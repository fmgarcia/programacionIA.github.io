# 游뱄 Unidad 7. M치quinas de Vectores de Soporte (SVM)

Las **M치quinas de Vectores de Soporte** (Support Vector Machines o SVM) son un conjunto de algoritmos de aprendizaje supervisado potentes y vers치tiles, utilizados tanto para **clasificaci칩n** (SVC) como para **regresi칩n** (SVR). Su objetivo principal es encontrar el hiperplano 칩ptimo que mejor separe las clases en el espacio de caracter칤sticas.

---

### 7.1. 쮺칩mo funciona el algoritmo?

La idea central de SVM es encontrar una l칤nea (en 2D), un plano (en 3D) o un **hiperplano** (en m치s dimensiones) que divida los datos en clases distintas. Pero no cualquier separaci칩n sirve; SVM busca la separaci칩n que tenga el **mayor margen** posible.

1.  **Hiperplano:** Es la frontera de decisi칩n que separa las clases.
2.  **Vectores de Soporte:** Son los puntos de datos m치s cercanos al hiperplano. Estos puntos son los m치s "dif칤ciles" de clasificar y son los 칰nicos que importan para definir la posici칩n del hiperplano.
3.  **Margen:** Es la distancia entre el hiperplano y los vectores de soporte m치s cercanos de cada clase. SVM intenta **maximizar** este margen para mejorar la generalizaci칩n del modelo.

---

### 7.2. Explicaci칩n Matem치tica y el "Kernel Trick"

Matem치ticamente, para un problema linealmente separable, buscamos los par치metros $w$ (vector de pesos) y $b$ (sesgo) tal que el hiperplano se defina como:
$$w \cdot x + b = 0$$

El objetivo es minimizar $||w||$ (lo que equivale a maximizar el margen) sujeto a que todas las muestras est칠n correctamente clasificadas fuera del margen.

#### El Truco del Kernel (Kernel Trick)
Cuando los datos no son separables linealmente (ej. un c칤rculo dentro de otro), SVM utiliza una t칠cnica llamada **Kernel Trick**.
Esta t칠cnica proyecta los datos originales a un espacio de **mayor dimensi칩n** donde s칤 son linealmente separables, sin necesidad de calcular expl칤citamente las coordenadas en ese espacio complejo (lo cual ser칤a computacionalmente costoso).

Kernels comunes:
*   **Lineal:** Para datos linealmente separables.
*   **Polin칩mico:** Mapea a espacios de dimensiones polin칩micas.
*   **RBF (Radial Basis Function):** El m치s popular. Mapea a un espacio de dimensi칩n infinita. Es muy efectivo para fronteras de decisi칩n complejas y curvas.

---

### 7.3. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **Alta Dimensionalidad:** Es muy efectivo en espacios con muchas dimensiones (incluso si hay m치s dimensiones que muestras). | **Grandes Datasets:** No escala bien con datasets muy grandes (el tiempo de entrenamiento crece c칰bicamente). |
| **Eficiencia de Memoria:** Solo usa un subconjunto de puntos de entrenamiento (los vectores de soporte) para definir el modelo. | **Ruido:** Es sensible al ruido y a clases que se solapan mucho (si no se ajustan bien los par치metros). |
| **Versatilidad:** Gracias a los Kernels, puede modelar relaciones lineales y no lineales complejas. | **Probabilidades:** No proporciona estimaciones de probabilidad directas (se calculan mediante validaci칩n cruzada costosa). |
| **Robustez:** Maximizar el margen ayuda a reducir el riesgo de overfitting. | **Ajuste de Par치metros:** Requiere un ajuste cuidadoso de hiperpar치metros clave ($C$, $\gamma$, Kernel). |

---

### 7.4. Ejemplo en Python con `scikit-learn`

Ejemplo de clasificaci칩n usando `SVC` (Support Vector Classification) con un kernel RBF.

```python
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# 1. Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# 2. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Escalar datos (IMPORTANTE para SVM)
# SVM es sensible a la escala porque intenta maximizar la distancia (margen).
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 4. Instanciar el modelo
# kernel='rbf' es el valor por defecto. C es el par치metro de regularizaci칩n.
clf = svm.SVC(kernel='rbf', C=1.0, gamma='scale')

# 5. Entrenar
clf.fit(X_train, y_train)

# 6. Predecir y Evaluar
y_pred = clf.predict(X_test)
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
```

---

### 7.5. Ejemplos Comunes de Uso

*   **Clasificaci칩n de Texto:** Categorizaci칩n de noticias, detecci칩n de spam y an치lisis de sentimientos. SVM maneja muy bien la alta dimensionalidad de los vectores de texto (Bag of Words).
*   **Reconocimiento de Im치genes:** Clasificaci칩n de im치genes, reconocimiento facial y reconocimiento de escritura a mano (OCR).
*   **Bioinform치tica:** Clasificaci칩n de prote칤nas y genes, donde los datos suelen tener muchas caracter칤sticas y pocas muestras.
*   **Detecci칩n de Intrusos:** Identificar actividad maliciosa en redes bas치ndose en patrones de tr치fico.

### 7.6. Aplicaciones Reales de SVM

SVM ha sido uno de los algoritmos m치s exitosos antes del auge del Deep Learning y sigue siendo muy relevante:

*   **Clasificaci칩n de Im치genes (Hist칩rico):** Antes de las redes neuronales convolucionales (CNN), SVM era el est치ndar para clasificaci칩n de im치genes y detecci칩n de objetos (ej. detecci칩n de peatones).
*   **Bioinform치tica (Clasificaci칩n de Prote칤nas):** Se utiliza para clasificar prote칤nas en familias funcionales y predecir la estructura secundaria de las prote칤nas, dado que maneja muy bien la alta dimensionalidad de los datos gen칩micos.
    *   [SVM en Bioinform치tica](https://github.com/topics/bioinformatics-machine-learning)
*   **Reconocimiento de Escritura:** SVM ha demostrado ser muy eficaz en el reconocimiento de caracteres manuscritos (OCR), compitiendo con redes neuronales en datasets como MNIST.
*   **Geolog칤a y Miner칤a:** Clasificaci칩n de tipos de suelo y rocas a partir de datos s칤smicos o im치genes satelitales.

---

### 7.7. Consideraciones Finales

1.  **Par치metro C (Regularizaci칩n):**
    *   Controla el equilibrio entre tener un margen amplio y clasificar correctamente los puntos de entrenamiento.
    *   **C alto:** Intenta clasificar todo correctamente (riesgo de Overfitting, margen estrecho).
    *   **C bajo:** Permite algunos errores para obtener un margen m치s amplio (mejor generalizaci칩n, margen suave).

2.  **Par치metro Gamma ($\gamma$) (Solo para kernels RBF/Poly):**
    *   Define qu칠 tan lejos llega la influencia de un solo ejemplo de entrenamiento.
    *   **Gamma alto:** Solo los puntos muy cercanos influyen. Puede llevar a fronteras de decisi칩n muy ajustadas e irregulares (Overfitting).
    *   **Gamma bajo:** La influencia llega lejos. La frontera de decisi칩n es m치s suave (Underfitting si es muy bajo).

3.  **Escalado:** Al igual que KNN, SVM se basa en distancias. Es **cr칤tico** estandarizar los datos antes de entrenar.

---

游늰 **Fecha de creaci칩n:** 19/11/2025
九꽲잺 **Autor:** Fran Garc칤a
