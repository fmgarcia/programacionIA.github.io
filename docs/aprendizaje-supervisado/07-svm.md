# 游뱄 Unidad 7. M치quinas de Vectores de Soporte (SVM)

Las **M치quinas de Vectores de Soporte** (Support Vector Machines o SVM) son un conjunto de algoritmos de aprendizaje supervisado potentes y vers치tiles, utilizados tanto para **clasificaci칩n** (SVC) como para **regresi칩n** (SVR). Su objetivo principal es encontrar el hiperplano 칩ptimo que mejor separe las clases en el espacio de caracter칤sticas.

---

### 7.1. 쮺칩mo funciona el algoritmo?

La idea central de SVM es encontrar una l칤nea (en 2D), un plano (en 3D) o un **hiperplano** (en m치s dimensiones) que divida los datos en clases distintas. Pero no cualquier separaci칩n sirve; SVM busca la separaci칩n que tenga el **mayor margen** posible.

1. **Hiperplano:** Es la frontera de decisi칩n que separa las clases.
2. **Vectores de Soporte:** Son los puntos de datos m치s cercanos al hiperplano. Estos puntos son los m치s "dif칤ciles" de clasificar y son los 칰nicos que importan para definir la posici칩n del hiperplano.
3. **Margen:** Es la distancia entre el hiperplano y los vectores de soporte m치s cercanos de cada clase. SVM intenta **maximizar** este margen para mejorar la generalizaci칩n del modelo.

---

### 7.2. Explicaci칩n Matem치tica y el "Kernel Trick"

Matem치ticamente, para un problema linealmente separable, buscamos los par치metros $w$ (vector de pesos) y $b$ (sesgo) tal que el hiperplano se defina como:
$$w \cdot x + b = 0$$

El objetivo es minimizar $||w||$ (lo que equivale a maximizar el margen) sujeto a que todas las muestras est칠n correctamente clasificadas fuera del margen.

#### El Truco del Kernel (Kernel Trick)

Cuando los datos no son separables linealmente (ej. un c칤rculo dentro de otro), SVM utiliza una t칠cnica llamada **Kernel Trick**.
Esta t칠cnica proyecta los datos originales a un espacio de **mayor dimensi칩n** donde s칤 son linealmente separables, sin necesidad de calcular expl칤citamente las coordenadas en ese espacio complejo (lo cual ser칤a computacionalmente costoso).

Kernels comunes:

* **Lineal:** Para datos linealmente separables.
* **Polin칩mico:** Mapea a espacios de dimensiones polin칩micas.
* **RBF (Radial Basis Function):** El m치s popular. Mapea a un espacio de dimensi칩n infinita. Es muy efectivo para fronteras de decisi칩n complejas y curvas.

---

### 7.3. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **Alta Dimensionalidad:** Es muy efectivo en espacios con muchas dimensiones (incluso si hay m치s dimensiones que muestras). | **Grandes Datasets:** No escala bien con datasets muy grandes (el tiempo de entrenamiento crece c칰bicamente). |
| **Eficiencia de Memoria:** Solo usa un subconjunto de puntos de entrenamiento (los vectores de soporte) para definir el modelo. | **Ruido:** Es sensible al ruido y a clases que se solapan mucho (si no se ajustan bien los par치metros). |
| **Versatilidad:** Gracias a los Kernels, puede modelar relaciones lineales y no lineales complejas. | **Probabilidades:** No proporciona estimaciones de probabilidad directas (se calculan mediante validaci칩n cruzada costosa). |
| **Robustez:** Maximizar el margen ayuda a reducir el riesgo de overfitting. | **Ajuste de Par치metros:** Requiere un ajuste cuidadoso de hiperpar치metros clave ($C$, $\gamma$, Kernel). |

---

### 7.4. Ejemplo en Python con `scikit-learn`

Ejemplo de clasificaci칩n usando `SVC` (Support Vector Classification) con un kernel RBF.

```python
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# 1. Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# 2. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Escalar datos (IMPORTANTE para SVM)
# SVM es sensible a la escala porque intenta maximizar la distancia (margen).
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 4. Instanciar el modelo
# kernel='rbf' es el valor por defecto. C es el par치metro de regularizaci칩n.
clf = svm.SVC(kernel='rbf', C=1.0, gamma='scale')

# 5. Entrenar
clf.fit(X_train, y_train)

# 6. Predecir y Evaluar
y_pred = clf.predict(X_test)
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("\nReporte de Clasificaci칩n:")
print(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))

# Comparar diferentes kernels
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

kernels = ['linear', 'poly', 'rbf', 'sigmoid']
C_values = [0.1, 1, 10, 100]
results = {}

print("\n" + "="*60)
print("COMPARACI칍N DE KERNELS")
print("="*60)

for kernel in kernels:
    scores = []
    for C in C_values:
        svm_model = svm.SVC(kernel=kernel, C=C, gamma='scale')
        cv_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=5)
        scores.append(cv_scores.mean())
    results[kernel] = scores
    print(f"\n{kernel.upper()} Kernel:")
    for C, score in zip(C_values, scores):
        print(f"  C={C:5.1f}: Accuracy={score:.4f}")

# Visualizar comparaci칩n
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Gr치fico 1: Comparaci칩n de kernels
for kernel, scores in results.items():
    axes[0].plot(C_values, scores, marker='o', label=kernel, linewidth=2)
axes[0].set_xlabel('Valor de C (Regularizaci칩n)')
axes[0].set_ylabel('Accuracy (Cross-Validation)')
axes[0].set_title('Comparaci칩n de Kernels SVM')
axes[0].set_xscale('log')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Gr치fico 2: Fronteras de decisi칩n (2D)
from matplotlib.colors import ListedColormap

# Usar solo 2 caracter칤sticas para visualizaci칩n
X_2d = iris.data[:, [2, 3]]  # petal length y width
y_2d = iris.target
X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(X_2d, y_2d, test_size=0.3, random_state=42)

# Escalar
scaler_2d = StandardScaler()
X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)
X_test_2d_scaled = scaler_2d.transform(X_test_2d)

# Entrenar SVM RBF
svm_rbf = svm.SVC(kernel='rbf', C=10, gamma='scale')
svm_rbf.fit(X_train_2d_scaled, y_train_2d)

# Crear malla
h = 0.02
x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1
y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
axes[1].contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)
axes[1].scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], c=y_train_2d,
                cmap=cmap_bold, edgecolor='k', s=50, alpha=0.7)

# Marcar vectores de soporte
support_vectors = svm_rbf.support_vectors_
axes[1].scatter(support_vectors[:, 0], support_vectors[:, 1], 
                s=200, linewidth=2, facecolors='none', edgecolors='black',
                label='Vectores de Soporte')

axes[1].set_xlabel('Petal Length (scaled)')
axes[1].set_ylabel('Petal Width (scaled)')
axes[1].set_title('SVM con Kernel RBF - Fronteras de Decisi칩n')
axes[1].legend()
plt.tight_layout()
plt.show()

print(f"\nN칰mero de vectores de soporte: {len(svm_rbf.support_vectors_)}")
print(f"Accuracy en test (2D): {svm_rbf.score(X_test_2d_scaled, y_test_2d):.4f}")
```

---

### 7.5. Ejemplos Comunes de Uso

* **Clasificaci칩n de Texto:** Categorizaci칩n de noticias, detecci칩n de spam y an치lisis de sentimientos. SVM maneja muy bien la alta dimensionalidad de los vectores de texto (Bag of Words).
* **Reconocimiento de Im치genes:** Clasificaci칩n de im치genes, reconocimiento facial y reconocimiento de escritura a mano (OCR).
* **Bioinform치tica:** Clasificaci칩n de prote칤nas y genes, donde los datos suelen tener muchas caracter칤sticas y pocas muestras.
* **Detecci칩n de Intrusos:** Identificar actividad maliciosa en redes bas치ndose en patrones de tr치fico.

### 7.6. Aplicaciones Reales de SVM

SVM ha sido uno de los algoritmos m치s exitosos antes del auge del Deep Learning y sigue siendo muy relevante:

* **Clasificaci칩n de Im치genes (Hist칩rico):** Antes de las redes neuronales convolucionales (CNN), SVM era el est치ndar para clasificaci칩n de im치genes y detecci칩n de objetos (ej. detecci칩n de peatones).
* **Bioinform치tica (Clasificaci칩n de Prote칤nas):** Se utiliza para clasificar prote칤nas en familias funcionales y predecir la estructura secundaria de las prote칤nas, dado que maneja muy bien la alta dimensionalidad de los datos gen칩micos.
  * [SVM en Bioinform치tica](https://github.com/topics/bioinformatics-machine-learning)
* **Reconocimiento de Escritura:** SVM ha demostrado ser muy eficaz en el reconocimiento de caracteres manuscritos (OCR), compitiendo con redes neuronales en datasets como MNIST.
* **Geolog칤a y Miner칤a:** Clasificaci칩n de tipos de suelo y rocas a partir de datos s칤smicos o im치genes satelitales.

---

### 7.7. Consideraciones Finales

1. **Par치metro C (Regularizaci칩n):**
    * Controla el equilibrio entre tener un margen amplio y clasificar correctamente los puntos de entrenamiento.
    * **C alto:** Intenta clasificar todo correctamente (riesgo de Overfitting, margen estrecho).
    * **C bajo:** Permite algunos errores para obtener un margen m치s amplio (mejor generalizaci칩n, margen suave).

2. **Par치metro Gamma ($\gamma$) (Solo para kernels RBF/Poly):**
    * Define qu칠 tan lejos llega la influencia de un solo ejemplo de entrenamiento.
    * **Gamma alto:** Solo los puntos muy cercanos influyen. Puede llevar a fronteras de decisi칩n muy ajustadas e irregulares (Overfitting).
    * **Gamma bajo:** La influencia llega lejos. La frontera de decisi칩n es m치s suave (Underfitting si es muy bajo).

3. **Escalado:** Al igual que KNN, SVM se basa en distancias. Es **cr칤tico** estandarizar los datos antes de entrenar.

---

游늰 **Fecha de creaci칩n:** 19/11/2025
九꽲잺 **Autor:** Fran Garc칤a
