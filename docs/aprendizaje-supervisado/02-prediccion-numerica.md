# 游뱄 Unidad 2. Regresi칩n Lineal para la Inteligencia Artificial

La regresi칩n lineal es uno de los modelos m치s simples y fundamentales en el campo de la inteligencia artificial y el aprendizaje autom치tico. A pesar de su simplicidad, proporciona una base s칩lida para entender c칩mo los algoritmos de regresi칩n pueden ser usados para hacer predicciones. En este art칤culo exploraremos c칩mo funciona la regresi칩n lineal, sus aplicaciones, y la compararemos con otros modelos de regresi칩n como Ridge y Lasso.

## 쯈u칠 es la Regresi칩n Lineal?

La **regresi칩n lineal** es un m칠todo estad칤stico que intenta modelar la relaci칩n entre una variable dependiente y una o m치s variables independientes mediante una l칤nea recta. La ecuaci칩n que representa una regresi칩n lineal simple tiene la siguiente forma:

$$
y = b_0 + b_1X + \epsilon
$$

- **y**: Variable dependiente (la que se intenta predecir).
- **X**: Variable independiente (el predictor).
- **b_0**: Intercepto, valor de **y** cuando **X** es cero.
- **b_1**: Coeficiente que representa la pendiente de la l칤nea.
- **$\epsilon$**: Error o ruido, la diferencia entre la predicci칩n y el valor real.

La regresi칩n lineal se utiliza principalmente para problemas de predicci칩n num칠rica, como el precio de una vivienda, el rendimiento de una acci칩n o cualquier otra situaci칩n en la que exista una relaci칩n lineal entre las variables.

#### Ejemplo en Python: Regresi칩n Lineal Simple

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Generar datos de ejemplo
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # Variable independiente
y = 2.5 * X.flatten() + 5 + np.random.randn(100) * 2  # y = 2.5x + 5 + ruido

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear y entrenar modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Predicciones
y_pred = model.predict(X_test)

# Evaluaci칩n
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Coeficiente (pendiente): {model.coef_[0]:.4f}")
print(f"Intercepto: {model.intercept_:.4f}")
print(f"MSE: {mse:.4f}")
print(f"R Score: {r2:.4f}")

# Visualizaci칩n
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='blue', label='Datos reales', alpha=0.6)
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicci칩n')
plt.xlabel('Variable Independiente (X)')
plt.ylabel('Variable Dependiente (y)')
plt.title('Regresi칩n Lineal Simple')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Aplicaciones de la Regresi칩n Lineal

La regresi칩n lineal es ampliamente utilizada en una variedad de aplicaciones, como:

- **Econom칤a**: Predicci칩n de precios de bienes y servicios. Por ejemplo, podemos usar la regresi칩n lineal para modelar la relaci칩n entre la inflaci칩n y el precio de los alimentos. En este caso, la regresi칩n lineal simple puede ser suficiente si se trata de una relaci칩n clara y directa. [Ejemplo en Python](https://github.com/scikit-learn/scikit-learn/blob/main/examples/linear_model/plot_ols.py)

- **Finanzas**: Estimaci칩n del rendimiento de acciones o bonos. La regresi칩n lineal puede ayudar a estimar c칩mo factores como las tasas de inter칠s y el crecimiento econ칩mico afectan los precios de las acciones. Si existen muchas variables correlacionadas, **Ridge Regression** ser칤a una mejor opci칩n para estabilizar el modelo y evitar el sobreajuste. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)

- **Salud**: Modelado de la relaci칩n entre la dosis de un medicamento y la respuesta del paciente. En este 치mbito, se podr칤a usar la regresi칩n lineal para entender c칩mo var칤a la presi칩n sangu칤nea en respuesta a diferentes dosis de un medicamento. Si existen m칰ltiples factores (como edad, peso, y otras condiciones de salud), **Lasso Regression** podr칤a ayudar a identificar cu치les son las caracter칤sticas m치s relevantes. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html)

- **Marketing**: Determinaci칩n de la relaci칩n entre el gasto publicitario y las ventas. La regresi칩n lineal se utiliza para estimar el impacto de diferentes estrategias publicitarias en las ventas. Si existen muchas campa침as publicitarias y se necesita identificar cu치les son las m치s efectivas, **Lasso Regression** podr칤a ayudar a eliminar las menos significativas y reducir la complejidad del modelo.

- **Educaci칩n**: Predicci칩n de calificaciones de estudiantes en funci칩n de variables como el tiempo de estudio y la asistencia. Si el objetivo es identificar los factores que tienen mayor influencia en el rendimiento acad칠mico, **Lasso Regression** ser칤a 칰til para seleccionar solo las caracter칤sticas m치s relevantes, como participaci칩n en clase, tiempo de estudio, o participaci칩n en actividades extracurriculares.

- **Inmobiliaria**: Predicci칩n del valor de una propiedad con base en caracter칤sticas como la ubicaci칩n, el tama침o y el n칰mero de habitaciones. En este contexto, **Ridge Regression** puede ser 칰til para manejar la multicolinealidad, ya que caracter칤sticas como la ubicaci칩n y el tama침o de una propiedad suelen estar correlacionadas. Ridge ayuda a estabilizar los coeficientes y mejorar la capacidad predictiva del modelo.

- **Agricultura**: Estimaci칩n del rendimiento de cultivos en funci칩n de factores como el clima, la cantidad de fertilizante y el tipo de suelo. **Ridge Regression** es adecuada cuando hay m칰ltiples factores que pueden estar correlacionados, como la temperatura y la precipitaci칩n. Esto ayuda a manejar mejor la multicolinealidad y a mejorar la generalizaci칩n del modelo. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)

### 쯈u칠 t칠cnica es m치s apropiada?

- **Econom칤a y Finanzas**: En estos campos, la **regresi칩n lineal** puede ser 칰til cuando se trata de problemas simples, como la predicci칩n de precios basada en una o dos caracter칤sticas. Sin embargo, si hay muchas variables que est치n altamente correlacionadas, **Ridge Regression** ser칤a m치s apropiada para evitar el sobreajuste. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)
- **Salud**: Para el modelado de la relaci칩n entre la dosis de un medicamento y la respuesta del paciente, **Lasso Regression** ser칤a adecuada si hay muchas caracter칤sticas potenciales, ya que podr칤a simplificar el modelo eliminando caracter칤sticas irrelevantes. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html)
- **Marketing**: Si hay muchas variables de marketing, como diferentes tipos de publicidad, **Lasso Regression** puede ayudar a identificar cu치les de ellas son las m치s importantes, eliminando las menos significativas.
- **Inmobiliaria**: En el caso de la predicci칩n de precios de propiedades, **Ridge Regression** puede ser 칰til para manejar la multicolinealidad, ya que a menudo las caracter칤sticas como ubicaci칩n, tama침o y tipo de propiedad est치n correlacionadas.
- **Educaci칩n**: Si queremos predecir las calificaciones de los estudiantes y hay muchas caracter칤sticas (como el historial acad칠mico, asistencia, participaci칩n en clase, etc.), **Lasso** ser칤a 칰til para identificar las variables m치s relevantes y eliminar las menos importantes.
- **Agricultura**: Para la estimaci칩n del rendimiento de cultivos, **Ridge Regression** ser칤a adecuada si existen m칰ltiples factores correlacionados, ya que permite manejar mejor la multicolinealidad. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)

## Limitaciones de la Regresi칩n Lineal

Aunque la regresi칩n lineal es f치cil de entender y usar, presenta algunas limitaciones importantes que deben tenerse en cuenta al aplicar este modelo:

- **Supone una relaci칩n lineal**: La regresi칩n lineal solo puede modelar relaciones lineales entre las variables. Si la relaci칩n es no lineal, el modelo tendr치 un rendimiento pobre. Esto implica que, si los datos muestran una relaci칩n m치s compleja (por ejemplo, cuadr치tica o exponencial), la regresi칩n lineal no podr치 capturar dicha complejidad, resultando en predicciones inexactas. En estos casos, ser칤a mejor utilizar modelos que puedan capturar la no linealidad, como la regresi칩n polin칩mica o t칠cnicas m치s avanzadas como redes neuronales.

- **Sensibilidad a los outliers**: La presencia de valores at칤picos puede afectar significativamente el ajuste de la l칤nea, ya que la regresi칩n lineal minimiza la suma de los errores al cuadrado. Los outliers, al tener errores m치s grandes, influyen desproporcionadamente en la l칤nea de ajuste, lo cual puede distorsionar el modelo. Para mitigar este problema, se pueden utilizar t칠cnicas como la detecci칩n y eliminaci칩n de outliers, o emplear m칠todos de regresi칩n robusta que minimicen el impacto de estos valores extremos.

- **Multicolinealidad**: Cuando las variables independientes est치n altamente correlacionadas, el modelo puede producir resultados inestables. La multicolinealidad genera problemas en la estimaci칩n de los coeficientes, haciendo que sean muy sensibles a peque침as variaciones en los datos y, por lo tanto, menos interpretables. Esto puede llevar a una disminuci칩n en la precisi칩n de las predicciones y a problemas en la generalizaci칩n del modelo. En estos casos, se recomienda usar t칠cnicas de regularizaci칩n, como **Ridge Regression**, que penaliza los coeficientes grandes y ayuda a reducir los efectos de la multicolinealidad, estabilizando el modelo.

## Ridge Regression y Lasso Regression

Para superar algunas de las limitaciones de la regresi칩n lineal est치ndar, se han desarrollado t칠cnicas de regularizaci칩n como **Ridge** y **Lasso**. Ambas t칠cnicas son versiones modificadas de la regresi칩n lineal que incluyen un t칠rmino de penalizaci칩n para mejorar el rendimiento y evitar el sobreajuste.

### Ridge Regression

**Ridge Regression**, tambi칠n conocida como **regresi칩n de cresta**, a침ade un t칠rmino de regularizaci칩n L2 a la funci칩n de p칠rdida. Esto significa que el modelo penaliza los coeficientes grandes, haciendo que los valores de los par치metros sean m치s peque침os y estables. La ecuaci칩n para Ridge es:

$$
J( heta) = \sum (y_i - \hat{y_i})^2 + \lambda \sum  heta_j^2
$$

- **\lambda**: Par치metro de regularizaci칩n que controla la cantidad de penalizaci칩n.
- **$heta_j$**: Coeficientes del modelo.

El t칠rmino de penalizaci칩n ayuda a reducir la complejidad del modelo, lo cual resulta 칰til especialmente cuando existen m칰ltiples variables independientes correlacionadas (multicolinealidad).

#### Ventajas de Ridge Regression

- **Reducci칩n del sobreajuste**: Ridge ayuda a reducir el riesgo de sobreajuste al penalizar los coeficientes grandes.
- **Mejora la estabilidad**: Especialmente en presencia de multicolinealidad, el modelo Ridge tiende a ser m치s estable.

#### Ejemplo en Python: Ridge Regression

```python
import numpy as np
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import make_regression

# Generar datos con multicolinealidad
X, y = make_regression(n_samples=200, n_features=10, n_informative=5, 
                       noise=10, random_state=42)

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Escalar datos (importante para Ridge)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Comparar Regresi칩n Lineal vs Ridge
linear_model = LinearRegression()
ridge_model = Ridge(alpha=1.0)  # alpha es el par치metro de regularizaci칩n 풭

# Entrenar modelos
linear_model.fit(X_train_scaled, y_train)
ridge_model.fit(X_train_scaled, y_train)

# Predicciones
y_pred_linear = linear_model.predict(X_test_scaled)
y_pred_ridge = ridge_model.predict(X_test_scaled)

# Evaluaci칩n
print("Regresi칩n Lineal:")
print(f"  MSE: {mean_squared_error(y_test, y_pred_linear):.4f}")
print(f"  R: {r2_score(y_test, y_pred_linear):.4f}")
print(f"  Suma de coeficientes al cuadrado: {np.sum(linear_model.coef_**2):.4f}")

print("\nRidge Regression:")
print(f"  MSE: {mean_squared_error(y_test, y_pred_ridge):.4f}")
print(f"  R: {r2_score(y_test, y_pred_ridge):.4f}")
print(f"  Suma de coeficientes al cuadrado: {np.sum(ridge_model.coef_**2):.4f}")

# Comparar magnitud de coeficientes
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(linear_model.coef_)), linear_model.coef_)
plt.title('Coeficientes - Regresi칩n Lineal')
plt.xlabel('Feature')
plt.ylabel('Coeficiente')
plt.subplot(1, 2, 2)
plt.bar(range(len(ridge_model.coef_)), ridge_model.coef_)
plt.title('Coeficientes - Ridge (alpha=1.0)')
plt.xlabel('Feature')
plt.ylabel('Coeficiente')
plt.tight_layout()
plt.show()
```

### Lasso Regression

**Lasso Regression** a침ade un t칠rmino de regularizaci칩n L1 a la funci칩n de p칠rdida. Este t칠rmino tiene la capacidad de hacer que algunos coeficientes sean exactamente cero, eliminando efectivamente ciertas caracter칤sticas del modelo. La ecuaci칩n de Lasso es:

$$
J( heta) = \sum (y_i - \hat{y_i})^2 + \lambda \sum | heta_j|
$$

- **$\lambda$**: Par치metro de regularizaci칩n que controla la penalizaci칩n.

Lasso es 칰til no solo para reducir el sobreajuste, sino tambi칠n para la **selecci칩n de caracter칤sticas**, ya que elimina autom치ticamente aquellas que no son 칰tiles para la predicci칩n.

#### Ventajas de Lasso Regression

- **Selecci칩n de caracter칤sticas**: Lasso simplifica el modelo seleccionando solo las caracter칤sticas m치s relevantes.
- **Reducci칩n del sobreajuste**: Similar a Ridge, Lasso ayuda a evitar el sobreajuste del modelo.

#### Ejemplo en Python: Lasso Regression

```python
import numpy as np
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import make_regression

# Generar datos con caracter칤sticas irrelevantes
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, 
                       noise=10, random_state=42)

# Dividir y escalar datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entrenar modelos
linear_model = LinearRegression()
lasso_model = Lasso(alpha=1.0)  # alpha es el par치metro de regularizaci칩n 풭

linear_model.fit(X_train_scaled, y_train)
lasso_model.fit(X_train_scaled, y_train)

# Predicciones
y_pred_linear = linear_model.predict(X_test_scaled)
y_pred_lasso = lasso_model.predict(X_test_scaled)

# Evaluaci칩n
print("Regresi칩n Lineal:")
print(f"  MSE: {mean_squared_error(y_test, y_pred_linear):.4f}")
print(f"  R: {r2_score(y_test, y_pred_linear):.4f}")
print(f"  Coeficientes no-cero: {np.sum(np.abs(linear_model.coef_) > 0.01)}")

print("\nLasso Regression:")
print(f"  MSE: {mean_squared_error(y_test, y_pred_lasso):.4f}")
print(f"  R: {r2_score(y_test, y_pred_lasso):.4f}")
print(f"  Coeficientes no-cero: {np.sum(np.abs(lasso_model.coef_) > 0.01)}")
print(f"  Caracter칤sticas eliminadas: {np.sum(lasso_model.coef_ == 0)}")

# Visualizar selecci칩n de caracter칤sticas
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(linear_model.coef_)), linear_model.coef_)
plt.title('Coeficientes - Regresi칩n Lineal')
plt.xlabel('Feature')
plt.ylabel('Coeficiente')
plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)

plt.subplot(1, 2, 2)
plt.bar(range(len(lasso_model.coef_)), lasso_model.coef_)
plt.title('Coeficientes - Lasso (alpha=1.0)')
plt.xlabel('Feature')
plt.ylabel('Coeficiente')
plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

# Ejemplo con RidgePath para visualizar el efecto de alpha
from sklearn.linear_model import lasso_path
alphas = np.logspace(-4, 1, 50)
coefs, _ = lasso_path(X_train_scaled, y_train, alphas=alphas)

plt.figure(figsize=(10, 6))
for coef in coefs:
    plt.plot(alphas, coef)
plt.xscale('log')
plt.xlabel('Alpha (regularizaci칩n)')
plt.ylabel('Coeficientes')
plt.title('Lasso Path - C칩mo var칤an los coeficientes con alpha')
plt.grid(True, alpha=0.3)
plt.show()
```

## Comparaci칩n entre Regresi칩n Lineal, Ridge y Lasso

| Caracter칤stica              | Regresi칩n Lineal  | Ridge Regression  | Lasso Regression  |
|----------------------------|-------------------|-------------------|-------------------|
| **Regularizaci칩n**         | No                | L2                | L1                |
| **Penalizaci칩n**           | Ninguna           | Penaliza coeficientes grandes | Algunos coeficientes se hacen cero |
| **Sobreajuste**            | Alta posibilidad  | Baja              | Baja              |
| **Multicolinealidad**      | Problemas con multicolinealidad | Mejor manejo | Mejor manejo     |
| **Selecci칩n de caracter칤sticas** | No            | No                | S칤                |

- **Regresi칩n Lineal**: Ideal para problemas simples y cuando existe una relaci칩n lineal clara entre las variables. Sin embargo, es propensa al sobreajuste si no se maneja adecuadamente.
- **Ridge Regression**: 칔til cuando existe multicolinealidad, ya que la regularizaci칩n L2 ayuda a estabilizar el modelo. No elimina caracter칤sticas, pero hace que los coeficientes sean m치s peque침os.
- **Lasso Regression**: 칔til para la selecci칩n de caracter칤sticas, ya que fuerza algunos coeficientes a ser exactamente cero. Esto resulta en un modelo m치s sencillo y f치cil de interpretar.

## Conclusiones

La regresi칩n lineal es una excelente herramienta para comenzar a entender los modelos de regresi칩n. Sin embargo, cuando nos enfrentamos a datos m치s complejos, con m칰ltiples caracter칤sticas y posibles problemas de sobreajuste, **Ridge** y **Lasso** se presentan como mejores alternativas. Estos modelos ayudan a mejorar la capacidad de generalizaci칩n del modelo y a reducir la complejidad, haciendo que la predicci칩n sea m치s precisa y confiable.

La elecci칩n entre la regresi칩n lineal, Ridge y Lasso depender치 de la naturaleza de los datos y los objetivos del an치lisis. Si se desea simplicidad y no hay riesgo de multicolinealidad, la regresi칩n lineal puede ser suficiente. Si el modelo tiende a sobreajustarse o hay muchas caracter칤sticas correlacionadas, Ridge y Lasso son opciones a considerar, siendo Lasso ideal si se desea simplificar el modelo eliminando caracter칤sticas irrelevantes.

## Ejemplos Adicionales de Uso

- **Predicci칩n de Ventas Minoristas**: En un negocio minorista donde existen m칰ltiples caracter칤sticas que afectan las ventas (promociones, temporadas, clima, ubicaci칩n), **Ridge Regression** ser칤a 칰til para manejar la posible multicolinealidad entre estas caracter칤sticas. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html)
- **Modelado de la Demanda Energ칠tica**: En la predicci칩n del consumo de energ칤a el칠ctrica, que depende de variables como temperatura, hora del d칤a, y tipo de d칤a (laboral o festivo), **Ridge** podr칤a ayudar a manejar la complejidad y multicolinealidad.
- **An치lisis de Sentimientos**: Al predecir la polaridad de una opini칩n (positiva o negativa) en base a muchas palabras o frases, **Lasso Regression** ser칤a ideal para seleccionar las palabras m치s relevantes y reducir la dimensionalidad.
- **Predicci칩n de Costos de Seguros M칠dicos**: Para estimar los costos de seguros m칠dicos en funci칩n de caracter칤sticas como edad, estado de salud, h치bitos de vida y ubicaci칩n geogr치fica, **Lasso** podr칤a ayudar a eliminar caracter칤sticas redundantes, haciendo el modelo m치s interpretable.
- **Optimizaci칩n de Cadenas de Suministro**: Para predecir el tiempo de entrega de productos considerando m칰ltiples variables (tr치fico, distancia, clima, inventario), **Ridge Regression** puede ser 칰til para manejar la correlaci칩n entre factores como tr치fico y distancia.
- **Reconocimiento de Actividad Humana**: En la clasificaci칩n de actividades humanas usando sensores port치tiles (como aceler칩metros y giroscopios), **Lasso** podr칤a ayudar a identificar cu치les de las se침ales del sensor son m치s importantes para diferenciar entre actividades como caminar, correr o estar de pie. [Ejemplo en Python](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_recovery.html)
