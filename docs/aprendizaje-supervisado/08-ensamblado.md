# 游뱄 Unidad 8. Algoritmos de Ensamblado (Ensemble Learning)

Los **Algoritmos de Ensamblado** (Ensemble Methods) son una t칠cnica de Machine Learning que combina las predicciones de m칰ltiples modelos base (conocidos como *weak learners* o aprendices d칠biles) para construir un modelo final m치s robusto y preciso (*strong learner*).

La intuici칩n detr치s de esto es la **"Sabidur칤a de las Masas"**: as칤 como la opini칩n colectiva de un grupo de expertos suele ser mejor que la de un solo experto, un grupo de modelos predictivos suele superar el rendimiento de un modelo individual.

---

### 8.1. Conceptos Clave y Categor칤as

El objetivo principal es reducir el **sesgo** (bias) o la **varianza** (variance), o ambos. Los m칠todos de ensamblado se dividen principalmente en tres categor칤as seg칰n c칩mo combinan los modelos:

1. **Voting (Votaci칩n):** Se entrenan varios modelos diferentes (ej. KNN, SVM, 츼rbol) y se "vota" para decidir la clase final.
2. **Bagging (Bootstrap Aggregating):** Se entrena el *mismo* algoritmo muchas veces en paralelo, pero con diferentes subconjuntos aleatorios de los datos de entrenamiento. Su objetivo es reducir la **varianza** (evitar overfitting). El ejemplo cl치sico es **Random Forest**.
3. **Boosting:** Se entrena el *mismo* algoritmo de forma **secuencial**. Cada nuevo modelo intenta corregir los errores cometidos por el modelo anterior. Su objetivo es reducir el **sesgo** (evitar underfitting). Ejemplos: AdaBoost, XGBoost.

---

### 8.2. Voting Classifiers (Votaci칩n)

Es la forma m치s simple de ensamblado. Consiste en agregar las predicciones de clasificadores totalmente diferentes.

#### Tipos de Votaci칩n

* **Hard Voting (Votaci칩n Dura):** Cada clasificador vota por una clase. La clase con la mayor칤a de votos gana (moda).
* **Soft Voting (Votaci칩n Suave):** Si los clasificadores pueden estimar probabilidades (tienen m칠todo `predict_proba`), se promedian las probabilidades de cada clase. La clase con el promedio de probabilidad m치s alto gana. El *Soft Voting* suele funcionar mejor porque da m치s peso a los votos con "alta confianza".

#### Ejemplo en Python (Voting)

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Datos de ejemplo
X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Modelos individuales
log_clf = LogisticRegression(random_state=42)
rnd_clf = DecisionTreeClassifier(random_state=42)
svm_clf = SVC(probability=True, random_state=42) # probability=True necesario para Soft Voting

# Ensamblado por Votaci칩n (Soft)
voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='soft'
)

# Entrenamiento y Comparaci칩n
for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(f"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}")
```

---

### 8.3. Bagging y Random Forest

**Bagging** (Bootstrap Aggregating) implica entrenar el mismo algoritmo en diferentes subconjuntos aleatorios del dataset de entrenamiento.

* **Bootstrap:** El muestreo se hace *con reemplazo* (una misma muestra puede aparecer varias veces en el mismo subconjunto).
* **Pasting:** El muestreo se hace *sin reemplazo*.

Una vez entrenados, los modelos agregan sus predicciones (moda para clasificaci칩n, promedio para regresi칩n).

#### Random Forest (Bosques Aleatorios)

Es una implementaci칩n espec칤fica y optimizada de Bagging usando **츼rboles de Decisi칩n**.
Introduce aleatoriedad extra: al dividir un nodo en el 치rbol, no busca la mejor caracter칤stica de *todas* las disponibles, sino la mejor caracter칤stica dentro de un **subconjunto aleatorio de caracter칤sticas**. Esto hace que los 치rboles sean m치s diversos (descorrelacionados), lo que reduce dr치sticamente la varianza.

**Hiperpar치metros Clave:**

* `n_estimators`: N칰mero de 치rboles (m치s es mejor, pero m치s lento).
* `max_features`: N칰mero m치ximo de caracter칤sticas a considerar en cada divisi칩n.
* `bootstrap`: Si usar muestreo con reemplazo (True por defecto).
* `n_jobs`: N칰mero de n칰cleos de CPU a usar (-1 para todos).

#### Ejemplo en Python (Random Forest)

```python
from sklearn.ensemble import RandomForestClassifier

# Instanciar Random Forest
# 500 치rboles, usando todos los n칰cleos de CPU
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)

rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")

# Importancia de Caracter칤sticas
# Random Forest permite ver qu칠 variables son m치s 칰tiles
for name, score in zip(["Feature 1", "Feature 2"], rnd_clf.feature_importances_):
    print(f"{name}: {score}")
```

---

### 8.4. Boosting (Impulso)

El Boosting entrena predictores secuencialmente, cada uno intentando corregir a su predecesor.

#### 8.4.1. AdaBoost (Adaptive Boosting)

El algoritmo presta m치s atenci칩n a las instancias de entrenamiento que el predecesor clasific칩 incorrectamente.

1. Entrena un clasificador base.
2. Aumenta el **peso relativo** de las instancias mal clasificadas.
3. Entrena un segundo clasificador con los pesos actualizados.
4. Repite el proceso.

**Hiperpar치metros Clave:**

* `n_estimators`: N칰mero de iteraciones.
* `learning_rate`: Cu치nto contribuye cada modelo. Un valor bajo requiere m치s estimadores.

**Ejemplo Python (AdaBoost):**

```python
from sklearn.ensemble import AdaBoostClassifier

# AdaBoost usando 츼rboles de Decisi칩n muy simples (stumps)
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm="SAMME.R", learning_rate=0.5, random_state=42
)
ada_clf.fit(X_train, y_train)
```

#### 8.4.2. Gradient Boosting Machine (GBM)

En lugar de ajustar los pesos de las instancias, GBM intenta ajustar el nuevo predictor a los **errores residuales** (la diferencia entre el valor real y el predicho) del predictor anterior.

**Ejemplo Python (GradientBoosting de sklearn):**

```python
from sklearn.ensemble import GradientBoostingClassifier

gbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
gbrt.fit(X_train, y_train)
```

#### 8.4.3. XGBoost (Extreme Gradient Boosting)

Es una versi칩n optimizada de Gradient Boosting dise침ada para ser altamente eficiente, flexible y port치til. Es el algoritmo dominante en competiciones de Machine Learning (Kaggle).

* **Regularizaci칩n:** Incluye regularizaci칩n L1 y L2 para evitar overfitting.
* **Paralelizaci칩n:** Construcci칩n de 치rboles en paralelo.
* **Manejo de nulos:** Aprende autom치ticamente la mejor direcci칩n para valores faltantes.

**Hiperpar치metros Clave:**

* `eta` (learning_rate): Paso de reducci칩n de pesos para prevenir overfitting.
* `max_depth`: Profundidad m치xima del 치rbol.
* `subsample`: Ratio de muestras de entrenamiento usadas.
* `colsample_bytree`: Ratio de columnas usadas por 치rbol.

**Ejemplo Python (XGBoost):**

```python
import xgboost as xgb

# XGBoost tiene su propia estructura de datos optimizada (DMatrix), pero es compatible con sklearn
xgb_clf = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    n_jobs=-1,
    random_state=42
)

xgb_clf.fit(X_train, y_train)
print(f"XGBoost Accuracy: {accuracy_score(y_test, xgb_clf.predict(X_test)):.4f}")

# Comparaci칩n exhaustiva de todos los m칠todos de ensamblado
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, 
                              GradientBoostingClassifier, VotingClassifier)
import matplotlib.pyplot as plt
import numpy as np

print("\n" + "="*70)
print("COMPARACI칍N COMPLETA DE M칄TODOS DE ENSAMBLADO")
print("="*70)

# Modelos a comparar
modelos = {
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42),
    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=3, random_state=42)
}

# Entrenar y evaluar cada modelo
resultados = {}
for nombre, modelo in modelos.items():
    # Cross-validation
    from sklearn.model_selection import cross_val_score
    cv_scores = cross_val_score(modelo, X_train, y_train, cv=5)
    
    # Entrenar y predecir
    modelo.fit(X_train, y_train)
    y_pred_modelo = modelo.predict(X_test)
    
    # Guardar resultados
    resultados[nombre] = {
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_accuracy': accuracy_score(y_test, y_pred_modelo)
    }
    
    print(f"\n{nombre}:")
    print(f"  CV Score: {cv_scores.mean():.4f} (췀{cv_scores.std():.4f})")
    print(f"  Test Accuracy: {accuracy_score(y_test, y_pred_modelo):.4f}")

# Visualizaci칩n comparativa
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Gr치fico 1: Comparaci칩n de accuracies
modelo_names = list(resultados.keys())
cv_means = [resultados[m]['cv_mean'] for m in modelo_names]
test_accs = [resultados[m]['test_accuracy'] for m in modelo_names]

x_pos = np.arange(len(modelo_names))
width = 0.35

axes[0].bar(x_pos - width/2, cv_means, width, label='CV Score', alpha=0.8)
axes[0].bar(x_pos + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)
axes[0].set_xlabel('Modelo')
axes[0].set_ylabel('Accuracy')
axes[0].set_title('Comparaci칩n de M칠todos de Ensamblado')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(modelo_names, rotation=45, ha='right')
axes[0].legend()
axes[0].grid(True, alpha=0.3, axis='y')
axes[0].set_ylim([0.5, 1.0])

# Gr치fico 2: Importancia de caracter칤sticas (Random Forest vs XGBoost)
feature_names = [f'Feature {i}' for i in range(X_train.shape[1])]

rf_importance = modelos['Random Forest'].feature_importances_
xgb_importance = modelos['XGBoost'].feature_importances_

x_pos_feat = np.arange(len(feature_names))
axes[1].barh(x_pos_feat - width/2, rf_importance, width, label='Random Forest', alpha=0.8)
axes[1].barh(x_pos_feat + width/2, xgb_importance, width, label='XGBoost', alpha=0.8)
axes[1].set_yticks(x_pos_feat)
axes[1].set_yticklabels(feature_names)
axes[1].set_xlabel('Importancia')
axes[1].set_title('Importancia de Caracter칤sticas')
axes[1].legend()
axes[1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

# Voting Classifier combinando los mejores modelos
print("\n" + "="*70)
print("VOTING CLASSIFIER (Combinaci칩n de Modelos)")
print("="*70)

voting_clf = VotingClassifier(
    estimators=[
        ('rf', modelos['Random Forest']),
        ('gb', modelos['Gradient Boosting']),
        ('xgb', modelos['XGBoost'])
    ],
    voting='soft'
)

voting_clf.fit(X_train, y_train)
y_pred_voting = voting_clf.predict(X_test)

print(f"\nVoting Classifier Accuracy: {accuracy_score(y_test, y_pred_voting):.4f}")
print("\nComparaci칩n Final:")
print(f"  Mejor modelo individual: {max(resultados.items(), key=lambda x: x[1]['test_accuracy'])[0]}")
print(f"  Accuracy m치xima individual: {max(r['test_accuracy'] for r in resultados.values()):.4f}")
print(f"  Voting Classifier: {accuracy_score(y_test, y_pred_voting):.4f}")
```

#### 8.4.4. LightGBM (Light Gradient Boosting Machine)

Desarrollado por Microsoft. A diferencia de otros que crecen el 치rbol por niveles (level-wise), LightGBM crece por hojas (**leaf-wise**). Elige la hoja con mayor p칠rdida para crecer.

* **Ventajas:** Mucho m치s r치pido que XGBoost en grandes datasets y consume menos memoria.
* **Desventajas:** Puede hacer overfitting f치cilmente en datasets peque침os (< 10,000 filas).

**Hiperpar치metros Clave:**

* `num_leaves`: Par치metro principal para controlar la complejidad (en lugar de max_depth).
* `min_data_in_leaf`: Importante para evitar overfitting.

**Ejemplo Python (LightGBM):**

```python
import lightgbm as lgb

lgb_clf = lgb.LGBMClassifier(
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=100,
    random_state=42
)

lgb_clf.fit(X_train, y_train)
print(f"LightGBM Accuracy: {accuracy_score(y_test, lgb_clf.predict(X_test)):.4f}")
```

---

### 8.5. Resumen Comparativo

| T칠cnica | Algoritmo Principal | Estrategia | Objetivo | Paralelizable |
| :--- | :--- | :--- | :--- | :--- |
| **Voting** | VotingClassifier | Promedio de modelos distintos | Robustez general | S칤 |
| **Bagging** | Random Forest | Modelos iguales, datos aleatorios (independientes) | Reducir Varianza | S칤 (Muy r치pido) |
| **Boosting** | AdaBoost, XGBoost | Modelos iguales, secuenciales (dependientes) | Reducir Sesgo | No (Secuencial)* |

*\* Nota: XGBoost y LightGBM paralelizan la construcci칩n dentro del 치rbol, pero los 치rboles se crean secuencialmente.*

### 8.6. Aplicaciones Reales de Algoritmos de Ensamblado

Los m칠todos de ensamblado dominan actualmente las competiciones de ciencia de datos y las aplicaciones industriales en datos estructurados:

* **Detecci칩n de Fraude (Banca):** Algoritmos como XGBoost y Random Forest son el est치ndar en la industria financiera para detectar transacciones fraudulentas en tiempo real debido a su alta precisi칩n y velocidad.
  * [Detecci칩n de fraude con XGBoost](https://github.com/topics/fraud-detection)
* **Diagn칩stico M칠dico:** Random Forest se utiliza para diagnosticar enfermedades (como la retinopat칤a diab칠tica) analizando m칰ltiples variables de pacientes, ya que proporciona una medida de qu칠 s칤ntomas son m치s relevantes.
* **Ranking de B칰squeda (Search Engines):** Motores de b칰squeda utilizan Gradient Boosting para ordenar los resultados de b칰squeda (Learning to Rank), optimizando la relevancia para el usuario.
  * [Learning to Rank con LightGBM](https://github.com/microsoft/LightGBM/tree/master/examples/lambdarank)
* **Predicci칩n de Demanda (Retail):** Cadenas de suministro usan estos modelos para predecir la demanda futura de productos, optimizando el inventario y reduciendo desperdicios.

---

### 8.7. Consideraciones Finales

1. **Random Forest** es una excelente "primera opci칩n". Es robusto, requiere poco ajuste de hiperpar치metros y nos da la importancia de las caracter칤sticas.
2. **XGBoost / LightGBM** suelen ofrecer el **mejor rendimiento** (Accuracy) en datos tabulares estructurados, pero requieren m치s ajuste de hiperpar치metros y cuidado con el overfitting.
3. **Escalado:** Los algoritmos basados en 치rboles (Random Forest, Boosting) **NO requieren escalado** de caracter칤sticas (StandardScaler), lo cual es una gran ventaja pr치ctica.
4. **Interpretabilidad:** Los modelos de ensamblado son "Cajas Negras". Perdemos la interpretabilidad simple de un solo 츼rbol de Decisi칩n o una Regresi칩n Lineal, aunque podemos usar la "Importancia de Caracter칤sticas" para entender qu칠 variables pesan m치s.

---

游늰 **Fecha de creaci칩n:** 19/11/2025
九꽲잺 **Autor:** Fran Garc칤a
