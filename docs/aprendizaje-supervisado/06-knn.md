# 游뱄 Unidad 6. Algoritmo K-Nearest Neighbors (KNN)

El algoritmo **K-Nearest Neighbors** (K-Vecinos M치s Cercanos) es uno de los m칠todos m치s simples y efectivos en Machine Learning supervisado. Se utiliza tanto para problemas de **clasificaci칩n** como de **regresi칩n**. Es un algoritmo **no param칠trico** (no hace suposiciones sobre la distribuci칩n de los datos subyacentes) y de **aprendizaje perezoso** (lazy learning), lo que significa que no "aprende" un modelo discriminativo durante la fase de entrenamiento, sino que memoriza los datos de entrenamiento para realizar predicciones en el momento necesario.

![Ilustraci칩n de knn](../assets/images/knn.svg)
---

### 6.1. 쮺칩mo funciona el algoritmo?

La intuici칩n detr치s de KNN es sencilla y se basa en la proximidad: "Dime con qui칠n andas y te dir칠 qui칠n eres". Para clasificar un nuevo punto de datos, el algoritmo busca en todo el conjunto de datos de entrenamiento los **'k'** puntos m치s cercanos (vecinos) a ese nuevo punto.

1. **Calcular distancias:** Se calcula la distancia matem치tica entre el punto nuevo que queremos predecir y todos los puntos existentes en el dataset.
2. **Buscar vecinos:** Se seleccionan los $k$ puntos con las distancias m치s cortas.
3. **Votaci칩n (Clasificaci칩n):** La clase del nuevo punto se determina por mayor칤a de votos de sus vecinos. La clase m치s frecuente entre los $k$ vecinos se asigna al nuevo punto.
4. **Promedio (Regresi칩n):** El valor del nuevo punto es el promedio (o media ponderada) de los valores num칠ricos de sus vecinos.

---

### 6.2. Explicaci칩n Matem치tica

El n칰cleo de KNN es la medici칩n de la distancia para determinar la similitud. La m칠trica m치s com칰n es la **Distancia Euclidiana**, aunque existen otras dependiendo del tipo de datos y el problema.

Dados dos puntos $P$ y $Q$ en un espacio n-dimensional (donde $n$ es el n칰mero de caracter칤sticas):
$P = (p_1, p_2, ..., p_n)$
$Q = (q_1, q_2, ..., q_n)$

* **Distancia Euclidiana (L2):** Es la distancia en l칤nea recta "a vuelo de p치jaro". Es la m치s utilizada por defecto.
    $$d(P, Q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

* **Distancia Manhattan (L1):** Suma de las diferencias absolutas. Es 칰til en sistemas tipo cuadr칤cula (como manzanas de una ciudad).
    $$d(P, Q) = \sum_{i=1}^{n} |q_i - p_i|$$

* **Distancia Minkowski:** Una generalizaci칩n matem치tica de las anteriores.
    $$d(P, Q) = (\sum_{i=1}^{n} |q_i - p_i|^p)^{1/p}$$
    (Si $p=1$ es Manhattan, si $p=2$ es Euclidiana).

---

### 6.3. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **Simplicidad:** Es extremadamente f치cil de entender, explicar e implementar. | **Costo Computacional:** Es lento en la fase de predicci칩n con grandes datasets, ya que debe calcular distancias con *todos* los puntos cada vez. |
| **Sin Entrenamiento:** La fase de entrenamiento es casi instant치nea (solo almacena datos), ideal si los datos cambian constantemente. | **Sensible a Outliers:** Los valores at칤picos o ruido pueden afectar significativamente la predicci칩n si $k$ es peque침o. |
| **Versatilidad:** Sirve tanto para tareas de clasificaci칩n como de regresi칩n. | **Sensible a la Escala:** Requiere estrictamente que los datos est칠n normalizados o estandarizados. |
| **No Lineal:** Se adapta bien a fronteras de decisi칩n irregulares y complejas. | **Maldici칩n de la Dimensionalidad:** Su rendimiento decae dr치sticamente cuando hay muchas dimensiones (features) irrelevantes. |

---

### 6.4. Ejemplo en Python con `scikit-learn`

A continuaci칩n, un ejemplo b치sico de clasificaci칩n usando el dataset Iris y la clase `KNeighborsClassifier`.

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# 1. Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# 2. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Escalar datos (CRUCIAL para KNN)
# Como KNN usa distancias, las variables con rangos grandes dominar치n a las peque침as.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 4. Instanciar el modelo (k=3)
knn = KNeighborsClassifier(n_neighbors=3)

# 5. Entrenar (En KNN esto es solo almacenar los datos)
knn.fit(X_train, y_train)

# 6. Predecir y Evaluar
y_pred = knn.predict(X_test)
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
print("\nReporte de Clasificaci칩n:")
print(metrics.classification_report(y_test, y_pred, target_names=iris.target_names))

# 7. Encontrar el mejor valor de k
from sklearn.model_selection import cross_val_score

k_values = range(1, 31)
cv_scores = []

for k in k_values:
    knn_temp = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn_temp, X_train, y_train, cv=5, scoring='accuracy')
    cv_scores.append(scores.mean())

# Visualizar la elecci칩n de k
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(k_values, cv_scores, marker='o', linewidth=2)
plt.xlabel('Valor de k')
plt.ylabel('Accuracy (Cross-Validation)')
plt.title('M칠todo del Codo para Elegir k')
plt.grid(True, alpha=0.3)
optimal_k = k_values[np.argmax(cv_scores)]
plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'k 칩ptimo = {optimal_k}')
plt.legend()

# 8. Visualizar fronteras de decisi칩n (usando 2 caracter칤sticas)
from matplotlib.colors import ListedColormap

# Usar solo 2 caracter칤sticas para visualizaci칩n
X_2d = X[:, [2, 3]]  # petal length y petal width
X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.3, random_state=42)

# Escalar
scaler_2d = StandardScaler()
X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)
X_test_2d_scaled = scaler_2d.transform(X_test_2d)

# Entrenar KNN con k 칩ptimo
knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k)
knn_optimal.fit(X_train_2d_scaled, y_train_2d)

# Crear malla para visualizar fronteras
h = 0.02
x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1
y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = knn_optimal.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.subplot(1, 2, 2)
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)
plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], c=y_train_2d, 
            cmap=cmap_bold, edgecolor='k', s=50, alpha=0.7)
plt.xlabel('Petal Length (scaled)')
plt.ylabel('Petal Width (scaled)')
plt.title(f'Fronteras de Decisi칩n KNN (k={optimal_k})')
plt.tight_layout()
plt.show()

print(f"\nAccuracy con k={optimal_k}: {knn_optimal.score(X_test_2d_scaled, y_test_2d):.4f}")
```

---

### 6.5. Ejemplos Comunes de Uso

* **Sistemas de Recomendaci칩n:** Sugerir productos, pel칤culas o m칰sica bas치ndose en las preferencias de usuarios "vecinos" con gustos similares (Filtrado Colaborativo).
* **Reconocimiento de Patrones:** Reconocimiento de caracteres escritos a mano (OCR) o clasificaci칩n de im치genes simples bas치ndose en la similitud de p칤xeles.
* **Detecci칩n de Anomal칤as:** Identificar fraudes bancarios o intrusiones en redes detectando eventos que est치n "lejos" de los grupos de vecinos normales.
* **Imputaci칩n de Datos Faltantes:** Rellenar valores nulos en un dataset bas치ndose en los valores de los vecinos m치s cercanos (`KNNImputer`).
* **Medicina:** Clasificaci칩n de pacientes con perfiles similares para predecir riesgos de enfermedades.

### 6.6. Aplicaciones Reales de KNN

Aunque es un algoritmo simple, KNN se utiliza en sistemas donde la interpretabilidad y la simplicidad son clave:

* **Sistemas de Recomendaci칩n (Retail):** Empresas como Amazon o Netflix utilizan variantes de algoritmos basados en vecindad para recomendar productos ("Los usuarios que compraron X tambi칠n compraron Y").
  * [Sistemas de recomendaci칩n con KNN](https://github.com/topics/recommendation-system)
* **Reconocimiento de Escritura a Mano:** El servicio postal de EE.UU. (USPS) utiliz칩 m칠todos basados en vecindad para reconocer d칤gitos escritos a mano en c칩digos postales.
  * [Dataset MNIST y KNN](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)
* **Detecci칩n de Intrusiones (Ciberseguridad):** Clasificar actividades de red como normales o sospechosas bas치ndose en su similitud con patrones de ataques conocidos.
* **Bioinform치tica:** Clasificaci칩n de muestras de genes o prote칤nas bas치ndose en su similitud con perfiles conocidos para el diagn칩stico de enfermedades.

---

### 6.7. Consideraciones Finales

1. **Elecci칩n de 'k' (Hiperpar치metro clave):**
    * Un $k$ muy peque침o (ej. $k=1$) hace que el modelo sea muy sensible al ruido (**Overfitting**).
    * Un $k$ muy grande suaviza demasiado la frontera de decisi칩n y puede incluir vecinos de otras clases lejanas (**Underfitting**).
    * Se suele elegir un $k$ **impar** para evitar empates en clasificaci칩n binaria.
    * El valor 칩ptimo se encuentra usualmente mediante validaci칩n cruzada (t칠cnica del codo o *Elbow Method*).

2. **Escalado de Caracter칤sticas:**
    * Dado que KNN se basa puramente en distancias, es **obligatorio** escalar las variables (usando `StandardScaler` o `MinMaxScaler`). Si una variable tiene una magnitud mucho mayor que otra (ej. Salario [1000-5000] vs Edad [20-60]), la variable de mayor magnitud dominar치 completamente el c치lculo de la distancia, haciendo que la otra sea irrelevante.

---

游늰 **Fecha de creaci칩n:** 19/11/2025
九꽲잺 **Autor:** Fran Garc칤a
