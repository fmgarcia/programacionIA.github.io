# 游뱄 Unidad 6. Algoritmo K-Nearest Neighbors (KNN)

El algoritmo **K-Nearest Neighbors** (K-Vecinos M치s Cercanos) es uno de los m칠todos m치s simples y efectivos en Machine Learning supervisado. Se utiliza tanto para problemas de **clasificaci칩n** como de **regresi칩n**. Es un algoritmo **no param칠trico** (no hace suposiciones sobre la distribuci칩n de los datos subyacentes) y de **aprendizaje perezoso** (lazy learning), lo que significa que no "aprende" un modelo discriminativo durante la fase de entrenamiento, sino que memoriza los datos de entrenamiento para realizar predicciones en el momento necesario.

---

### 6.1. 쮺칩mo funciona el algoritmo?

La intuici칩n detr치s de KNN es sencilla y se basa en la proximidad: "Dime con qui칠n andas y te dir칠 qui칠n eres". Para clasificar un nuevo punto de datos, el algoritmo busca en todo el conjunto de datos de entrenamiento los **'k'** puntos m치s cercanos (vecinos) a ese nuevo punto.

1.  **Calcular distancias:** Se calcula la distancia matem치tica entre el punto nuevo que queremos predecir y todos los puntos existentes en el dataset.
2.  **Buscar vecinos:** Se seleccionan los $k$ puntos con las distancias m치s cortas.
3.  **Votaci칩n (Clasificaci칩n):** La clase del nuevo punto se determina por mayor칤a de votos de sus vecinos. La clase m치s frecuente entre los $k$ vecinos se asigna al nuevo punto.
4.  **Promedio (Regresi칩n):** El valor del nuevo punto es el promedio (o media ponderada) de los valores num칠ricos de sus vecinos.

---

### 6.2. Explicaci칩n Matem치tica

El n칰cleo de KNN es la medici칩n de la distancia para determinar la similitud. La m칠trica m치s com칰n es la **Distancia Euclidiana**, aunque existen otras dependiendo del tipo de datos y el problema.

Dados dos puntos $P$ y $Q$ en un espacio n-dimensional (donde $n$ es el n칰mero de caracter칤sticas):
$P = (p_1, p_2, ..., p_n)$
$Q = (q_1, q_2, ..., q_n)$

*   **Distancia Euclidiana (L2):** Es la distancia en l칤nea recta "a vuelo de p치jaro". Es la m치s utilizada por defecto.
    $$d(P, Q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

*   **Distancia Manhattan (L1):** Suma de las diferencias absolutas. Es 칰til en sistemas tipo cuadr칤cula (como manzanas de una ciudad).
    $$d(P, Q) = \sum_{i=1}^{n} |q_i - p_i|$$

*   **Distancia Minkowski:** Una generalizaci칩n matem치tica de las anteriores.
    $$d(P, Q) = (\sum_{i=1}^{n} |q_i - p_i|^p)^{1/p}$$
    (Si $p=1$ es Manhattan, si $p=2$ es Euclidiana).

---

### 6.3. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **Simplicidad:** Es extremadamente f치cil de entender, explicar e implementar. | **Costo Computacional:** Es lento en la fase de predicci칩n con grandes datasets, ya que debe calcular distancias con *todos* los puntos cada vez. |
| **Sin Entrenamiento:** La fase de entrenamiento es casi instant치nea (solo almacena datos), ideal si los datos cambian constantemente. | **Sensible a Outliers:** Los valores at칤picos o ruido pueden afectar significativamente la predicci칩n si $k$ es peque침o. |
| **Versatilidad:** Sirve tanto para tareas de clasificaci칩n como de regresi칩n. | **Sensible a la Escala:** Requiere estrictamente que los datos est칠n normalizados o estandarizados. |
| **No Lineal:** Se adapta bien a fronteras de decisi칩n irregulares y complejas. | **Maldici칩n de la Dimensionalidad:** Su rendimiento decae dr치sticamente cuando hay muchas dimensiones (features) irrelevantes. |

---

### 6.4. Ejemplo en Python con `scikit-learn`

A continuaci칩n, un ejemplo b치sico de clasificaci칩n usando el dataset Iris y la clase `KNeighborsClassifier`.

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# 1. Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# 2. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. Escalar datos (CRUCIAL para KNN)
# Como KNN usa distancias, las variables con rangos grandes dominar치n a las peque침as.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 4. Instanciar el modelo (k=3)
knn = KNeighborsClassifier(n_neighbors=3)

# 5. Entrenar (En KNN esto es solo almacenar los datos)
knn.fit(X_train, y_train)

# 6. Predecir y Evaluar
y_pred = knn.predict(X_test)
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
```

---

### 6.5. Ejemplos Comunes de Uso

*   **Sistemas de Recomendaci칩n:** Sugerir productos, pel칤culas o m칰sica bas치ndose en las preferencias de usuarios "vecinos" con gustos similares (Filtrado Colaborativo).
*   **Reconocimiento de Patrones:** Reconocimiento de caracteres escritos a mano (OCR) o clasificaci칩n de im치genes simples bas치ndose en la similitud de p칤xeles.
*   **Detecci칩n de Anomal칤as:** Identificar fraudes bancarios o intrusiones en redes detectando eventos que est치n "lejos" de los grupos de vecinos normales.
*   **Imputaci칩n de Datos Faltantes:** Rellenar valores nulos en un dataset bas치ndose en los valores de los vecinos m치s cercanos (`KNNImputer`).
*   **Medicina:** Clasificaci칩n de pacientes con perfiles similares para predecir riesgos de enfermedades.

### 6.6. Aplicaciones Reales de KNN

Aunque es un algoritmo simple, KNN se utiliza en sistemas donde la interpretabilidad y la simplicidad son clave:

*   **Sistemas de Recomendaci칩n (Retail):** Empresas como Amazon o Netflix utilizan variantes de algoritmos basados en vecindad para recomendar productos ("Los usuarios que compraron X tambi칠n compraron Y").
    *   [Sistemas de recomendaci칩n con KNN](https://github.com/topics/recommendation-system)
*   **Reconocimiento de Escritura a Mano:** El servicio postal de EE.UU. (USPS) utiliz칩 m칠todos basados en vecindad para reconocer d칤gitos escritos a mano en c칩digos postales.
    *   [Dataset MNIST y KNN](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)
*   **Detecci칩n de Intrusiones (Ciberseguridad):** Clasificar actividades de red como normales o sospechosas bas치ndose en su similitud con patrones de ataques conocidos.
*   **Bioinform치tica:** Clasificaci칩n de muestras de genes o prote칤nas bas치ndose en su similitud con perfiles conocidos para el diagn칩stico de enfermedades.

---

### 6.7. Consideraciones Finales

1.  **Elecci칩n de 'k' (Hiperpar치metro clave):**
    *   Un $k$ muy peque침o (ej. $k=1$) hace que el modelo sea muy sensible al ruido (**Overfitting**).
    *   Un $k$ muy grande suaviza demasiado la frontera de decisi칩n y puede incluir vecinos de otras clases lejanas (**Underfitting**).
    *   Se suele elegir un $k$ **impar** para evitar empates en clasificaci칩n binaria.
    *   El valor 칩ptimo se encuentra usualmente mediante validaci칩n cruzada (t칠cnica del codo o *Elbow Method*).

2.  **Escalado de Caracter칤sticas:**
    *   Dado que KNN se basa puramente en distancias, es **obligatorio** escalar las variables (usando `StandardScaler` o `MinMaxScaler`). Si una variable tiene una magnitud mucho mayor que otra (ej. Salario [1000-5000] vs Edad [20-60]), la variable de mayor magnitud dominar치 completamente el c치lculo de la distancia, haciendo que la otra sea irrelevante.

---

游늰 **Fecha de creaci칩n:** 19/11/2025
九꽲잺 **Autor:** Fran Garc칤a
