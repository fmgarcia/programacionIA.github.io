# 游뱄 Unidad 4. 츼rbol de Decisi칩n en Inteligencia Artificial: Explicaci칩n Detallada

El algoritmo de **츼rbol de Decisi칩n (Decision Tree)** es un modelo de aprendizaje supervisado que se utiliza tanto para problemas de clasificaci칩n como de regresi칩n. Su objetivo es dividir el espacio de datos en subconjuntos homog칠neos bas치ndose en una serie de reglas, de modo que cada subconjunto sea lo m치s puro posible con respecto a la variable objetivo. Los 치rboles de decisi칩n son f치ciles de interpretar, muy 칰tiles para entender las relaciones en los datos, y se aplican ampliamente en una variedad de campos.

A continuaci칩n, exploraremos la teor칤a detr치s de los 치rboles de decisi칩n, c칩mo se construyen, y daremos ejemplos que ilustran c칩mo funciona este algoritmo en la pr치ctica. Tambi칠n incluiremos casos reales en los que este algoritmo ha demostrado ser 칰til, as칤 como c칩mo encontrar los mejores valores para los metapar치metros del modelo.

#### 1. **Estructura de un 츼rbol de Decisi칩n**

Un **치rbol de decisi칩n** est치 compuesto por varios elementos fundamentales:

- **Nodos de Decisi칩n**: Representan la divisi칩n de los datos seg칰n una caracter칤stica espec칤fica. Aqu칤 se toma una decisi칩n sobre qu칠 atributo se usa para dividir el conjunto de datos.
- **Ramas**: Las conexiones entre nodos representan el resultado de una decisi칩n. Cada rama lleva a un nuevo nodo o a un nodo hoja.
- **Nodos Hoja**: Son los puntos finales del 치rbol. Representan la categor칤a final o el valor predicho para una determinada observaci칩n.

![Estructura de 츼rbol de Decisi칩n](../assets/images/decision_tree.svg)

Cada divisi칩n en un 치rbol de decisi칩n intenta dividir los datos de manera que maximice la pureza de los subconjuntos resultantes, es decir, que agrupe datos similares juntos. Este proceso contin칰a hasta que se cumplen ciertas condiciones, como alcanzar un n칰mero m칤nimo de muestras en un nodo o una profundidad m치xima del 치rbol.

#### 2. **Criterios de Divisi칩n y F칩rmulas Matem치ticas**

Los 치rboles de decisi칩n se construyen utilizando una serie de divisiones, cada una de las cuales se elige bas치ndose en un criterio que mide la calidad de la divisi칩n. Existen varias m칠tricas para seleccionar la caracter칤stica que mejor divide los datos:

- **Entrop칤a e 칈ndice de Ganancia de Informaci칩n**
- **칈ndice Gini**

##### **2.1. Entrop칤a e 칈ndice de Ganancia de Informaci칩n**

La **entrop칤a** mide la pureza de un nodo. Se define de la siguiente manera para un nodo que tiene dos clases (positiva y negativa):

$$
H(S) = -p_+ \cdot \log_2(p_+) - p_- \cdot \log_2(p_-)
$$

Donde:

- \( p_+ \) y \( p_- \) son las proporciones de ejemplos positivos y negativos en el nodo.

El objetivo es minimizar la entrop칤a en cada nodo, lo que equivale a hacer los nodos lo m치s homog칠neos posible.

La **ganancia de informaci칩n** mide la reducci칩n de la entrop칤a despu칠s de dividir un nodo. La f칩rmula para la ganancia de informaci칩n (\( IG \)) es:

$$
IG(S, A) = H(S) - \sum_{v \in Valores(A)} \frac{|S_v|}{|S|} H(S_v)
$$

Donde:

- S es el conjunto de datos original.
- A es el atributo por el cual se est치 dividiendo.
- S_v son los subconjuntos de S resultantes de la divisi칩n por el valor v de la caracter칤stica A.

##### **2.2. 칈ndice Gini**

El **칤ndice Gini** es otra medida utilizada para evaluar la calidad de una divisi칩n. Representa la probabilidad de que una observaci칩n seleccionada aleatoriamente sea clasificada incorrectamente si se realiza una predicci칩n aleatoria basada en la distribuci칩n de clases del nodo. La f칩rmula para el 칤ndice Gini es:

$$
Gini(S) = 1 - \sum_{i=1}^C p_i^2
$$

Donde \( p_i \) es la proporci칩n de elementos pertenecientes a la clase \( i \) en el conjunto \( S \), y \( C \) es el n칰mero de clases.

La idea detr치s del 칤ndice Gini es minimizar el valor de \( Gini(S) \) en cada divisi칩n, buscando nodos lo m치s puros posible.

#### 2.3. **C치lculo de Ejemplos con Entrop칤a e 칈ndice Gini**

Veamos un ejemplo detallado de c칩mo calcular la **entrop칤a** y el **칤ndice Gini** para una divisi칩n espec칤fica de datos.

Supongamos que tenemos un conjunto de datos con la siguiente distribuci칩n para la variable objetivo (S칤/No):

| Caracter칤stica  | Clase: S칤 | Clase: No |
|-----------------|-----------|-----------|
| A               | 4         | 2         |
| B               | 1         | 3         |

##### **2.3.1. C치lculo de la Entrop칤a**

Primero calculamos la **entrop칤a** para cada uno de los nodos resultantes de dividir el conjunto de datos seg칰n la caracter칤stica "A":

Para **A**:

- Total de ejemplos: \( 4 + 2 = 6 \)
- Proporci칩n de clase S칤 $( p_+ ): (\frac{4}{6})$
- Proporci칩n de clase No $( p_- ): (\frac{2}{6})$

La entrop칤a para la caracter칤stica A es:
$$
H(A) = -\left( \frac{4}{6} \right) \log_2\left( \frac{4}{6} \right) - \left( \frac{2}{6} \right) \log_2\left( \frac{2}{6} \right) = 0.918
$$

Para **B**:

- Total de ejemplos: \( 1 + 3 = 4 \)
- Proporci칩n de clase S칤 $( p_+ ): (\frac{1}{4})$
- Proporci칩n de clase No $( p_+ ): (\frac{3}{4})$

La entrop칤a para la caracter칤stica B es:
$$
H(B) = -\left( \frac{1}{4} \right) \log_2\left( \frac{1}{4} \right) - \left( \frac{3}{4} \right) \log_2\left( \frac{3}{4} \right) = 0.811
$$

##### **2.3.2. C치lculo del 칈ndice Gini**

Ahora calculamos el **칤ndice Gini** para la misma divisi칩n:

Para **A**:

- Proporci칩n de clase S칤 $( p_+ ): (\frac{4}{6})$
- Proporci칩n de clase No $( p_- ): (\frac{2}{6})$

El 칤ndice Gini para la caracter칤stica A es:
$$
Gini(A) = 1 - \left( \frac{4}{6} \right)^2 - \left( \frac{2}{6} \right)^2 = 0.444
$$

Para **B**:

- Proporci칩n de clase S칤 $( p_+ ): (\frac{1}{4})$
- Proporci칩n de clase No $( p_+ ): (\frac{3}{4})$

El 칤ndice Gini para la caracter칤stica B es:
$$
Gini(B) = 1 - \left( \frac{1}{4} \right)^2 - \left( \frac{3}{4} \right)^2 = 0.375
$$

Con estos valores, podemos comparar las caracter칤sticas y elegir cu치l proporciona una mejor divisi칩n de los datos seg칰n el criterio seleccionado (en este caso, el que minimice la entrop칤a o el 칤ndice Gini).

#### Ejemplo en Python: C치lculo de Entrop칤a y Gini

```python
import numpy as np
from scipy.stats import entropy

def calcular_entropia(clases):
    """Calcula la entrop칤a de un conjunto de clases"""
    total = len(clases)
    if total == 0:
        return 0
    
    # Contar cada clase
    valores, cuentas = np.unique(clases, return_counts=True)
    probabilidades = cuentas / total
    
    # Calcular entrop칤a: -sum(p * log2(p))
    entropia = -np.sum(probabilidades * np.log2(probabilidades))
    return entropia

def calcular_gini(clases):
    """Calcula el 칤ndice Gini de un conjunto de clases"""
    total = len(clases)
    if total == 0:
        return 0
    
    # Contar cada clase
    valores, cuentas = np.unique(clases, return_counts=True)
    probabilidades = cuentas / total
    
    # Calcular Gini: 1 - sum(p^2)
    gini = 1 - np.sum(probabilidades ** 2)
    return gini

# Ejemplo con los datos de la tabla
caracteristica_A = ['S칤', 'S칤', 'S칤', 'S칤', 'No', 'No']  # 4 S칤, 2 No
caracteristica_B = ['S칤', 'No', 'No', 'No']  # 1 S칤, 3 No

print("C치lculos para Caracter칤stica A:")
entropia_A = calcular_entropia(caracteristica_A)
gini_A = calcular_gini(caracteristica_A)
print(f"  Entrop칤a: {entropia_A:.4f}")
print(f"  칈ndice Gini: {gini_A:.4f}")

print("\nC치lculos para Caracter칤stica B:")
entropia_B = calcular_entropia(caracteristica_B)
gini_B = calcular_gini(caracteristica_B)
print(f"  Entrop칤a: {entropia_B:.4f}")
print(f"  칈ndice Gini: {gini_B:.4f}")

# Interpretaci칩n
print("\nInterpretaci칩n:")
if gini_A < gini_B:
    print(f"La caracter칤stica A es mejor (Gini m치s bajo: {gini_A:.4f} < {gini_B:.4f})")
else:
    print(f"La caracter칤stica B es mejor (Gini m치s bajo: {gini_B:.4f} < {gini_A:.4f})")
```

#### 3. **Construcci칩n de un 츼rbol de Decisi칩n**

La construcci칩n de un 치rbol de decisi칩n se realiza de manera recursiva, siguiendo estos pasos:

1. **Seleccionar el Mejor Atributo**: Se elige el atributo que maximiza la ganancia de informaci칩n o minimiza el 칤ndice Gini.
2. **Dividir el Conjunto de Datos**: Se divide el conjunto de datos en funci칩n del atributo seleccionado.
3. **Repetir el Proceso**: Se repiten los pasos anteriores para cada subconjunto resultante hasta alcanzar un criterio de parada.

**Criterios de Parada** pueden ser, por ejemplo, que todos los datos del nodo sean de la misma clase, que el nodo contenga muy pocas instancias (por debajo de un umbral m칤nimo), o que se haya alcanzado una profundidad m치xima predefinida.

#### Ejemplo en Python: Construcci칩n de 츼rbol de Decisi칩n

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import numpy as np

# Cargar datos
iris = load_iris()
X, y = iris.data, iris.target

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear 치rboles con diferentes criterios
tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)

# Entrenar ambos modelos
tree_gini.fit(X_train, y_train)
tree_entropy.fit(X_train, y_train)

# Predicciones
y_pred_gini = tree_gini.predict(X_test)
y_pred_entropy = tree_entropy.predict(X_test)

# Evaluaci칩n
print("\u00c1rbol con criterio Gini:")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_gini):.4f}")

print("\n\u00c1rbol con criterio Entrop칤a:")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_entropy):.4f}")

# Visualizar ambos 치rboles
fig, axes = plt.subplots(2, 1, figsize=(20, 20))

# 츼rbol con Gini
plot_tree(tree_gini, 
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=10,
          ax=axes[0])
axes[0].set_title('츼rbol de Decisi칩n - Criterio: Gini', fontsize=16, fontweight='bold')

# 츼rbol con Entrop칤a
plot_tree(tree_entropy, 
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=10,
          ax=axes[1])
axes[1].set_title('츼rbol de Decisi칩n - Criterio: Entrop칤a', fontsize=16, fontweight='bold')

plt.tight_layout()
plt.savefig('arboles_decision_comparacion.png', dpi=150, bbox_inches='tight')
plt.show()

# Informaci칩n sobre la importancia de caracter칤sticas
print("\nImportancia de Caracter칤sticas (Gini):")
for nombre, importancia in zip(iris.feature_names, tree_gini.feature_importances_):
    print(f"  {nombre}: {importancia:.4f}")
```

#### 4. **Ejemplo de 츼rbol de Decisi칩n**

Supongamos que queremos predecir si una persona har치 ejercicio al aire libre en funci칩n de dos variables: **tiempo (soleado, nublado, lluvioso)** y **temperatura (alta, baja)**.

1. **Ra칤z del 츼rbol**: Elegimos la primera divisi칩n. Si utilizamos la ganancia de informaci칩n, tal vez encontremos que la variable **tiempo** tiene la mayor ganancia.
   - **Nodo Ra칤z**: Tiempo
   - **Ramas**: Soleado, Nublado, Lluvioso

2. **Divisiones Subsiguientes**: Para cada valor del tiempo, examinamos la temperatura.
   - Para **tiempo = Soleado**, podemos tener otra divisi칩n por **temperatura**.

3. **Nodos Hoja**: Al final de las ramas, llegamos a los **nodos hoja**, que pueden ser "S칤" o "No" indicando si la persona har치 ejercicio o no.

#### 5. **Ventajas y Limitaciones de los 츼rboles de Decisi칩n**

- **Ventajas**:
  - **F치cil Interpretaci칩n**: Los 치rboles de decisi칩n son f치ciles de interpretar, ya que se asemejan a c칩mo los humanos toman decisiones.
  - **Pocos Supuestos sobre los Datos**: No necesitan normalizaci칩n de datos ni que las caracter칤sticas sean escaladas.
  - **Manejo de Datos Categ칩ricos y Num칠ricos**: Los 치rboles de decisi칩n pueden trabajar con ambos tipos de datos.

- **Limitaciones**:
  - **Sobreajuste**: Los 치rboles de decisi칩n tienden a sobreajustarse si no se limitan adecuadamente (por ejemplo, estableciendo una profundidad m치xima).
  - **Inestabilidad**: Los 치rboles de decisi칩n son sensibles a peque침as variaciones en los datos, lo cual puede generar 치rboles diferentes para conjuntos de datos similares.

#### 6. **Optimizaci칩n de los Metapar치metros**

La calidad de un 치rbol de decisi칩n depende en gran medida de los **metapar치metros** que se elijan. Algunos de los metapar치metros clave para un 치rbol de decisi칩n son:

1. **Profundidad M치xima (`max_depth`)**: Limitar la profundidad del 치rbol ayuda a evitar el sobreajuste. La **profundidad m치xima** determina cu치ntos niveles puede tener el 치rbol. Una profundidad muy alta puede llevar al sobreajuste, mientras que una profundidad muy baja puede causar un subajuste.

2. **N칰mero M칤nimo de Muestras por Hoja (`min_samples_leaf`)**: Controla el n칰mero m칤nimo de muestras que debe haber en un nodo hoja. Un valor m치s alto reduce el sobreajuste, ya que asegura que las hojas tengan un n칰mero significativo de ejemplos.

3. **N칰mero M칤nimo de Muestras para Dividir (`min_samples_split`)**: Especifica el n칰mero m칤nimo de muestras requerido para dividir un nodo. Un valor m치s alto evita divisiones innecesarias, lo cual ayuda a mantener el 치rbol m치s simple y reducir el riesgo de sobreajuste.

4. **Criterio de Divisi칩n (`criterion`)**: Define la funci칩n que se usa para medir la calidad de una divisi칩n. Los criterios comunes son `gini` e `entropy`. La elecci칩n del criterio puede influir en la estructura del 치rbol y su capacidad de generalizaci칩n.

##### **B칰squeda de los Mejores Valores de los Metapar치metros**

Para encontrar los valores 칩ptimos de estos metapar치metros, se suelen usar t칠cnicas como la **b칰squeda en cuadr칤cula (Grid Search)** o la **b칰squeda aleatoria (Random Search)**, combinadas con la validaci칩n cruzada.

- **Grid Search**: Busca de manera exhaustiva entre una lista predefinida de valores para cada metapar치metro. Es eficaz pero puede ser computacionalmente costosa si hay muchos par치metros y valores posibles.

- **Random Search**: Busca valores de metapar치metros de manera aleatoria dentro de un rango definido. Es m치s eficiente que Grid Search cuando se trabaja con un gran n칰mero de combinaciones posibles.

- **Validaci칩n Cruzada**: Tanto en Grid Search como en Random Search, se utiliza **validaci칩n cruzada** para evaluar el rendimiento del modelo para cada combinaci칩n de metapar치metros y seleccionar aquella que maximice la m칠trica de rendimiento, como la precisi칩n o el F1-score.

#### Ejemplo en Python: Optimizaci칩n de Hiperpar치metros

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

# Cargar datos
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("=== GRID SEARCH ===")
# Definir espacio de b칰squeda
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# Grid Search con validaci칩n cruzada
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"\nMejores par치metros: {grid_search.best_params_}")
print(f"Mejor score (CV): {grid_search.best_score_:.4f}")

# Evaluar en test
y_pred_grid = grid_search.best_estimator_.predict(X_test)
print(f"Accuracy en test: {accuracy_score(y_test, y_pred_grid):.4f}")

print("\n=== RANDOM SEARCH ===")
# Definir distribuciones para Random Search
param_dist = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [3, 5, 7, 10, 15, 20, None],
    'min_samples_split': np.arange(2, 20),
    'min_samples_leaf': np.arange(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

# Random Search
random_search = RandomizedSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_dist,
    n_iter=50,  # N칰mero de combinaciones a probar
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train, y_train)

print(f"\nMejores par치metros: {random_search.best_params_}")
print(f"Mejor score (CV): {random_search.best_score_:.4f}")

# Evaluar en test
y_pred_random = random_search.best_estimator_.predict(X_test)
print(f"Accuracy en test: {accuracy_score(y_test, y_pred_random):.4f}")

# Visualizar efecto de max_depth
max_depths = range(1, 21)
train_scores = []
test_scores = []

for depth in max_depths:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    train_scores.append(tree.score(X_train, y_train))
    test_scores.append(tree.score(X_test, y_test))

plt.figure(figsize=(10, 6))
plt.plot(max_depths, train_scores, label='Train Accuracy', marker='o')
plt.plot(max_depths, test_scores, label='Test Accuracy', marker='s')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Efecto de max_depth en el Rendimiento')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axvline(x=grid_search.best_params_['max_depth'], color='red', 
            linestyle='--', alpha=0.7, label='Mejor max_depth (Grid Search)')
plt.show()
```

#### 7. **Aplicaciones Reales de los 츼rboles de Decisi칩n**

Los 치rboles de decisi칩n se aplican en una amplia gama de problemas reales debido a su versatilidad y facilidad de interpretaci칩n. Algunos ejemplos son:

1. **Diagn칩stico M칠dico**: En la medicina, los 치rboles de decisi칩n se utilizan para ayudar a los m칠dicos a diagnosticar enfermedades bas치ndose en s칤ntomas y pruebas de laboratorio. Por ejemplo, un 치rbol de decisi칩n puede ayudar a predecir si un paciente tiene diabetes en funci칩n de caracter칤sticas como nivel de glucosa, presi칩n arterial y edad.

2. **Cr칠dito y Riesgo Financiero**: En el sector financiero, los 치rboles de decisi칩n se usan para evaluar la probabilidad de que un cliente incumpla un pr칠stamo. Las caracter칤sticas utilizadas pueden incluir el historial crediticio, los ingresos mensuales y el monto del pr칠stamo solicitado.

3. **M치rketing y Segmentaci칩n de Clientes**: En el marketing, los 치rboles de decisi칩n ayudan a segmentar a los clientes y a predecir si un cliente potencial realizar치 una compra. Los datos analizados pueden incluir el historial de compras, la interacci칩n con campa침as publicitarias y la demograf칤a del cliente.

4. **Control de Calidad en Manufactura**: En el sector manufacturero, los 치rboles de decisi칩n pueden ayudar a detectar productos defectuosos durante el proceso de producci칩n, bas치ndose en caracter칤sticas como la temperatura, el tiempo de producci칩n, y otras m칠tricas de calidad.

5. **Predicci칩n de Deserci칩n Escolar**: En educaci칩n, los 치rboles de decisi칩n se usan para predecir la probabilidad de que un estudiante abandone sus estudios, bas치ndose en factores como la asistencia, las calificaciones y el apoyo familiar.

6. **Clasificaci칩n de Especies**: En la biolog칤a, se utilizan para clasificar especies de plantas o animales seg칰n caracter칤sticas observadas. Un ejemplo cl치sico es el conjunto de datos Iris, donde se clasifica una flor en una de tres especies seg칰n el largo y ancho de los p칠talos y s칠palos.

#### 8. **Conclusi칩n**

Los **치rboles de decisi칩n** son una herramienta fundamental en el aprendizaje autom치tico debido a su capacidad para dividir los datos de manera iterativa y sencilla, maximizando la pureza de los nodos en cada divisi칩n. Aunque presentan ciertas limitaciones, como el riesgo de sobreajuste, son particularmente valiosos cuando se necesita una explicaci칩n clara y comprensible del proceso de decisi칩n. Los 치rboles de decisi칩n se utilizan ampliamente en muchos sectores, y sus aplicaciones van desde el diagn칩stico m칠dico hasta la predicci칩n del comportamiento de los clientes. Son una excelente elecci칩n cuando la interpretabilidad y la facilidad de uso son factores importantes a considerar.
