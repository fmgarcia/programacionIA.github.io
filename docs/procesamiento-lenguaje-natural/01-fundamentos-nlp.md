# üí¨ Unidad 1. Fundamentos del Procesamiento de Lenguaje Natural

El **Procesamiento de Lenguaje Natural (NLP)** es una disciplina en la intersecci√≥n de la ling√º√≠stica, la inform√°tica y la inteligencia artificial. Su objetivo es permitir que las m√°quinas comprendan, interpreten y generen lenguaje humano de forma √∫til.


![Ilustraci√≥n de nlp basics](../assets/images/nlp_basics.svg)
---

## 1.1. ¬øQu√© es el NLP?

El NLP abarca un amplio rango de tareas computacionales que involucran el lenguaje humano:

*   **Comprensi√≥n:** Extraer significado del texto (clasificaci√≥n, extracci√≥n de entidades, an√°lisis de sentimientos).
*   **Generaci√≥n:** Crear texto coherente (chatbots, res√∫menes, traducci√≥n).
*   **Transformaci√≥n:** Convertir texto de una forma a otra (correcci√≥n ortogr√°fica, parafraseo).

### ¬øPor qu√© es dif√≠cil?

El lenguaje humano presenta desaf√≠os √∫nicos para las m√°quinas:

*   **Ambig√ºedad:** Una palabra puede tener m√∫ltiples significados ("banco" puede ser una instituci√≥n financiera o un asiento).
*   **Contexto:** El significado cambia seg√∫n el contexto ("Hace fr√≠o" vs "Es un tipo fr√≠o").
*   **Variabilidad:** Sin√≥nimos, jerga, errores ortogr√°ficos, diferentes idiomas.
*   **Conocimiento impl√≠cito:** Los humanos entienden iron√≠a, sarcasmo y referencias culturales.

---

## 1.2. Pipeline T√≠pico de NLP

Un proyecto de NLP generalmente sigue estos pasos:

```
Texto Crudo ‚Üí Preprocesamiento ‚Üí Representaci√≥n Vectorial ‚Üí Modelo ML/DL ‚Üí Salida
```

1.  **Adquisici√≥n de Datos:** Obtener el corpus de texto (scraping, APIs, datasets p√∫blicos).
2.  **Preprocesamiento:** Limpiar y normalizar el texto.
3.  **Representaci√≥n:** Convertir texto a n√∫meros (vectores).
4.  **Modelado:** Aplicar algoritmos de ML o Deep Learning.
5.  **Evaluaci√≥n:** Medir el rendimiento con m√©tricas apropiadas.
6.  **Despliegue:** Poner el modelo en producci√≥n.

---

## 1.3. Preprocesamiento de Texto

El preprocesamiento es crucial para reducir el ruido y normalizar el texto.

### Tokenizaci√≥n

Dividir el texto en unidades m√°s peque√±as llamadas **tokens** (palabras, subpalabras o caracteres).

```python
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize

texto = "El NLP es fascinante. Permite crear chatbots incre√≠bles."

# Tokenizaci√≥n por oraciones
oraciones = sent_tokenize(texto, language='spanish')
print(oraciones)
# ['El NLP es fascinante.', 'Permite crear chatbots incre√≠bles.']

# Tokenizaci√≥n por palabras
palabras = word_tokenize(texto, language='spanish')
print(palabras)
# ['El', 'NLP', 'es', 'fascinante', '.', 'Permite', 'crear', 'chatbots', 'incre√≠bles', '.']
```

### Normalizaci√≥n de Texto

*   **Lowercasing:** Convertir todo a min√∫sculas para que "Casa" y "casa" sean iguales.
*   **Eliminaci√≥n de puntuaci√≥n y caracteres especiales.**
*   **Eliminaci√≥n de n√∫meros** (si no son relevantes).
*   **Eliminaci√≥n de espacios extra.**

```python
import re

def normalizar_texto(texto):
    # Min√∫sculas
    texto = texto.lower()
    # Eliminar caracteres especiales (mantener letras, n√∫meros y espacios)
    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±0-9\s]', '', texto)
    # Eliminar espacios m√∫ltiples
    texto = re.sub(r'\s+', ' ', texto).strip()
    return texto

print(normalizar_texto("¬°Hola! ¬øC√≥mo est√°s???  Muy   bien."))
# 'hola c√≥mo est√°s muy bien'
```

### Stop Words

Las **stop words** son palabras muy comunes que generalmente no aportan significado sem√°ntico (art√≠culos, preposiciones, conjunciones).

```python
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words = set(stopwords.words('spanish'))
print(list(stop_words)[:10])
# ['al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual']

palabras_filtradas = [w for w in palabras if w.lower() not in stop_words]
print(palabras_filtradas)
```

### Stemming vs Lematizaci√≥n

Ambas t√©cnicas reducen las palabras a su forma base, pero de forma diferente:

*   **Stemming:** Corta la palabra de forma heur√≠stica para obtener su ra√≠z. Es r√°pido pero puede producir ra√≠ces que no son palabras reales ("correr", "corriendo" ‚Üí "corr").

```python
from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer('spanish')
palabras = ["corriendo", "corr√≠", "correr", "corredores"]
stems = [stemmer.stem(p) for p in palabras]
print(stems)
# ['corr', 'corr', 'corr', 'corred']
```

*   **Lematizaci√≥n:** Utiliza un diccionario para encontrar el **lema** (forma can√≥nica) de la palabra. Es m√°s preciso pero m√°s lento ("corriendo" ‚Üí "correr").

```python
import spacy

nlp = spacy.load('es_core_news_sm')
doc = nlp("Los perros estaban corriendo por el parque")
lemmas = [token.lemma_ for token in doc]
print(lemmas)
# ['el', 'perro', 'estar', 'correr', 'por', 'el', 'parque']
```

---

## 1.4. Bibliotecas Fundamentales de NLP

| Biblioteca | Descripci√≥n | Fortalezas |
| :--- | :--- | :--- |
| **NLTK** | Natural Language Toolkit. La biblioteca cl√°sica para aprender NLP. | Educativa, muchos recursos, corpus incluidos. |
| **spaCy** | Biblioteca industrial de NLP. | R√°pida, pipelines preentrenados, NER, POS. |
| **Transformers (Hugging Face)** | Modelos de lenguaje preentrenados (BERT, GPT, etc.). | Estado del arte, f√°cil uso, muchos modelos. |
| **Gensim** | Especializada en modelado de t√≥picos y embeddings. | Word2Vec, Doc2Vec, LDA. |
| **TextBlob** | Interfaz simple para tareas comunes de NLP. | F√°cil de usar, an√°lisis de sentimientos. |

### Ejemplo Completo con spaCy

```python
import spacy

# Cargar modelo en espa√±ol
nlp = spacy.load('es_core_news_sm')

texto = "Apple est√° buscando comprar una startup del Reino Unido por 1.000 millones de d√≥lares."
doc = nlp(texto)

# Tokenizaci√≥n autom√°tica
print("Tokens:", [token.text for token in doc])

# Part-of-Speech Tagging (Etiquetado gramatical)
print("\nPOS Tagging:")
for token in doc:
    print(f"  {token.text}: {token.pos_} ({token.dep_})")

# Named Entity Recognition (NER)
print("\nEntidades:")
for ent in doc.ents:
    print(f"  {ent.text}: {ent.label_}")
# Apple: ORG, Reino Unido: LOC, 1.000 millones de d√≥lares: MONEY
```

---

## 1.5. Tareas Comunes de NLP

El campo del NLP abarca muchas tareas espec√≠ficas:

### Clasificaci√≥n de Texto

Asignar una categor√≠a a un documento.

*   **An√°lisis de Sentimientos:** Positivo / Negativo / Neutral.
*   **Detecci√≥n de Spam:** Spam / No Spam.
*   **Clasificaci√≥n de Noticias:** Deportes / Pol√≠tica / Tecnolog√≠a.

### Extracci√≥n de Informaci√≥n

*   **Named Entity Recognition (NER):** Identificar personas, lugares, organizaciones, fechas.
*   **Extracci√≥n de Relaciones:** Encontrar relaciones entre entidades ("Apple" - adquiri√≥ - "Startup").

### Generaci√≥n de Texto

*   **Resumen Autom√°tico:** Crear versiones cortas de textos largos.
*   **Traducci√≥n Autom√°tica:** Convertir texto entre idiomas.
*   **Chatbots:** Generar respuestas conversacionales.
*   **Completado de Texto:** GPT, modelos de lenguaje.

### Similitud y B√∫squeda

*   **B√∫squeda Sem√°ntica:** Encontrar documentos similares por significado.
*   **Question Answering (QA):** Responder preguntas bas√°ndose en un contexto.

---

## 1.6. Datasets Populares en NLP

| Dataset | Descripci√≥n | Tarea |
| :--- | :--- | :--- |
| **IMDB Reviews** | 50,000 rese√±as de pel√≠culas | An√°lisis de sentimientos |
| **AG News** | 120,000 art√≠culos de noticias | Clasificaci√≥n de texto |
| **SQuAD** | Preguntas sobre art√≠culos de Wikipedia | Question Answering |
| **GLUE / SuperGLUE** | Benchmark de m√∫ltiples tareas | Evaluaci√≥n de modelos |
| **CoNLL-2003** | Corpus anotado con entidades | NER |

---

## 1.7. Consideraciones √âticas en NLP

El NLP plantea importantes cuestiones √©ticas:

*   **Sesgo en los Datos:** Los modelos pueden perpetuar sesgos de g√©nero, raza o cultura presentes en los datos de entrenamiento.
*   **Privacidad:** Los modelos entrenados pueden memorizar informaci√≥n sensible.
*   **Desinformaci√≥n:** La generaci√≥n de texto puede usarse para crear fake news.
*   **Interpretabilidad:** Los modelos de Deep Learning son "cajas negras".

Es responsabilidad del profesional ser consciente de estos riesgos y tomar medidas para mitigarlos.

---

üìÖ **Fecha de creaci√≥n:** Enero 2026  
‚úçÔ∏è **Autor:** Fran Garc√≠a
