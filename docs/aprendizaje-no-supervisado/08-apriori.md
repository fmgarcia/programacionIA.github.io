# üõí Unidad 8. Apriori - Reglas de Asociaci√≥n

**Apriori** es un algoritmo cl√°sico de **miner√≠a de reglas de asociaci√≥n** utilizado para descubrir patrones frecuentes y relaciones entre √≠tems en grandes datasets transaccionales. Es famoso por su aplicaci√≥n en el **an√°lisis de la cesta de la compra** (Market Basket Analysis), donde se identifican qu√© productos suelen comprarse juntos. A diferencia de otros algoritmos de aprendizaje no supervisado, Apriori trabaja con datos transaccionales discretos.

---

## 8.1. ¬øQu√© son las Reglas de Asociaci√≥n?

### El Problema de la Cesta de la Compra

Imagina que tienes datos de transacciones de un supermercado:

| Transacci√≥n | √çtems Comprados |
| :---: | :--- |
| T1 | Pan, Leche, Mantequilla |
| T2 | Pan, Cerveza |
| T3 | Leche, Pa√±ales, Cerveza |
| T4 | Pan, Leche, Pa√±ales, Cerveza |
| T5 | Pan, Leche, Pa√±ales |

**Pregunta:** ¬øQu√© productos tienden a comprarse juntos?

Una **regla de asociaci√≥n** tiene la forma:

$$\text{Si } \{A, B\} \rightarrow \text{ entonces } \{C\}$$

Por ejemplo: *"Si un cliente compra Pan y Leche, es probable que tambi√©n compre Mantequilla"*

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ REGLA DE ASOCIACI√ìN                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ   {Pan, Leche} ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí {Mantequilla}               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   ‚ñ≤                               ‚ñ≤                        ‚îÇ
‚îÇ   ‚îÇ                               ‚îÇ                        ‚îÇ
‚îÇ   Antecedente (LHS)              Consecuente (RHS)         ‚îÇ
‚îÇ   "Si compra esto..."            "...probablemente         ‚îÇ
‚îÇ                                   tambi√©n compra esto"      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   M√©tricas de la regla:                                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Support (Soporte): ¬øQu√© tan frecuente es {Pan,Leche}?    ‚îÇ
‚îÇ   Confidence (Confianza): ¬øCon qu√© frecuencia se cumple?   ‚îÇ
‚îÇ   Lift: ¬øEs la asociaci√≥n significativa?                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 8.2. Conceptos Fundamentales y M√©tricas

### Itemset

Un **itemset** es un conjunto de √≠tems. Por ejemplo: {Pan, Leche, Mantequilla}

Un **itemset frecuente** es uno que aparece en al menos un n√∫mero m√≠nimo de transacciones.

### Support (Soporte)

El **soporte** mide qu√© tan frecuente es un itemset en el dataset:

$$Support(A) = \frac{\text{N√∫mero de transacciones que contienen } A}{\text{N√∫mero total de transacciones}}$$

Para una regla $A \rightarrow B$:

$$Support(A \rightarrow B) = \frac{|\{t : A \cup B \subseteq t\}|}{|T|}$$

**Ejemplo:** Si {Pan, Leche} aparece en 3 de 5 transacciones:
$$Support(\{Pan, Leche\}) = \frac{3}{5} = 0.6 = 60\%$$

### Confidence (Confianza)

La **confianza** mide qu√© tan a menudo se cumple la regla cuando el antecedente est√° presente:

$$Confidence(A \rightarrow B) = \frac{Support(A \cup B)}{Support(A)} = P(B|A)$$

**Ejemplo:** Si {Pan} aparece en 4 transacciones y {Pan, Leche} en 3:
$$Confidence(\{Pan\} \rightarrow \{Leche\}) = \frac{3}{4} = 0.75 = 75\%$$

### Lift

El **lift** mide si la asociaci√≥n es significativa o simplemente debida al azar:

$$Lift(A \rightarrow B) = \frac{Confidence(A \rightarrow B)}{Support(B)} = \frac{P(A \cap B)}{P(A) \cdot P(B)}$$

**Interpretaci√≥n:**
- **Lift > 1:** A y B aparecen juntos m√°s de lo esperado por azar ‚Üí Asociaci√≥n positiva
- **Lift = 1:** A y B son independientes
- **Lift < 1:** A y B aparecen juntos menos de lo esperado ‚Üí Asociaci√≥n negativa

### Conviction

La **conviction** mide qu√© tan diferente es la regla de una asociaci√≥n aleatoria:

$$Conviction(A \rightarrow B) = \frac{1 - Support(B)}{1 - Confidence(A \rightarrow B)}$$

**Interpretaci√≥n:**
- **Conviction alto:** La regla es muy √∫til
- **Conviction = 1:** A y B son independientes
- **Conviction = ‚àû:** La regla siempre se cumple

---

## 8.3. El Algoritmo Apriori

### La Propiedad Apriori (Antimonoton√≠a)

El algoritmo se basa en un principio clave:

> **"Si un itemset es infrecuente, todos sus superconjuntos tambi√©n ser√°n infrecuentes"**

Esto permite podar el espacio de b√∫squeda eficientemente.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PRINCIPIO APRIORI                                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ   Si {A, B} es INFRECUENTE                                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   Entonces NO necesitamos verificar:                       ‚îÇ
‚îÇ   - {A, B, C}                                               ‚îÇ
‚îÇ   - {A, B, D}                                               ‚îÇ
‚îÇ   - {A, B, C, D}                                            ‚îÇ
‚îÇ   - ... (todos los superconjuntos)                         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ   ¬°Esto ahorra much√≠simo c√≥mputo!                          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pasos del Algoritmo

1. **Paso 1:** Encontrar todos los itemsets de tama√±o 1 que cumplan min_support
2. **Paso 2:** Generar candidatos de tama√±o k+1 combinando itemsets frecuentes de tama√±o k
3. **Paso 3:** Podar candidatos que contengan subconjuntos infrecuentes
4. **Paso 4:** Calcular soporte de candidatos restantes
5. **Paso 5:** Repetir hasta que no haya m√°s itemsets frecuentes
6. **Paso 6:** Generar reglas de los itemsets frecuentes que cumplan min_confidence

---

## 8.4. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **F√°cil de entender e implementar:** Algoritmo intuitivo | **Puede ser lento:** M√∫ltiples escaneos del dataset |
| **Interpretable:** Las reglas son f√°ciles de explicar | **Genera muchos candidatos:** Especialmente con min_support bajo |
| **Escalable con poda:** La propiedad apriori reduce b√∫squeda | **Requiere datos discretos:** No funciona con datos continuos directamente |
| **Ampliamente usado:** Implementaciones optimizadas disponibles | **Sensible a par√°metros:** min_support y min_confidence afectan mucho |
| **Resultados accionables:** √ötil para decisiones de negocio | **No captura contexto:** Solo frecuencia, no causa-efecto |

---

## 8.5. Ejemplo B√°sico en Python

Este ejemplo usa la biblioteca `mlxtend` para implementar Apriori y reglas de asociaci√≥n.

```python
# ============================================================
# EJEMPLO B√ÅSICO: Apriori para an√°lisis de cesta de compra
# ============================================================

# Importar bibliotecas necesarias
import pandas as pd                          # Manipulaci√≥n de datos
import numpy as np                           # Operaciones num√©ricas
from mlxtend.preprocessing import TransactionEncoder  # Codificar transacciones
from mlxtend.frequent_patterns import apriori, association_rules  # Algoritmo Apriori

# -------------------------------------------------------------
# 1. CREAR DATASET DE TRANSACCIONES
# -------------------------------------------------------------
# Lista de transacciones (cada transacci√≥n es una lista de √≠tems)
transactions = [
    ['Pan', 'Leche', 'Mantequilla'],
    ['Pan', 'Cerveza'],
    ['Leche', 'Pa√±ales', 'Cerveza'],
    ['Pan', 'Leche', 'Pa√±ales', 'Cerveza'],
    ['Pan', 'Leche', 'Pa√±ales'],
    ['Leche', 'Pa√±ales', 'Cerveza'],
    ['Pan', 'Leche'],
    ['Pan', 'Cerveza', 'Pa√±ales'],
    ['Pan', 'Leche', 'Cerveza', 'Pa√±ales'],
    ['Leche', 'Mantequilla']
]

print("="*60)
print("APRIORI - AN√ÅLISIS DE CESTA DE LA COMPRA")
print("="*60)

print(f"\n--- Transacciones Originales ---")
for i, t in enumerate(transactions, 1):
    print(f"T{i}: {t}")

print(f"\nTotal de transacciones: {len(transactions)}")

# -------------------------------------------------------------
# 2. PREPARAR DATOS PARA APRIORI
# -------------------------------------------------------------
# TransactionEncoder convierte listas a matriz binaria
te = TransactionEncoder()
te_array = te.fit_transform(transactions)

# Crear DataFrame con nombres de columnas (√≠tems)
df = pd.DataFrame(te_array, columns=te.columns_)

print(f"\n--- Matriz de Transacciones (One-Hot Encoded) ---")
print(df)
print(f"\n√çtems √∫nicos: {list(te.columns_)}")

# -------------------------------------------------------------
# 3. ENCONTRAR ITEMSETS FRECUENTES
# -------------------------------------------------------------
print(f"\n--- Paso 1: Encontrar Itemsets Frecuentes ---")

# Aplicar algoritmo Apriori
# min_support: soporte m√≠nimo (% de transacciones)
frequent_itemsets = apriori(
    df, 
    min_support=0.3,  # Itemset debe aparecer en al menos 30% de transacciones
    use_colnames=True  # Usar nombres de √≠tems en lugar de √≠ndices
)

# Ordenar por soporte
frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)

print(f"\nItemsets frecuentes (min_support=30%):")
print(frequent_itemsets.to_string(index=False))

print(f"\nTotal de itemsets frecuentes: {len(frequent_itemsets)}")

# -------------------------------------------------------------
# 4. GENERAR REGLAS DE ASOCIACI√ìN
# -------------------------------------------------------------
print(f"\n--- Paso 2: Generar Reglas de Asociaci√≥n ---")

# Generar reglas con confianza m√≠nima
rules = association_rules(
    frequent_itemsets,
    metric='confidence',  # M√©trica para filtrar
    min_threshold=0.5     # Confianza m√≠nima del 50%
)

# Seleccionar columnas relevantes y ordenar
rules_display = rules[['antecedents', 'consequents', 'support', 
                        'confidence', 'lift', 'conviction']]
rules_display = rules_display.sort_values('lift', ascending=False)

print(f"\nReglas de asociaci√≥n (min_confidence=50%):")
print(rules_display.to_string(index=False))

print(f"\nTotal de reglas: {len(rules)}")

# -------------------------------------------------------------
# 5. INTERPRETAR REGLAS
# -------------------------------------------------------------
print(f"\n--- Interpretaci√≥n de las Mejores Reglas ---")

# Top 3 reglas por Lift
top_rules = rules.nlargest(3, 'lift')

for idx, (_, rule) in enumerate(top_rules.iterrows(), 1):
    ant = list(rule['antecedents'])
    cons = list(rule['consequents'])
    supp = rule['support']
    conf = rule['confidence']
    lift = rule['lift']
    
    print(f"\n{idx}. {ant} ‚Üí {cons}")
    print(f"   Soporte: {supp:.1%} (aparece en {supp*10:.0f}/10 transacciones)")
    print(f"   Confianza: {conf:.1%} (cuando se compra {ant}, {conf:.0%} compra {cons})")
    print(f"   Lift: {lift:.2f} ({lift:.2f}x m√°s probable que por azar)")

# -------------------------------------------------------------
# 6. VISUALIZAR REGLAS
# -------------------------------------------------------------
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Scatter: Support vs Confidence, coloreado por Lift
scatter = axes[0].scatter(rules['support'], rules['confidence'], 
                          c=rules['lift'], cmap='RdYlGn', s=100, alpha=0.7)
axes[0].set_xlabel('Support')
axes[0].set_ylabel('Confidence')
axes[0].set_title('Reglas: Support vs Confidence\n(color=Lift)')
plt.colorbar(scatter, ax=axes[0], label='Lift')

# Histograma de Lift
axes[1].hist(rules['lift'], bins=10, color='steelblue', edgecolor='black')
axes[1].axvline(x=1, color='red', linestyle='--', label='Lift=1 (independencia)')
axes[1].set_xlabel('Lift')
axes[1].set_ylabel('Frecuencia')
axes[1].set_title('Distribuci√≥n del Lift')
axes[1].legend()

# Top 5 reglas por Lift
top_5 = rules.nlargest(5, 'lift')
labels = [f"{list(r['antecedents'])} ‚Üí {list(r['consequents'])}" 
          for _, r in top_5.iterrows()]
labels = [l[:30] + '...' if len(l) > 30 else l for l in labels]  # Truncar

axes[2].barh(labels, top_5['lift'], color='steelblue')
axes[2].axvline(x=1, color='red', linestyle='--', alpha=0.5)
axes[2].set_xlabel('Lift')
axes[2].set_title('Top 5 Reglas por Lift')

plt.tight_layout()
plt.show()

# -------------------------------------------------------------
# 7. FILTRAR REGLAS √öTILES
# -------------------------------------------------------------
print(f"\n--- Reglas Recomendadas para el Negocio ---")

# Reglas con alto lift Y alta confianza
good_rules = rules[(rules['lift'] > 1.0) & (rules['confidence'] > 0.5)]
good_rules = good_rules.sort_values('lift', ascending=False)

print(f"\nReglas con Lift > 1 y Confidence > 50%:")
for _, rule in good_rules.head(5).iterrows():
    ant = ', '.join(list(rule['antecedents']))
    cons = ', '.join(list(rule['consequents']))
    print(f"  ‚Ä¢ Si compra [{ant}], es {rule['confidence']:.0%} probable que compre [{cons}]")
    print(f"    (Lift: {rule['lift']:.2f})")

print("""
Recomendaciones basadas en las reglas:
1. Colocar productos asociados cerca en el supermercado
2. Crear promociones de productos que se compran juntos
3. Recomendar productos complementarios en el checkout
""")
```

---

## 8.6. Ejemplo Avanzado: Dataset Real y Optimizaci√≥n

Este ejemplo trabaja con un dataset m√°s grande y explora diferentes par√°metros.

```python
# ============================================================
# EJEMPLO AVANZADO: Apriori con dataset de retail
# ============================================================

import pandas as pd
import numpy as np
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# -------------------------------------------------------------
# 1. CREAR DATASET SINT√âTICO REALISTA
# -------------------------------------------------------------
np.random.seed(42)

# Simular 1000 transacciones de supermercado
products = {
    'Desayuno': ['Pan', 'Leche', 'Cereales', 'Huevos', 'Yogurt', 'Caf√©', 'Zumo'],
    'Snacks': ['Galletas', 'Chocolate', 'Patatas', 'Frutos_secos'],
    'Bebidas': ['Agua', 'Cerveza', 'Vino', 'Refresco'],
    'Limpieza': ['Detergente', 'Jab√≥n', 'Papel_higienico'],
    'Beb√©s': ['Pa√±ales', 'Toallitas', 'Papilla']
}

all_products = [p for sublist in products.values() for p in sublist]

def generate_transaction():
    """Genera una transacci√≥n con productos correlacionados"""
    items = []
    
    # Desayuno: alta probabilidad de combinar
    if np.random.random() < 0.6:
        items.extend(np.random.choice(['Pan', 'Leche', 'Cereales'], 
                                      size=np.random.randint(2, 4), replace=False))
        if np.random.random() < 0.3:
            items.append('Mantequilla')
    
    # Cerveza y pa√±ales (ejemplo cl√°sico)
    if np.random.random() < 0.15:
        items.extend(['Cerveza', 'Pa√±ales'])
    
    # Productos aleatorios adicionales
    n_random = np.random.randint(1, 5)
    items.extend(np.random.choice(all_products, size=n_random, replace=False))
    
    return list(set(items))

# Generar transacciones
n_transactions = 1000
transactions = [generate_transaction() for _ in range(n_transactions)]

print("="*60)
print("APRIORI - AN√ÅLISIS AVANZADO")
print("="*60)

print(f"\nTransacciones generadas: {n_transactions}")
print(f"Productos √∫nicos: {len(all_products)}")

# Estad√≠sticas b√°sicas
sizes = [len(t) for t in transactions]
print(f"Tama√±o promedio de transacci√≥n: {np.mean(sizes):.1f} √≠tems")
print(f"Rango: {min(sizes)} - {max(sizes)} √≠tems")

# -------------------------------------------------------------
# 2. PREPARAR DATOS
# -------------------------------------------------------------
te = TransactionEncoder()
te_array = te.fit_transform(transactions)
df = pd.DataFrame(te_array, columns=te.columns_)

print(f"\nMatriz de transacciones: {df.shape}")

# Frecuencia de productos individuales
product_freq = df.sum().sort_values(ascending=False)
print(f"\n--- Top 10 Productos m√°s Frecuentes ---")
print(product_freq.head(10))

# -------------------------------------------------------------
# 3. COMPARAR PAR√ÅMETROS DE SOPORTE M√çNIMO
# -------------------------------------------------------------
print(f"\n--- Efecto del Soporte M√≠nimo ---")

min_supports = [0.01, 0.05, 0.1, 0.2, 0.3]
results = []

for min_sup in min_supports:
    freq_items = apriori(df, min_support=min_sup, use_colnames=True)
    n_items = len(freq_items)
    max_size = freq_items['itemsets'].apply(len).max() if n_items > 0 else 0
    results.append({
        'min_support': min_sup,
        'n_itemsets': n_items,
        'max_size': max_size
    })
    print(f"  min_support={min_sup:.0%}: {n_items:4d} itemsets, max tama√±o={max_size}")

# -------------------------------------------------------------
# 4. ENCONTRAR ITEMSETS FRECUENTES (APRIORI vs FP-GROWTH)
# -------------------------------------------------------------
print(f"\n--- Comparaci√≥n: Apriori vs FP-Growth ---")

import time

# Apriori
start = time.time()
freq_apriori = apriori(df, min_support=0.05, use_colnames=True)
time_apriori = time.time() - start

# FP-Growth (m√°s eficiente)
start = time.time()
freq_fpgrowth = fpgrowth(df, min_support=0.05, use_colnames=True)
time_fpgrowth = time.time() - start

print(f"  Apriori:   {len(freq_apriori):4d} itemsets en {time_apriori:.3f}s")
print(f"  FP-Growth: {len(freq_fpgrowth):4d} itemsets en {time_fpgrowth:.3f}s")
print(f"  Speedup FP-Growth: {time_apriori/time_fpgrowth:.2f}x")

# Usar FP-Growth para el resto del an√°lisis
frequent_itemsets = freq_fpgrowth

# -------------------------------------------------------------
# 5. GENERAR Y ANALIZAR REGLAS
# -------------------------------------------------------------
print(f"\n--- Generaci√≥n de Reglas de Asociaci√≥n ---")

# Generar reglas con diferentes m√©tricas
metrics = ['confidence', 'lift', 'conviction']

for metric in metrics:
    rules = association_rules(frequent_itemsets, metric=metric, 
                              min_threshold=0.5 if metric=='confidence' else 1.0)
    print(f"  M√©trica '{metric}': {len(rules)} reglas")

# Usar confianza como m√©trica principal
rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)
rules = rules[rules['lift'] > 1]  # Solo asociaciones positivas

print(f"\nReglas finales (confidence>50%, lift>1): {len(rules)}")

# -------------------------------------------------------------
# 6. TOP REGLAS POR DIFERENTES CRITERIOS
# -------------------------------------------------------------
print(f"\n--- Top Reglas por Diferentes Criterios ---")

def format_rule(row):
    ant = ', '.join(list(row['antecedents']))
    cons = ', '.join(list(row['consequents']))
    return f"{ant} ‚Üí {cons}"

# Por Lift (asociaci√≥n m√°s fuerte)
print(f"\n[Top 5 por LIFT - Asociaci√≥n m√°s fuerte]")
top_lift = rules.nlargest(5, 'lift')
for _, r in top_lift.iterrows():
    print(f"  {format_rule(r)}")
    print(f"    Lift: {r['lift']:.2f}, Conf: {r['confidence']:.1%}, Supp: {r['support']:.1%}")

# Por Confidence (m√°s confiables)
print(f"\n[Top 5 por CONFIDENCE - M√°s confiables]")
top_conf = rules.nlargest(5, 'confidence')
for _, r in top_conf.iterrows():
    print(f"  {format_rule(r)}")
    print(f"    Conf: {r['confidence']:.1%}, Lift: {r['lift']:.2f}, Supp: {r['support']:.1%}")

# Por Support (m√°s frecuentes)
print(f"\n[Top 5 por SUPPORT - M√°s frecuentes]")
top_supp = rules.nlargest(5, 'support')
for _, r in top_supp.iterrows():
    print(f"  {format_rule(r)}")
    print(f"    Supp: {r['support']:.1%}, Conf: {r['confidence']:.1%}, Lift: {r['lift']:.2f}")

# -------------------------------------------------------------
# 7. VISUALIZACI√ìN AVANZADA
# -------------------------------------------------------------
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# 1. Heatmap de m√©tricas
metrics_df = rules[['support', 'confidence', 'lift', 'leverage', 'conviction']].copy()
metrics_df.index = [format_rule(rules.iloc[i])[:20] + '...' for i in range(len(rules))]
# Tomar solo top 15 para visualizaci√≥n
metrics_sample = metrics_df.head(15)

ax1 = axes[0, 0]
from matplotlib.colors import Normalize
# Normalizar para heatmap
normalized = (metrics_sample - metrics_sample.min()) / (metrics_sample.max() - metrics_sample.min())
im = ax1.imshow(normalized.values, cmap='YlOrRd', aspect='auto')
ax1.set_xticks(range(len(metrics_sample.columns)))
ax1.set_xticklabels(metrics_sample.columns, rotation=45)
ax1.set_yticks(range(len(metrics_sample.index)))
ax1.set_yticklabels(metrics_sample.index, fontsize=8)
ax1.set_title('M√©tricas de Reglas (normalizado)')
plt.colorbar(im, ax=ax1)

# 2. Scatter 3D: Support, Confidence, Lift
from mpl_toolkits.mplot3d import Axes3D
ax2 = fig.add_subplot(2, 2, 2, projection='3d')
ax2.scatter(rules['support'], rules['confidence'], rules['lift'],
            c=rules['lift'], cmap='viridis', s=50)
ax2.set_xlabel('Support')
ax2.set_ylabel('Confidence')
ax2.set_zlabel('Lift')
ax2.set_title('Reglas en 3D')

# 3. An√°lisis de productos en reglas
ax3 = axes[1, 0]

# Contar apariciones de cada producto en reglas
product_counts = {}
for _, row in rules.iterrows():
    for item in row['antecedents']:
        product_counts[item] = product_counts.get(item, 0) + 1
    for item in row['consequents']:
        product_counts[item] = product_counts.get(item, 0) + 1

product_counts_sorted = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)[:10]
products_names, counts = zip(*product_counts_sorted)

ax3.barh(products_names, counts, color='steelblue')
ax3.set_xlabel('Apariciones en reglas')
ax3.set_title('Productos m√°s frecuentes en reglas')

# 4. Distribuci√≥n de tama√±os de itemsets
ax4 = axes[1, 1]

# Tama√±o de antecedentes y consecuentes
ant_sizes = rules['antecedents'].apply(len)
cons_sizes = rules['consequents'].apply(len)

ax4.hist(ant_sizes, bins=range(1, max(ant_sizes)+2), alpha=0.7, 
         label='Antecedentes', color='blue')
ax4.hist(cons_sizes, bins=range(1, max(cons_sizes)+2), alpha=0.7,
         label='Consecuentes', color='green')
ax4.set_xlabel('N√∫mero de √≠tems')
ax4.set_ylabel('Frecuencia')
ax4.set_title('Tama√±o de Antecedentes y Consecuentes')
ax4.legend()

plt.tight_layout()
plt.show()

# -------------------------------------------------------------
# 8. FILTRAR REGLAS ACCIONABLES
# -------------------------------------------------------------
print(f"\n--- Reglas Accionables para el Negocio ---")

# Definir umbrales para reglas √∫tiles
actionable_rules = rules[
    (rules['support'] > 0.05) &      # Al menos 5% de transacciones
    (rules['confidence'] > 0.6) &     # Al menos 60% de confianza
    (rules['lift'] > 1.2)             # Al menos 20% mejor que azar
].copy()

actionable_rules = actionable_rules.sort_values('lift', ascending=False)

print(f"\nReglas accionables encontradas: {len(actionable_rules)}")
print("\n[Recomendaciones estrat√©gicas]")

for i, (_, rule) in enumerate(actionable_rules.head(5).iterrows(), 1):
    ant = ', '.join(list(rule['antecedents']))
    cons = ', '.join(list(rule['consequents']))
    
    print(f"\n{i}. Cuando el cliente compra: {ant}")
    print(f"   ‚Üí Recomendar: {cons}")
    print(f"   Efectividad: {rule['confidence']:.0%} de probabilidad")
    print(f"   Impacto: {rule['lift']:.2f}x m√°s probable que aleatorio")

# -------------------------------------------------------------
# 9. AN√ÅLISIS DE PATRONES ESPEC√çFICOS
# -------------------------------------------------------------
print(f"\n--- An√°lisis de Patrones Espec√≠ficos ---")

# Buscar reglas que contengan un producto espec√≠fico
target_product = 'Cerveza'

rules_with_target = rules[
    rules['antecedents'].apply(lambda x: target_product in x) |
    rules['consequents'].apply(lambda x: target_product in x)
]

print(f"\nReglas relacionadas con '{target_product}': {len(rules_with_target)}")
for _, r in rules_with_target.head(3).iterrows():
    print(f"  {format_rule(r)} (Lift: {r['lift']:.2f})")

# -------------------------------------------------------------
# 10. RESUMEN
# -------------------------------------------------------------
print("\n" + "="*60)
print("RESUMEN Y MEJORES PR√ÅCTICAS")
print("="*60)

print(f"""
An√°lisis completado:
- Transacciones analizadas: {n_transactions:,}
- Itemsets frecuentes encontrados: {len(frequent_itemsets)}
- Reglas generadas: {len(rules)}
- Reglas accionables: {len(actionable_rules)}

Mejores pr√°cticas para Apriori:
1. min_support: Empezar alto (0.1) y bajar si hay pocos resultados
2. min_confidence: 0.5-0.7 para reglas confiables
3. Lift: Siempre filtrar por lift > 1 para asociaciones reales
4. FP-Growth: Usar en lugar de Apriori para datasets grandes
5. Validaci√≥n: Revisar reglas con expertos del dominio

Uso empresarial:
- Cross-selling: Recomendar productos complementarios
- Layout de tienda: Colocar productos asociados juntos
- Promociones: Crear bundles basados en asociaciones
- Inventario: Mantener stock de productos asociados
""")
```

---

## 8.7. Hiperpar√°metros y Par√°metros

### Par√°metros de Apriori (mlxtend)

| Par√°metro | Descripci√≥n | Valores | Recomendaci√≥n |
| :--- | :--- | :--- | :--- |
| `min_support` | Soporte m√≠nimo para itemsets | 0.0-1.0 | 0.01-0.1 (depende del dataset) |
| `use_colnames` | Usar nombres de columnas | bool | True |
| `max_len` | Tama√±o m√°ximo de itemsets | int | None (sin l√≠mite) |
| `low_memory` | Modo bajo memoria | bool | False |

### Par√°metros de Association Rules (mlxtend)

| Par√°metro | Descripci√≥n | Valores | Recomendaci√≥n |
| :--- | :--- | :--- | :--- |
| `metric` | M√©trica para filtrar | 'support', 'confidence', 'lift', etc. | 'confidence' o 'lift' |
| `min_threshold` | Umbral m√≠nimo de la m√©trica | float | Depende de la m√©trica |

---

## 8.8. Alternativas a Apriori

### FP-Growth

**FP-Growth** (Frequent Pattern Growth) es m√°s eficiente que Apriori:
- No genera candidatos expl√≠citamente
- Usa una estructura de √°rbol (FP-tree)
- Solo requiere 2 escaneos del dataset

```python
from mlxtend.frequent_patterns import fpgrowth

frequent_itemsets = fpgrowth(df, min_support=0.05, use_colnames=True)
```

### Eclat

**Eclat** usa intersecci√≥n de conjuntos verticales:
- Puede ser m√°s r√°pido para datasets densos

### SPMF

Para an√°lisis m√°s avanzado, la biblioteca **SPMF** ofrece muchos algoritmos de patrones secuenciales.

---

## 8.9. Aplicaciones Reales

### 1. Retail y E-commerce
An√°lisis de cesta de compra, recomendaciones de productos.
* [Amazon: "Customers who bought this also bought..."](https://www.amazon.com)

### 2. Banca y Finanzas
Detectar patrones de fraude, servicios complementarios.

### 3. Medicina
Asociaciones entre s√≠ntomas y diagn√≥sticos, efectos secundarios de medicamentos.

### 4. Web Mining
Analizar patrones de navegaci√≥n, p√°ginas visitadas juntas.

### 5. Telecomunicaciones
An√°lisis de servicios contratados juntos, patrones de uso.

---

## 8.10. Resumen y Checklist

### Checklist para usar Apriori

- [ ] **Datos en formato transaccional** (lista de √≠tems por transacci√≥n)
- [ ] **Codificar con TransactionEncoder** para matriz binaria
- [ ] **Empezar con min_support alto** y ajustar
- [ ] **Generar reglas con confidence** razonable (>50%)
- [ ] **Filtrar por lift > 1** para asociaciones reales
- [ ] **Validar reglas con conocimiento del dominio**
- [ ] **Usar FP-Growth** para datasets grandes

### ¬øCu√°ndo usar Apriori?

‚úÖ **Usar Apriori cuando:**
- Tienes datos transaccionales discretos
- Buscas patrones de co-ocurrencia
- Necesitas reglas interpretables
- Dataset de tama√±o moderado (<100K transacciones)

‚ùå **Considerar alternativas cuando:**
- Datos continuos ‚Üí Primero discretizar o usar clustering
- Dataset muy grande ‚Üí FP-Growth
- Patrones secuenciales ‚Üí GSP, PrefixSpan
- Necesitas predicci√≥n ‚Üí Modelos supervisados

---

üìÖ **Fecha de creaci√≥n:** Enero 2026  
‚úçÔ∏è **Autor:** Fran Garc√≠a
