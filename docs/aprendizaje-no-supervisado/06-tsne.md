# ğŸ”® Unidad 6. t-SNE - VisualizaciÃ³n de Alta DimensiÃ³n

**t-SNE** (t-distributed Stochastic Neighbor Embedding) es una tÃ©cnica de **reducciÃ³n de dimensionalidad no lineal** diseÃ±ada especÃ­ficamente para **visualizaciÃ³n** de datos de alta dimensiÃ³n. A diferencia de PCA que preserva la varianza global, t-SNE se enfoca en preservar la **estructura local**: los puntos que son similares en el espacio original permanecen cercanos en el espacio reducido. Esto lo hace excepcional para revelar clusters y patrones ocultos.


![IlustraciÃ³n de tsne](../assets/images/tsne.svg)
---

## 6.1. Â¿Por QuÃ© t-SNE?

### Limitaciones de PCA

PCA es una tÃ©cnica lineal que preserva la varianza global. Sin embargo:
- No captura relaciones no lineales
- No preserva bien la estructura de clusters
- Los datos proyectados pueden solaparse incluso si los clusters originales estÃ¡n bien separados

### La Idea de t-SNE

t-SNE se pregunta: *"Â¿CÃ³mo puedo proyectar los datos de manera que los vecinos cercanos en alta dimensiÃ³n sigan siendo vecinos cercanos en baja dimensiÃ³n?"*

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INTUICIÃ“N DE t-SNE                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ Espacio Original (Alta DimensiÃ³n)                           â”‚
â”‚                                                             â”‚
â”‚     A estÃ¡ cerca de B y C                                   â”‚
â”‚     A estÃ¡ lejos de X e Y                                   â”‚
â”‚                                                             â”‚
â”‚              A â— â— B                                        â”‚
â”‚                â— C                                          â”‚
â”‚                                                             â”‚
â”‚                           X â— â— Y                           â”‚
â”‚                                                             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                             â”‚
â”‚ t-SNE preserva estas relaciones de vecindad:                â”‚
â”‚                                                             â”‚
â”‚ Espacio Reducido (2D)                                       â”‚
â”‚                                                             â”‚
â”‚         â—A â—B               â—X                              â”‚
â”‚          â—C                  â—Y                             â”‚
â”‚                                                             â”‚
â”‚ âœ“ A sigue cerca de B y C                                   â”‚
â”‚ âœ“ A sigue lejos de X e Y                                   â”‚
â”‚ âœ“ Clusters visualmente separados                            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6.2. ExplicaciÃ³n MatemÃ¡tica

### Paso 1: Calcular Similitudes en Alta DimensiÃ³n

Para cada par de puntos $(x_i, x_j)$ en el espacio original, t-SNE calcula una **probabilidad condicional** $p_{j|i}$ que representa quÃ© tan probable es que $x_i$ elija a $x_j$ como su vecino si los vecinos se eligieran proporcionalmente a una distribuciÃ³n Gaussiana centrada en $x_i$:

$$p_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$$

Donde $\sigma_i$ es la varianza de la Gaussiana centrada en $x_i$ (se ajusta automÃ¡ticamente segÃºn el parÃ¡metro **perplexity**).

Las probabilidades se simetrizan:
$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$

### Paso 2: Calcular Similitudes en Baja DimensiÃ³n

En el espacio reducido, t-SNE usa una **distribuciÃ³n t de Student** (con 1 grado de libertad, es decir, distribuciÃ³n de Cauchy) en lugar de Gaussiana:

$$q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l}(1 + ||y_k - y_l||^2)^{-1}}$$

Donde $y_i$ y $y_j$ son las representaciones en baja dimensiÃ³n.

### Â¿Por QuÃ© la DistribuciÃ³n t?

La distribuciÃ³n t tiene **colas mÃ¡s pesadas** que la Gaussiana:

```
          Gaussiana          DistribuciÃ³n t
             ___                  ___
            /   \               /     \
           /     \             /       \
          /       \           /         \
      ___/         \___     _/           \_
           â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Colas ligeras       Colas pesadas
```

Esto resuelve el **problema del amontonamiento (crowding problem)**:
- En alta dimensiÃ³n hay mucho "espacio" para que los puntos se dispersen
- En baja dimensiÃ³n hay menos espacio
- Las colas pesadas permiten que puntos moderadamente lejanos se separen mÃ¡s, dejando espacio para los clusters

### Paso 3: Minimizar la Divergencia KL

t-SNE minimiza la **divergencia de Kullback-Leibler** entre las distribuciones P (alta dim) y Q (baja dim):

$$KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

Esta funciÃ³n de costo penaliza fuertemente cuando:
- Puntos cercanos en alta dimensiÃ³n ($p_{ij}$ alto) quedan lejos en baja dimensiÃ³n ($q_{ij}$ bajo)

La minimizaciÃ³n se hace mediante **descenso de gradiente**.

### El ParÃ¡metro Perplexity

La **perplexity** es el hiperparÃ¡metro mÃ¡s importante de t-SNE. Intuitivamente, es una medida del nÃºmero efectivo de vecinos cercanos:

$$Perplexity = 2^{H(P_i)}$$

Donde $H(P_i)$ es la entropÃ­a de Shannon de la distribuciÃ³n de probabilidad centrada en $x_i$.

- **Perplexity baja (5-10):** Solo considera vecinos muy cercanos â†’ estructura muy local
- **Perplexity alta (30-50):** Considera mÃ¡s vecinos â†’ estructura mÃ¡s global
- **Regla:** Debe ser menor que el nÃºmero de puntos

---

## 6.3. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **Excelente para visualizaciÃ³n:** Revela clusters claramente | **Solo para visualizaciÃ³n:** No usar para preprocesamiento de ML |
| **Preserva estructura local:** Vecinos cercanos permanecen juntos | **Lento:** Complejidad $O(n^2)$, aunque hay aproximaciones |
| **No lineal:** Captura relaciones complejas | **No determinÃ­stico:** Diferentes ejecuciones dan diferentes resultados |
| **Funciona bien con clusters:** Separa grupos visualmente | **Distancias no interpretables:** Las distancias entre clusters no tienen significado |
| **HiperparÃ¡metros simples:** Principalmente perplexity | **Sensible a hiperparÃ¡metros:** Perplexity afecta mucho el resultado |

---

## 6.4. Ejemplo BÃ¡sico en Python

Este ejemplo muestra el uso bÃ¡sico de t-SNE para visualizar el dataset de dÃ­gitos.

```python
# ============================================================
# EJEMPLO BÃSICO: t-SNE para visualizaciÃ³n de dÃ­gitos
# ============================================================

# Importar bibliotecas necesarias
import numpy as np                          # Operaciones numÃ©ricas
import matplotlib.pyplot as plt             # VisualizaciÃ³n
from sklearn.manifold import TSNE           # Algoritmo t-SNE
from sklearn.preprocessing import StandardScaler  # EstandarizaciÃ³n
from sklearn.datasets import load_digits    # Dataset de dÃ­gitos

# -------------------------------------------------------------
# 1. CARGAR DATOS
# -------------------------------------------------------------
digits = load_digits()
X = digits.data     # 1797 muestras Ã— 64 caracterÃ­sticas (8Ã—8 pÃ­xeles)
y = digits.target   # Etiquetas de dÃ­gitos (0-9)

print("="*50)
print("t-SNE - EJEMPLO BÃSICO CON DÃGITOS")
print("="*50)
print(f"\nDimensiones originales: {X.shape}")
print(f"Clases: {np.unique(y)}")

# -------------------------------------------------------------
# 2. ESTANDARIZAR LOS DATOS
# -------------------------------------------------------------
# Aunque t-SNE es robusto a la escala, es buena prÃ¡ctica estandarizar
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------------------------------------
# 3. APLICAR t-SNE
# -------------------------------------------------------------
# Reducir de 64 dimensiones a 2 para visualizaciÃ³n
print("\nAplicando t-SNE (puede tardar un momento)...")

tsne = TSNE(
    n_components=2,         # Reducir a 2 dimensiones
    perplexity=30,          # NÃºmero efectivo de vecinos (tÃ­pico: 5-50)
    random_state=42,        # Reproducibilidad
    n_iter=1000,            # NÃºmero de iteraciones de optimizaciÃ³n
    learning_rate='auto'    # Tasa de aprendizaje automÃ¡tica
)

# fit_transform: ajusta el modelo y transforma los datos
X_tsne = tsne.fit_transform(X_scaled)

print(f"Dimensiones despuÃ©s de t-SNE: {X_tsne.shape}")
print(f"Divergencia KL final: {tsne.kl_divergence_:.4f}")

# -------------------------------------------------------------
# 4. VISUALIZAR RESULTADOS
# -------------------------------------------------------------
plt.figure(figsize=(12, 10))

# Crear scatter plot con colores por dÃ­gito
scatter = plt.scatter(
    X_tsne[:, 0], X_tsne[:, 1],
    c=y,                    # Color segÃºn el dÃ­gito
    cmap='tab10',           # Paleta de 10 colores
    alpha=0.7,              # Transparencia
    edgecolors='w',         # Borde blanco
    s=30                    # TamaÃ±o de puntos
)

# AÃ±adir etiquetas en los centroides de cada cluster
for digit in range(10):
    mask = y == digit
    centroid = X_tsne[mask].mean(axis=0)
    plt.annotate(
        str(digit),
        centroid,
        fontsize=20,
        fontweight='bold',
        ha='center',
        va='center',
        color='black',
        bbox=dict(boxstyle='circle', facecolor='white', alpha=0.8)
    )

plt.xlabel('t-SNE DimensiÃ³n 1', fontsize=12)
plt.ylabel('t-SNE DimensiÃ³n 2', fontsize=12)
plt.title('VisualizaciÃ³n de DÃ­gitos con t-SNE (64D â†’ 2D)', fontsize=14)
plt.colorbar(scatter, label='DÃ­gito')
plt.grid(True, alpha=0.3)
plt.show()

# -------------------------------------------------------------
# 5. COMPARAR CON PCA
# -------------------------------------------------------------
from sklearn.decomposition import PCA

# Aplicar PCA para comparaciÃ³n
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Comparar visualizaciones
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# PCA
scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10',
                           alpha=0.7, edgecolors='w', s=20)
axes[0].set_xlabel('PC1')
axes[0].set_ylabel('PC2')
axes[0].set_title('PCA (lineal) - DÃ­gitos')
plt.colorbar(scatter1, ax=axes[0])

# t-SNE
scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',
                           alpha=0.7, edgecolors='w', s=20)
axes[1].set_xlabel('t-SNE 1')
axes[1].set_ylabel('t-SNE 2')
axes[1].set_title('t-SNE (no lineal) - DÃ­gitos')
plt.colorbar(scatter2, ax=axes[1])

plt.tight_layout()
plt.show()

print("""
Observaciones:
- t-SNE separa claramente los clusters de dÃ­gitos
- PCA muestra mÃ¡s solapamiento entre clases
- t-SNE es superior para visualizar estructura de clusters
- Las distancias en t-SNE no son interpretables (solo la estructura)
""")
```

---

## 6.5. Ejemplo Avanzado: Efecto de HiperparÃ¡metros y Buenas PrÃ¡cticas

Este ejemplo explora el efecto de la perplexity y otros parÃ¡metros.

```python
# ============================================================
# EJEMPLO AVANZADO: AnÃ¡lisis de hiperparÃ¡metros de t-SNE
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_digits
import time

# -------------------------------------------------------------
# 1. CARGAR Y PREPARAR DATOS
# -------------------------------------------------------------
digits = load_digits()
X = digits.data
y = digits.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("="*60)
print("ANÃLISIS DE HIPERPARÃMETROS DE t-SNE")
print("="*60)

# -------------------------------------------------------------
# 2. EFECTO DE LA PERPLEXITY
# -------------------------------------------------------------
print("\n[1] EFECTO DE LA PERPLEXITY")
print("-"*40)

perplexities = [5, 15, 30, 50, 100]

fig, axes = plt.subplots(1, len(perplexities), figsize=(20, 4))

for i, perp in enumerate(perplexities):
    print(f"  Calculando perplexity={perp}...", end=" ")
    start = time.time()
    
    tsne = TSNE(n_components=2, perplexity=perp, random_state=42,
                n_iter=1000, learning_rate='auto')
    X_tsne = tsne.fit_transform(X_scaled)
    
    elapsed = time.time() - start
    print(f"({elapsed:.1f}s)")
    
    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',
                    alpha=0.6, s=10, edgecolors='none')
    axes[i].set_title(f'Perplexity = {perp}')
    axes[i].set_xticks([])
    axes[i].set_yticks([])

plt.suptitle('Efecto de la Perplexity en t-SNE', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

print("""
InterpretaciÃ³n de Perplexity:
- Perplexity baja (5-10): Estructura muy local, clusters pequeÃ±os
- Perplexity media (30): Balance entre local y global (recomendado)
- Perplexity alta (50-100): Estructura mÃ¡s global, clusters mÃ¡s grandes
- Perplexity > n_samples/3 puede causar problemas
""")

# -------------------------------------------------------------
# 3. EFECTO DEL NÃšMERO DE ITERACIONES
# -------------------------------------------------------------
print("\n[2] EFECTO DEL NÃšMERO DE ITERACIONES")
print("-"*40)

n_iters = [250, 500, 1000, 2000]

fig, axes = plt.subplots(1, len(n_iters), figsize=(16, 4))

for i, n_iter in enumerate(n_iters):
    print(f"  Calculando n_iter={n_iter}...", end=" ")
    start = time.time()
    
    tsne = TSNE(n_components=2, perplexity=30, random_state=42,
                n_iter=n_iter, learning_rate='auto')
    X_tsne = tsne.fit_transform(X_scaled)
    
    elapsed = time.time() - start
    print(f"KL={tsne.kl_divergence_:.4f} ({elapsed:.1f}s)")
    
    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',
                    alpha=0.6, s=10, edgecolors='none')
    axes[i].set_title(f'n_iter={n_iter}\nKL={tsne.kl_divergence_:.3f}')
    axes[i].set_xticks([])
    axes[i].set_yticks([])

plt.suptitle('Efecto del NÃºmero de Iteraciones', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

print("""
InterpretaciÃ³n:
- Muy pocas iteraciones: t-SNE no converge (estructura incompleta)
- 1000 iteraciones suele ser suficiente para la mayorÃ­a de casos
- MÃ¡s iteraciones mejoran hasta un punto, luego estabilizan
""")

# -------------------------------------------------------------
# 4. ESTABILIDAD: MÃšLTIPLES EJECUCIONES
# -------------------------------------------------------------
print("\n[3] ESTABILIDAD DE t-SNE")
print("-"*40)

fig, axes = plt.subplots(1, 4, figsize=(16, 4))

for i in range(4):
    # Diferentes random_state
    tsne = TSNE(n_components=2, perplexity=30, random_state=i*10,
                n_iter=1000, learning_rate='auto')
    X_tsne = tsne.fit_transform(X_scaled)
    
    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',
                    alpha=0.6, s=10, edgecolors='none')
    axes[i].set_title(f'random_state={i*10}')
    axes[i].set_xticks([])
    axes[i].set_yticks([])

plt.suptitle('Diferentes Inicializaciones de t-SNE', fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

print("""
Observaciones sobre estabilidad:
- t-SNE NO es determinÃ­stico (diferente resultado cada vez)
- Los CLUSTERS se preservan, pero su POSICIÃ“N y ORIENTACIÃ“N cambian
- No comparar posiciones entre diferentes ejecuciones
- Usar random_state fijo para reproducibilidad
""")

# -------------------------------------------------------------
# 5. INICIALIZACIÃ“N CON PCA
# -------------------------------------------------------------
print("\n[4] INICIALIZACIÃ“N CON PCA (Recomendado)")
print("-"*40)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Sin inicializaciÃ³n PCA
tsne_random = TSNE(n_components=2, perplexity=30, random_state=42,
                   n_iter=1000, init='random', learning_rate='auto')
X_tsne_random = tsne_random.fit_transform(X_scaled)

# Con inicializaciÃ³n PCA
tsne_pca = TSNE(n_components=2, perplexity=30, random_state=42,
                n_iter=1000, init='pca', learning_rate='auto')
X_tsne_pca = tsne_pca.fit_transform(X_scaled)

axes[0].scatter(X_tsne_random[:, 0], X_tsne_random[:, 1], c=y, 
                cmap='tab10', alpha=0.6, s=15)
axes[0].set_title(f"init='random'\nKL={tsne_random.kl_divergence_:.4f}")
axes[0].set_xticks([])
axes[0].set_yticks([])

axes[1].scatter(X_tsne_pca[:, 0], X_tsne_pca[:, 1], c=y,
                cmap='tab10', alpha=0.6, s=15)
axes[1].set_title(f"init='pca' (recomendado)\nKL={tsne_pca.kl_divergence_:.4f}")
axes[1].set_xticks([])
axes[1].set_yticks([])

plt.suptitle('Efecto de la InicializaciÃ³n', fontsize=14)
plt.tight_layout()
plt.show()

print("""
init='pca' es recomendado porque:
- MÃ¡s reproducible
- Convergencia mÃ¡s rÃ¡pida
- Mejor preservaciÃ³n de la estructura global
""")

# -------------------------------------------------------------
# 6. t-SNE CON DATOS GRANDES (Barnes-Hut)
# -------------------------------------------------------------
print("\n[5] ESCALABILIDAD: Barnes-Hut vs Exact")
print("-"*40)

print("""
Para datasets grandes, usar method='barnes_hut':
- Complejidad: O(nÂ²) â†’ O(n log n)
- AproximaciÃ³n del algoritmo exacto
- Por defecto cuando n_samples > 10000
""")

# Ejemplo con datos mÃ¡s grandes
from sklearn.datasets import make_blobs
X_large, y_large = make_blobs(n_samples=5000, n_features=50, centers=10, random_state=42)
X_large_scaled = StandardScaler().fit_transform(X_large)

# Barnes-Hut (aproximado)
print("  Barnes-Hut (aproximado)...", end=" ")
start = time.time()
tsne_bh = TSNE(n_components=2, perplexity=30, method='barnes_hut', 
               random_state=42, n_iter=1000)
X_bh = tsne_bh.fit_transform(X_large_scaled)
print(f"{time.time()-start:.1f}s")

# Exact (para comparaciÃ³n - serÃ¡ mÃ¡s lento)
print("  Exact...", end=" ")
start = time.time()
tsne_exact = TSNE(n_components=2, perplexity=30, method='exact',
                  random_state=42, n_iter=1000)
X_exact = tsne_exact.fit_transform(X_large_scaled)
print(f"{time.time()-start:.1f}s")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].scatter(X_bh[:, 0], X_bh[:, 1], c=y_large, cmap='tab10', alpha=0.5, s=5)
axes[0].set_title("method='barnes_hut' (RÃ¡pido)")

axes[1].scatter(X_exact[:, 0], X_exact[:, 1], c=y_large, cmap='tab10', alpha=0.5, s=5)
axes[1].set_title("method='exact' (Preciso)")

plt.suptitle('ComparaciÃ³n de MÃ©todos para n=5000', fontsize=14)
plt.tight_layout()
plt.show()

# -------------------------------------------------------------
# 7. INTERPRETACIÃ“N CORRECTA DE t-SNE
# -------------------------------------------------------------
print("\n" + "="*60)
print("CÃ“MO INTERPRETAR (Y NO INTERPRETAR) t-SNE")
print("="*60)

print("""
âœ… LO QUE SÃ PUEDES INTERPRETAR:
   - La existencia de clusters separados
   - Puntos cercanos en t-SNE â†’ similares en alta dimensiÃ³n
   - Estructura general de los datos

âŒ LO QUE NO PUEDES INTERPRETAR:
   - TamaÃ±o de los clusters (distorsionado)
   - Distancia entre clusters (no tiene significado)
   - Densidad de los clusters
   - PosiciÃ³n absoluta (rotaciÃ³n/reflejo arbitrarios)

âš ï¸ ERRORES COMUNES:
   1. "El cluster A es mÃ¡s grande que B" â†’ FALSO
   2. "Los clusters A y B estÃ¡n mÃ¡s cerca que A y C" â†’ PUEDE SER FALSO
   3. "Hay mÃ¡s densidad en esta regiÃ³n" â†’ NO NECESARIAMENTE
   4. Usar t-SNE como preprocesamiento para ML â†’ NO RECOMENDADO
""")

# DemostraciÃ³n del problema de distancias entre clusters
print("\n[DemostraciÃ³n: Distancias entre clusters NO son confiables]")

# Crear datos con distancias conocidas
from sklearn.datasets import make_blobs
centers = [[0, 0], [10, 0], [100, 0]]  # Distancias 10 y 90
X_demo, y_demo = make_blobs(n_samples=300, centers=centers, 
                            cluster_std=1, random_state=42)

# Aplicar t-SNE
tsne_demo = TSNE(n_components=2, perplexity=30, random_state=42)
X_demo_tsne = tsne_demo.fit_transform(X_demo)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Espacio original
axes[0].scatter(X_demo[:, 0], X_demo[:, 1], c=y_demo, cmap='tab10', alpha=0.7)
axes[0].set_title('Espacio Original\nDistancias: A-B=10, B-C=90')
axes[0].set_xlabel('X')
axes[0].set_ylabel('Y')

# Espacio t-SNE
axes[1].scatter(X_demo_tsne[:, 0], X_demo_tsne[:, 1], c=y_demo, cmap='tab10', alpha=0.7)
axes[1].set_title('Espacio t-SNE\nÂ¿Se preservan las distancias relativas?')
axes[1].set_xlabel('t-SNE 1')
axes[1].set_ylabel('t-SNE 2')

plt.tight_layout()
plt.show()

print("""
ConclusiÃ³n: Las distancias relativas entre clusters NO se preservan en t-SNE
El cluster C que estaba 9x mÃ¡s lejos puede aparecer a distancia similar en t-SNE
""")

# -------------------------------------------------------------
# 8. RESUMEN DE MEJORES PRÃCTICAS
# -------------------------------------------------------------
print("\n" + "="*60)
print("MEJORES PRÃCTICAS PARA t-SNE")
print("="*60)

print("""
1. PREPROCESAMIENTO:
   - Siempre estandarizar (StandardScaler)
   - Considerar reducir con PCA primero si dim > 50

2. HIPERPARÃMETROS:
   - perplexity: 5-50, tÃ­picamente 30
   - n_iter: al menos 1000, verificar convergencia (KL divergence)
   - learning_rate: 'auto' o n_samples/12
   - init: 'pca' para mayor reproducibilidad

3. VISUALIZACIÃ“N:
   - No confiar en tamaÃ±os de clusters
   - No confiar en distancias entre clusters
   - Ejecutar varias veces para verificar estabilidad

4. NO USAR PARA:
   - Preprocesamiento de ML
   - Clustering (usar los datos originales)
   - Comparar posiciones entre diferentes ejecuciones
""")

print("\n" + "="*60)
print("ANÃLISIS COMPLETADO")
print("="*60)
```

---

## 6.6. HiperparÃ¡metros de t-SNE en scikit-learn

| ParÃ¡metro | DescripciÃ³n | Valores | RecomendaciÃ³n |
| :--- | :--- | :--- | :--- |
| `n_components` | Dimensiones de salida | 2 o 3 | 2 para visualizaciÃ³n |
| `perplexity` | NÃºmero efectivo de vecinos | 5-50 | 30 es un buen inicio |
| `learning_rate` | Tasa de aprendizaje | 'auto', 10-1000 | 'auto' (n_samples/12) |
| `n_iter` | NÃºmero de iteraciones | int > 0 | 1000 mÃ­nimo |
| `init` | InicializaciÃ³n | 'random', 'pca' | 'pca' para reproducibilidad |
| `method` | Algoritmo | 'barnes_hut', 'exact' | 'barnes_hut' si n > 10000 |
| `metric` | MÃ©trica de distancia | 'euclidean', 'cosine', etc. | 'euclidean' |
| `random_state` | Semilla | int o None | Fijar para reproducibilidad |

---

## 6.7. Aplicaciones Reales

### 1. VisualizaciÃ³n de Word Embeddings

Visualizar relaciones semÃ¡nticas entre palabras (Word2Vec, GloVe).
* [Tutorial: Visualizing Word Embeddings](https://www.kaggle.com/code/jeffd23/visualizing-word-vectors-with-t-sne)

### 2. AnÃ¡lisis de ImÃ¡genes

Explorar similitud entre imÃ¡genes en datasets como MNIST, CIFAR.
* [Ejemplo: t-SNE on MNIST](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)

### 3. BioinformÃ¡tica

Visualizar expresiÃ³n gÃ©nica, scRNA-seq (single-cell RNA sequencing).

### 4. DetecciÃ³n de Fraude

Visualizar transacciones para identificar patrones anÃ³malos.

---

## 6.8. t-SNE vs Otras TÃ©cnicas de VisualizaciÃ³n

| TÃ©cnica | Tipo | Velocidad | Estructura | Mejor para |
| :--- | :--- | :--- | :--- | :--- |
| **PCA** | Lineal | Muy rÃ¡pida | Global | Preprocesamiento, interpretaciÃ³n |
| **t-SNE** | No lineal | Lenta | Local | VisualizaciÃ³n de clusters |
| **UMAP** | No lineal | RÃ¡pida | Local + Global | VisualizaciÃ³n + ML |
| **MDS** | No lineal | Media | Global | Preservar distancias |

### UMAP: La Alternativa Moderna

**UMAP** (Uniform Manifold Approximation and Projection) es una alternativa mÃ¡s reciente a t-SNE:

```python
# pip install umap-learn
import umap

reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)
X_umap = reducer.fit_transform(X_scaled)
```

Ventajas de UMAP sobre t-SNE:
- MÃ¡s rÃ¡pido
- Mejor preservaciÃ³n de estructura global
- Se puede usar para ML (transformar nuevos datos)

---

## 6.9. Resumen y Mejores PrÃ¡cticas

### Checklist para usar t-SNE

- [ ] **Estandarizar los datos**
- [ ] **Reducir dimensionalidad primero con PCA** si dim > 50
- [ ] **Empezar con perplexity=30** y ajustar
- [ ] **Usar n_iter >= 1000** y verificar convergencia
- [ ] **Usar init='pca'** para reproducibilidad
- [ ] **Ejecutar mÃºltiples veces** para verificar estabilidad
- [ ] **NO interpretar tamaÃ±os ni distancias entre clusters**

### Â¿CuÃ¡ndo usar t-SNE?

âœ… **Usar t-SNE cuando:**
- Quieres visualizar datos de alta dimensiÃ³n
- Buscas identificar clusters visualmente
- El dataset es de tamaÃ±o moderado (< 50K puntos)
- Solo necesitas visualizaciÃ³n (no ML downstream)

âŒ **Considerar alternativas cuando:**
- Necesitas velocidad con datos grandes â†’ UMAP
- Quieres preservar distancias globales â†’ PCA, MDS
- Necesitas transformar nuevos datos â†’ UMAP, PCA
- Quieres interpretabilidad â†’ PCA

---

ğŸ“… **Fecha de creaciÃ³n:** Enero 2026  
âœï¸ **Autor:** Fran GarcÃ­a
