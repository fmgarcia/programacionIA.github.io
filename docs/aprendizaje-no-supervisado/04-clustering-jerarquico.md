# üå≤ Unidad 4. Clustering Jer√°rquico

El **Clustering Jer√°rquico** es una familia de algoritmos que construyen una **jerarqu√≠a de clusters** en lugar de una partici√≥n plana. Su caracter√≠stica distintiva es que produce un **dendrograma**: una estructura de √°rbol que muestra c√≥mo se forman o dividen los clusters a diferentes niveles de similitud. Este enfoque permite explorar la estructura de los datos a m√∫ltiples escalas sin necesidad de especificar el n√∫mero de clusters a priori.

![Ilustraci√≥n de hierarchical](../assets/images/hierarchical.svg)
---

## 4.1. ¬øC√≥mo Funciona el Clustering Jer√°rquico?

### Dos Enfoques Principales

Existen dos estrategias opuestas para construir la jerarqu√≠a:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIPOS DE CLUSTERING JER√ÅRQUICO                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ 1. AGLOMERATIVO (Bottom-Up) - El m√°s com√∫n                  ‚îÇ
‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÇ
‚îÇ    ‚Ä¢ Empieza: cada punto es su propio cluster               ‚îÇ
‚îÇ    ‚Ä¢ Proceso: fusiona los dos clusters m√°s cercanos         ‚îÇ
‚îÇ    ‚Ä¢ Termina: todos los puntos en un √∫nico cluster          ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ         ‚óã ‚óã ‚óã ‚óã ‚óã   ‚Üí   ‚óã‚óã ‚óã ‚óã‚óã   ‚Üí   ‚óã‚óã‚óã ‚óã‚óã   ‚Üí   ‚óã‚óã‚óã‚óã‚óã    ‚îÇ
‚îÇ         5 clusters     4 clusters    2 clusters   1 cluster ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 2. DIVISIVO (Top-Down) - Menos com√∫n                        ‚îÇ
‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚îÇ
‚îÇ    ‚Ä¢ Empieza: todos los puntos en un √∫nico cluster          ‚îÇ
‚îÇ    ‚Ä¢ Proceso: divide el cluster menos coherente             ‚îÇ
‚îÇ    ‚Ä¢ Termina: cada punto es su propio cluster               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ         ‚óã‚óã‚óã‚óã‚óã   ‚Üí   ‚óã‚óã‚óã ‚óã‚óã   ‚Üí   ‚óã‚óã ‚óã ‚óã‚óã   ‚Üí   ‚óã ‚óã ‚óã ‚óã ‚óã    ‚îÇ
‚îÇ         1 cluster   2 clusters   4 clusters   5 clusters    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Algoritmo Aglomerativo Paso a Paso

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ALGORITMO AGLOMERATIVO (AGNES)                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Entrada: Datos X con n puntos, criterio de enlace           ‚îÇ
‚îÇ Salida: Dendrograma (√°rbol de fusiones)                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 1. INICIALIZACI√ìN:                                          ‚îÇ
‚îÇ    - Crear n clusters (uno por cada punto)                  ‚îÇ
‚îÇ    - Calcular matriz de distancias entre todos los pares    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 2. REPETIR n-1 veces:                                       ‚îÇ
‚îÇ    a) Encontrar los dos clusters m√°s cercanos               ‚îÇ
‚îÇ    b) Fusionarlos en un nuevo cluster                       ‚îÇ
‚îÇ    c) Actualizar la matriz de distancias                    ‚îÇ
‚îÇ    d) Registrar la fusi√≥n y su altura en el dendrograma     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ 3. RESULTADO:                                               ‚îÇ
‚îÇ    - Dendrograma completo                                   ‚îÇ
‚îÇ    - Para obtener k clusters: "cortar" el dendrograma       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### El Dendrograma

El dendrograma es la visualizaci√≥n clave del clustering jer√°rquico:

```
Altura
(distancia)
   ‚îÇ
   ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
   ‚îÇ     ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ  ‚îå‚îÄ‚îÄ‚î§        ‚îÇ
   ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ‚îÄ‚îÄ‚î§      ‚îÇ
   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            A  B  C  D  E  (puntos)
```

- **Eje vertical:** altura de fusi√≥n (distancia entre clusters fusionados)
- **Eje horizontal:** puntos o clusters individuales
- **L√≠neas horizontales:** fusiones entre clusters
- **Cortar horizontalmente:** obtener un n√∫mero espec√≠fico de clusters

---

## 4.2. Explicaci√≥n Matem√°tica

### Matriz de Distancias

El clustering jer√°rquico comienza calculando una **matriz de distancias** $D$ de tama√±o $n \times n$:

$$D_{ij} = d(x_i, x_j)$$

Donde $d$ es una funci√≥n de distancia (t√≠picamente Euclidiana).

### Criterios de Enlace (Linkage)

La clave del clustering jer√°rquico es c√≥mo se calcula la distancia entre clusters. Existen varios **criterios de enlace**:

#### 1. Enlace Simple (Single Linkage) - "Vecino m√°s cercano"

Distancia entre los dos puntos m√°s cercanos de cada cluster:

$$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$$

- ‚úÖ Puede detectar clusters de formas arbitrarias
- ‚ùå Sensible al "efecto cadena" (clusters elongados)

#### 2. Enlace Completo (Complete Linkage) - "Vecino m√°s lejano"

Distancia entre los dos puntos m√°s lejanos de cada cluster:

$$d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)$$

- ‚úÖ Produce clusters compactos de tama√±o similar
- ‚ùå Sensible a outliers

#### 3. Enlace Promedio (Average Linkage - UPGMA)

Promedio de todas las distancias entre pares de puntos:

$$d(C_i, C_j) = \frac{1}{|C_i| \cdot |C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y)$$

- ‚úÖ Balance entre single y complete
- ‚úÖ Menos sensible a outliers

#### 4. Enlace de Ward (Ward's Method)

Minimiza el incremento en la varianza total al fusionar clusters:

$$d(C_i, C_j) = \sqrt{\frac{2|C_i||C_j|}{|C_i|+|C_j|}} ||\mu_i - \mu_j||$$

Donde $\mu_i$ y $\mu_j$ son los centroides de los clusters.

Equivalentemente, Ward minimiza:

$$\Delta = \sum_{x \in C_i \cup C_j} ||x - \mu_{ij}||^2 - \sum_{x \in C_i} ||x - \mu_i||^2 - \sum_{x \in C_j} ||x - \mu_j||^2$$

- ‚úÖ Tiende a producir clusters esf√©ricos y de tama√±o similar
- ‚úÖ Similar a K-Means pero jer√°rquico
- ‚ùå Asume clusters esf√©ricos

### Visualizaci√≥n de Criterios de Enlace

```
          Cluster A              Cluster B
         
           ‚óè  ‚óè                    ‚ñ≤  ‚ñ≤
         ‚óè      ‚óè                ‚ñ≤      ‚ñ≤
           ‚óè  ‚óè                    ‚ñ≤  ‚ñ≤

Single:   d = distancia m√≠nima (m√°s corta)
Complete: d = distancia m√°xima (m√°s larga)
Average:  d = promedio de todas las distancias
Ward:     d = incremento m√≠nimo en varianza
```

---

## 4.3. Pros y Contras

| Ventajas | Desventajas |
| :--- | :--- |
| **No requiere k:** El n√∫mero de clusters se elige despu√©s | **Complejidad alta:** $O(n^3)$ tiempo, $O(n^2)$ espacio |
| **Dendrograma informativo:** Permite explorar estructura a m√∫ltiples niveles | **No escalable:** Impracticable para datasets grandes (>10K puntos) |
| **Flexibilidad:** Diferentes criterios de enlace para diferentes necesidades | **Irreversible:** Una fusi√≥n mala no puede deshacerse |
| **Determin√≠stico:** Mismo resultado cada ejecuci√≥n | **Sensible a outliers:** Especialmente con complete linkage |
| **Interpretable:** El dendrograma es f√°cil de entender | **Sensible a la elecci√≥n de linkage:** Resultados muy diferentes |

---

## 4.4. Ejemplo B√°sico en Python

Este ejemplo muestra el uso b√°sico del clustering jer√°rquico con visualizaci√≥n del dendrograma.

```python
# ============================================================
# EJEMPLO B√ÅSICO: Clustering Jer√°rquico con Dendrograma
# ============================================================

# Importar bibliotecas necesarias
import numpy as np                          # Operaciones num√©ricas
import matplotlib.pyplot as plt             # Visualizaci√≥n
from scipy.cluster.hierarchy import (       # Funciones de scipy
    linkage,        # Calcular el enlace jer√°rquico
    dendrogram,     # Crear el dendrograma
    fcluster        # Obtener clusters del dendrograma
)
from sklearn.datasets import make_blobs     # Datos sint√©ticos
from sklearn.preprocessing import StandardScaler  # Escalado

# -------------------------------------------------------------
# 1. GENERAR DATOS DE EJEMPLO
# -------------------------------------------------------------
# Crear 4 clusters bien definidos
X, y_true = make_blobs(
    n_samples=50,       # 50 puntos (peque√±o para visualizaci√≥n clara)
    centers=4,          # 4 clusters
    cluster_std=0.60,   # Dispersi√≥n moderada
    random_state=42
)

print(f"Forma de los datos: {X.shape}")

# -------------------------------------------------------------
# 2. CALCULAR LA MATRIZ DE ENLACE
# -------------------------------------------------------------
# linkage() calcula el clustering jer√°rquico
# Devuelve una matriz Z de (n-1) x 4:
# - Columnas 0 y 1: √≠ndices de clusters fusionados
# - Columna 2: distancia entre ellos (altura del enlace)
# - Columna 3: n√∫mero de puntos en el nuevo cluster

Z = linkage(
    X,                  # Datos
    method='ward',      # Criterio de enlace
    metric='euclidean'  # M√©trica de distancia
)

print(f"\nMatriz de enlace Z (primeras 5 filas):")
print(f"[cluster_1, cluster_2, distancia, n_puntos]")
print(Z[:5])

# -------------------------------------------------------------
# 3. VISUALIZAR EL DENDROGRAMA
# -------------------------------------------------------------
plt.figure(figsize=(14, 8))

# dendrogram() crea la visualizaci√≥n del √°rbol jer√°rquico
# truncate_mode='level' limita el n√∫mero de niveles mostrados
dn = dendrogram(
    Z,
    truncate_mode='lastp',  # Mostrar los √∫ltimos p clusters fusionados
    p=20,                    # N√∫mero de clusters a mostrar
    leaf_rotation=90,        # Rotar etiquetas de hojas
    leaf_font_size=10,       # Tama√±o de fuente
    show_contracted=True     # Mostrar clusters contra√≠dos
)

plt.xlabel('Punto o Cluster', fontsize=12)
plt.ylabel('Distancia (Altura)', fontsize=12)
plt.title('Dendrograma - Clustering Jer√°rquico (Ward Linkage)', fontsize=14)

# A√±adir l√≠nea horizontal para "cortar" el dendrograma
# Esta l√≠nea indica d√≥nde cortar√≠amos para obtener cierto n√∫mero de clusters
corte = 7  # Altura de corte
plt.axhline(y=corte, color='r', linestyle='--', 
            label=f'Corte en altura={corte}')
plt.legend()
plt.tight_layout()
plt.show()

# -------------------------------------------------------------
# 4. OBTENER ASIGNACIONES DE CLUSTER
# -------------------------------------------------------------
# fcluster() "corta" el dendrograma para obtener clusters
# t: umbral de corte
# criterion: c√≥mo interpretar t

# Opci√≥n 1: Cortar por distancia (altura)
labels_dist = fcluster(Z, t=corte, criterion='distance')
print(f"\nClusters (corte por distancia={corte}): {np.unique(labels_dist)}")
print(f"Distribuci√≥n: {np.bincount(labels_dist)[1:]}")  # [1:] porque fcluster empieza en 1

# Opci√≥n 2: Especificar n√∫mero de clusters directamente
labels_k = fcluster(Z, t=4, criterion='maxclust')
print(f"\nClusters (k=4): {np.unique(labels_k)}")
print(f"Distribuci√≥n: {np.bincount(labels_k)[1:]}")

# -------------------------------------------------------------
# 5. VISUALIZAR CLUSTERS RESULTANTES
# -------------------------------------------------------------
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Clusters por corte de distancia
scatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=labels_dist,
                           cmap='viridis', edgecolors='w', s=50)
axes[0].set_title(f'Clusters (corte altura={corte})')
axes[0].set_xlabel('Feature 1')
axes[0].set_ylabel('Feature 2')
plt.colorbar(scatter1, ax=axes[0], label='Cluster')

# Clusters especificando k=4
scatter2 = axes[1].scatter(X[:, 0], X[:, 1], c=labels_k,
                           cmap='viridis', edgecolors='w', s=50)
axes[1].set_title('Clusters (k=4 especificado)')
axes[1].set_xlabel('Feature 1')
axes[1].set_ylabel('Feature 2')
plt.colorbar(scatter2, ax=axes[1], label='Cluster')

plt.tight_layout()
plt.show()

print("\n‚úÖ El dendrograma permite explorar la estructura a diferentes niveles")
print("‚úÖ Cortando a diferentes alturas obtenemos diferentes n√∫meros de clusters")
```

---

## 4.5. Ejemplo Avanzado: Comparaci√≥n de Criterios de Enlace

Este ejemplo compara diferentes criterios de enlace y muestra c√≥mo elegir el n√∫mero √≥ptimo de clusters.

```python
# ============================================================
# EJEMPLO AVANZADO: An√°lisis Completo de Clustering Jer√°rquico
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet
from scipy.spatial.distance import pdist
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.datasets import load_iris

# -------------------------------------------------------------
# 1. CARGAR Y PREPARAR DATOS
# -------------------------------------------------------------
iris = load_iris()
X = iris.data
y_true = iris.target
feature_names = iris.feature_names

print("="*60)
print("AN√ÅLISIS DE CLUSTERING JER√ÅRQUICO - DATASET IRIS")
print("="*60)
print(f"\nDimensiones: {X.shape}")
print(f"Features: {feature_names}")

# Estandarizar datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------------------------------------
# 2. COMPARAR DIFERENTES CRITERIOS DE ENLACE
# -------------------------------------------------------------
print("\n" + "="*60)
print("COMPARACI√ìN DE CRITERIOS DE ENLACE")
print("="*60)

# M√©todos de enlace a comparar
methods = ['single', 'complete', 'average', 'ward']
method_names = {
    'single': 'Single (Vecino m√°s cercano)',
    'complete': 'Complete (Vecino m√°s lejano)',
    'average': 'Average (Promedio)',
    'ward': 'Ward (Minimiza varianza)'
}

# Crear figura para dendrogramas
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()

# Almacenar resultados
linkage_results = {}

for idx, method in enumerate(methods):
    # Calcular enlace
    Z = linkage(X_scaled, method=method)
    linkage_results[method] = Z
    
    # Calcular correlaci√≥n cophenetica
    # Mide qu√© tan bien el dendrograma preserva las distancias originales
    c, _ = cophenet(Z, pdist(X_scaled))
    
    # Dibujar dendrograma
    ax = axes[idx]
    dendrogram(Z, ax=ax, truncate_mode='lastp', p=30,
               leaf_rotation=90, leaf_font_size=8,
               show_contracted=True)
    ax.set_title(f'{method_names[method]}\nCorrelaci√≥n Cophenetica: {c:.3f}')
    ax.set_xlabel('Muestra')
    ax.set_ylabel('Distancia')

plt.tight_layout()
plt.show()

# Mostrar correlaciones copheneticas
print("\nCorrelaci√≥n Cophenetica por m√©todo:")
print("(Mayor = mejor preservaci√≥n de distancias originales)")
for method in methods:
    c, _ = cophenet(linkage_results[method], pdist(X_scaled))
    print(f"  {method_names[method]}: {c:.4f}")

# -------------------------------------------------------------
# 3. AN√ÅLISIS DEL DENDROGRAMA (Ward)
# -------------------------------------------------------------
print("\n" + "="*60)
print("AN√ÅLISIS DETALLADO - M√âTODO WARD")
print("="*60)

Z_ward = linkage_results['ward']

# Analizar las √∫ltimas fusiones (m√°s informativas)
print("\n√öltimas 10 fusiones:")
print("Fusi√≥n | Cluster1 | Cluster2 | Distancia | Tama√±o")
print("-"*55)
for i in range(-10, 0):
    row = Z_ward[i]
    print(f"{len(Z_ward)+i+1:6} | {int(row[0]):8} | {int(row[1]):8} | {row[2]:9.3f} | {int(row[3]):6}")

# -------------------------------------------------------------
# 4. DETERMINAR N√öMERO √ìPTIMO DE CLUSTERS
# -------------------------------------------------------------
print("\n" + "="*60)
print("DETERMINACI√ìN DEL N√öMERO √ìPTIMO DE CLUSTERS")
print("="*60)

# M√©todo 1: An√°lisis de las distancias de fusi√≥n
# Buscar "saltos" grandes en las distancias
heights = Z_ward[:, 2]
height_diffs = np.diff(heights)

# Las √∫ltimas fusiones (las m√°s significativas)
print("\nSaltos de distancia en √∫ltimas fusiones:")
for i in range(-5, 0):
    n_clusters = len(Z_ward) - len(Z_ward) - i
    print(f"  {n_clusters} ‚Üí {n_clusters-1} clusters: salto = {height_diffs[i]:.3f}")

# M√©todo 2: Usar m√©tricas de evaluaci√≥n
k_range = range(2, 11)
metrics = {'k': [], 'silhouette': [], 'calinski': []}

for k in k_range:
    labels = fcluster(Z_ward, t=k, criterion='maxclust')
    
    metrics['k'].append(k)
    metrics['silhouette'].append(silhouette_score(X_scaled, labels))
    metrics['calinski'].append(calinski_harabasz_score(X_scaled, labels))

df_metrics = pd.DataFrame(metrics)
print("\nM√©tricas por n√∫mero de clusters:")
print(df_metrics.round(4).to_string(index=False))

# Visualizar m√©tricas
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Dendrograma con l√≠neas de corte
ax1 = axes[0]
dendrogram(Z_ward, ax=ax1, truncate_mode='lastp', p=20)
ax1.axhline(y=8, color='r', linestyle='--', label='k=3')
ax1.axhline(y=5, color='g', linestyle='--', label='k=5')
ax1.set_title('Dendrograma Ward')
ax1.legend()

# Silhouette
ax2 = axes[1]
ax2.plot(df_metrics['k'], df_metrics['silhouette'], 'bo-')
ax2.set_xlabel('N√∫mero de clusters')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silueta vs k')
ax2.grid(True, alpha=0.3)

# Calinski-Harabasz
ax3 = axes[2]
ax3.plot(df_metrics['k'], df_metrics['calinski'], 'go-')
ax3.set_xlabel('N√∫mero de clusters')
ax3.set_ylabel('Calinski-Harabasz')
ax3.set_title('Calinski-Harabasz vs k')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# -------------------------------------------------------------
# 5. MODELO FINAL CON SKLEARN
# -------------------------------------------------------------
print("\n" + "="*60)
print("MODELO FINAL CON AgglomerativeClustering")
print("="*60)

k_optimo = 3  # Basado en an√°lisis

# Usando sklearn.cluster.AgglomerativeClustering
# M√°s flexible y permite especificar k directamente
model = AgglomerativeClustering(
    n_clusters=k_optimo,        # N√∫mero de clusters
    metric='euclidean',         # M√©trica de distancia
    linkage='ward',             # Criterio de enlace
    # Par√°metros adicionales:
    # compute_full_tree: bool, calcular √°rbol completo aunque n_clusters est√© especificado
    # distance_threshold: None o float, distancia para el corte (si se usa, n_clusters debe ser None)
)

# Entrenar y obtener etiquetas
labels = model.fit_predict(X_scaled)

print(f"\nResultados:")
print(f"  N√∫mero de clusters: {model.n_clusters_}")
print(f"  N√∫mero de hojas: {model.n_leaves_}")
print(f"  N√∫mero de componentes conectados: {model.n_connected_components_}")

# Distribuci√≥n
print(f"\nDistribuci√≥n de puntos:")
for i in range(k_optimo):
    count = np.sum(labels == i)
    print(f"  Cluster {i}: {count} puntos ({count/len(labels)*100:.1f}%)")

# M√©tricas
print(f"\nM√©tricas de evaluaci√≥n:")
print(f"  Silhouette Score: {silhouette_score(X_scaled, labels):.4f}")
print(f"  Calinski-Harabasz: {calinski_harabasz_score(X_scaled, labels):.4f}")

# -------------------------------------------------------------
# 6. COMPARACI√ìN CON GROUND TRUTH
# -------------------------------------------------------------
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

print(f"\n" + "="*60)
print("COMPARACI√ìN CON ETIQUETAS REALES")
print("="*60)

ari = adjusted_rand_score(y_true, labels)
nmi = normalized_mutual_info_score(y_true, labels)

print(f"\n  Adjusted Rand Index: {ari:.4f}")
print(f"  Normalized Mutual Info: {nmi:.4f}")

# Matriz de contingencia
print("\nMatriz de Contingencia:")
contingency = pd.crosstab(
    pd.Series(labels, name='Cluster'),
    pd.Series(y_true, name='Especie'),
    margins=True
)
contingency.columns = list(iris.target_names) + ['Total']
print(contingency)

# -------------------------------------------------------------
# 7. PERFIL DE CLUSTERS
# -------------------------------------------------------------
print(f"\n" + "="*60)
print("PERFIL DE CADA CLUSTER")
print("="*60)

df_result = pd.DataFrame(X, columns=feature_names)
df_result['cluster'] = labels

print("\nMedia por cluster (valores originales):")
print(df_result.groupby('cluster').mean().round(2))

# -------------------------------------------------------------
# 8. VISUALIZACI√ìN FINAL
# -------------------------------------------------------------
from sklearn.decomposition import PCA

# Reducir a 2D para visualizaci√≥n
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Clusters jer√°rquicos
scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels,
                           cmap='viridis', edgecolors='w', s=50)
axes[0].set_title('Clustering Jer√°rquico (Ward)')
axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
plt.colorbar(scatter1, ax=axes[0])

# Ground truth
scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_true,
                           cmap='viridis', edgecolors='w', s=50)
axes[1].set_title('Especies Reales')
axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
plt.colorbar(scatter2, ax=axes[1])

# Comparaci√≥n de m√©todos de enlace (para k=3)
labels_by_method = {}
for method in methods:
    labels_by_method[method] = fcluster(linkage_results[method], 
                                         t=3, criterion='maxclust')

# Mostrar silueta por m√©todo
silhouettes = [silhouette_score(X_scaled, labels_by_method[m]) for m in methods]
bars = axes[2].bar(methods, silhouettes, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
axes[2].set_ylabel('Silhouette Score')
axes[2].set_title('Silueta por Criterio de Enlace (k=3)')
axes[2].set_ylim([0, max(silhouettes) * 1.2])

# A√±adir valores sobre las barras
for bar, val in zip(bars, silhouettes):
    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                 f'{val:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# -------------------------------------------------------------
# 9. EJEMPLO CON DISTANCE_THRESHOLD
# -------------------------------------------------------------
print("\n" + "="*60)
print("ALTERNATIVA: Clustering por Umbral de Distancia")
print("="*60)

# En lugar de especificar k, especificar distancia de corte
model_dist = AgglomerativeClustering(
    n_clusters=None,            # No especificar k
    distance_threshold=10,      # Umbral de distancia
    metric='euclidean',
    linkage='ward'
)

labels_dist = model_dist.fit_predict(X_scaled)
n_clusters_found = len(np.unique(labels_dist))

print(f"\nCon distance_threshold=10:")
print(f"  Clusters encontrados: {n_clusters_found}")
print(f"  Distribuci√≥n: {np.bincount(labels_dist)}")

print("\n" + "="*60)
print("AN√ÅLISIS COMPLETADO")
print("="*60)
```

---

## 4.6. Hiperpar√°metros en scikit-learn

### scipy.cluster.hierarchy.linkage

| Par√°metro | Descripci√≥n | Valores |
| :--- | :--- | :--- |
| `y` | Datos o matriz de distancias | array (n, d) o condensada |
| `method` | Criterio de enlace | 'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward' |
| `metric` | M√©trica de distancia | 'euclidean', 'cityblock', 'cosine', etc. |
| `optimal_ordering` | Reordenar hojas para minimizar distancias | True/False |

### sklearn.cluster.AgglomerativeClustering

| Par√°metro | Descripci√≥n | Valores |
| :--- | :--- | :--- |
| `n_clusters` | N√∫mero de clusters (None si se usa distance_threshold) | int o None |
| `distance_threshold` | Umbral de distancia para corte | float o None |
| `metric` | M√©trica de distancia | 'euclidean', 'manhattan', 'cosine', etc. |
| `linkage` | Criterio de enlace | 'ward', 'complete', 'average', 'single' |
| `compute_full_tree` | Calcular √°rbol completo | 'auto', True, False |
| `compute_distances` | Calcular distancias entre clusters | True/False |

---

## 4.7. Criterio de Enlace: ¬øCu√°l Elegir?

| Criterio | Forma de Clusters | Sensibilidad a Outliers | Cu√°ndo Usar |
| :--- | :--- | :--- | :--- |
| **Single** | Arbitraria, elongada | Baja | Detectar clusters de forma irregular |
| **Complete** | Compacta, esf√©rica | Alta | Clusters de tama√±o similar |
| **Average** | Moderada | Media | Balance general |
| **Ward** | Compacta, esf√©rica | Media | Similar a K-Means, varianza m√≠nima |

### Visualizaci√≥n del Efecto del Enlace

```python
# Datos con forma irregular (dos lunas)
from sklearn.datasets import make_moons
X_moons, _ = make_moons(n_samples=200, noise=0.05)

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
methods = ['single', 'complete', 'average', 'ward']

for ax, method in zip(axes.flatten(), methods):
    model = AgglomerativeClustering(n_clusters=2, linkage=method)
    labels = model.fit_predict(X_moons)
    ax.scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis')
    ax.set_title(f'{method.capitalize()} Linkage')

plt.tight_layout()
plt.show()
# Single linkage funcionar√° mejor con estos datos irregulares
```

---

## 4.8. Aplicaciones Reales

### 1. An√°lisis Filogen√©tico (Biolog√≠a)

Construir √°rboles evolutivos basados en similitud gen√©tica.
* **Tutorial:** [Phylogenetic Trees with Hierarchical Clustering](https://www.kaggle.com/code/andradaolteanu/hierarchical-clustering-phylogenetic-trees)

### 2. Segmentaci√≥n de Documentos

Agrupar documentos similares para organizaci√≥n autom√°tica.

### 3. An√°lisis de Expresi√≥n G√©nica

Identificar grupos de genes con patrones de expresi√≥n similares.
* **Ejemplo:** [Gene Expression Clustering](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html)

### 4. Segmentaci√≥n de Mercado

El dendrograma permite identificar segmentos a diferentes niveles de granularidad.

---

## 4.9. Comparaci√≥n con Otros M√©todos

| Aspecto | Jer√°rquico | K-Means | DBSCAN |
| :--- | :--- | :--- | :--- |
| Requiere k | No (a priori) | S√≠ | No |
| Escalabilidad | Baja ($O(n^3)$) | Alta ($O(nk)$) | Media ($O(n^2)$) |
| Formas de cluster | Depende del enlace | Esf√©ricas | Arbitrarias |
| Outliers | No los detecta | No los detecta | S√≠ los detecta |
| Interpretabilidad | Alta (dendrograma) | Alta (centroides) | Moderada |
| Exploraci√≥n multinivel | S√≠ | No | No |

---

## 4.10. Resumen y Mejores Pr√°cticas

### Checklist para Clustering Jer√°rquico

- [ ] **Escalar los datos** (especialmente para Ward)
- [ ] **Elegir criterio de enlace** apropiado para la forma esperada de clusters
- [ ] **Calcular correlaci√≥n cophenetica** para validar el dendrograma
- [ ] **Analizar el dendrograma** visualmente antes de cortar
- [ ] **Probar varios puntos de corte** y evaluar con m√©tricas
- [ ] **Comparar con otros m√©todos** (K-Means, DBSCAN)

### ¬øCu√°ndo Elegir Clustering Jer√°rquico?

‚úÖ **Usar Jer√°rquico cuando:**
- Dataset peque√±o-mediano (< 10K puntos)
- Quieres explorar la estructura a m√∫ltiples niveles
- El dendrograma es informativo para el dominio
- No sabes cu√°ntos clusters hay

‚ùå **Considerar alternativas cuando:**
- Dataset grande ‚Üí K-Means o Mini-Batch K-Means
- Necesitas identificar outliers ‚Üí DBSCAN
- Clusters de formas muy irregulares ‚Üí DBSCAN + Single linkage
- Eficiencia computacional es cr√≠tica

---

üìÖ **Fecha de creaci√≥n:** Enero 2026  
‚úçÔ∏è **Autor:** Fran Garc√≠a
