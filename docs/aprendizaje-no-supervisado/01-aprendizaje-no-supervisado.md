# ğŸ” Unidad 1. Fundamentos del Aprendizaje No Supervisado

Esta unidad introduce los conceptos fundamentales del **Aprendizaje No Supervisado**, sus diferencias con el aprendizaje supervisado, las bibliotecas Python necesarias, el flujo de trabajo tÃ­pico, y las metodologÃ­as esenciales para preparar datos y evaluar resultados en ausencia de etiquetas.


![IlustraciÃ³n de unsup overview](../assets/images/unsup_overview.svg)
---

## 1.1. Â¿QuÃ© es el Aprendizaje No Supervisado?

El **Aprendizaje No Supervisado** es una rama del Machine Learning donde los algoritmos trabajan con datos **sin etiquetas**. A diferencia del aprendizaje supervisado, no existe una "respuesta correcta" predefinida que guÃ­e el entrenamiento.

### Definiciones Clave

* **DefiniciÃ³n formal:** "El aprendizaje no supervisado es el entrenamiento de un modelo usando informaciÃ³n que no estÃ¡ clasificada ni etiquetada, permitiendo al algoritmo actuar sobre esa informaciÃ³n sin guÃ­a."

* **Objetivo principal:** Descubrir **estructuras ocultas**, **patrones** o **agrupaciones** inherentes en los datos que no son evidentes a simple vista.

* **AnalogÃ­a:** Imagina que te dan una caja con miles de fotografÃ­as sin ninguna descripciÃ³n. El aprendizaje no supervisado serÃ­a como organizarlas automÃ¡ticamente en grupos (paisajes, retratos, animales, etc.) basÃ¡ndose Ãºnicamente en las similitudes visuales entre ellas.

### Diferencias con el Aprendizaje Supervisado

| Aspecto | Supervisado | No Supervisado |
| :--- | :--- | :--- |
| **Datos** | Etiquetados (X, y) | Sin etiquetas (solo X) |
| **Objetivo** | Predecir una variable objetivo | Descubrir estructura en los datos |
| **EvaluaciÃ³n** | MÃ©tricas claras (accuracy, F1, MSE) | MÃ©tricas indirectas (silueta, inercia) |
| **Ejemplos** | ClasificaciÃ³n, RegresiÃ³n | Clustering, ReducciÃ³n de dimensionalidad |
| **Feedback** | Conocemos si la predicciÃ³n es correcta | No hay "respuesta correcta" |

### Tipos de Problemas No Supervisados

El aprendizaje no supervisado abarca principalmente cuatro tipos de problemas:

1. **Clustering (Agrupamiento):**
   * **Objetivo:** Dividir los datos en grupos (clusters) donde los elementos dentro de un grupo son similares entre sÃ­ y diferentes a los de otros grupos.
   * **Algoritmos:** K-Means, DBSCAN, Clustering JerÃ¡rquico, OPTICS, Mean Shift.
   * **AplicaciÃ³n:** SegmentaciÃ³n de clientes, agrupaciÃ³n de documentos.

2. **ReducciÃ³n de Dimensionalidad:**
   * **Objetivo:** Reducir el nÃºmero de variables (features) manteniendo la mayor cantidad de informaciÃ³n posible.
   * **Algoritmos:** PCA, t-SNE, UMAP, LDA, Autoencoders.
   * **AplicaciÃ³n:** VisualizaciÃ³n de datos, compresiÃ³n, preprocesamiento.

3. **DetecciÃ³n de AnomalÃ­as:**
   * **Objetivo:** Identificar puntos de datos que se desvÃ­an significativamente del comportamiento normal.
   * **Algoritmos:** Isolation Forest, One-Class SVM, LOF (Local Outlier Factor).
   * **AplicaciÃ³n:** DetecciÃ³n de fraudes, mantenimiento predictivo.

4. **Reglas de AsociaciÃ³n:**
   * **Objetivo:** Descubrir relaciones interesantes entre variables en grandes conjuntos de datos.
   * **Algoritmos:** Apriori, FP-Growth, Eclat.
   * **AplicaciÃ³n:** AnÃ¡lisis de cesta de compra, sistemas de recomendaciÃ³n.

---

## 1.2. Flujo de Trabajo del Aprendizaje No Supervisado

El proceso general para aplicar tÃ©cnicas no supervisadas sigue estos pasos:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. DefiniciÃ³n  â”‚â”€â”€â”€â–¶â”‚  2. PreparaciÃ³n â”‚â”€â”€â”€â–¶â”‚  3. SelecciÃ³n   â”‚
â”‚  del Problema   â”‚    â”‚    de Datos     â”‚    â”‚  del Algoritmo  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. AplicaciÃ³n   â”‚â—€â”€â”€â”€â”‚ 5. ValidaciÃ³n   â”‚â—€â”€â”€â”€â”‚ 4. Entrenamientoâ”‚
â”‚ e InterpretaciÃ³nâ”‚    â”‚   y EvaluaciÃ³n  â”‚    â”‚    del Modelo   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. DefiniciÃ³n del Problema
* Â¿QuÃ© queremos descubrir? Â¿Grupos de clientes? Â¿Patrones anÃ³malos?
* Â¿CuÃ¡ntas dimensiones tienen los datos? Â¿Son visualizables?
* Â¿Hay conocimiento del dominio que pueda guiar la interpretaciÃ³n?

### 2. PreparaciÃ³n de Datos
* **Limpieza:** Manejo de valores faltantes y outliers.
* **Escalado:** Crucial para algoritmos basados en distancias (K-Means, DBSCAN).
* **SelecciÃ³n de caracterÃ­sticas:** Eliminar features irrelevantes o redundantes.

### 3. SelecciÃ³n del Algoritmo
* Depende del tipo de problema y las caracterÃ­sticas de los datos.
* Considerar: tamaÃ±o del dataset, nÃºmero de clusters esperado, forma de los clusters.

### 4. Entrenamiento del Modelo
* No hay etiquetas, por lo que no hay conjunto de "validaciÃ³n" tradicional.
* Se ajustan hiperparÃ¡metros mediante tÃ©cnicas especÃ­ficas (mÃ©todo del codo, silueta).

### 5. ValidaciÃ³n y EvaluaciÃ³n
* MÃ©tricas internas: Silueta, Inercia, Davies-Bouldin.
* ValidaciÃ³n visual: GrÃ¡ficos de clusters, dendrogramas.
* ValidaciÃ³n externa (si hay etiquetas disponibles): NMI, ARI.

### 6. AplicaciÃ³n e InterpretaciÃ³n
* Asignar significado a los clusters descubiertos.
* Integrar resultados en procesos de negocio o anÃ¡lisis posteriores.

---

## 1.3. Bibliotecas Python para Aprendizaje No Supervisado

### InstalaciÃ³n de Bibliotecas Esenciales

```bash
# InstalaciÃ³n con pip
pip install numpy pandas matplotlib seaborn scikit-learn

# Bibliotecas adicionales especÃ­ficas
pip install mlxtend          # Para reglas de asociaciÃ³n (Apriori)
pip install umap-learn       # Para UMAP (reducciÃ³n de dimensionalidad)
pip install hdbscan          # Para HDBSCAN (clustering avanzado)
pip install yellowbrick      # Para visualizaciÃ³n de ML
```

### Bibliotecas Principales y sus MÃ³dulos

| Biblioteca | MÃ³dulo | Funcionalidad | Algoritmos/Funciones |
| :--- | :--- | :--- | :--- |
| `scikit-learn` | `sklearn.cluster` | Clustering | `KMeans`, `DBSCAN`, `AgglomerativeClustering` |
| `scikit-learn` | `sklearn.decomposition` | ReducciÃ³n de dimensionalidad | `PCA`, `TruncatedSVD`, `NMF` |
| `scikit-learn` | `sklearn.manifold` | Embedding no lineal | `TSNE`, `MDS`, `Isomap` |
| `scikit-learn` | `sklearn.ensemble` | DetecciÃ³n de anomalÃ­as | `IsolationForest` |
| `scikit-learn` | `sklearn.neighbors` | DetecciÃ³n de outliers | `LocalOutlierFactor` |
| `scikit-learn` | `sklearn.metrics` | MÃ©tricas de evaluaciÃ³n | `silhouette_score`, `calinski_harabasz_score` |
| `mlxtend` | `mlxtend.frequent_patterns` | Reglas de asociaciÃ³n | `apriori`, `association_rules` |
| `scipy` | `scipy.cluster.hierarchy` | Clustering jerÃ¡rquico | `linkage`, `dendrogram`, `fcluster` |

### Imports TÃ­picos para Aprendizaje No Supervisado

```python
# Bibliotecas bÃ¡sicas
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocesamiento
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer

# Algoritmos de Clustering
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# ReducciÃ³n de Dimensionalidad
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# DetecciÃ³n de AnomalÃ­as
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# Reglas de AsociaciÃ³n
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# MÃ©tricas de EvaluaciÃ³n
from sklearn.metrics import (
    silhouette_score,
    silhouette_samples,
    calinski_harabasz_score,
    davies_bouldin_score
)

# Clustering JerÃ¡rquico (scipy)
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
```

---

## 1.4. Preprocesamiento de Datos para Aprendizaje No Supervisado

El preprocesamiento es **aÃºn mÃ¡s crÃ­tico** en el aprendizaje no supervisado que en el supervisado, ya que los algoritmos son muy sensibles a la escala y calidad de los datos.

### 1.4.1. Escalado de CaracterÃ­sticas

**Â¿Por quÃ© es obligatorio?**

La mayorÃ­a de algoritmos no supervisados se basan en **medidas de distancia** (Euclidiana, Manhattan, etc.). Si las variables tienen escalas muy diferentes, las de mayor magnitud dominarÃ¡n completamente el cÃ¡lculo.

**Ejemplo del problema:**
```
Cliente A: Edad=25, Salario=50000
Cliente B: Edad=35, Salario=51000
Cliente C: Edad=26, Salario=80000
```
Sin escalado, la diferencia de salario (miles) dominarÃ¡ sobre la edad (decenas).

**MÃ©todos de Escalado:**

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# 1. EstandarizaciÃ³n (Z-score) - El mÃ¡s comÃºn
# Transforma datos para tener media=0 y desviaciÃ³n estÃ¡ndar=1
# FÃ³rmula: z = (x - Î¼) / Ïƒ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. NormalizaciÃ³n Min-Max
# Escala valores al rango [0, 1]
# FÃ³rmula: x_norm = (x - x_min) / (x_max - x_min)
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# 3. RobustScaler - Resistente a outliers
# Usa mediana y rango intercuartÃ­lico en lugar de media y std
scaler = RobustScaler()
X_robust = scaler.fit_transform(X)
```

**Â¿CuÃ¡ndo usar cada uno?**

| MÃ©todo | CuÃ¡ndo usar |
| :--- | :--- |
| `StandardScaler` | Datos aproximadamente normales, sin muchos outliers |
| `MinMaxScaler` | Cuando necesitas valores acotados [0,1], ej. para redes neuronales |
| `RobustScaler` | Cuando hay outliers significativos en los datos |

### 1.4.2. Manejo de Valores Faltantes

En aprendizaje no supervisado, los valores faltantes son problemÃ¡ticos porque:
- Muchos algoritmos no los aceptan directamente
- Pueden distorsionar las medidas de distancia

```python
from sklearn.impute import SimpleImputer, KNNImputer

# 1. ImputaciÃ³n simple (media, mediana, moda)
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# 2. ImputaciÃ³n basada en KNN (mÃ¡s sofisticada)
# Imputa usando los k vecinos mÃ¡s cercanos
imputer = KNNImputer(n_neighbors=5)
X_imputed = imputer.fit_transform(X)
```

### 1.4.3. Manejo de Datos CategÃ³ricos

Los algoritmos de clustering generalmente requieren datos numÃ©ricos:

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Para variables nominales: One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=['categoria'])

# Alternativa: OneHotEncoder de sklearn
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
X_encoded = encoder.fit_transform(df[['categoria']])
```

**Nota:** Para clustering con variables categÃ³ricas puras, considerar algoritmos especializados como **K-Modes** o **K-Prototypes** (biblioteca `kmodes`).

---

## 1.5. MÃ©tricas de EvaluaciÃ³n en Aprendizaje No Supervisado

Evaluar modelos no supervisados es mÃ¡s complejo porque **no hay etiquetas de referencia**. Existen dos tipos de mÃ©tricas:

### 1.5.1. MÃ©tricas Internas (sin etiquetas reales)

EvalÃºan la calidad del clustering basÃ¡ndose Ãºnicamente en los datos y las asignaciones de cluster.

#### Coeficiente de Silueta (Silhouette Score)

Mide quÃ© tan similar es un punto a su propio cluster comparado con otros clusters.

**FÃ³rmula para un punto $i$:**
$$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$

Donde:
- $a(i)$ = distancia media de $i$ a los otros puntos de su mismo cluster (cohesiÃ³n)
- $b(i)$ = distancia media mÃ­nima de $i$ a los puntos del cluster mÃ¡s cercano (separaciÃ³n)

**InterpretaciÃ³n:**
- $s(i) \approx 1$: El punto estÃ¡ bien asignado a su cluster
- $s(i) \approx 0$: El punto estÃ¡ en la frontera entre clusters
- $s(i) < 0$: El punto probablemente estÃ¡ mal asignado

```python
from sklearn.metrics import silhouette_score, silhouette_samples

# Silueta promedio del clustering
score = silhouette_score(X, labels)
print(f"Silhouette Score: {score:.3f}")

# Silueta por cada muestra (para anÃ¡lisis detallado)
sample_scores = silhouette_samples(X, labels)
```

#### Inercia (Within-Cluster Sum of Squares - WCSS)

Suma de las distancias al cuadrado de cada punto al centroide de su cluster. **Solo para K-Means.**

$$WCSS = \sum_{i=1}^{k}\sum_{x \in C_i} ||x - \mu_i||^2$$

**InterpretaciÃ³n:**
- Menor inercia = clusters mÃ¡s compactos
- Se usa en el **MÃ©todo del Codo** para encontrar el nÃºmero Ã³ptimo de clusters

```python
# La inercia se obtiene directamente del modelo KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
print(f"Inercia: {kmeans.inertia_}")
```

#### Ãndice Calinski-Harabasz (Variance Ratio Criterion)

Ratio entre la dispersiÃ³n entre clusters y la dispersiÃ³n dentro de clusters.

$$CH = \frac{SS_B / (k-1)}{SS_W / (n-k)}$$

Donde:
- $SS_B$ = dispersiÃ³n entre clusters
- $SS_W$ = dispersiÃ³n dentro de clusters
- $k$ = nÃºmero de clusters
- $n$ = nÃºmero de muestras

**InterpretaciÃ³n:** Mayor valor = mejor clustering

```python
from sklearn.metrics import calinski_harabasz_score

score = calinski_harabasz_score(X, labels)
print(f"Calinski-Harabasz Score: {score:.3f}")
```

#### Ãndice Davies-Bouldin

Mide la similitud promedio entre cada cluster y su cluster mÃ¡s similar.

$$DB = \frac{1}{k}\sum_{i=1}^{k}\max_{j \neq i}\left(\frac{s_i + s_j}{d_{ij}}\right)$$

Donde:
- $s_i$ = dispersiÃ³n media del cluster $i$
- $d_{ij}$ = distancia entre centroides de clusters $i$ y $j$

**InterpretaciÃ³n:** Menor valor = mejor clustering (clusters mÃ¡s separados y compactos)

```python
from sklearn.metrics import davies_bouldin_score

score = davies_bouldin_score(X, labels)
print(f"Davies-Bouldin Score: {score:.3f}")
```

### 1.5.2. MÃ©tricas Externas (con etiquetas reales)

Cuando disponemos de etiquetas reales (ground truth), podemos comparar los clusters descubiertos con las clases verdaderas.

#### Adjusted Rand Index (ARI)

Mide la similitud entre dos asignaciones de clusters, ajustada por azar.

```python
from sklearn.metrics import adjusted_rand_score

ari = adjusted_rand_score(y_true, labels_pred)
# Rango: [-1, 1], donde 1 = asignaciÃ³n perfecta
```

#### Normalized Mutual Information (NMI)

Mide la informaciÃ³n mutua entre las asignaciones, normalizada.

```python
from sklearn.metrics import normalized_mutual_info_score

nmi = normalized_mutual_info_score(y_true, labels_pred)
# Rango: [0, 1], donde 1 = asignaciÃ³n perfecta
```

### 1.5.3. Tabla Resumen de MÃ©tricas

| MÃ©trica | Tipo | Rango | Mejor valor | Uso principal |
| :--- | :--- | :--- | :--- | :--- |
| Silhouette | Interna | [-1, 1] | Cercano a 1 | Evaluar calidad general |
| Inercia (WCSS) | Interna | [0, âˆ) | Menor | MÃ©todo del codo |
| Calinski-Harabasz | Interna | [0, âˆ) | Mayor | Comparar configuraciones |
| Davies-Bouldin | Interna | [0, âˆ) | Menor | Comparar configuraciones |
| ARI | Externa | [-1, 1] | Cercano a 1 | Validar con ground truth |
| NMI | Externa | [0, 1] | Cercano a 1 | Validar con ground truth |

---

## 1.6. El MÃ©todo del Codo (Elbow Method)

Es la tÃ©cnica mÃ¡s popular para determinar el nÃºmero Ã³ptimo de clusters en K-Means.

### Concepto

1. Ejecutar K-Means con diferentes valores de $k$ (nÃºmero de clusters)
2. Para cada $k$, calcular la inercia (WCSS)
3. Graficar $k$ vs. inercia
4. Buscar el "codo": el punto donde la reducciÃ³n de inercia se desacelera significativamente

### ImplementaciÃ³n Completa

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Generar datos de ejemplo
X, _ = make_blobs(n_samples=500, centers=4, random_state=42)

# Calcular inercia para diferentes valores de k
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# Graficar el mÃ©todo del codo
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
plt.xlabel('NÃºmero de Clusters (k)', fontsize=12)
plt.ylabel('Inercia (WCSS)', fontsize=12)
plt.title('MÃ©todo del Codo para Determinar k Ã“ptimo', fontsize=14)
plt.xticks(K_range)
plt.grid(True, alpha=0.3)
plt.show()
```

### InterpretaciÃ³n Visual

```
Inercia
   â”‚
   â”‚\
   â”‚ \
   â”‚  \
   â”‚   \____ â† "Codo" (k Ã³ptimo)
   â”‚        \____
   â”‚             \____
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ k
```

---

## 1.7. VisualizaciÃ³n de Resultados

La visualizaciÃ³n es fundamental en aprendizaje no supervisado para interpretar y comunicar resultados.

### 1.7.1. VisualizaciÃ³n de Clusters en 2D

```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Crear datos
X, y_true = make_blobs(n_samples=300, centers=4, random_state=42)

# Aplicar K-Means
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# Visualizar
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', 
                      alpha=0.6, edgecolors='w', s=50)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', 
            s=200, edgecolors='black', linewidths=2, label='Centroides')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Resultados del Clustering K-Means')
plt.legend()
plt.colorbar(scatter, label='Cluster')
plt.show()
```

### 1.7.2. GrÃ¡fico de Silueta

```python
from sklearn.metrics import silhouette_samples
import numpy as np

def plot_silhouette(X, labels, n_clusters):
    """Grafica el anÃ¡lisis de silueta por cluster."""
    fig, ax = plt.subplots(figsize=(10, 8))
    
    silhouette_avg = silhouette_score(X, labels)
    sample_silhouette_values = silhouette_samples(X, labels)
    
    y_lower = 10
    for i in range(n_clusters):
        # Valores de silueta para el cluster i
        cluster_silhouette_values = sample_silhouette_values[labels == i]
        cluster_silhouette_values.sort()
        
        cluster_size = cluster_silhouette_values.shape[0]
        y_upper = y_lower + cluster_size
        
        color = plt.cm.viridis(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)
        
        ax.text(-0.05, y_lower + 0.5 * cluster_size, str(i))
        y_lower = y_upper + 10
    
    ax.axvline(x=silhouette_avg, color="red", linestyle="--", 
               label=f'Silueta media: {silhouette_avg:.3f}')
    ax.set_xlabel("Coeficiente de Silueta")
    ax.set_ylabel("Cluster")
    ax.legend()
    plt.title("AnÃ¡lisis de Silueta")
    plt.show()
```

---

## 1.8. Ejemplos PrÃ¡cticos y Recursos Externos

### Recursos y Tutoriales Recomendados

* **DocumentaciÃ³n oficial de scikit-learn - Clustering:**
  [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)

* **DocumentaciÃ³n oficial de scikit-learn - ReducciÃ³n de Dimensionalidad:**
  [https://scikit-learn.org/stable/modules/decomposition.html](https://scikit-learn.org/stable/modules/decomposition.html)

* **Tutorial de K-Means con datos reales (Customer Segmentation):**
  [https://www.kaggle.com/code/kushal1996/customer-segmentation-k-means-analysis](https://www.kaggle.com/code/kushal1996/customer-segmentation-k-means-analysis)

* **Ejemplo completo de PCA para visualizaciÃ³n:**
  [https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html)

* **Market Basket Analysis con Apriori:**
  [https://www.kaggle.com/code/datatheque/association-rules-mining-market-basket-analysis](https://www.kaggle.com/code/datatheque/association-rules-mining-market-basket-analysis)

* **DetecciÃ³n de anomalÃ­as con Isolation Forest:**
  [https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html](https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html)

---

## 1.9. Comparativa de Algoritmos de Clustering

| Algoritmo | Forma clusters | Escalabilidad | Requiere k | Maneja ruido | Complejidad |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **K-Means** | EsfÃ©ricos | Muy alta | SÃ­ | No | O(nÂ·kÂ·i) |
| **DBSCAN** | Arbitraria | Media | No | SÃ­ | O(nÂ²) o O(n log n) |
| **JerÃ¡rquico** | Arbitraria | Baja | No* | No | O(nÂ³) |
| **Gaussian Mixture** | ElÃ­pticos | Alta | SÃ­ | No | O(nÂ·kÂ·i) |
| **OPTICS** | Arbitraria | Media | No | SÃ­ | O(nÂ²) |

*El clustering jerÃ¡rquico no requiere k a priori, pero sÃ­ para "cortar" el dendrograma.

---

## 1.10. Buenas PrÃ¡cticas

### âœ… Hacer siempre:

1. **Escalar los datos** antes de aplicar algoritmos basados en distancias
2. **Explorar los datos** visualmente antes del clustering (EDA)
3. **Probar mÃºltiples algoritmos** y comparar resultados
4. **Usar mÃºltiples mÃ©tricas** para evaluar la calidad
5. **Validar los resultados** con conocimiento del dominio
6. **Documentar las decisiones** (por quÃ© se eligiÃ³ cierto k, algoritmo, etc.)

### âŒ Evitar:

1. Asumir que existe una estructura de clusters cuando puede no haberla
2. Confiar ciegamente en una sola mÃ©trica
3. Ignorar outliers sin investigarlos
4. Aplicar algoritmos sin entender sus supuestos
5. Sobrevalorar el nÃºmero de clusters (mÃ¡s no siempre es mejor)

---

## 1.11. Ejercicio Integrador: Pipeline Completo

```python
"""
Pipeline completo de clustering con evaluaciÃ³n
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.datasets import load_iris

# 1. CARGAR DATOS
iris = load_iris()
X = iris.data
feature_names = iris.feature_names

print("="*50)
print("PIPELINE DE CLUSTERING NO SUPERVISADO")
print("="*50)

# 2. PREPROCESAMIENTO
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print(f"\n[1] Datos escalados: {X_scaled.shape}")

# 3. MÃ‰TODO DEL CODO
inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))

# Visualizar
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(K_range, inertias, 'bo-', linewidth=2)
axes[0].set_xlabel('NÃºmero de Clusters (k)')
axes[0].set_ylabel('Inercia')
axes[0].set_title('MÃ©todo del Codo')
axes[0].grid(True, alpha=0.3)

axes[1].plot(K_range, silhouettes, 'go-', linewidth=2)
axes[1].set_xlabel('NÃºmero de Clusters (k)')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('Silueta vs k')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 4. MODELO FINAL (k=3 basado en anÃ¡lisis)
k_optimo = 3
kmeans_final = KMeans(n_clusters=k_optimo, random_state=42, n_init=10)
labels = kmeans_final.fit_predict(X_scaled)

print(f"\n[2] Clustering con k={k_optimo}")
print(f"    DistribuciÃ³n de clusters: {np.bincount(labels)}")

# 5. EVALUACIÃ“N
print(f"\n[3] MÃ‰TRICAS DE EVALUACIÃ“N:")
print(f"    - Silhouette Score:      {silhouette_score(X_scaled, labels):.4f}")
print(f"    - Calinski-Harabasz:     {calinski_harabasz_score(X_scaled, labels):.4f}")
print(f"    - Davies-Bouldin:        {davies_bouldin_score(X_scaled, labels):.4f}")

# 6. ANÃLISIS DE RESULTADOS
print(f"\n[4] ANÃLISIS POR CLUSTER:")
df = pd.DataFrame(X, columns=feature_names)
df['cluster'] = labels
print(df.groupby('cluster').mean().round(2))
```

---

ğŸ“… **Fecha de creaciÃ³n:** Enero 2026  
âœï¸ **Autor:** Fran GarcÃ­a
