{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd16 Programaci\u00f3n en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 Este sitio web recopila documentaci\u00f3n y recursos sobre diferentes \u00e1reas de la Inteligencia Artificial (IA) y el Machine Learning (ML).</p>"},{"location":"#contenidos","title":"\ud83d\udcda Contenidos","text":"<p>Explora las diferentes secciones disponibles:</p>"},{"location":"#aprendizaje-supervisado","title":"\ud83c\udfaf Aprendizaje Supervisado","text":"<p>Aprende sobre algoritmos que utilizan datos etiquetados para realizar predicciones. Incluye t\u00e9cnicas de clasificaci\u00f3n y regresi\u00f3n como: - Machine Learning para an\u00e1lisis de datos - \u00c1rboles de decisi\u00f3n - Naive Bayes - K-Nearest Neighbors (KNN) - Support Vector Machines (SVM) - Algoritmos de ensamblado</p>"},{"location":"#aprendizaje-no-supervisado","title":"\ud83d\udd0d Aprendizaje No Supervisado","text":"<p>Descubre t\u00e9cnicas para encontrar patrones en datos no etiquetados: - Clustering (K-Means, DBSCAN, Jer\u00e1rquico) - Reducci\u00f3n de dimensionalidad (PCA, t-SNE) - An\u00e1lisis de asociaci\u00f3n</p>"},{"location":"#procesamiento-de-lenguaje-natural","title":"\ud83d\udcac Procesamiento de Lenguaje Natural","text":"<p>Explora c\u00f3mo las m\u00e1quinas comprenden y generan lenguaje humano: - Tokenizaci\u00f3n y preprocesamiento de texto - Modelos de lenguaje - An\u00e1lisis de sentimientos - Transformers y atenci\u00f3n</p>"},{"location":"#deep-learning","title":"\ud83e\udde0 Deep Learning","text":"<p>Profundiza en redes neuronales y arquitecturas avanzadas: - Redes neuronales artificiales - Redes convolucionales (CNN) - Redes recurrentes (RNN, LSTM) - Arquitecturas modernas</p>"},{"location":"#sobre-este-sitio","title":"\ud83c\udf93 Sobre este sitio","text":"<p>Este recurso est\u00e1 dise\u00f1ado como material de apoyo para el aprendizaje de t\u00e9cnicas de Inteligencia Artificial y Machine Learning, con ejemplos pr\u00e1cticos en Python.</p> <p>\ud83d\udcc5 \u00daltima actualizaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/","title":"\ud83d\udd0d Aprendizaje No Supervisado","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Aprendizaje No Supervisado! \ud83c\udf89</p>"},{"location":"aprendizaje-no-supervisado/#que-es-el-aprendizaje-no-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje No Supervisado?","text":"<p>El aprendizaje no supervisado es un tipo de aprendizaje autom\u00e1tico en el que el modelo trabaja con datos sin etiquetas. El objetivo es descubrir patrones ocultos, estructuras o agrupaciones en los datos sin tener una respuesta correcta predefinida.</p> <p>A diferencia del aprendizaje supervisado, aqu\u00ed no hay una variable objetivo que predecir; el algoritmo debe encontrar por s\u00ed mismo las relaciones entre los datos.</p>"},{"location":"aprendizaje-no-supervisado/#tipos-de-problemas-no-supervisados","title":"\ud83e\udde0 Tipos de Problemas No Supervisados","text":"<ol> <li> <p>Clustering (Agrupamiento):    Agrupa datos similares en clusters o grupos.    \ud83d\udccd Ejemplo: Segmentaci\u00f3n de clientes seg\u00fan su comportamiento de compra.</p> </li> <li> <p>Reducci\u00f3n de Dimensionalidad:    Reduce el n\u00famero de variables manteniendo la informaci\u00f3n m\u00e1s relevante.    \ud83d\udccd Ejemplo: Visualizar datos de alta dimensi\u00f3n en 2D o 3D.</p> </li> <li> <p>Detecci\u00f3n de Anomal\u00edas:    Identifica datos que se desv\u00edan significativamente del patr\u00f3n normal.    \ud83d\udccd Ejemplo: Detectar transacciones fraudulentas.</p> </li> <li> <p>Reglas de Asociaci\u00f3n:    Encuentra relaciones entre variables en grandes conjuntos de datos.    \ud83d\udccd Ejemplo: An\u00e1lisis de cesta de compra (\u00bfqu\u00e9 productos se compran juntos?).</p> </li> </ol>"},{"location":"aprendizaje-no-supervisado/#algoritmos-comunes","title":"\ud83d\udd0d Algoritmos Comunes","text":"<ul> <li>K-Means</li> <li>DBSCAN</li> <li>Clustering Jer\u00e1rquico</li> <li>PCA (An\u00e1lisis de Componentes Principales)</li> <li>t-SNE</li> <li>Isolation Forest</li> <li>Apriori</li> </ul>"},{"location":"aprendizaje-no-supervisado/#contenido-en-construccion","title":"\ud83d\udea7 Contenido en construcci\u00f3n","text":"<p>Esta secci\u00f3n est\u00e1 siendo desarrollada. Pr\u00f3ximamente encontrar\u00e1s:</p> <ul> <li>[ ] Algoritmos de Clustering</li> <li>[ ] Reducci\u00f3n de dimensionalidad</li> <li>[ ] Detecci\u00f3n de anomal\u00edas</li> <li>[ ] Ejemplos pr\u00e1cticos con Python</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/","title":"\ud83d\udd0d Unidad 1. Fundamentos del Aprendizaje No Supervisado","text":"<p>Esta unidad introduce los conceptos fundamentales del Aprendizaje No Supervisado, sus diferencias con el aprendizaje supervisado, las bibliotecas Python necesarias, el flujo de trabajo t\u00edpico, y las metodolog\u00edas esenciales para preparar datos y evaluar resultados en ausencia de etiquetas.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#11-que-es-el-aprendizaje-no-supervisado","title":"1.1. \u00bfQu\u00e9 es el Aprendizaje No Supervisado?","text":"<p>El Aprendizaje No Supervisado es una rama del Machine Learning donde los algoritmos trabajan con datos sin etiquetas. A diferencia del aprendizaje supervisado, no existe una \"respuesta correcta\" predefinida que gu\u00ede el entrenamiento.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#definiciones-clave","title":"Definiciones Clave","text":"<ul> <li> <p>Definici\u00f3n formal: \"El aprendizaje no supervisado es el entrenamiento de un modelo usando informaci\u00f3n que no est\u00e1 clasificada ni etiquetada, permitiendo al algoritmo actuar sobre esa informaci\u00f3n sin gu\u00eda.\"</p> </li> <li> <p>Objetivo principal: Descubrir estructuras ocultas, patrones o agrupaciones inherentes en los datos que no son evidentes a simple vista.</p> </li> <li> <p>Analog\u00eda: Imagina que te dan una caja con miles de fotograf\u00edas sin ninguna descripci\u00f3n. El aprendizaje no supervisado ser\u00eda como organizarlas autom\u00e1ticamente en grupos (paisajes, retratos, animales, etc.) bas\u00e1ndose \u00fanicamente en las similitudes visuales entre ellas.</p> </li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#diferencias-con-el-aprendizaje-supervisado","title":"Diferencias con el Aprendizaje Supervisado","text":"Aspecto Supervisado No Supervisado Datos Etiquetados (X, y) Sin etiquetas (solo X) Objetivo Predecir una variable objetivo Descubrir estructura en los datos Evaluaci\u00f3n M\u00e9tricas claras (accuracy, F1, MSE) M\u00e9tricas indirectas (silueta, inercia) Ejemplos Clasificaci\u00f3n, Regresi\u00f3n Clustering, Reducci\u00f3n de dimensionalidad Feedback Conocemos si la predicci\u00f3n es correcta No hay \"respuesta correcta\""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#tipos-de-problemas-no-supervisados","title":"Tipos de Problemas No Supervisados","text":"<p>El aprendizaje no supervisado abarca principalmente cuatro tipos de problemas:</p> <ol> <li>Clustering (Agrupamiento):</li> <li>Objetivo: Dividir los datos en grupos (clusters) donde los elementos dentro de un grupo son similares entre s\u00ed y diferentes a los de otros grupos.</li> <li>Algoritmos: K-Means, DBSCAN, Clustering Jer\u00e1rquico, OPTICS, Mean Shift.</li> <li> <p>Aplicaci\u00f3n: Segmentaci\u00f3n de clientes, agrupaci\u00f3n de documentos.</p> </li> <li> <p>Reducci\u00f3n de Dimensionalidad:</p> </li> <li>Objetivo: Reducir el n\u00famero de variables (features) manteniendo la mayor cantidad de informaci\u00f3n posible.</li> <li>Algoritmos: PCA, t-SNE, UMAP, LDA, Autoencoders.</li> <li> <p>Aplicaci\u00f3n: Visualizaci\u00f3n de datos, compresi\u00f3n, preprocesamiento.</p> </li> <li> <p>Detecci\u00f3n de Anomal\u00edas:</p> </li> <li>Objetivo: Identificar puntos de datos que se desv\u00edan significativamente del comportamiento normal.</li> <li>Algoritmos: Isolation Forest, One-Class SVM, LOF (Local Outlier Factor).</li> <li> <p>Aplicaci\u00f3n: Detecci\u00f3n de fraudes, mantenimiento predictivo.</p> </li> <li> <p>Reglas de Asociaci\u00f3n:</p> </li> <li>Objetivo: Descubrir relaciones interesantes entre variables en grandes conjuntos de datos.</li> <li>Algoritmos: Apriori, FP-Growth, Eclat.</li> <li>Aplicaci\u00f3n: An\u00e1lisis de cesta de compra, sistemas de recomendaci\u00f3n.</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#12-flujo-de-trabajo-del-aprendizaje-no-supervisado","title":"1.2. Flujo de Trabajo del Aprendizaje No Supervisado","text":"<p>El proceso general para aplicar t\u00e9cnicas no supervisadas sigue estos pasos:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Definici\u00f3n  \u2502\u2500\u2500\u2500\u25b6\u2502  2. Preparaci\u00f3n \u2502\u2500\u2500\u2500\u25b6\u2502  3. Selecci\u00f3n   \u2502\n\u2502  del Problema   \u2502    \u2502    de Datos     \u2502    \u2502  del Algoritmo  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 6. Aplicaci\u00f3n   \u2502\u25c0\u2500\u2500\u2500\u2502 5. Validaci\u00f3n   \u2502\u25c0\u2500\u2500\u2500\u2502 4. Entrenamiento\u2502\n\u2502 e Interpretaci\u00f3n\u2502    \u2502   y Evaluaci\u00f3n  \u2502    \u2502    del Modelo   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#1-definicion-del-problema","title":"1. Definici\u00f3n del Problema","text":"<ul> <li>\u00bfQu\u00e9 queremos descubrir? \u00bfGrupos de clientes? \u00bfPatrones an\u00f3malos?</li> <li>\u00bfCu\u00e1ntas dimensiones tienen los datos? \u00bfSon visualizables?</li> <li>\u00bfHay conocimiento del dominio que pueda guiar la interpretaci\u00f3n?</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#2-preparacion-de-datos","title":"2. Preparaci\u00f3n de Datos","text":"<ul> <li>Limpieza: Manejo de valores faltantes y outliers.</li> <li>Escalado: Crucial para algoritmos basados en distancias (K-Means, DBSCAN).</li> <li>Selecci\u00f3n de caracter\u00edsticas: Eliminar features irrelevantes o redundantes.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#3-seleccion-del-algoritmo","title":"3. Selecci\u00f3n del Algoritmo","text":"<ul> <li>Depende del tipo de problema y las caracter\u00edsticas de los datos.</li> <li>Considerar: tama\u00f1o del dataset, n\u00famero de clusters esperado, forma de los clusters.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#4-entrenamiento-del-modelo","title":"4. Entrenamiento del Modelo","text":"<ul> <li>No hay etiquetas, por lo que no hay conjunto de \"validaci\u00f3n\" tradicional.</li> <li>Se ajustan hiperpar\u00e1metros mediante t\u00e9cnicas espec\u00edficas (m\u00e9todo del codo, silueta).</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#5-validacion-y-evaluacion","title":"5. Validaci\u00f3n y Evaluaci\u00f3n","text":"<ul> <li>M\u00e9tricas internas: Silueta, Inercia, Davies-Bouldin.</li> <li>Validaci\u00f3n visual: Gr\u00e1ficos de clusters, dendrogramas.</li> <li>Validaci\u00f3n externa (si hay etiquetas disponibles): NMI, ARI.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#6-aplicacion-e-interpretacion","title":"6. Aplicaci\u00f3n e Interpretaci\u00f3n","text":"<ul> <li>Asignar significado a los clusters descubiertos.</li> <li>Integrar resultados en procesos de negocio o an\u00e1lisis posteriores.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#13-bibliotecas-python-para-aprendizaje-no-supervisado","title":"1.3. Bibliotecas Python para Aprendizaje No Supervisado","text":""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#instalacion-de-bibliotecas-esenciales","title":"Instalaci\u00f3n de Bibliotecas Esenciales","text":"<pre><code># Instalaci\u00f3n con pip\npip install numpy pandas matplotlib seaborn scikit-learn\n\n# Bibliotecas adicionales espec\u00edficas\npip install mlxtend          # Para reglas de asociaci\u00f3n (Apriori)\npip install umap-learn       # Para UMAP (reducci\u00f3n de dimensionalidad)\npip install hdbscan          # Para HDBSCAN (clustering avanzado)\npip install yellowbrick      # Para visualizaci\u00f3n de ML\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#bibliotecas-principales-y-sus-modulos","title":"Bibliotecas Principales y sus M\u00f3dulos","text":"Biblioteca M\u00f3dulo Funcionalidad Algoritmos/Funciones <code>scikit-learn</code> <code>sklearn.cluster</code> Clustering <code>KMeans</code>, <code>DBSCAN</code>, <code>AgglomerativeClustering</code> <code>scikit-learn</code> <code>sklearn.decomposition</code> Reducci\u00f3n de dimensionalidad <code>PCA</code>, <code>TruncatedSVD</code>, <code>NMF</code> <code>scikit-learn</code> <code>sklearn.manifold</code> Embedding no lineal <code>TSNE</code>, <code>MDS</code>, <code>Isomap</code> <code>scikit-learn</code> <code>sklearn.ensemble</code> Detecci\u00f3n de anomal\u00edas <code>IsolationForest</code> <code>scikit-learn</code> <code>sklearn.neighbors</code> Detecci\u00f3n de outliers <code>LocalOutlierFactor</code> <code>scikit-learn</code> <code>sklearn.metrics</code> M\u00e9tricas de evaluaci\u00f3n <code>silhouette_score</code>, <code>calinski_harabasz_score</code> <code>mlxtend</code> <code>mlxtend.frequent_patterns</code> Reglas de asociaci\u00f3n <code>apriori</code>, <code>association_rules</code> <code>scipy</code> <code>scipy.cluster.hierarchy</code> Clustering jer\u00e1rquico <code>linkage</code>, <code>dendrogram</code>, <code>fcluster</code>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#imports-tipicos-para-aprendizaje-no-supervisado","title":"Imports T\u00edpicos para Aprendizaje No Supervisado","text":"<pre><code># Bibliotecas b\u00e1sicas\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocesamiento\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Algoritmos de Clustering\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\n\n# Reducci\u00f3n de Dimensionalidad\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Detecci\u00f3n de Anomal\u00edas\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# Reglas de Asociaci\u00f3n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# M\u00e9tricas de Evaluaci\u00f3n\nfrom sklearn.metrics import (\n    silhouette_score,\n    silhouette_samples,\n    calinski_harabasz_score,\n    davies_bouldin_score\n)\n\n# Clustering Jer\u00e1rquico (scipy)\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#14-preprocesamiento-de-datos-para-aprendizaje-no-supervisado","title":"1.4. Preprocesamiento de Datos para Aprendizaje No Supervisado","text":"<p>El preprocesamiento es a\u00fan m\u00e1s cr\u00edtico en el aprendizaje no supervisado que en el supervisado, ya que los algoritmos son muy sensibles a la escala y calidad de los datos.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#141-escalado-de-caracteristicas","title":"1.4.1. Escalado de Caracter\u00edsticas","text":"<p>\u00bfPor qu\u00e9 es obligatorio?</p> <p>La mayor\u00eda de algoritmos no supervisados se basan en medidas de distancia (Euclidiana, Manhattan, etc.). Si las variables tienen escalas muy diferentes, las de mayor magnitud dominar\u00e1n completamente el c\u00e1lculo.</p> <p>Ejemplo del problema: <pre><code>Cliente A: Edad=25, Salario=50000\nCliente B: Edad=35, Salario=51000\nCliente C: Edad=26, Salario=80000\n</code></pre> Sin escalado, la diferencia de salario (miles) dominar\u00e1 sobre la edad (decenas).</p> <p>M\u00e9todos de Escalado:</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# 1. Estandarizaci\u00f3n (Z-score) - El m\u00e1s com\u00fan\n# Transforma datos para tener media=0 y desviaci\u00f3n est\u00e1ndar=1\n# F\u00f3rmula: z = (x - \u03bc) / \u03c3\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2. Normalizaci\u00f3n Min-Max\n# Escala valores al rango [0, 1]\n# F\u00f3rmula: x_norm = (x - x_min) / (x_max - x_min)\nscaler = MinMaxScaler()\nX_normalized = scaler.fit_transform(X)\n\n# 3. RobustScaler - Resistente a outliers\n# Usa mediana y rango intercuart\u00edlico en lugar de media y std\nscaler = RobustScaler()\nX_robust = scaler.fit_transform(X)\n</code></pre> <p>\u00bfCu\u00e1ndo usar cada uno?</p> M\u00e9todo Cu\u00e1ndo usar <code>StandardScaler</code> Datos aproximadamente normales, sin muchos outliers <code>MinMaxScaler</code> Cuando necesitas valores acotados [0,1], ej. para redes neuronales <code>RobustScaler</code> Cuando hay outliers significativos en los datos"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#142-manejo-de-valores-faltantes","title":"1.4.2. Manejo de Valores Faltantes","text":"<p>En aprendizaje no supervisado, los valores faltantes son problem\u00e1ticos porque: - Muchos algoritmos no los aceptan directamente - Pueden distorsionar las medidas de distancia</p> <pre><code>from sklearn.impute import SimpleImputer, KNNImputer\n\n# 1. Imputaci\u00f3n simple (media, mediana, moda)\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X)\n\n# 2. Imputaci\u00f3n basada en KNN (m\u00e1s sofisticada)\n# Imputa usando los k vecinos m\u00e1s cercanos\nimputer = KNNImputer(n_neighbors=5)\nX_imputed = imputer.fit_transform(X)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#143-manejo-de-datos-categoricos","title":"1.4.3. Manejo de Datos Categ\u00f3ricos","text":"<p>Los algoritmos de clustering generalmente requieren datos num\u00e9ricos:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# Para variables nominales: One-Hot Encoding\ndf_encoded = pd.get_dummies(df, columns=['categoria'])\n\n# Alternativa: OneHotEncoder de sklearn\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nX_encoded = encoder.fit_transform(df[['categoria']])\n</code></pre> <p>Nota: Para clustering con variables categ\u00f3ricas puras, considerar algoritmos especializados como K-Modes o K-Prototypes (biblioteca <code>kmodes</code>).</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#15-metricas-de-evaluacion-en-aprendizaje-no-supervisado","title":"1.5. M\u00e9tricas de Evaluaci\u00f3n en Aprendizaje No Supervisado","text":"<p>Evaluar modelos no supervisados es m\u00e1s complejo porque no hay etiquetas de referencia. Existen dos tipos de m\u00e9tricas:</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#151-metricas-internas-sin-etiquetas-reales","title":"1.5.1. M\u00e9tricas Internas (sin etiquetas reales)","text":"<p>Eval\u00faan la calidad del clustering bas\u00e1ndose \u00fanicamente en los datos y las asignaciones de cluster.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#coeficiente-de-silueta-silhouette-score","title":"Coeficiente de Silueta (Silhouette Score)","text":"<p>Mide qu\u00e9 tan similar es un punto a su propio cluster comparado con otros clusters.</p> <p>F\u00f3rmula para un punto \\(i\\): \\(\\(s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\\)\\)</p> <p>Donde: - \\(a(i)\\) = distancia media de \\(i\\) a los otros puntos de su mismo cluster (cohesi\u00f3n) - \\(b(i)\\) = distancia media m\u00ednima de \\(i\\) a los puntos del cluster m\u00e1s cercano (separaci\u00f3n)</p> <p>Interpretaci\u00f3n: - \\(s(i) \\approx 1\\): El punto est\u00e1 bien asignado a su cluster - \\(s(i) \\approx 0\\): El punto est\u00e1 en la frontera entre clusters - \\(s(i) &lt; 0\\): El punto probablemente est\u00e1 mal asignado</p> <pre><code>from sklearn.metrics import silhouette_score, silhouette_samples\n\n# Silueta promedio del clustering\nscore = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {score:.3f}\")\n\n# Silueta por cada muestra (para an\u00e1lisis detallado)\nsample_scores = silhouette_samples(X, labels)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#inercia-within-cluster-sum-of-squares-wcss","title":"Inercia (Within-Cluster Sum of Squares - WCSS)","text":"<p>Suma de las distancias al cuadrado de cada punto al centroide de su cluster. Solo para K-Means.</p> \\[WCSS = \\sum_{i=1}^{k}\\sum_{x \\in C_i} ||x - \\mu_i||^2\\] <p>Interpretaci\u00f3n: - Menor inercia = clusters m\u00e1s compactos - Se usa en el M\u00e9todo del Codo para encontrar el n\u00famero \u00f3ptimo de clusters</p> <pre><code># La inercia se obtiene directamente del modelo KMeans\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nprint(f\"Inercia: {kmeans.inertia_}\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#indice-calinski-harabasz-variance-ratio-criterion","title":"\u00cdndice Calinski-Harabasz (Variance Ratio Criterion)","text":"<p>Ratio entre la dispersi\u00f3n entre clusters y la dispersi\u00f3n dentro de clusters.</p> \\[CH = \\frac{SS_B / (k-1)}{SS_W / (n-k)}\\] <p>Donde: - \\(SS_B\\) = dispersi\u00f3n entre clusters - \\(SS_W\\) = dispersi\u00f3n dentro de clusters - \\(k\\) = n\u00famero de clusters - \\(n\\) = n\u00famero de muestras</p> <p>Interpretaci\u00f3n: Mayor valor = mejor clustering</p> <pre><code>from sklearn.metrics import calinski_harabasz_score\n\nscore = calinski_harabasz_score(X, labels)\nprint(f\"Calinski-Harabasz Score: {score:.3f}\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#indice-davies-bouldin","title":"\u00cdndice Davies-Bouldin","text":"<p>Mide la similitud promedio entre cada cluster y su cluster m\u00e1s similar.</p> \\[DB = \\frac{1}{k}\\sum_{i=1}^{k}\\max_{j \\neq i}\\left(\\frac{s_i + s_j}{d_{ij}}\\right)\\] <p>Donde: - \\(s_i\\) = dispersi\u00f3n media del cluster \\(i\\) - \\(d_{ij}\\) = distancia entre centroides de clusters \\(i\\) y \\(j\\)</p> <p>Interpretaci\u00f3n: Menor valor = mejor clustering (clusters m\u00e1s separados y compactos)</p> <pre><code>from sklearn.metrics import davies_bouldin_score\n\nscore = davies_bouldin_score(X, labels)\nprint(f\"Davies-Bouldin Score: {score:.3f}\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#152-metricas-externas-con-etiquetas-reales","title":"1.5.2. M\u00e9tricas Externas (con etiquetas reales)","text":"<p>Cuando disponemos de etiquetas reales (ground truth), podemos comparar los clusters descubiertos con las clases verdaderas.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#adjusted-rand-index-ari","title":"Adjusted Rand Index (ARI)","text":"<p>Mide la similitud entre dos asignaciones de clusters, ajustada por azar.</p> <pre><code>from sklearn.metrics import adjusted_rand_score\n\nari = adjusted_rand_score(y_true, labels_pred)\n# Rango: [-1, 1], donde 1 = asignaci\u00f3n perfecta\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#normalized-mutual-information-nmi","title":"Normalized Mutual Information (NMI)","text":"<p>Mide la informaci\u00f3n mutua entre las asignaciones, normalizada.</p> <pre><code>from sklearn.metrics import normalized_mutual_info_score\n\nnmi = normalized_mutual_info_score(y_true, labels_pred)\n# Rango: [0, 1], donde 1 = asignaci\u00f3n perfecta\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#153-tabla-resumen-de-metricas","title":"1.5.3. Tabla Resumen de M\u00e9tricas","text":"M\u00e9trica Tipo Rango Mejor valor Uso principal Silhouette Interna [-1, 1] Cercano a 1 Evaluar calidad general Inercia (WCSS) Interna [0, \u221e) Menor M\u00e9todo del codo Calinski-Harabasz Interna [0, \u221e) Mayor Comparar configuraciones Davies-Bouldin Interna [0, \u221e) Menor Comparar configuraciones ARI Externa [-1, 1] Cercano a 1 Validar con ground truth NMI Externa [0, 1] Cercano a 1 Validar con ground truth"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#16-el-metodo-del-codo-elbow-method","title":"1.6. El M\u00e9todo del Codo (Elbow Method)","text":"<p>Es la t\u00e9cnica m\u00e1s popular para determinar el n\u00famero \u00f3ptimo de clusters en K-Means.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#concepto","title":"Concepto","text":"<ol> <li>Ejecutar K-Means con diferentes valores de \\(k\\) (n\u00famero de clusters)</li> <li>Para cada \\(k\\), calcular la inercia (WCSS)</li> <li>Graficar \\(k\\) vs. inercia</li> <li>Buscar el \"codo\": el punto donde la reducci\u00f3n de inercia se desacelera significativamente</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#implementacion-completa","title":"Implementaci\u00f3n Completa","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Generar datos de ejemplo\nX, _ = make_blobs(n_samples=500, centers=4, random_state=42)\n\n# Calcular inercia para diferentes valores de k\ninertias = []\nK_range = range(1, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\n# Graficar el m\u00e9todo del codo\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('N\u00famero de Clusters (k)', fontsize=12)\nplt.ylabel('Inercia (WCSS)', fontsize=12)\nplt.title('M\u00e9todo del Codo para Determinar k \u00d3ptimo', fontsize=14)\nplt.xticks(K_range)\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#interpretacion-visual","title":"Interpretaci\u00f3n Visual","text":"<pre><code>Inercia\n   \u2502\n   \u2502\\\n   \u2502 \\\n   \u2502  \\\n   \u2502   \\____ \u2190 \"Codo\" (k \u00f3ptimo)\n   \u2502        \\____\n   \u2502             \\____\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 k\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#17-visualizacion-de-resultados","title":"1.7. Visualizaci\u00f3n de Resultados","text":"<p>La visualizaci\u00f3n es fundamental en aprendizaje no supervisado para interpretar y comunicar resultados.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#171-visualizacion-de-clusters-en-2d","title":"1.7.1. Visualizaci\u00f3n de Clusters en 2D","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Crear datos\nX, y_true = make_blobs(n_samples=300, centers=4, random_state=42)\n\n# Aplicar K-Means\nkmeans = KMeans(n_clusters=4, random_state=42)\nlabels = kmeans.fit_predict(X)\ncentroids = kmeans.cluster_centers_\n\n# Visualizar\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', \n                      alpha=0.6, edgecolors='w', s=50)\nplt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', \n            s=200, edgecolors='black', linewidths=2, label='Centroides')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Resultados del Clustering K-Means')\nplt.legend()\nplt.colorbar(scatter, label='Cluster')\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#172-grafico-de-silueta","title":"1.7.2. Gr\u00e1fico de Silueta","text":"<pre><code>from sklearn.metrics import silhouette_samples\nimport numpy as np\n\ndef plot_silhouette(X, labels, n_clusters):\n    \"\"\"Grafica el an\u00e1lisis de silueta por cluster.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    silhouette_avg = silhouette_score(X, labels)\n    sample_silhouette_values = silhouette_samples(X, labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Valores de silueta para el cluster i\n        cluster_silhouette_values = sample_silhouette_values[labels == i]\n        cluster_silhouette_values.sort()\n\n        cluster_size = cluster_silhouette_values.shape[0]\n        y_upper = y_lower + cluster_size\n\n        color = plt.cm.viridis(float(i) / n_clusters)\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                         0, cluster_silhouette_values,\n                         facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax.text(-0.05, y_lower + 0.5 * cluster_size, str(i))\n        y_lower = y_upper + 10\n\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", \n               label=f'Silueta media: {silhouette_avg:.3f}')\n    ax.set_xlabel(\"Coeficiente de Silueta\")\n    ax.set_ylabel(\"Cluster\")\n    ax.legend()\n    plt.title(\"An\u00e1lisis de Silueta\")\n    plt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#18-ejemplos-practicos-y-recursos-externos","title":"1.8. Ejemplos Pr\u00e1cticos y Recursos Externos","text":""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#recursos-y-tutoriales-recomendados","title":"Recursos y Tutoriales Recomendados","text":"<ul> <li> <p>Documentaci\u00f3n oficial de scikit-learn - Clustering: https://scikit-learn.org/stable/modules/clustering.html</p> </li> <li> <p>Documentaci\u00f3n oficial de scikit-learn - Reducci\u00f3n de Dimensionalidad: https://scikit-learn.org/stable/modules/decomposition.html</p> </li> <li> <p>Tutorial de K-Means con datos reales (Customer Segmentation): https://www.kaggle.com/code/kushal1996/customer-segmentation-k-means-analysis</p> </li> <li> <p>Ejemplo completo de PCA para visualizaci\u00f3n: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html</p> </li> <li> <p>Market Basket Analysis con Apriori: https://www.kaggle.com/code/datatheque/association-rules-mining-market-basket-analysis</p> </li> <li> <p>Detecci\u00f3n de anomal\u00edas con Isolation Forest: https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html</p> </li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#19-comparativa-de-algoritmos-de-clustering","title":"1.9. Comparativa de Algoritmos de Clustering","text":"Algoritmo Forma clusters Escalabilidad Requiere k Maneja ruido Complejidad K-Means Esf\u00e9ricos Muy alta S\u00ed No O(n\u00b7k\u00b7i) DBSCAN Arbitraria Media No S\u00ed O(n\u00b2) o O(n log n) Jer\u00e1rquico Arbitraria Baja No* No O(n\u00b3) Gaussian Mixture El\u00edpticos Alta S\u00ed No O(n\u00b7k\u00b7i) OPTICS Arbitraria Media No S\u00ed O(n\u00b2) <p>*El clustering jer\u00e1rquico no requiere k a priori, pero s\u00ed para \"cortar\" el dendrograma.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#110-buenas-practicas","title":"1.10. Buenas Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#hacer-siempre","title":"\u2705 Hacer siempre:","text":"<ol> <li>Escalar los datos antes de aplicar algoritmos basados en distancias</li> <li>Explorar los datos visualmente antes del clustering (EDA)</li> <li>Probar m\u00faltiples algoritmos y comparar resultados</li> <li>Usar m\u00faltiples m\u00e9tricas para evaluar la calidad</li> <li>Validar los resultados con conocimiento del dominio</li> <li>Documentar las decisiones (por qu\u00e9 se eligi\u00f3 cierto k, algoritmo, etc.)</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#evitar","title":"\u274c Evitar:","text":"<ol> <li>Asumir que existe una estructura de clusters cuando puede no haberla</li> <li>Confiar ciegamente en una sola m\u00e9trica</li> <li>Ignorar outliers sin investigarlos</li> <li>Aplicar algoritmos sin entender sus supuestos</li> <li>Sobrevalorar el n\u00famero de clusters (m\u00e1s no siempre es mejor)</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#111-ejercicio-integrador-pipeline-completo","title":"1.11. Ejercicio Integrador: Pipeline Completo","text":"<pre><code>\"\"\"\nPipeline completo de clustering con evaluaci\u00f3n\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.datasets import load_iris\n\n# 1. CARGAR DATOS\niris = load_iris()\nX = iris.data\nfeature_names = iris.feature_names\n\nprint(\"=\"*50)\nprint(\"PIPELINE DE CLUSTERING NO SUPERVISADO\")\nprint(\"=\"*50)\n\n# 2. PREPROCESAMIENTO\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(f\"\\n[1] Datos escalados: {X_scaled.shape}\")\n\n# 3. M\u00c9TODO DEL CODO\ninertias = []\nsilhouettes = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# Visualizar\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(K_range, inertias, 'bo-', linewidth=2)\naxes[0].set_xlabel('N\u00famero de Clusters (k)')\naxes[0].set_ylabel('Inercia')\naxes[0].set_title('M\u00e9todo del Codo')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(K_range, silhouettes, 'go-', linewidth=2)\naxes[1].set_xlabel('N\u00famero de Clusters (k)')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].set_title('Silueta vs k')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 4. MODELO FINAL (k=3 basado en an\u00e1lisis)\nk_optimo = 3\nkmeans_final = KMeans(n_clusters=k_optimo, random_state=42, n_init=10)\nlabels = kmeans_final.fit_predict(X_scaled)\n\nprint(f\"\\n[2] Clustering con k={k_optimo}\")\nprint(f\"    Distribuci\u00f3n de clusters: {np.bincount(labels)}\")\n\n# 5. EVALUACI\u00d3N\nprint(f\"\\n[3] M\u00c9TRICAS DE EVALUACI\u00d3N:\")\nprint(f\"    - Silhouette Score:      {silhouette_score(X_scaled, labels):.4f}\")\nprint(f\"    - Calinski-Harabasz:     {calinski_harabasz_score(X_scaled, labels):.4f}\")\nprint(f\"    - Davies-Bouldin:        {davies_bouldin_score(X_scaled, labels):.4f}\")\n\n# 6. AN\u00c1LISIS DE RESULTADOS\nprint(f\"\\n[4] AN\u00c1LISIS POR CLUSTER:\")\ndf = pd.DataFrame(X, columns=feature_names)\ndf['cluster'] = labels\nprint(df.groupby('cluster').mean().round(2))\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/","title":"\ud83c\udfaf Unidad 2. Algoritmo K-Means","text":"<p>El algoritmo K-Means es el m\u00e9todo de clustering m\u00e1s popular y ampliamente utilizado en Machine Learning. Su simplicidad, eficiencia y escalabilidad lo convierten en la primera opci\u00f3n para muchas tareas de agrupamiento. K-Means particiona los datos en k grupos (clusters) donde cada observaci\u00f3n pertenece al cluster con el centroide (media) m\u00e1s cercano.</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#21-como-funciona-k-means","title":"2.1. \u00bfC\u00f3mo Funciona K-Means?","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#intuicion-del-algoritmo","title":"Intuici\u00f3n del Algoritmo","text":"<p>K-Means busca dividir \\(n\\) observaciones en \\(k\\) clusters de manera que se minimice la varianza dentro de cada cluster. Es un algoritmo iterativo que alterna entre dos pasos hasta converger:</p> <ol> <li>Asignaci\u00f3n: Cada punto se asigna al centroide m\u00e1s cercano</li> <li>Actualizaci\u00f3n: Cada centroide se recalcula como la media de los puntos asignados</li> </ol>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#algoritmo-paso-a-paso","title":"Algoritmo Paso a Paso","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ALGORITMO K-MEANS                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Entrada: Datos X, n\u00famero de clusters k                      \u2502\n\u2502 Salida: Asignaciones de cluster y centroides                \u2502\n\u2502                                                             \u2502\n\u2502 1. INICIALIZACI\u00d3N                                           \u2502\n\u2502    Seleccionar k puntos aleatorios como centroides iniciales\u2502\n\u2502                                                             \u2502\n\u2502 2. REPETIR hasta convergencia:                              \u2502\n\u2502    a) ASIGNACI\u00d3N: Asignar cada punto al centroide m\u00e1s      \u2502\n\u2502       cercano (seg\u00fan distancia Euclidiana)                  \u2502\n\u2502    b) ACTUALIZACI\u00d3N: Recalcular cada centroide como la     \u2502\n\u2502       media de todos los puntos asignados a \u00e9l              \u2502\n\u2502                                                             \u2502\n\u2502 3. CONVERGENCIA cuando:                                     \u2502\n\u2502    - Los centroides no cambian, O                           \u2502\n\u2502    - Las asignaciones no cambian, O                         \u2502\n\u2502    - Se alcanza el n\u00famero m\u00e1ximo de iteraciones             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#visualizacion-del-proceso","title":"Visualizaci\u00f3n del Proceso","text":"<pre><code>Iteraci\u00f3n 0 (Inicial)     Iteraci\u00f3n 1           Iteraci\u00f3n 2 (Final)\n\n    \u00d7  \u2022  \u2022                 \u00d7 \u2022 \u2022                  \u00d7 \u2022 \u2022\n   \u2022  \u2022    \u2022               \u2022 \u2022   \u2022                \u2022 \u2022   \u2022\n      \u00d7  \u2022                    \u00d7\u2022                     \u00d7\u2022\n  \u2022     \u2022  \u2022              \u2022    \u2022 \u2022               \u2022    \u2022 \u2022\n    \u2022  \u00d7                    \u2022 \u00d7                    \u2022 \u00d7\n\n\u00d7 = Centroide             \u00d7 se mueve              \u00d7 converge\n\u2022 = Punto de datos        \n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#22-explicacion-matematica","title":"2.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#funcion-objetivo","title":"Funci\u00f3n Objetivo","text":"<p>K-Means busca minimizar la inercia (Within-Cluster Sum of Squares - WCSS):</p> \\[J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\] <p>Donde: - \\(k\\) = n\u00famero de clusters - \\(C_i\\) = conjunto de puntos en el cluster \\(i\\) - \\(\\mu_i\\) = centroide del cluster \\(i\\) - \\(||x - \\mu_i||^2\\) = distancia Euclidiana al cuadrado</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#paso-de-asignacion","title":"Paso de Asignaci\u00f3n","text":"<p>Para cada punto \\(x_j\\), se asigna al cluster cuyo centroide est\u00e1 m\u00e1s cerca:</p> \\[c_j = \\arg\\min_{i} ||x_j - \\mu_i||^2\\] <p>Donde \\(c_j\\) es la asignaci\u00f3n de cluster para el punto \\(x_j\\).</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#paso-de-actualizacion","title":"Paso de Actualizaci\u00f3n","text":"<p>Cada centroide se actualiza como la media aritm\u00e9tica de todos los puntos asignados:</p> \\[\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x\\] <p>Donde \\(|C_i|\\) es el n\u00famero de puntos en el cluster \\(i\\).</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#distancia-euclidiana","title":"Distancia Euclidiana","text":"<p>La m\u00e9trica est\u00e1ndar utilizada:</p> \\[d(x, \\mu) = \\sqrt{\\sum_{j=1}^{p} (x_j - \\mu_j)^2}\\] <p>Donde \\(p\\) es el n\u00famero de dimensiones (features).</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#23-pros-y-contras","title":"2.3. Pros y Contras","text":"Ventajas Desventajas Simplicidad: F\u00e1cil de entender e implementar Requiere especificar k: El n\u00famero de clusters debe definirse a priori Eficiencia: Complejidad \\(O(n \\cdot k \\cdot i \\cdot d)\\) donde \\(i\\) = iteraciones, \\(d\\) = dimensiones Sensible a inicializaci\u00f3n: Puede converger a m\u00ednimos locales Escalabilidad: Funciona bien con grandes datasets Asume clusters esf\u00e9ricos: No funciona bien con clusters de formas irregulares Interpretabilidad: Los centroides tienen significado real Sensible a outliers: Los valores at\u00edpicos distorsionan los centroides Garant\u00eda de convergencia: Siempre converge (aunque quiz\u00e1 a \u00f3ptimo local) Clusters de tama\u00f1o similar: Tiende a crear clusters de tama\u00f1o comparable"},{"location":"aprendizaje-no-supervisado/02-kmeans/#24-ejemplo-basico-en-python","title":"2.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso m\u00e1s simple de K-Means con datos sint\u00e9ticos.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: K-Means con datos sint\u00e9ticos\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.cluster import KMeans          # Algoritmo K-Means\nfrom sklearn.datasets import make_blobs     # Generar datos sint\u00e9ticos\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS DE EJEMPLO\n# -------------------------------------------------------------\n# make_blobs crea clusters esf\u00e9ricos bien definidos\n# n_samples: n\u00famero total de puntos\n# centers: n\u00famero de clusters a generar\n# cluster_std: desviaci\u00f3n est\u00e1ndar de cada cluster (dispersi\u00f3n)\n# random_state: semilla para reproducibilidad\nX, y_true = make_blobs(\n    n_samples=300,      # 300 puntos en total\n    centers=4,          # 4 clusters reales\n    cluster_std=0.60,   # Clusters bastante compactos\n    random_state=42     # Reproducible\n)\n\nprint(f\"Forma de los datos: {X.shape}\")\nprint(f\"Primeras 5 filas:\\n{X[:5]}\")\n\n# -------------------------------------------------------------\n# 2. CREAR Y ENTRENAR EL MODELO K-MEANS\n# -------------------------------------------------------------\n# Instanciar el modelo con k=4 clusters\nkmeans = KMeans(\n    n_clusters=4,       # N\u00famero de clusters a formar\n    random_state=42     # Reproducibilidad\n)\n\n# fit_predict: entrena el modelo Y devuelve las etiquetas de cluster\n# Equivalente a: kmeans.fit(X) seguido de kmeans.predict(X)\nlabels = kmeans.fit_predict(X)\n\nprint(f\"\\nEtiquetas asignadas (primeras 10): {labels[:10]}\")\nprint(f\"Distribuci\u00f3n de clusters: {np.bincount(labels)}\")\n\n# -------------------------------------------------------------\n# 3. OBTENER INFORMACI\u00d3N DEL MODELO ENTRENADO\n# -------------------------------------------------------------\n# cluster_centers_: coordenadas de los centroides\ncentroids = kmeans.cluster_centers_\nprint(f\"\\nCentroides:\\n{centroids}\")\n\n# inertia_: suma de distancias al cuadrado (WCSS)\nprint(f\"\\nInercia (WCSS): {kmeans.inertia_:.2f}\")\n\n# n_iter_: n\u00famero de iteraciones hasta convergencia\nprint(f\"Iteraciones hasta convergencia: {kmeans.n_iter_}\")\n\n# -------------------------------------------------------------\n# 4. VISUALIZACI\u00d3N DE RESULTADOS\n# -------------------------------------------------------------\nplt.figure(figsize=(10, 8))\n\n# Graficar puntos coloreados por cluster asignado\n# c=labels asigna color seg\u00fan el cluster\n# cmap='viridis' es la paleta de colores\nscatter = plt.scatter(\n    X[:, 0], X[:, 1],           # Coordenadas x, y\n    c=labels,                    # Color seg\u00fan cluster\n    cmap='viridis',             # Paleta de colores\n    alpha=0.6,                   # Transparencia\n    edgecolors='w',             # Borde blanco\n    s=50                         # Tama\u00f1o de puntos\n)\n\n# Graficar centroides como estrellas rojas\nplt.scatter(\n    centroids[:, 0], centroids[:, 1],  # Coordenadas centroides\n    c='red',                            # Color rojo\n    marker='*',                         # Forma de estrella\n    s=300,                              # Tama\u00f1o grande\n    edgecolors='black',                 # Borde negro\n    linewidths=2,\n    label='Centroides'\n)\n\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('Feature 2', fontsize=12)\nplt.title('Clustering K-Means (k=4)', fontsize=14)\nplt.legend()\nplt.colorbar(scatter, label='Cluster')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Salida esperada: <pre><code>Forma de los datos: (300, 2)\nEtiquetas asignadas (primeras 10): [1 0 2 2 0 3 1 0 3 2]\nDistribuci\u00f3n de clusters: [75 75 75 75]\nInercia (WCSS): 211.93\nIteraciones hasta convergencia: 3\n</code></pre></p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#25-ejemplo-avanzado-pipeline-completo-con-hiperparametros","title":"2.5. Ejemplo Avanzado: Pipeline Completo con Hiperpar\u00e1metros","text":"<p>Este ejemplo implementa un an\u00e1lisis completo con selecci\u00f3n autom\u00e1tica de \\(k\\), evaluaci\u00f3n de m\u00e9tricas y optimizaci\u00f3n de hiperpar\u00e1metros.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: K-Means con Pipeline Completo\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.datasets import load_iris\n\n# -------------------------------------------------------------\n# 1. CARGAR Y PREPARAR DATOS REALES (IRIS)\n# -------------------------------------------------------------\n# Cargar el dataset Iris (sin usar las etiquetas)\niris = load_iris()\nX = iris.data                    # 150 muestras \u00d7 4 caracter\u00edsticas\nfeature_names = iris.feature_names\ny_true = iris.target             # Solo para validaci\u00f3n final\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS DE CLUSTERING K-MEANS - DATASET IRIS\")\nprint(\"=\"*60)\nprint(f\"\\nDimensiones del dataset: {X.shape}\")\nprint(f\"Caracter\u00edsticas: {feature_names}\")\n\n# -------------------------------------------------------------\n# 2. PREPROCESAMIENTO: ESTANDARIZACI\u00d3N\n# -------------------------------------------------------------\n# K-Means es sensible a la escala, por lo que debemos estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"\\nEstad\u00edsticas despu\u00e9s de estandarizar:\")\nprint(f\"  Media por feature: {X_scaled.mean(axis=0).round(2)}\")\nprint(f\"  Std por feature:   {X_scaled.std(axis=0).round(2)}\")\n\n# -------------------------------------------------------------\n# 3. M\u00c9TODO DEL CODO + AN\u00c1LISIS DE SILUETA\n# -------------------------------------------------------------\n# Probar diferentes valores de k y calcular m\u00e9tricas\nK_range = range(2, 11)  # k de 2 a 10\nmetrics = {\n    'k': [],\n    'inertia': [],\n    'silhouette': [],\n    'calinski': [],\n    'davies_bouldin': []\n}\n\nprint(\"\\nCalculando m\u00e9tricas para diferentes valores de k...\")\n\nfor k in K_range:\n    # Crear modelo con hiperpar\u00e1metros espec\u00edficos\n    kmeans = KMeans(\n        n_clusters=k,           # N\u00famero de clusters\n        init='k-means++',       # Inicializaci\u00f3n inteligente\n        n_init=10,              # N\u00famero de inicializaciones\n        max_iter=300,           # M\u00e1ximo de iteraciones\n        tol=1e-4,               # Tolerancia para convergencia\n        random_state=42         # Reproducibilidad\n    )\n\n    # Entrenar y obtener etiquetas\n    labels = kmeans.fit_predict(X_scaled)\n\n    # Almacenar m\u00e9tricas\n    metrics['k'].append(k)\n    metrics['inertia'].append(kmeans.inertia_)\n    metrics['silhouette'].append(silhouette_score(X_scaled, labels))\n    metrics['calinski'].append(calinski_harabasz_score(X_scaled, labels))\n    metrics['davies_bouldin'].append(davies_bouldin_score(X_scaled, labels))\n\n# Convertir a DataFrame para mejor visualizaci\u00f3n\ndf_metrics = pd.DataFrame(metrics)\nprint(\"\\nM\u00e9tricas por n\u00famero de clusters:\")\nprint(df_metrics.round(3).to_string(index=False))\n\n# -------------------------------------------------------------\n# 4. VISUALIZACI\u00d3N: M\u00c9TODO DEL CODO Y SILUETA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 4.1 M\u00e9todo del Codo (Inercia)\naxes[0, 0].plot(df_metrics['k'], df_metrics['inertia'], 'bo-', linewidth=2, markersize=8)\naxes[0, 0].set_xlabel('N\u00famero de Clusters (k)')\naxes[0, 0].set_ylabel('Inercia (WCSS)')\naxes[0, 0].set_title('M\u00e9todo del Codo')\naxes[0, 0].axvline(x=3, color='r', linestyle='--', alpha=0.7, label='k=3 (codo)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 4.2 Silhouette Score\naxes[0, 1].plot(df_metrics['k'], df_metrics['silhouette'], 'go-', linewidth=2, markersize=8)\naxes[0, 1].set_xlabel('N\u00famero de Clusters (k)')\naxes[0, 1].set_ylabel('Silhouette Score')\naxes[0, 1].set_title('An\u00e1lisis de Silueta')\naxes[0, 1].axvline(x=3, color='r', linestyle='--', alpha=0.7, label='k=3')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 4.3 Calinski-Harabasz Score\naxes[1, 0].plot(df_metrics['k'], df_metrics['calinski'], 'mo-', linewidth=2, markersize=8)\naxes[1, 0].set_xlabel('N\u00famero de Clusters (k)')\naxes[1, 0].set_ylabel('Calinski-Harabasz Score')\naxes[1, 0].set_title('\u00cdndice Calinski-Harabasz (mayor=mejor)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4.4 Davies-Bouldin Score\naxes[1, 1].plot(df_metrics['k'], df_metrics['davies_bouldin'], 'co-', linewidth=2, markersize=8)\naxes[1, 1].set_xlabel('N\u00famero de Clusters (k)')\naxes[1, 1].set_ylabel('Davies-Bouldin Score')\naxes[1, 1].set_title('\u00cdndice Davies-Bouldin (menor=mejor)')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 5. MODELO FINAL CON k \u00d3PTIMO\n# -------------------------------------------------------------\nk_optimo = 3  # Basado en an\u00e1lisis del codo y silueta\n\nprint(f\"\\n{'='*60}\")\nprint(f\"MODELO FINAL CON k={k_optimo}\")\nprint(f\"{'='*60}\")\n\n# Modelo final con todos los hiperpar\u00e1metros optimizados\nkmeans_final = KMeans(\n    n_clusters=k_optimo,    # N\u00famero \u00f3ptimo de clusters\n    init='k-means++',       # Inicializaci\u00f3n inteligente (ver secci\u00f3n 2.6)\n    n_init=20,              # M\u00e1s inicializaciones para robustez\n    max_iter=500,           # M\u00e1s iteraciones permitidas\n    tol=1e-6,               # Mayor precisi\u00f3n en convergencia\n    algorithm='lloyd',      # Algoritmo cl\u00e1sico de Lloyd\n    random_state=42\n)\n\n# Entrenar modelo final\nlabels_final = kmeans_final.fit_predict(X_scaled)\n\n# M\u00e9tricas finales\nprint(f\"\\nResultados del clustering:\")\nprint(f\"  - Inercia: {kmeans_final.inertia_:.2f}\")\nprint(f\"  - Silueta: {silhouette_score(X_scaled, labels_final):.4f}\")\nprint(f\"  - Iteraciones: {kmeans_final.n_iter_}\")\n\n# Distribuci\u00f3n de clusters\nprint(f\"\\nDistribuci\u00f3n de puntos por cluster:\")\nfor i in range(k_optimo):\n    count = np.sum(labels_final == i)\n    print(f\"  Cluster {i}: {count} puntos ({count/len(labels_final)*100:.1f}%)\")\n\n# -------------------------------------------------------------\n# 6. AN\u00c1LISIS DE LOS CENTROIDES\n# -------------------------------------------------------------\nprint(f\"\\nCentroides (valores estandarizados):\")\ncentroids_df = pd.DataFrame(\n    kmeans_final.cluster_centers_,\n    columns=feature_names,\n    index=[f'Cluster {i}' for i in range(k_optimo)]\n)\nprint(centroids_df.round(3))\n\n# Centroides en escala original\ncentroids_original = scaler.inverse_transform(kmeans_final.cluster_centers_)\nprint(f\"\\nCentroides (valores originales):\")\ncentroids_original_df = pd.DataFrame(\n    centroids_original,\n    columns=feature_names,\n    index=[f'Cluster {i}' for i in range(k_optimo)]\n)\nprint(centroids_original_df.round(2))\n\n# -------------------------------------------------------------\n# 7. PERFIL DE CADA CLUSTER\n# -------------------------------------------------------------\nprint(f\"\\n{'='*60}\")\nprint(\"PERFIL DE CADA CLUSTER\")\nprint(f\"{'='*60}\")\n\n# Crear DataFrame con datos y etiquetas\ndf_result = pd.DataFrame(X, columns=feature_names)\ndf_result['cluster'] = labels_final\n\n# Estad\u00edsticas por cluster\nprint(\"\\nMedia por cluster:\")\nprint(df_result.groupby('cluster').mean().round(2))\n\nprint(\"\\nDesviaci\u00f3n est\u00e1ndar por cluster:\")\nprint(df_result.groupby('cluster').std().round(2))\n\n# -------------------------------------------------------------\n# 8. VALIDACI\u00d3N CON ETIQUETAS REALES (si disponibles)\n# -------------------------------------------------------------\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nprint(f\"\\n{'='*60}\")\nprint(\"VALIDACI\u00d3N CON ETIQUETAS REALES\")\nprint(f\"{'='*60}\")\n\nari = adjusted_rand_score(y_true, labels_final)\nnmi = normalized_mutual_info_score(y_true, labels_final)\n\nprint(f\"\\n  Adjusted Rand Index (ARI): {ari:.4f}\")\nprint(f\"  Normalized Mutual Info (NMI): {nmi:.4f}\")\n\n# Matriz de contingencia\nprint(\"\\nMatriz de contingencia (filas=clusters, cols=especies reales):\")\ncontingency = pd.crosstab(\n    pd.Series(labels_final, name='Cluster'),\n    pd.Series(y_true, name='Especie Real')\n)\ncontingency.columns = iris.target_names\nprint(contingency)\n\n# -------------------------------------------------------------\n# 9. VISUALIZACI\u00d3N 2D CON PCA\n# -------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n\n# Reducir a 2D para visualizaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ncentroids_pca = pca.transform(kmeans_final.cluster_centers_)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Clusters encontrados\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_final, \n                           cmap='viridis', alpha=0.6, s=50, edgecolors='w')\naxes[0].scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', \n                marker='*', s=300, edgecolors='black', linewidths=2, label='Centroides')\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\naxes[0].set_title('Clusters K-Means')\naxes[0].legend()\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# Clases reales para comparaci\u00f3n\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, \n                           cmap='viridis', alpha=0.6, s=50, edgecolors='w')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\naxes[1].set_title('Clases Reales (Ground Truth)')\nplt.colorbar(scatter2, ax=axes[1], label='Especie')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#26-hiperparametros-de-k-means-en-scikit-learn","title":"2.6. Hiperpar\u00e1metros de K-Means en scikit-learn","text":"Hiperpar\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_clusters</code> N\u00famero de clusters a formar int &gt; 0 Usar m\u00e9todo del codo o silueta <code>init</code> M\u00e9todo de inicializaci\u00f3n de centroides <code>'k-means++'</code>, <code>'random'</code>, array <code>'k-means++'</code> (por defecto, m\u00e1s robusto) <code>n_init</code> N\u00famero de inicializaciones diferentes int &gt; 0 10 (default) o m\u00e1s para resultados robustos <code>max_iter</code> M\u00e1ximo de iteraciones por inicializaci\u00f3n int &gt; 0 300 (default), aumentar si no converge <code>tol</code> Tolerancia para declarar convergencia float &gt; 0 1e-4 (default) <code>algorithm</code> Algoritmo de K-Means <code>'lloyd'</code>, <code>'elkan'</code> <code>'lloyd'</code> para datasets densos <code>random_state</code> Semilla para reproducibilidad int o None Fijar para reproducibilidad"},{"location":"aprendizaje-no-supervisado/02-kmeans/#inicializacion-k-means","title":"Inicializaci\u00f3n K-Means++","text":"<p>La inicializaci\u00f3n <code>k-means++</code> es crucial para obtener buenos resultados. Funciona as\u00ed:</p> <ol> <li>Seleccionar el primer centroide aleatoriamente</li> <li>Para cada centroide siguiente:</li> <li>Calcular la distancia de cada punto al centroide m\u00e1s cercano ya seleccionado</li> <li>Seleccionar el siguiente centroide con probabilidad proporcional a \\(D(x)^2\\)</li> </ol> <p>Esto asegura que los centroides iniciales est\u00e9n bien distribuidos en el espacio de datos.</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#27-limitaciones-y-cuando-no-usar-k-means","title":"2.7. Limitaciones y Cu\u00e1ndo NO Usar K-Means","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#formas-de-clusters-no-esfericas","title":"Formas de Clusters No Esf\u00e9ricas","text":"<p>K-Means asume clusters convexos y esf\u00e9ricos. Falla con:</p> <pre><code>from sklearn.datasets import make_moons\n\n# Datos con forma de media luna\nX_moons, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n\n# K-Means NO funcionar\u00e1 bien aqu\u00ed\nkmeans = KMeans(n_clusters=2, random_state=42)\nlabels_moons = kmeans.fit_predict(X_moons)\n\n# Visualizar el problema\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap='viridis')\nplt.title('K-Means (INCORRECTO)')\n\n# Alternativa: DBSCAN maneja esto mejor\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=0.2, min_samples=5)\nlabels_dbscan = dbscan.fit_predict(X_moons)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_dbscan, cmap='viridis')\nplt.title('DBSCAN (CORRECTO)')\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#clusters-de-tamanos-muy-diferentes","title":"Clusters de Tama\u00f1os Muy Diferentes","text":"<p>K-Means tiende a crear clusters de tama\u00f1o similar, lo cual puede ser problem\u00e1tico:</p> <pre><code># Clusters de tama\u00f1os muy desiguales\nX1 = np.random.randn(500, 2) * 0.5 + [0, 0]    # Cluster grande\nX2 = np.random.randn(50, 2) * 0.5 + [5, 5]     # Cluster peque\u00f1o\nX_unequal = np.vstack([X1, X2])\n\n# K-Means puede dividir el cluster grande en lugar de encontrar el peque\u00f1o\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#28-aplicaciones-reales-de-k-means","title":"2.8. Aplicaciones Reales de K-Means","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#1-segmentacion-de-clientes-marketing","title":"1. Segmentaci\u00f3n de Clientes (Marketing)","text":"<pre><code># Ejemplo: RFM Analysis (Recency, Frequency, Monetary)\n# Datos t\u00edpicos: \u00faltima compra, frecuencia de compras, gasto total\n# K-Means agrupa clientes en segmentos como:\n# - VIP (alta frecuencia, alto gasto)\n# - En riesgo (baja recencia)\n# - Nuevos (alta recencia, baja frecuencia)\n</code></pre> <ul> <li>Tutorial completo: Customer Segmentation with K-Means</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#2-compresion-de-imagenes","title":"2. Compresi\u00f3n de Im\u00e1genes","text":"<pre><code># K-Means puede reducir los colores de una imagen\n# Los centroides representan la paleta de colores reducida\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\n\ndef compress_image(image_path, n_colors=16):\n    \"\"\"Comprime una imagen reduciendo el n\u00famero de colores.\"\"\"\n    img = np.array(Image.open(image_path))\n    pixels = img.reshape(-1, 3)  # Aplanar a lista de p\u00edxeles RGB\n\n    kmeans = KMeans(n_clusters=n_colors, random_state=42)\n    labels = kmeans.fit_predict(pixels)\n\n    # Reemplazar cada p\u00edxel por el color de su centroide\n    compressed = kmeans.cluster_centers_[labels].reshape(img.shape)\n    return compressed.astype(np.uint8)\n</code></pre> <ul> <li>Tutorial completo: Image Compression with K-Means</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#3-deteccion-de-anomalias-simple","title":"3. Detecci\u00f3n de Anomal\u00edas Simple","text":"<pre><code># Puntos muy lejos de todos los centroides pueden ser anomal\u00edas\ndistances = kmeans.transform(X)  # Distancia a cada centroide\nmin_distances = distances.min(axis=1)  # Distancia al centroide m\u00e1s cercano\nthreshold = np.percentile(min_distances, 95)  # Top 5% como anomal\u00edas\nanomalies = min_distances &gt; threshold\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#4-preprocesamiento-para-otros-algoritmos","title":"4. Preprocesamiento para Otros Algoritmos","text":"<ul> <li>Feature Engineering: Las distancias a centroides como nuevas features</li> <li>Inicializaci\u00f3n: Clusters de K-Means para inicializar GMM u otros modelos</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#29-variantes-de-k-means","title":"2.9. Variantes de K-Means","text":"Variante Descripci\u00f3n Uso Mini-Batch K-Means Usa subconjuntos aleatorios en cada iteraci\u00f3n Datasets muy grandes K-Medoids (PAM) Usa puntos reales como centroides (medoides) Robustez a outliers Fuzzy C-Means Asignaci\u00f3n probabil\u00edstica a m\u00faltiples clusters Cuando la pertenencia no es binaria K-Means++ Mejor inicializaci\u00f3n de centroides Est\u00e1ndar en sklearn Bisecting K-Means Divisi\u00f3n jer\u00e1rquica de clusters Clusters jer\u00e1rquicos"},{"location":"aprendizaje-no-supervisado/02-kmeans/#mini-batch-k-means-para-big-data","title":"Mini-Batch K-Means para Big Data","text":"<pre><code>from sklearn.cluster import MiniBatchKMeans\n\n# Para datasets de millones de puntos\nmbkmeans = MiniBatchKMeans(\n    n_clusters=5,\n    batch_size=1000,       # Tama\u00f1o del mini-batch\n    max_iter=100,\n    random_state=42\n)\nlabels = mbkmeans.fit_predict(X_large)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#210-resumen-y-mejores-practicas","title":"2.10. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#checklist-para-usar-k-means","title":"Checklist para usar K-Means","text":"<ul> <li>[ ] Escalar los datos (StandardScaler o MinMaxScaler)</li> <li>[ ] Determinar k usando m\u00e9todo del codo y/o silueta</li> <li>[ ] Usar init='k-means++' para mejor inicializaci\u00f3n</li> <li>[ ] Aumentar n_init (10-20) para robustez</li> <li>[ ] Verificar convergencia (revisar <code>n_iter_</code>)</li> <li>[ ] Evaluar con m\u00faltiples m\u00e9tricas (silueta, Calinski-Harabasz)</li> <li>[ ] Visualizar resultados para interpretaci\u00f3n</li> <li>[ ] Validar con conocimiento del dominio</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#cuando-elegir-k-means","title":"\u00bfCu\u00e1ndo elegir K-Means?","text":"<p>\u2705 Usar K-Means cuando: - Los clusters son aproximadamente esf\u00e9ricos - Se conoce (o puede estimarse) el n\u00famero de clusters - El dataset es grande (escalabilidad) - Se necesita interpretabilidad (centroides)</p> <p>\u274c Considerar alternativas cuando: - Clusters de formas arbitrarias \u2192 DBSCAN - Clusters jer\u00e1rquicos \u2192 Agglomerative Clustering - Outliers significativos \u2192 K-Medoids - Incertidumbre en el n\u00famero de clusters \u2192 DBSCAN, Mean Shift</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/","title":"\ud83d\udd2c Unidad 3. Algoritmo DBSCAN","text":"<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad. A diferencia de K-Means, DBSCAN puede descubrir clusters de formas arbitrarias y es capaz de identificar autom\u00e1ticamente puntos de ruido (outliers). No requiere especificar el n\u00famero de clusters a priori, lo cual lo hace extremadamente \u00fatil cuando no conocemos la estructura de los datos.</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#31-como-funciona-dbscan","title":"3.1. \u00bfC\u00f3mo Funciona DBSCAN?","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#intuicion-del-algoritmo","title":"Intuici\u00f3n del Algoritmo","text":"<p>DBSCAN agrupa puntos que est\u00e1n densamente empaquetados juntos, marcando como outliers los puntos que est\u00e1n aislados en regiones de baja densidad. La idea central es:</p> <p>\"Un cluster es una regi\u00f3n de alta densidad separada de otras regiones por \u00e1reas de baja densidad.\"</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#conceptos-fundamentales","title":"Conceptos Fundamentales","text":"<p>DBSCAN define tres tipos de puntos bas\u00e1ndose en dos par\u00e1metros: \u03b5 (epsilon) y MinPts:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 CONCEPTOS CLAVE DE DBSCAN                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u03b5 (epsilon): Radio de vecindad                             \u2502\n\u2502  MinPts: N\u00famero m\u00ednimo de puntos para formar un n\u00facleo      \u2502\n\u2502                                                             \u2502\n\u2502  TIPOS DE PUNTOS:                                           \u2502\n\u2502                                                             \u2502\n\u2502  1. PUNTO N\u00daCLEO (Core Point):                              \u2502\n\u2502     Tiene al menos MinPts puntos dentro de su \u03b5-vecindad    \u2502\n\u2502     (incluy\u00e9ndose a s\u00ed mismo)                               \u2502\n\u2502                                                             \u2502\n\u2502  2. PUNTO FRONTERA (Border Point):                          \u2502\n\u2502     No es un punto n\u00facleo, pero est\u00e1 dentro de la           \u2502\n\u2502     \u03b5-vecindad de un punto n\u00facleo                           \u2502\n\u2502                                                             \u2502\n\u2502  3. PUNTO DE RUIDO (Noise Point):                           \u2502\n\u2502     No es ni punto n\u00facleo ni punto frontera                 \u2502\n\u2502     \u2192 Se considera OUTLIER                                  \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#visualizacion-de-conceptos","title":"Visualizaci\u00f3n de Conceptos","text":"<pre><code>        \u03b5-vecindad\n           ___\n         /     \\\n        |   \u25cf   |  \u2190 Punto N\u00facleo (\u2265 MinPts vecinos)\n        | \u25cf \u25cf \u25cf |\n         \\_\u25cf_\u25cf_/\n\n     \u25cb               \u2190 Punto Frontera (en vecindad de n\u00facleo)\n\n                 \u00d7   \u2190 Punto de Ruido (aislado)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#algoritmo-paso-a-paso","title":"Algoritmo Paso a Paso","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ALGORITMO DBSCAN                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Entrada: Datos X, epsilon \u03b5, MinPts                         \u2502\n\u2502 Salida: Asignaciones de cluster (o -1 para ruido)           \u2502\n\u2502                                                             \u2502\n\u2502 1. Para cada punto P no visitado:                           \u2502\n\u2502    a) Marcar P como visitado                                \u2502\n\u2502    b) Encontrar todos los vecinos de P en radio \u03b5           \u2502\n\u2502    c) SI |vecinos| &lt; MinPts:                                \u2502\n\u2502          Marcar P como RUIDO (temporalmente)                \u2502\n\u2502       SINO:                                                 \u2502\n\u2502          Crear nuevo cluster C                              \u2502\n\u2502          Agregar P a C                                      \u2502\n\u2502          Para cada punto Q en vecinos:                      \u2502\n\u2502            - Si Q no visitado: visitar y expandir           \u2502\n\u2502            - Si Q no asignado: agregar Q a C                \u2502\n\u2502                                                             \u2502\n\u2502 2. Los puntos de ruido que est\u00e1n en la \u03b5-vecindad de un    \u2502\n\u2502    punto n\u00facleo se reasignan como puntos frontera           \u2502\n\u2502                                                             \u2502\n\u2502 NOTA: No hay fase de \"entrenamiento\" tradicional            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#32-explicacion-matematica","title":"3.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#-vecindad-epsilon-neighborhood","title":"\u03b5-Vecindad (Epsilon-Neighborhood)","text":"<p>La \u03b5-vecindad de un punto \\(p\\) es el conjunto de todos los puntos que est\u00e1n a una distancia menor o igual a \u03b5:</p> \\[N_\\varepsilon(p) = \\{q \\in D \\mid dist(p, q) \\leq \\varepsilon\\}\\] <p>Donde: - \\(D\\) = conjunto de datos - \\(dist(p, q)\\) = funci\u00f3n de distancia (t\u00edpicamente Euclidiana)</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#definiciones-formales","title":"Definiciones Formales","text":"<p>Punto N\u00facleo (Core Point): Un punto \\(p\\) es un punto n\u00facleo si: \\(\\(|N_\\varepsilon(p)| \\geq MinPts\\)\\)</p> <p>Directamente Alcanzable por Densidad: Un punto \\(q\\) es directamente alcanzable por densidad desde \\(p\\) si: 1. \\(p\\) es un punto n\u00facleo 2. \\(q \\in N_\\varepsilon(p)\\)</p> <p>Alcanzable por Densidad: Un punto \\(q\\) es alcanzable por densidad desde \\(p\\) si existe una cadena de puntos \\(p_1, p_2, ..., p_n\\) donde: - \\(p_1 = p\\) - \\(p_n = q\\) - \\(p_{i+1}\\) es directamente alcanzable por densidad desde \\(p_i\\)</p> <p>Conectado por Densidad: Dos puntos \\(p\\) y \\(q\\) est\u00e1n conectados por densidad si existe un punto \\(o\\) tal que tanto \\(p\\) como \\(q\\) son alcanzables por densidad desde \\(o\\).</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#definicion-de-cluster","title":"Definici\u00f3n de Cluster","text":"<p>Un cluster \\(C\\) en DBSCAN satisface: 1. Maximalidad: Si \\(p \\in C\\) y \\(q\\) es alcanzable por densidad desde \\(p\\), entonces \\(q \\in C\\) 2. Conectividad: Todos los puntos en \\(C\\) est\u00e1n conectados por densidad</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#metricas-de-distancia","title":"M\u00e9tricas de Distancia","text":"<p>DBSCAN puede usar diferentes m\u00e9tricas de distancia:</p> <p>Distancia Euclidiana (por defecto): \\(\\(d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\\)\\)</p> <p>Distancia Manhattan: \\(\\(d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\\)\\)</p> <p>Distancia de Minkowski (generalizaci\u00f3n): \\(\\(d(p, q) = \\left(\\sum_{i=1}^{n} |p_i - q_i|^p\\right)^{1/p}\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#33-pros-y-contras","title":"3.3. Pros y Contras","text":"Ventajas Desventajas No requiere k: No necesita especificar el n\u00famero de clusters a priori Sensible a par\u00e1metros: \u03b5 y MinPts son cr\u00edticos y dif\u00edciles de elegir Formas arbitrarias: Puede encontrar clusters de cualquier forma Densidad variable: No maneja bien clusters con densidades muy diferentes Detecci\u00f3n de outliers: Identifica autom\u00e1ticamente puntos de ruido Alta dimensionalidad: Sufre de la \"maldici\u00f3n de la dimensionalidad\" Robusto a outliers: Los outliers no afectan los centroides (no los hay) Computacionalmente costoso: \\(O(n^2)\\) sin \u00edndice espacial Determin\u00edstico: Mismo resultado (casi) con mismos par\u00e1metros Puntos frontera ambiguos: Pueden asignarse arbitrariamente si est\u00e1n entre clusters"},{"location":"aprendizaje-no-supervisado/03-dbscan/#34-ejemplo-basico-en-python","title":"3.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico de DBSCAN con datos que tienen formas no esf\u00e9ricas.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: DBSCAN con datos de forma arbitraria\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.cluster import DBSCAN          # Algoritmo DBSCAN\nfrom sklearn.datasets import make_moons     # Datos en forma de luna\nfrom sklearn.preprocessing import StandardScaler  # Escalado\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS DE EJEMPLO (FORMA DE MEDIA LUNA)\n# -------------------------------------------------------------\n# make_moons genera dos semic\u00edrculos entrelazados\n# Estos datos NO pueden ser correctamente agrupados por K-Means\nX, y_true = make_moons(\n    n_samples=300,      # 300 puntos en total (150 por luna)\n    noise=0.05,         # Peque\u00f1o ruido a\u00f1adido\n    random_state=42     # Reproducibilidad\n)\n\n# A\u00f1adir algunos outliers manualmente para demostraci\u00f3n\noutliers = np.array([[-0.5, 0.8], [1.5, -0.3], [2.0, 0.8]])\nX = np.vstack([X, outliers])\n\nprint(f\"Forma de los datos: {X.shape}\")\nprint(f\"Muestra de datos:\\n{X[:5]}\")\n\n# -------------------------------------------------------------\n# 2. PREPROCESAMIENTO: ESTANDARIZACI\u00d3N\n# -------------------------------------------------------------\n# Aunque DBSCAN no requiere estrictamente escalado,\n# es buena pr\u00e1ctica para que epsilon sea m\u00e1s interpretable\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. CREAR Y APLICAR DBSCAN\n# -------------------------------------------------------------\n# Instanciar DBSCAN con par\u00e1metros b\u00e1sicos\ndbscan = DBSCAN(\n    eps=0.3,            # Radio de vecindad (epsilon)\n    min_samples=5       # M\u00ednimo de puntos para ser n\u00facleo (MinPts)\n)\n\n# fit_predict: aplica DBSCAN y devuelve las etiquetas\n# Etiquetas: 0, 1, 2, ... para clusters\n#            -1 para puntos de RUIDO (outliers)\nlabels = dbscan.fit_predict(X_scaled)\n\nprint(f\"\\nEtiquetas \u00fanicas: {np.unique(labels)}\")\nprint(f\"N\u00famero de clusters encontrados: {len(set(labels)) - (1 if -1 in labels else 0)}\")\nprint(f\"N\u00famero de outliers: {np.sum(labels == -1)}\")\n\n# Distribuci\u00f3n de puntos por cluster\nprint(\"\\nDistribuci\u00f3n de puntos:\")\nfor label in np.unique(labels):\n    count = np.sum(labels == label)\n    if label == -1:\n        print(f\"  Ruido (outliers): {count} puntos\")\n    else:\n        print(f\"  Cluster {label}: {count} puntos\")\n\n# -------------------------------------------------------------\n# 4. OBTENER INFORMACI\u00d3N ADICIONAL\n# -------------------------------------------------------------\n# core_sample_indices_: \u00edndices de los puntos n\u00facleo\ncore_samples = dbscan.core_sample_indices_\nprint(f\"\\nN\u00famero de puntos n\u00facleo: {len(core_samples)}\")\n\n# components_: los puntos n\u00facleo como array\nprint(f\"Forma de components_: {dbscan.components_.shape}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZACI\u00d3N DE RESULTADOS\n# -------------------------------------------------------------\nplt.figure(figsize=(12, 5))\n\n# 5.1 Clusters encontrados por DBSCAN\nplt.subplot(1, 2, 1)\n\n# Crear m\u00e1scara para puntos n\u00facleo\ncore_mask = np.zeros_like(labels, dtype=bool)\ncore_mask[core_samples] = True\n\n# Colores: usar colormap para clusters, negro para ruido\nunique_labels = set(labels)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        # Puntos de ruido en negro\n        color = 'black'\n        marker = 'x'\n        size = 50\n        label_name = 'Ruido'\n    else:\n        marker = 'o'\n        size = 50\n        label_name = f'Cluster {label}'\n\n    mask = labels == label\n    plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1],\n                c=[color], marker=marker, s=size,\n                label=label_name, alpha=0.7, edgecolors='w')\n\n# Resaltar puntos n\u00facleo con borde\nplt.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n            facecolors='none', edgecolors='red', s=100,\n            linewidths=1.5, label='Puntos n\u00facleo')\n\nplt.xlabel('Feature 1 (escalada)')\nplt.ylabel('Feature 2 (escalada)')\nplt.title(f'DBSCAN (eps={dbscan.eps}, min_samples={dbscan.min_samples})')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\n\n# 5.2 Comparaci\u00f3n con las clases reales\nplt.subplot(1, 2, 2)\ny_true_extended = np.concatenate([y_true, [-1, -1, -1]])  # Outliers\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_true_extended,\n            cmap='viridis', alpha=0.7, edgecolors='w', s=50)\nplt.xlabel('Feature 1 (escalada)')\nplt.ylabel('Feature 2 (escalada)')\nplt.title('Datos originales (Ground Truth)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Salida esperada: <pre><code>Forma de los datos: (303, 2)\nEtiquetas \u00fanicas: [-1  0  1]\nN\u00famero de clusters encontrados: 2\nN\u00famero de outliers: 3\nDistribuci\u00f3n de puntos:\n  Ruido (outliers): 3 puntos\n  Cluster 0: 150 puntos\n  Cluster 1: 150 puntos\n</code></pre></p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#35-ejemplo-avanzado-seleccion-de-parametros-y-analisis-completo","title":"3.5. Ejemplo Avanzado: Selecci\u00f3n de Par\u00e1metros y An\u00e1lisis Completo","text":"<p>Este ejemplo muestra c\u00f3mo elegir los par\u00e1metros \u03b5 y MinPts de forma sistem\u00e1tica.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: DBSCAN con selecci\u00f3n de par\u00e1metros\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS CON RUIDO\n# -------------------------------------------------------------\n# Crear clusters con diferentes densidades y a\u00f1adir ruido\nnp.random.seed(42)\n\n# Cluster 1: denso\nX1 = np.random.randn(200, 2) * 0.5 + [0, 0]\n\n# Cluster 2: m\u00e1s disperso\nX2 = np.random.randn(150, 2) * 0.8 + [5, 5]\n\n# Cluster 3: alargado\nX3 = np.random.randn(100, 2) * np.array([0.3, 1.5]) + [2, -3]\n\n# Ruido uniforme\nnoise = np.random.uniform(-3, 8, size=(30, 2))\n\n# Combinar todos los datos\nX = np.vstack([X1, X2, X3, noise])\ny_true = np.array([0]*200 + [1]*150 + [2]*100 + [-1]*30)\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETO DE DBSCAN\")\nprint(\"=\"*60)\nprint(f\"\\nDatos: {X.shape[0]} puntos, {X.shape[1]} dimensiones\")\nprint(f\"Clusters reales: 3 + ruido\")\n\n# -------------------------------------------------------------\n# 2. PREPROCESAMIENTO\n# -------------------------------------------------------------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. M\u00c9TODO K-DISTANCE PARA ENCONTRAR EPSILON \u00d3PTIMO\n# -------------------------------------------------------------\n# El m\u00e9todo k-distance graph ayuda a encontrar un buen valor de \u03b5\n# Idea: Graficar la distancia al k-\u00e9simo vecino m\u00e1s cercano (ordenado)\n# Buscar el \"codo\" donde la distancia aumenta significativamente\n\ndef plot_k_distance(X, k=5):\n    \"\"\"\n    Grafica el k-distance para ayudar a elegir epsilon.\n\n    El m\u00e9todo funciona as\u00ed:\n    1. Para cada punto, calcula la distancia a su k-\u00e9simo vecino m\u00e1s cercano\n    2. Ordena estas distancias de menor a mayor\n    3. El \"codo\" de la curva sugiere un buen valor de epsilon\n    \"\"\"\n    # Calcular distancias a los k vecinos m\u00e1s cercanos\n    neighbors = NearestNeighbors(n_neighbors=k)\n    neighbors.fit(X)\n    distances, _ = neighbors.kneighbors(X)\n\n    # Tomar la distancia al k-\u00e9simo vecino (\u00faltima columna)\n    k_distances = distances[:, -1]\n\n    # Ordenar de menor a mayor\n    k_distances_sorted = np.sort(k_distances)\n\n    return k_distances_sorted\n\n# Probar diferentes valores de k\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor k in [4, 5, 10]:\n    k_dist = plot_k_distance(X_scaled, k=k)\n    axes[0].plot(range(len(k_dist)), k_dist, label=f'k={k}')\n\naxes[0].set_xlabel('Puntos (ordenados)')\naxes[0].set_ylabel(f'Distancia al k-\u00e9simo vecino')\naxes[0].set_title('M\u00e9todo K-Distance para elegir \u03b5')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Marcar el \"codo\" sugerido\nk_dist_5 = plot_k_distance(X_scaled, k=5)\nknee_idx = 450  # Aproximadamente donde est\u00e1 el codo\naxes[0].axhline(y=k_dist_5[knee_idx], color='r', linestyle='--', \n                label=f'\u03b5 sugerido \u2248 {k_dist_5[knee_idx]:.2f}')\naxes[0].axvline(x=knee_idx, color='r', linestyle='--', alpha=0.5)\naxes[0].legend()\n\n# Zoom en la regi\u00f3n del codo\naxes[1].plot(range(len(k_dist_5)), k_dist_5)\naxes[1].set_xlim([350, 500])\naxes[1].set_ylim([0, 1.5])\naxes[1].set_xlabel('Puntos (ordenados)')\naxes[1].set_ylabel('Distancia al 5-\u00e9simo vecino')\naxes[1].set_title('Zoom en la regi\u00f3n del codo')\naxes[1].axhline(y=k_dist_5[knee_idx], color='r', linestyle='--')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nEpsilon sugerido por m\u00e9todo k-distance: {k_dist_5[knee_idx]:.3f}\")\n\n# -------------------------------------------------------------\n# 4. B\u00daSQUEDA DE PAR\u00c1METROS \u00d3PTIMOS\n# -------------------------------------------------------------\n# Probar diferentes combinaciones de eps y min_samples\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"B\u00daSQUEDA DE PAR\u00c1METROS\")\nprint(\"=\"*60)\n\neps_values = [0.2, 0.3, 0.4, 0.5, 0.6]\nmin_samples_values = [3, 5, 7, 10]\n\nresults = []\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(X_scaled)\n\n        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n        n_noise = np.sum(labels == -1)\n\n        # Silhouette score (solo si hay m\u00e1s de 1 cluster y no todos son ruido)\n        if n_clusters &gt; 1 and n_noise &lt; len(labels) - 1:\n            # Excluir puntos de ruido para el c\u00e1lculo de silueta\n            mask = labels != -1\n            if np.sum(mask) &gt; n_clusters:\n                sil_score = silhouette_score(X_scaled[mask], labels[mask])\n            else:\n                sil_score = -1\n        else:\n            sil_score = -1\n\n        results.append({\n            'eps': eps,\n            'min_samples': min_samples,\n            'n_clusters': n_clusters,\n            'n_noise': n_noise,\n            'noise_pct': n_noise / len(labels) * 100,\n            'silhouette': sil_score\n        })\n\n# Mostrar resultados como DataFrame\ndf_results = pd.DataFrame(results)\nprint(\"\\nResultados de la b\u00fasqueda de par\u00e1metros:\")\nprint(df_results.round(3).to_string(index=False))\n\n# Encontrar mejor configuraci\u00f3n\nbest_idx = df_results[df_results['silhouette'] &gt; 0]['silhouette'].idxmax()\nbest_params = df_results.loc[best_idx]\nprint(f\"\\nMejor configuraci\u00f3n (por Silhouette):\")\nprint(f\"  eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\nprint(f\"  Clusters: {best_params['n_clusters']}, Silueta: {best_params['silhouette']:.3f}\")\n\n# -------------------------------------------------------------\n# 5. MODELO FINAL CON PAR\u00c1METROS \u00d3PTIMOS\n# -------------------------------------------------------------\neps_optimo = best_params['eps']\nmin_samples_optimo = int(best_params['min_samples'])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"MODELO FINAL: eps={eps_optimo}, min_samples={min_samples_optimo}\")\nprint(f\"{'='*60}\")\n\n# Crear modelo final\ndbscan_final = DBSCAN(\n    eps=eps_optimo,\n    min_samples=min_samples_optimo,\n    metric='euclidean',     # M\u00e9trica de distancia\n    algorithm='auto',       # Algoritmo para calcular vecinos\n    leaf_size=30,           # Tama\u00f1o de hoja para ball_tree o kd_tree\n    n_jobs=-1               # Usar todos los n\u00facleos disponibles\n)\n\nlabels_final = dbscan_final.fit_predict(X_scaled)\n\n# Estad\u00edsticas del modelo final\nn_clusters_final = len(set(labels_final)) - (1 if -1 in labels_final else 0)\nn_noise_final = np.sum(labels_final == -1)\nn_core = len(dbscan_final.core_sample_indices_)\n\nprint(f\"\\nResultados del modelo final:\")\nprint(f\"  Clusters encontrados: {n_clusters_final}\")\nprint(f\"  Puntos de ruido: {n_noise_final} ({n_noise_final/len(X)*100:.1f}%)\")\nprint(f\"  Puntos n\u00facleo: {n_core} ({n_core/len(X)*100:.1f}%)\")\n\n# Distribuci\u00f3n por cluster\nprint(\"\\nDistribuci\u00f3n de puntos por cluster:\")\nfor label in sorted(np.unique(labels_final)):\n    count = np.sum(labels_final == label)\n    if label == -1:\n        print(f\"  Ruido: {count} puntos ({count/len(X)*100:.1f}%)\")\n    else:\n        print(f\"  Cluster {label}: {count} puntos ({count/len(X)*100:.1f}%)\")\n\n# -------------------------------------------------------------\n# 6. AN\u00c1LISIS DE CALIDAD DEL CLUSTERING\n# -------------------------------------------------------------\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nprint(f\"\\n{'='*60}\")\nprint(\"M\u00c9TRICAS DE EVALUACI\u00d3N\")\nprint(f\"{'='*60}\")\n\n# Silueta (excluyendo ruido)\nmask_no_noise = labels_final != -1\nif np.sum(mask_no_noise) &gt; 0 and len(np.unique(labels_final[mask_no_noise])) &gt; 1:\n    sil_final = silhouette_score(X_scaled[mask_no_noise], labels_final[mask_no_noise])\n    print(f\"\\nSilhouette Score (sin ruido): {sil_final:.4f}\")\nelse:\n    print(\"\\nNo se puede calcular Silueta (muy pocos clusters o muchos outliers)\")\n\n# Comparaci\u00f3n con ground truth\nari = adjusted_rand_score(y_true, labels_final)\nnmi = normalized_mutual_info_score(y_true, labels_final)\nprint(f\"Adjusted Rand Index: {ari:.4f}\")\nprint(f\"Normalized Mutual Info: {nmi:.4f}\")\n\n# Matriz de contingencia\nprint(\"\\nMatriz de Contingencia:\")\ncontingency = pd.crosstab(\n    pd.Series(labels_final, name='DBSCAN'),\n    pd.Series(y_true, name='Real')\n)\nprint(contingency)\n\n# -------------------------------------------------------------\n# 7. VISUALIZACI\u00d3N COMPLETA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 7.1 Clusters DBSCAN\nax1 = axes[0, 0]\nunique_labels = set(labels_final)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        color = 'black'\n        marker = 'x'\n        alpha = 0.5\n    else:\n        marker = 'o'\n        alpha = 0.7\n\n    mask = labels_final == label\n    ax1.scatter(X_scaled[mask, 0], X_scaled[mask, 1],\n                c=[color], marker=marker, alpha=alpha, s=30)\n\n# Resaltar puntos n\u00facleo\ncore_mask = np.zeros_like(labels_final, dtype=bool)\ncore_mask[dbscan_final.core_sample_indices_] = True\nax1.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n            facecolors='none', edgecolors='red', s=80, linewidths=1)\nax1.set_title(f'DBSCAN (eps={eps_optimo}, min_samples={min_samples_optimo})')\nax1.set_xlabel('Feature 1')\nax1.set_ylabel('Feature 2')\nax1.grid(True, alpha=0.3)\n\n# 7.2 Ground Truth\nax2 = axes[0, 1]\nscatter = ax2.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_true,\n                      cmap='viridis', alpha=0.7, s=30)\nax2.set_title('Ground Truth')\nax2.set_xlabel('Feature 1')\nax2.set_ylabel('Feature 2')\nax2.grid(True, alpha=0.3)\n\n# 7.3 Puntos clasificados por tipo\nax3 = axes[1, 0]\n# Puntos n\u00facleo\nax3.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n            c='green', label=f'N\u00facleo ({np.sum(core_mask)})', alpha=0.6, s=30)\n# Puntos frontera (no n\u00facleo pero no ruido)\nborder_mask = ~core_mask &amp; (labels_final != -1)\nax3.scatter(X_scaled[border_mask, 0], X_scaled[border_mask, 1],\n            c='blue', label=f'Frontera ({np.sum(border_mask)})', alpha=0.6, s=30)\n# Ruido\nnoise_mask = labels_final == -1\nax3.scatter(X_scaled[noise_mask, 0], X_scaled[noise_mask, 1],\n            c='red', marker='x', label=f'Ruido ({np.sum(noise_mask)})', alpha=0.8, s=50)\nax3.set_title('Clasificaci\u00f3n de Puntos')\nax3.set_xlabel('Feature 1')\nax3.set_ylabel('Feature 2')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 7.4 Comparaci\u00f3n con K-Means\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels_kmeans = kmeans.fit_predict(X_scaled)\n\nax4 = axes[1, 1]\nax4.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_kmeans,\n            cmap='viridis', alpha=0.7, s=30)\nax4.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='*', s=200, edgecolors='black')\nax4.set_title('K-Means (k=3) para comparaci\u00f3n')\nax4.set_xlabel('Feature 1')\nax4.set_ylabel('Feature 2')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 8. HEUR\u00cdSTICA PARA ELEGIR MIN_SAMPLES\n# -------------------------------------------------------------\nprint(f\"\\n{'='*60}\")\nprint(\"HEUR\u00cdSTICAS PARA ELEGIR PAR\u00c1METROS\")\nprint(f\"{'='*60}\")\nprint(\"\"\"\nReglas generales para elegir par\u00e1metros:\n\n1. min_samples (MinPts):\n   - Regla b\u00e1sica: min_samples &gt;= dimensiones + 1\n   - En 2D: min_samples &gt;= 3 (m\u00ednimo te\u00f3rico)\n   - Pr\u00e1ctica com\u00fan: min_samples = 2 * dimensiones\n   - Para nuestros datos (2D): min_samples \u2248 4-5\n\n2. epsilon (eps):\n   - Usar el m\u00e9todo k-distance graph\n   - Buscar el \"codo\" donde la pendiente cambia\n   - k = min_samples para consistencia\n\n3. Iteraci\u00f3n:\n   - Empezar con min_samples = 2*dim\n   - Usar k-distance para eps inicial\n   - Ajustar bas\u00e1ndose en resultados\n\"\"\")\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#36-hiperparametros-de-dbscan-en-scikit-learn","title":"3.6. Hiperpar\u00e1metros de DBSCAN en scikit-learn","text":"Hiperpar\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>eps</code> Radio de vecindad \u03b5 float &gt; 0 Usar m\u00e9todo k-distance <code>min_samples</code> M\u00ednimo de puntos para ser n\u00facleo int &gt; 0 \\(\\geq dim + 1\\), t\u00edpico: \\(2 \\times dim\\) <code>metric</code> M\u00e9trica de distancia 'euclidean', 'manhattan', 'cosine', etc. 'euclidean' por defecto <code>algorithm</code> Algoritmo para calcular vecinos 'auto', 'ball_tree', 'kd_tree', 'brute' 'auto' (elige autom\u00e1ticamente) <code>leaf_size</code> Tama\u00f1o de hoja para ball_tree/kd_tree int &gt; 0 30 (default), ajustar para memoria <code>p</code> Potencia para Minkowski float &gt; 0 2 para Euclidiana, 1 para Manhattan <code>n_jobs</code> N\u00facleos para paralelizaci\u00f3n int o -1 -1 para usar todos"},{"location":"aprendizaje-no-supervisado/03-dbscan/#complejidad-computacional","title":"Complejidad Computacional","text":"Algoritmo Sin \u00edndice Con \u00edndice (kd_tree/ball_tree) Tiempo \\(O(n^2)\\) \\(O(n \\log n)\\) Espacio \\(O(n^2)\\) \\(O(n)\\)"},{"location":"aprendizaje-no-supervisado/03-dbscan/#37-aplicaciones-reales-de-dbscan","title":"3.7. Aplicaciones Reales de DBSCAN","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#1-deteccion-de-anomalias-en-sensores","title":"1. Detecci\u00f3n de Anomal\u00edas en Sensores","text":"<pre><code># Los puntos etiquetados como -1 son outliers autom\u00e1ticos\n# Ideal para detectar lecturas an\u00f3malas de sensores\nanomalies = X[labels == -1]\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#2-analisis-geoespacial","title":"2. An\u00e1lisis Geoespacial","text":"<p>DBSCAN es muy popular para agrupar datos geogr\u00e1ficos: - Clustering de ubicaciones GPS - Identificaci\u00f3n de zonas de alta densidad de eventos - Detecci\u00f3n de hotspots de crimen</p> <ul> <li>Tutorial: Geospatial Clustering with DBSCAN</li> </ul>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#3-segmentacion-de-imagenes","title":"3. Segmentaci\u00f3n de Im\u00e1genes","text":"<pre><code># Usar coordenadas de p\u00edxeles + valores RGB como features\n# DBSCAN agrupa p\u00edxeles similares y cercanos espacialmente\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#4-deteccion-de-comunidades-en-redes-sociales","title":"4. Detecci\u00f3n de Comunidades en Redes Sociales","text":"<ul> <li>Documentaci\u00f3n sklearn: DBSCAN Examples</li> </ul>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#38-dbscan-vs-k-means-cuando-usar-cada-uno","title":"3.8. DBSCAN vs K-Means: \u00bfCu\u00e1ndo Usar Cada Uno?","text":"Criterio K-Means DBSCAN Forma de clusters Esf\u00e9ricos Cualquier forma N\u00famero de clusters Debe especificarse Autom\u00e1tico Outliers Los asigna a un cluster Los identifica (-1) Escalabilidad Muy buena Moderada Densidad variable No maneja bien No maneja bien* Interpretabilidad Alta (centroides) Moderada <p>Para densidad variable, considerar OPTICS o HDBSCAN*.</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#ejemplo-visual-de-comparacion","title":"Ejemplo Visual de Comparaci\u00f3n","text":"<pre><code>from sklearn.datasets import make_circles\n\n# Datos conc\u00e9ntricos (K-Means fallar\u00e1)\nX_circles, _ = make_circles(n_samples=500, factor=0.5, noise=0.05)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Original\naxes[0].scatter(X_circles[:, 0], X_circles[:, 1], alpha=0.7)\naxes[0].set_title('Datos originales')\n\n# K-Means (incorrecto)\nlabels_km = KMeans(n_clusters=2).fit_predict(X_circles)\naxes[1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_km, cmap='viridis')\naxes[1].set_title('K-Means (INCORRECTO)')\n\n# DBSCAN (correcto)\nlabels_db = DBSCAN(eps=0.2, min_samples=5).fit_predict(X_circles)\naxes[2].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_db, cmap='viridis')\naxes[2].set_title('DBSCAN (CORRECTO)')\n\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#39-variantes-y-alternativas","title":"3.9. Variantes y Alternativas","text":"Algoritmo Descripci\u00f3n Cu\u00e1ndo usar OPTICS Ordenamiento de puntos para identificar estructura Clusters con densidades variables HDBSCAN DBSCAN jer\u00e1rquico Clusters con densidades variables, m\u00e1s robusto Mean Shift Basado en gradientes de densidad No requiere n\u00famero de clusters ni eps"},{"location":"aprendizaje-no-supervisado/03-dbscan/#hdbscan-hierarchical-dbscan","title":"HDBSCAN (Hierarchical DBSCAN)","text":"<pre><code># pip install hdbscan\nimport hdbscan\n\n# HDBSCAN maneja mejor la variaci\u00f3n de densidad\nclusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\nlabels = clusterer.fit_predict(X_scaled)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#310-resumen-y-mejores-practicas","title":"3.10. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#checklist-para-usar-dbscan","title":"Checklist para usar DBSCAN","text":"<ul> <li>[ ] Escalar los datos (especialmente si tienen diferentes unidades)</li> <li>[ ] Usar m\u00e9todo k-distance para estimar epsilon</li> <li>[ ] Empezar con min_samples = 2*dimensiones</li> <li>[ ] Probar varios valores de eps y min_samples</li> <li>[ ] Evaluar con silueta (excluyendo outliers)</li> <li>[ ] Analizar los outliers (pueden ser informaci\u00f3n valiosa)</li> <li>[ ] Visualizar resultados para validaci\u00f3n</li> </ul>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#cuando-elegir-dbscan","title":"\u00bfCu\u00e1ndo elegir DBSCAN?","text":"<p>\u2705 Usar DBSCAN cuando: - No conoces el n\u00famero de clusters - Los clusters tienen formas no esf\u00e9ricas - Hay outliers en los datos - Los clusters tienen densidades similares</p> <p>\u274c Considerar alternativas cuando: - Clusters con densidades muy diferentes \u2192 HDBSCAN, OPTICS - Datasets muy grandes \u2192 Mini-Batch K-Means - Necesitas centroides interpretables \u2192 K-Means - Datos de muy alta dimensionalidad \u2192 Reducir dimensiones primero</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/","title":"\ud83c\udf32 Unidad 4. Clustering Jer\u00e1rquico","text":"<p>El Clustering Jer\u00e1rquico es una familia de algoritmos que construyen una jerarqu\u00eda de clusters en lugar de una partici\u00f3n plana. Su caracter\u00edstica distintiva es que produce un dendrograma: una estructura de \u00e1rbol que muestra c\u00f3mo se forman o dividen los clusters a diferentes niveles de similitud. Este enfoque permite explorar la estructura de los datos a m\u00faltiples escalas sin necesidad de especificar el n\u00famero de clusters a priori.</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#41-como-funciona-el-clustering-jerarquico","title":"4.1. \u00bfC\u00f3mo Funciona el Clustering Jer\u00e1rquico?","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#dos-enfoques-principales","title":"Dos Enfoques Principales","text":"<p>Existen dos estrategias opuestas para construir la jerarqu\u00eda:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TIPOS DE CLUSTERING JER\u00c1RQUICO                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502 1. AGLOMERATIVO (Bottom-Up) - El m\u00e1s com\u00fan                  \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                    \u2502\n\u2502    \u2022 Empieza: cada punto es su propio cluster               \u2502\n\u2502    \u2022 Proceso: fusiona los dos clusters m\u00e1s cercanos         \u2502\n\u2502    \u2022 Termina: todos los puntos en un \u00fanico cluster          \u2502\n\u2502                                                             \u2502\n\u2502         \u25cb \u25cb \u25cb \u25cb \u25cb   \u2192   \u25cb\u25cb \u25cb \u25cb\u25cb   \u2192   \u25cb\u25cb\u25cb \u25cb\u25cb   \u2192   \u25cb\u25cb\u25cb\u25cb\u25cb    \u2502\n\u2502         5 clusters     4 clusters    2 clusters   1 cluster \u2502\n\u2502                                                             \u2502\n\u2502 2. DIVISIVO (Top-Down) - Menos com\u00fan                        \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                           \u2502\n\u2502    \u2022 Empieza: todos los puntos en un \u00fanico cluster          \u2502\n\u2502    \u2022 Proceso: divide el cluster menos coherente             \u2502\n\u2502    \u2022 Termina: cada punto es su propio cluster               \u2502\n\u2502                                                             \u2502\n\u2502         \u25cb\u25cb\u25cb\u25cb\u25cb   \u2192   \u25cb\u25cb\u25cb \u25cb\u25cb   \u2192   \u25cb\u25cb \u25cb \u25cb\u25cb   \u2192   \u25cb \u25cb \u25cb \u25cb \u25cb    \u2502\n\u2502         1 cluster   2 clusters   4 clusters   5 clusters    \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#algoritmo-aglomerativo-paso-a-paso","title":"Algoritmo Aglomerativo Paso a Paso","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ALGORITMO AGLOMERATIVO (AGNES)                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Entrada: Datos X con n puntos, criterio de enlace           \u2502\n\u2502 Salida: Dendrograma (\u00e1rbol de fusiones)                     \u2502\n\u2502                                                             \u2502\n\u2502 1. INICIALIZACI\u00d3N:                                          \u2502\n\u2502    - Crear n clusters (uno por cada punto)                  \u2502\n\u2502    - Calcular matriz de distancias entre todos los pares    \u2502\n\u2502                                                             \u2502\n\u2502 2. REPETIR n-1 veces:                                       \u2502\n\u2502    a) Encontrar los dos clusters m\u00e1s cercanos               \u2502\n\u2502    b) Fusionarlos en un nuevo cluster                       \u2502\n\u2502    c) Actualizar la matriz de distancias                    \u2502\n\u2502    d) Registrar la fusi\u00f3n y su altura en el dendrograma     \u2502\n\u2502                                                             \u2502\n\u2502 3. RESULTADO:                                               \u2502\n\u2502    - Dendrograma completo                                   \u2502\n\u2502    - Para obtener k clusters: \"cortar\" el dendrograma       \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#el-dendrograma","title":"El Dendrograma","text":"<p>El dendrograma es la visualizaci\u00f3n clave del clustering jer\u00e1rquico:</p> <pre><code>Altura\n(distancia)\n   \u2502\n   \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502     \u250c\u2500\u2500\u2500\u2524         \u2502\n   \u2502     \u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n   \u2502  \u250c\u2500\u2500\u2524        \u2502\n   \u2502  \u2502  \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n   \u2502\u2500\u2500\u2524      \u2502\n   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502         \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            A  B  C  D  E  (puntos)\n</code></pre> <ul> <li>Eje vertical: altura de fusi\u00f3n (distancia entre clusters fusionados)</li> <li>Eje horizontal: puntos o clusters individuales</li> <li>L\u00edneas horizontales: fusiones entre clusters</li> <li>Cortar horizontalmente: obtener un n\u00famero espec\u00edfico de clusters</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#42-explicacion-matematica","title":"4.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#matriz-de-distancias","title":"Matriz de Distancias","text":"<p>El clustering jer\u00e1rquico comienza calculando una matriz de distancias \\(D\\) de tama\u00f1o \\(n \\times n\\):</p> \\[D_{ij} = d(x_i, x_j)\\] <p>Donde \\(d\\) es una funci\u00f3n de distancia (t\u00edpicamente Euclidiana).</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#criterios-de-enlace-linkage","title":"Criterios de Enlace (Linkage)","text":"<p>La clave del clustering jer\u00e1rquico es c\u00f3mo se calcula la distancia entre clusters. Existen varios criterios de enlace:</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#1-enlace-simple-single-linkage-vecino-mas-cercano","title":"1. Enlace Simple (Single Linkage) - \"Vecino m\u00e1s cercano\"","text":"<p>Distancia entre los dos puntos m\u00e1s cercanos de cada cluster:</p> \\[d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x, y)\\] <ul> <li>\u2705 Puede detectar clusters de formas arbitrarias</li> <li>\u274c Sensible al \"efecto cadena\" (clusters elongados)</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#2-enlace-completo-complete-linkage-vecino-mas-lejano","title":"2. Enlace Completo (Complete Linkage) - \"Vecino m\u00e1s lejano\"","text":"<p>Distancia entre los dos puntos m\u00e1s lejanos de cada cluster:</p> \\[d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} d(x, y)\\] <ul> <li>\u2705 Produce clusters compactos de tama\u00f1o similar</li> <li>\u274c Sensible a outliers</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#3-enlace-promedio-average-linkage-upgma","title":"3. Enlace Promedio (Average Linkage - UPGMA)","text":"<p>Promedio de todas las distancias entre pares de puntos:</p> \\[d(C_i, C_j) = \\frac{1}{|C_i| \\cdot |C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} d(x, y)\\] <ul> <li>\u2705 Balance entre single y complete</li> <li>\u2705 Menos sensible a outliers</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#4-enlace-de-ward-wards-method","title":"4. Enlace de Ward (Ward's Method)","text":"<p>Minimiza el incremento en la varianza total al fusionar clusters:</p> \\[d(C_i, C_j) = \\sqrt{\\frac{2|C_i||C_j|}{|C_i|+|C_j|}} ||\\mu_i - \\mu_j||\\] <p>Donde \\(\\mu_i\\) y \\(\\mu_j\\) son los centroides de los clusters.</p> <p>Equivalentemente, Ward minimiza:</p> \\[\\Delta = \\sum_{x \\in C_i \\cup C_j} ||x - \\mu_{ij}||^2 - \\sum_{x \\in C_i} ||x - \\mu_i||^2 - \\sum_{x \\in C_j} ||x - \\mu_j||^2\\] <ul> <li>\u2705 Tiende a producir clusters esf\u00e9ricos y de tama\u00f1o similar</li> <li>\u2705 Similar a K-Means pero jer\u00e1rquico</li> <li>\u274c Asume clusters esf\u00e9ricos</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#visualizacion-de-criterios-de-enlace","title":"Visualizaci\u00f3n de Criterios de Enlace","text":"<pre><code>          Cluster A              Cluster B\n\n           \u25cf  \u25cf                    \u25b2  \u25b2\n         \u25cf      \u25cf                \u25b2      \u25b2\n           \u25cf  \u25cf                    \u25b2  \u25b2\n\nSingle:   d = distancia m\u00ednima (m\u00e1s corta)\nComplete: d = distancia m\u00e1xima (m\u00e1s larga)\nAverage:  d = promedio de todas las distancias\nWard:     d = incremento m\u00ednimo en varianza\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#43-pros-y-contras","title":"4.3. Pros y Contras","text":"Ventajas Desventajas No requiere k: El n\u00famero de clusters se elige despu\u00e9s Complejidad alta: \\(O(n^3)\\) tiempo, \\(O(n^2)\\) espacio Dendrograma informativo: Permite explorar estructura a m\u00faltiples niveles No escalable: Impracticable para datasets grandes (&gt;10K puntos) Flexibilidad: Diferentes criterios de enlace para diferentes necesidades Irreversible: Una fusi\u00f3n mala no puede deshacerse Determin\u00edstico: Mismo resultado cada ejecuci\u00f3n Sensible a outliers: Especialmente con complete linkage Interpretable: El dendrograma es f\u00e1cil de entender Sensible a la elecci\u00f3n de linkage: Resultados muy diferentes"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#44-ejemplo-basico-en-python","title":"4.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico del clustering jer\u00e1rquico con visualizaci\u00f3n del dendrograma.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: Clustering Jer\u00e1rquico con Dendrograma\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom scipy.cluster.hierarchy import (       # Funciones de scipy\n    linkage,        # Calcular el enlace jer\u00e1rquico\n    dendrogram,     # Crear el dendrograma\n    fcluster        # Obtener clusters del dendrograma\n)\nfrom sklearn.datasets import make_blobs     # Datos sint\u00e9ticos\nfrom sklearn.preprocessing import StandardScaler  # Escalado\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS DE EJEMPLO\n# -------------------------------------------------------------\n# Crear 4 clusters bien definidos\nX, y_true = make_blobs(\n    n_samples=50,       # 50 puntos (peque\u00f1o para visualizaci\u00f3n clara)\n    centers=4,          # 4 clusters\n    cluster_std=0.60,   # Dispersi\u00f3n moderada\n    random_state=42\n)\n\nprint(f\"Forma de los datos: {X.shape}\")\n\n# -------------------------------------------------------------\n# 2. CALCULAR LA MATRIZ DE ENLACE\n# -------------------------------------------------------------\n# linkage() calcula el clustering jer\u00e1rquico\n# Devuelve una matriz Z de (n-1) x 4:\n# - Columnas 0 y 1: \u00edndices de clusters fusionados\n# - Columna 2: distancia entre ellos (altura del enlace)\n# - Columna 3: n\u00famero de puntos en el nuevo cluster\n\nZ = linkage(\n    X,                  # Datos\n    method='ward',      # Criterio de enlace\n    metric='euclidean'  # M\u00e9trica de distancia\n)\n\nprint(f\"\\nMatriz de enlace Z (primeras 5 filas):\")\nprint(f\"[cluster_1, cluster_2, distancia, n_puntos]\")\nprint(Z[:5])\n\n# -------------------------------------------------------------\n# 3. VISUALIZAR EL DENDROGRAMA\n# -------------------------------------------------------------\nplt.figure(figsize=(14, 8))\n\n# dendrogram() crea la visualizaci\u00f3n del \u00e1rbol jer\u00e1rquico\n# truncate_mode='level' limita el n\u00famero de niveles mostrados\ndn = dendrogram(\n    Z,\n    truncate_mode='lastp',  # Mostrar los \u00faltimos p clusters fusionados\n    p=20,                    # N\u00famero de clusters a mostrar\n    leaf_rotation=90,        # Rotar etiquetas de hojas\n    leaf_font_size=10,       # Tama\u00f1o de fuente\n    show_contracted=True     # Mostrar clusters contra\u00eddos\n)\n\nplt.xlabel('Punto o Cluster', fontsize=12)\nplt.ylabel('Distancia (Altura)', fontsize=12)\nplt.title('Dendrograma - Clustering Jer\u00e1rquico (Ward Linkage)', fontsize=14)\n\n# A\u00f1adir l\u00ednea horizontal para \"cortar\" el dendrograma\n# Esta l\u00ednea indica d\u00f3nde cortar\u00edamos para obtener cierto n\u00famero de clusters\ncorte = 7  # Altura de corte\nplt.axhline(y=corte, color='r', linestyle='--', \n            label=f'Corte en altura={corte}')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 4. OBTENER ASIGNACIONES DE CLUSTER\n# -------------------------------------------------------------\n# fcluster() \"corta\" el dendrograma para obtener clusters\n# t: umbral de corte\n# criterion: c\u00f3mo interpretar t\n\n# Opci\u00f3n 1: Cortar por distancia (altura)\nlabels_dist = fcluster(Z, t=corte, criterion='distance')\nprint(f\"\\nClusters (corte por distancia={corte}): {np.unique(labels_dist)}\")\nprint(f\"Distribuci\u00f3n: {np.bincount(labels_dist)[1:]}\")  # [1:] porque fcluster empieza en 1\n\n# Opci\u00f3n 2: Especificar n\u00famero de clusters directamente\nlabels_k = fcluster(Z, t=4, criterion='maxclust')\nprint(f\"\\nClusters (k=4): {np.unique(labels_k)}\")\nprint(f\"Distribuci\u00f3n: {np.bincount(labels_k)[1:]}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZAR CLUSTERS RESULTANTES\n# -------------------------------------------------------------\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Clusters por corte de distancia\nscatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=labels_dist,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[0].set_title(f'Clusters (corte altura={corte})')\naxes[0].set_xlabel('Feature 1')\naxes[0].set_ylabel('Feature 2')\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# Clusters especificando k=4\nscatter2 = axes[1].scatter(X[:, 0], X[:, 1], c=labels_k,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[1].set_title('Clusters (k=4 especificado)')\naxes[1].set_xlabel('Feature 1')\naxes[1].set_ylabel('Feature 2')\nplt.colorbar(scatter2, ax=axes[1], label='Cluster')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2705 El dendrograma permite explorar la estructura a diferentes niveles\")\nprint(\"\u2705 Cortando a diferentes alturas obtenemos diferentes n\u00fameros de clusters\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#45-ejemplo-avanzado-comparacion-de-criterios-de-enlace","title":"4.5. Ejemplo Avanzado: Comparaci\u00f3n de Criterios de Enlace","text":"<p>Este ejemplo compara diferentes criterios de enlace y muestra c\u00f3mo elegir el n\u00famero \u00f3ptimo de clusters.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: An\u00e1lisis Completo de Clustering Jer\u00e1rquico\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\nfrom scipy.spatial.distance import pdist\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.datasets import load_iris\n\n# -------------------------------------------------------------\n# 1. CARGAR Y PREPARAR DATOS\n# -------------------------------------------------------------\niris = load_iris()\nX = iris.data\ny_true = iris.target\nfeature_names = iris.feature_names\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS DE CLUSTERING JER\u00c1RQUICO - DATASET IRIS\")\nprint(\"=\"*60)\nprint(f\"\\nDimensiones: {X.shape}\")\nprint(f\"Features: {feature_names}\")\n\n# Estandarizar datos\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 2. COMPARAR DIFERENTES CRITERIOS DE ENLACE\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARACI\u00d3N DE CRITERIOS DE ENLACE\")\nprint(\"=\"*60)\n\n# M\u00e9todos de enlace a comparar\nmethods = ['single', 'complete', 'average', 'ward']\nmethod_names = {\n    'single': 'Single (Vecino m\u00e1s cercano)',\n    'complete': 'Complete (Vecino m\u00e1s lejano)',\n    'average': 'Average (Promedio)',\n    'ward': 'Ward (Minimiza varianza)'\n}\n\n# Crear figura para dendrogramas\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\n# Almacenar resultados\nlinkage_results = {}\n\nfor idx, method in enumerate(methods):\n    # Calcular enlace\n    Z = linkage(X_scaled, method=method)\n    linkage_results[method] = Z\n\n    # Calcular correlaci\u00f3n cophenetica\n    # Mide qu\u00e9 tan bien el dendrograma preserva las distancias originales\n    c, _ = cophenet(Z, pdist(X_scaled))\n\n    # Dibujar dendrograma\n    ax = axes[idx]\n    dendrogram(Z, ax=ax, truncate_mode='lastp', p=30,\n               leaf_rotation=90, leaf_font_size=8,\n               show_contracted=True)\n    ax.set_title(f'{method_names[method]}\\nCorrelaci\u00f3n Cophenetica: {c:.3f}')\n    ax.set_xlabel('Muestra')\n    ax.set_ylabel('Distancia')\n\nplt.tight_layout()\nplt.show()\n\n# Mostrar correlaciones copheneticas\nprint(\"\\nCorrelaci\u00f3n Cophenetica por m\u00e9todo:\")\nprint(\"(Mayor = mejor preservaci\u00f3n de distancias originales)\")\nfor method in methods:\n    c, _ = cophenet(linkage_results[method], pdist(X_scaled))\n    print(f\"  {method_names[method]}: {c:.4f}\")\n\n# -------------------------------------------------------------\n# 3. AN\u00c1LISIS DEL DENDROGRAMA (Ward)\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS DETALLADO - M\u00c9TODO WARD\")\nprint(\"=\"*60)\n\nZ_ward = linkage_results['ward']\n\n# Analizar las \u00faltimas fusiones (m\u00e1s informativas)\nprint(\"\\n\u00daltimas 10 fusiones:\")\nprint(\"Fusi\u00f3n | Cluster1 | Cluster2 | Distancia | Tama\u00f1o\")\nprint(\"-\"*55)\nfor i in range(-10, 0):\n    row = Z_ward[i]\n    print(f\"{len(Z_ward)+i+1:6} | {int(row[0]):8} | {int(row[1]):8} | {row[2]:9.3f} | {int(row[3]):6}\")\n\n# -------------------------------------------------------------\n# 4. DETERMINAR N\u00daMERO \u00d3PTIMO DE CLUSTERS\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"DETERMINACI\u00d3N DEL N\u00daMERO \u00d3PTIMO DE CLUSTERS\")\nprint(\"=\"*60)\n\n# M\u00e9todo 1: An\u00e1lisis de las distancias de fusi\u00f3n\n# Buscar \"saltos\" grandes en las distancias\nheights = Z_ward[:, 2]\nheight_diffs = np.diff(heights)\n\n# Las \u00faltimas fusiones (las m\u00e1s significativas)\nprint(\"\\nSaltos de distancia en \u00faltimas fusiones:\")\nfor i in range(-5, 0):\n    n_clusters = len(Z_ward) - len(Z_ward) - i\n    print(f\"  {n_clusters} \u2192 {n_clusters-1} clusters: salto = {height_diffs[i]:.3f}\")\n\n# M\u00e9todo 2: Usar m\u00e9tricas de evaluaci\u00f3n\nk_range = range(2, 11)\nmetrics = {'k': [], 'silhouette': [], 'calinski': []}\n\nfor k in k_range:\n    labels = fcluster(Z_ward, t=k, criterion='maxclust')\n\n    metrics['k'].append(k)\n    metrics['silhouette'].append(silhouette_score(X_scaled, labels))\n    metrics['calinski'].append(calinski_harabasz_score(X_scaled, labels))\n\ndf_metrics = pd.DataFrame(metrics)\nprint(\"\\nM\u00e9tricas por n\u00famero de clusters:\")\nprint(df_metrics.round(4).to_string(index=False))\n\n# Visualizar m\u00e9tricas\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Dendrograma con l\u00edneas de corte\nax1 = axes[0]\ndendrogram(Z_ward, ax=ax1, truncate_mode='lastp', p=20)\nax1.axhline(y=8, color='r', linestyle='--', label='k=3')\nax1.axhline(y=5, color='g', linestyle='--', label='k=5')\nax1.set_title('Dendrograma Ward')\nax1.legend()\n\n# Silhouette\nax2 = axes[1]\nax2.plot(df_metrics['k'], df_metrics['silhouette'], 'bo-')\nax2.set_xlabel('N\u00famero de clusters')\nax2.set_ylabel('Silhouette Score')\nax2.set_title('Silueta vs k')\nax2.grid(True, alpha=0.3)\n\n# Calinski-Harabasz\nax3 = axes[2]\nax3.plot(df_metrics['k'], df_metrics['calinski'], 'go-')\nax3.set_xlabel('N\u00famero de clusters')\nax3.set_ylabel('Calinski-Harabasz')\nax3.set_title('Calinski-Harabasz vs k')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 5. MODELO FINAL CON SKLEARN\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODELO FINAL CON AgglomerativeClustering\")\nprint(\"=\"*60)\n\nk_optimo = 3  # Basado en an\u00e1lisis\n\n# Usando sklearn.cluster.AgglomerativeClustering\n# M\u00e1s flexible y permite especificar k directamente\nmodel = AgglomerativeClustering(\n    n_clusters=k_optimo,        # N\u00famero de clusters\n    metric='euclidean',         # M\u00e9trica de distancia\n    linkage='ward',             # Criterio de enlace\n    # Par\u00e1metros adicionales:\n    # compute_full_tree: bool, calcular \u00e1rbol completo aunque n_clusters est\u00e9 especificado\n    # distance_threshold: None o float, distancia para el corte (si se usa, n_clusters debe ser None)\n)\n\n# Entrenar y obtener etiquetas\nlabels = model.fit_predict(X_scaled)\n\nprint(f\"\\nResultados:\")\nprint(f\"  N\u00famero de clusters: {model.n_clusters_}\")\nprint(f\"  N\u00famero de hojas: {model.n_leaves_}\")\nprint(f\"  N\u00famero de componentes conectados: {model.n_connected_components_}\")\n\n# Distribuci\u00f3n\nprint(f\"\\nDistribuci\u00f3n de puntos:\")\nfor i in range(k_optimo):\n    count = np.sum(labels == i)\n    print(f\"  Cluster {i}: {count} puntos ({count/len(labels)*100:.1f}%)\")\n\n# M\u00e9tricas\nprint(f\"\\nM\u00e9tricas de evaluaci\u00f3n:\")\nprint(f\"  Silhouette Score: {silhouette_score(X_scaled, labels):.4f}\")\nprint(f\"  Calinski-Harabasz: {calinski_harabasz_score(X_scaled, labels):.4f}\")\n\n# -------------------------------------------------------------\n# 6. COMPARACI\u00d3N CON GROUND TRUTH\n# -------------------------------------------------------------\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"COMPARACI\u00d3N CON ETIQUETAS REALES\")\nprint(\"=\"*60)\n\nari = adjusted_rand_score(y_true, labels)\nnmi = normalized_mutual_info_score(y_true, labels)\n\nprint(f\"\\n  Adjusted Rand Index: {ari:.4f}\")\nprint(f\"  Normalized Mutual Info: {nmi:.4f}\")\n\n# Matriz de contingencia\nprint(\"\\nMatriz de Contingencia:\")\ncontingency = pd.crosstab(\n    pd.Series(labels, name='Cluster'),\n    pd.Series(y_true, name='Especie'),\n    margins=True\n)\ncontingency.columns = list(iris.target_names) + ['Total']\nprint(contingency)\n\n# -------------------------------------------------------------\n# 7. PERFIL DE CLUSTERS\n# -------------------------------------------------------------\nprint(f\"\\n\" + \"=\"*60)\nprint(\"PERFIL DE CADA CLUSTER\")\nprint(\"=\"*60)\n\ndf_result = pd.DataFrame(X, columns=feature_names)\ndf_result['cluster'] = labels\n\nprint(\"\\nMedia por cluster (valores originales):\")\nprint(df_result.groupby('cluster').mean().round(2))\n\n# -------------------------------------------------------------\n# 8. VISUALIZACI\u00d3N FINAL\n# -------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n\n# Reducir a 2D para visualizaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Clusters jer\u00e1rquicos\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[0].set_title('Clustering Jer\u00e1rquico (Ward)')\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\nplt.colorbar(scatter1, ax=axes[0])\n\n# Ground truth\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_true,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[1].set_title('Especies Reales')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\nplt.colorbar(scatter2, ax=axes[1])\n\n# Comparaci\u00f3n de m\u00e9todos de enlace (para k=3)\nlabels_by_method = {}\nfor method in methods:\n    labels_by_method[method] = fcluster(linkage_results[method], \n                                         t=3, criterion='maxclust')\n\n# Mostrar silueta por m\u00e9todo\nsilhouettes = [silhouette_score(X_scaled, labels_by_method[m]) for m in methods]\nbars = axes[2].bar(methods, silhouettes, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\naxes[2].set_ylabel('Silhouette Score')\naxes[2].set_title('Silueta por Criterio de Enlace (k=3)')\naxes[2].set_ylim([0, max(silhouettes) * 1.2])\n\n# A\u00f1adir valores sobre las barras\nfor bar, val in zip(bars, silhouettes):\n    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                 f'{val:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 9. EJEMPLO CON DISTANCE_THRESHOLD\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALTERNATIVA: Clustering por Umbral de Distancia\")\nprint(\"=\"*60)\n\n# En lugar de especificar k, especificar distancia de corte\nmodel_dist = AgglomerativeClustering(\n    n_clusters=None,            # No especificar k\n    distance_threshold=10,      # Umbral de distancia\n    metric='euclidean',\n    linkage='ward'\n)\n\nlabels_dist = model_dist.fit_predict(X_scaled)\nn_clusters_found = len(np.unique(labels_dist))\n\nprint(f\"\\nCon distance_threshold=10:\")\nprint(f\"  Clusters encontrados: {n_clusters_found}\")\nprint(f\"  Distribuci\u00f3n: {np.bincount(labels_dist)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#46-hiperparametros-en-scikit-learn","title":"4.6. Hiperpar\u00e1metros en scikit-learn","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#scipyclusterhierarchylinkage","title":"scipy.cluster.hierarchy.linkage","text":"Par\u00e1metro Descripci\u00f3n Valores <code>y</code> Datos o matriz de distancias array (n, d) o condensada <code>method</code> Criterio de enlace 'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward' <code>metric</code> M\u00e9trica de distancia 'euclidean', 'cityblock', 'cosine', etc. <code>optimal_ordering</code> Reordenar hojas para minimizar distancias True/False"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#sklearnclusteragglomerativeclustering","title":"sklearn.cluster.AgglomerativeClustering","text":"Par\u00e1metro Descripci\u00f3n Valores <code>n_clusters</code> N\u00famero de clusters (None si se usa distance_threshold) int o None <code>distance_threshold</code> Umbral de distancia para corte float o None <code>metric</code> M\u00e9trica de distancia 'euclidean', 'manhattan', 'cosine', etc. <code>linkage</code> Criterio de enlace 'ward', 'complete', 'average', 'single' <code>compute_full_tree</code> Calcular \u00e1rbol completo 'auto', True, False <code>compute_distances</code> Calcular distancias entre clusters True/False"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#47-criterio-de-enlace-cual-elegir","title":"4.7. Criterio de Enlace: \u00bfCu\u00e1l Elegir?","text":"Criterio Forma de Clusters Sensibilidad a Outliers Cu\u00e1ndo Usar Single Arbitraria, elongada Baja Detectar clusters de forma irregular Complete Compacta, esf\u00e9rica Alta Clusters de tama\u00f1o similar Average Moderada Media Balance general Ward Compacta, esf\u00e9rica Media Similar a K-Means, varianza m\u00ednima"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#visualizacion-del-efecto-del-enlace","title":"Visualizaci\u00f3n del Efecto del Enlace","text":"<pre><code># Datos con forma irregular (dos lunas)\nfrom sklearn.datasets import make_moons\nX_moons, _ = make_moons(n_samples=200, noise=0.05)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nmethods = ['single', 'complete', 'average', 'ward']\n\nfor ax, method in zip(axes.flatten(), methods):\n    model = AgglomerativeClustering(n_clusters=2, linkage=method)\n    labels = model.fit_predict(X_moons)\n    ax.scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis')\n    ax.set_title(f'{method.capitalize()} Linkage')\n\nplt.tight_layout()\nplt.show()\n# Single linkage funcionar\u00e1 mejor con estos datos irregulares\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#48-aplicaciones-reales","title":"4.8. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#1-analisis-filogenetico-biologia","title":"1. An\u00e1lisis Filogen\u00e9tico (Biolog\u00eda)","text":"<p>Construir \u00e1rboles evolutivos basados en similitud gen\u00e9tica. * Tutorial: Phylogenetic Trees with Hierarchical Clustering</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#2-segmentacion-de-documentos","title":"2. Segmentaci\u00f3n de Documentos","text":"<p>Agrupar documentos similares para organizaci\u00f3n autom\u00e1tica.</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#3-analisis-de-expresion-genica","title":"3. An\u00e1lisis de Expresi\u00f3n G\u00e9nica","text":"<p>Identificar grupos de genes con patrones de expresi\u00f3n similares. * Ejemplo: Gene Expression Clustering</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#4-segmentacion-de-mercado","title":"4. Segmentaci\u00f3n de Mercado","text":"<p>El dendrograma permite identificar segmentos a diferentes niveles de granularidad.</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#49-comparacion-con-otros-metodos","title":"4.9. Comparaci\u00f3n con Otros M\u00e9todos","text":"Aspecto Jer\u00e1rquico K-Means DBSCAN Requiere k No (a priori) S\u00ed No Escalabilidad Baja (\\(O(n^3)\\)) Alta (\\(O(nk)\\)) Media (\\(O(n^2)\\)) Formas de cluster Depende del enlace Esf\u00e9ricas Arbitrarias Outliers No los detecta No los detecta S\u00ed los detecta Interpretabilidad Alta (dendrograma) Alta (centroides) Moderada Exploraci\u00f3n multinivel S\u00ed No No"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#410-resumen-y-mejores-practicas","title":"4.10. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#checklist-para-clustering-jerarquico","title":"Checklist para Clustering Jer\u00e1rquico","text":"<ul> <li>[ ] Escalar los datos (especialmente para Ward)</li> <li>[ ] Elegir criterio de enlace apropiado para la forma esperada de clusters</li> <li>[ ] Calcular correlaci\u00f3n cophenetica para validar el dendrograma</li> <li>[ ] Analizar el dendrograma visualmente antes de cortar</li> <li>[ ] Probar varios puntos de corte y evaluar con m\u00e9tricas</li> <li>[ ] Comparar con otros m\u00e9todos (K-Means, DBSCAN)</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#cuando-elegir-clustering-jerarquico","title":"\u00bfCu\u00e1ndo Elegir Clustering Jer\u00e1rquico?","text":"<p>\u2705 Usar Jer\u00e1rquico cuando: - Dataset peque\u00f1o-mediano (&lt; 10K puntos) - Quieres explorar la estructura a m\u00faltiples niveles - El dendrograma es informativo para el dominio - No sabes cu\u00e1ntos clusters hay</p> <p>\u274c Considerar alternativas cuando: - Dataset grande \u2192 K-Means o Mini-Batch K-Means - Necesitas identificar outliers \u2192 DBSCAN - Clusters de formas muy irregulares \u2192 DBSCAN + Single linkage - Eficiencia computacional es cr\u00edtica</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/05-pca/","title":"\ud83d\udcc9 Unidad 5. PCA - An\u00e1lisis de Componentes Principales","text":"<p>El An\u00e1lisis de Componentes Principales (PCA, del ingl\u00e9s Principal Component Analysis) es la t\u00e9cnica de reducci\u00f3n de dimensionalidad m\u00e1s utilizada en Machine Learning y estad\u00edstica. PCA transforma los datos a un nuevo sistema de coordenadas donde las primeras dimensiones (componentes principales) capturan la mayor parte de la varianza de los datos. Es una t\u00e9cnica fundamental para visualizaci\u00f3n, compresi\u00f3n de datos y preprocesamiento.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#51-que-es-la-reduccion-de-dimensionalidad","title":"5.1. \u00bfQu\u00e9 es la Reducci\u00f3n de Dimensionalidad?","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#el-problema-de-la-alta-dimensionalidad","title":"El Problema de la Alta Dimensionalidad","text":"<p>En muchos datasets reales, el n\u00famero de caracter\u00edsticas (dimensiones) puede ser muy alto: - Im\u00e1genes: miles de p\u00edxeles - Texto: miles de palabras - Gen\u00f3mica: miles de genes - Finanzas: cientos de indicadores</p> <p>Esto causa varios problemas:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROBLEMAS DE LA ALTA DIMENSIONALIDAD                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502 1. \"MALDICI\u00d3N DE LA DIMENSIONALIDAD\"                        \u2502\n\u2502    - Los datos se vuelven muy dispersos                     \u2502\n\u2502    - Las distancias pierden significado                     \u2502\n\u2502    - Aumenta el overfitting                                 \u2502\n\u2502                                                             \u2502\n\u2502 2. VISUALIZACI\u00d3N IMPOSIBLE                                  \u2502\n\u2502    - No podemos visualizar m\u00e1s de 3 dimensiones             \u2502\n\u2502                                                             \u2502\n\u2502 3. COSTO COMPUTACIONAL                                      \u2502\n\u2502    - M\u00e1s dimensiones = m\u00e1s tiempo y memoria                 \u2502\n\u2502                                                             \u2502\n\u2502 4. MULTICOLINEALIDAD                                        \u2502\n\u2502    - Variables correlacionadas = redundancia                \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#que-hace-pca","title":"\u00bfQu\u00e9 Hace PCA?","text":"<p>PCA encuentra las direcciones de m\u00e1xima varianza en los datos y proyecta los datos en estas direcciones:</p> <pre><code>Datos originales (2D)         Despu\u00e9s de PCA\n        \u2502                          \u2502\n    y   \u2502    \u2571\u2571\u2571\u2571                   \u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        \u2502   \u2571\u2571\u2571\u2571                    \u2502    PC1 (m\u00e1xima varianza)\n        \u2502  \u2571\u2571\u2571\u2571                     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 x                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n   Los datos tienen              PC1 captura la direcci\u00f3n\n   varianza en diagonal          de m\u00e1xima dispersi\u00f3n\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#52-explicacion-matematica","title":"5.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#intuicion-geometrica","title":"Intuici\u00f3n Geom\u00e9trica","text":"<p>PCA busca los ejes de m\u00e1xima varianza: 1. Encontrar la direcci\u00f3n donde los datos est\u00e1n m\u00e1s dispersos \u2192 PC1 2. Encontrar la siguiente direcci\u00f3n ortogonal con m\u00e1xima varianza \u2192 PC2 3. Repetir hasta tener d componentes (d = dimensiones originales)</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#pasos-matematicos","title":"Pasos Matem\u00e1ticos","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#paso-1-centrar-los-datos","title":"Paso 1: Centrar los Datos","text":"<p>Restar la media de cada variable para que los datos est\u00e9n centrados en el origen:</p> \\[X_{centered} = X - \\bar{X}\\] <p>Donde \\(\\bar{X}\\) es el vector de medias de cada columna.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-2-calcular-la-matriz-de-covarianza","title":"Paso 2: Calcular la Matriz de Covarianza","text":"<p>La matriz de covarianza \\(\\Sigma\\) captura las relaciones entre variables:</p> \\[\\Sigma = \\frac{1}{n-1} X_{centered}^T X_{centered}\\] <p>Para dos variables \\(x\\) y \\(y\\): \\(\\(Cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\)\\)</p> <p>La matriz de covarianza es de tama\u00f1o \\(d \\times d\\) (donde \\(d\\) = n\u00famero de variables):</p> \\[\\Sigma = \\begin{bmatrix} Var(x_1) &amp; Cov(x_1, x_2) &amp; \\cdots \\\\ Cov(x_2, x_1) &amp; Var(x_2) &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\]"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-3-calcular-autovalores-y-autovectores","title":"Paso 3: Calcular Autovalores y Autovectores","text":"<p>Los autovectores de \\(\\Sigma\\) son las direcciones de los componentes principales. Los autovalores correspondientes indican cu\u00e1nta varianza captura cada componente.</p> \\[\\Sigma v = \\lambda v\\] <p>Donde: - \\(v\\) = autovector (direcci\u00f3n del componente principal) - \\(\\lambda\\) = autovalor (varianza explicada por ese componente)</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-4-ordenar-y-seleccionar-componentes","title":"Paso 4: Ordenar y Seleccionar Componentes","text":"<p>Los componentes se ordenan por autovalor descendente: - \\(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_d\\) - PC1 tiene el mayor autovalor, PC2 el segundo, etc.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-5-proyectar-los-datos","title":"Paso 5: Proyectar los Datos","text":"<p>Para reducir de \\(d\\) dimensiones a \\(k\\) dimensiones:</p> \\[X_{reducido} = X_{centered} \\cdot W_k\\] <p>Donde \\(W_k\\) es la matriz de los \\(k\\) primeros autovectores (columnas).</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#varianza-explicada","title":"Varianza Explicada","text":"<p>La varianza explicada por cada componente es:</p> \\[\\text{Varianza explicada}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\\] <p>La varianza acumulada indica cu\u00e1nta informaci\u00f3n total preservamos:</p> \\[\\text{Varianza acumulada}_k = \\sum_{i=1}^{k} \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\\]"},{"location":"aprendizaje-no-supervisado/05-pca/#53-pros-y-contras","title":"5.3. Pros y Contras","text":"Ventajas Desventajas Reducci\u00f3n efectiva: Elimina redundancia y ruido Transformaci\u00f3n lineal: No captura relaciones no lineales Visualizaci\u00f3n: Permite visualizar datos de alta dimensi\u00f3n P\u00e9rdida de informaci\u00f3n: Los componentes descartados contienen algo de informaci\u00f3n Decorrelaci\u00f3n: Los componentes son ortogonales (no correlacionados) Dif\u00edcil interpretaci\u00f3n: Los componentes son combinaciones de todas las variables Sin hiperpar\u00e1metros: Solo decidir cu\u00e1ntos componentes mantener Sensible a escala: Requiere estandarizaci\u00f3n previa Eficiente: Complejidad \\(O(nd^2 + d^3)\\) Sensible a outliers: Los outliers afectan la direcci\u00f3n de m\u00e1xima varianza"},{"location":"aprendizaje-no-supervisado/05-pca/#54-ejemplo-basico-en-python","title":"5.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra c\u00f3mo aplicar PCA para visualizar datos de alta dimensi\u00f3n.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: PCA para visualizaci\u00f3n del dataset Iris\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.decomposition import PCA       # Algoritmo PCA\nfrom sklearn.preprocessing import StandardScaler  # Estandarizaci\u00f3n\nfrom sklearn.datasets import load_iris      # Dataset de ejemplo\n\n# -------------------------------------------------------------\n# 1. CARGAR DATOS\n# -------------------------------------------------------------\niris = load_iris()\nX = iris.data           # 150 muestras \u00d7 4 caracter\u00edsticas\ny = iris.target         # Etiquetas de especie\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\nprint(\"=\"*50)\nprint(\"PCA - EJEMPLO B\u00c1SICO CON DATASET IRIS\")\nprint(\"=\"*50)\nprint(f\"\\nDimensiones originales: {X.shape}\")\nprint(f\"Features: {feature_names}\")\n\n# -------------------------------------------------------------\n# 2. ESTANDARIZAR LOS DATOS (CRUCIAL)\n# -------------------------------------------------------------\n# PCA es sensible a la escala, por lo que SIEMPRE debemos estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"\\nDatos estandarizados:\")\nprint(f\"  Media por feature: {X_scaled.mean(axis=0).round(4)}\")\nprint(f\"  Std por feature: {X_scaled.std(axis=0).round(4)}\")\n\n# -------------------------------------------------------------\n# 3. APLICAR PCA\n# -------------------------------------------------------------\n# Reducir de 4 dimensiones a 2 para poder visualizar\npca = PCA(n_components=2)   # Mantener solo 2 componentes\n\n# fit_transform: calcula los componentes y transforma los datos\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"\\nDimensiones despu\u00e9s de PCA: {X_pca.shape}\")\n\n# -------------------------------------------------------------\n# 4. ANALIZAR LOS COMPONENTES PRINCIPALES\n# -------------------------------------------------------------\n# explained_variance_ratio_: proporci\u00f3n de varianza explicada por cada componente\nprint(f\"\\nVarianza explicada por componente:\")\nfor i, var in enumerate(pca.explained_variance_ratio_):\n    print(f\"  PC{i+1}: {var*100:.2f}%\")\nprint(f\"  Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n\n# explained_variance_: autovalores (varianza absoluta)\nprint(f\"\\nAutovalores (varianza absoluta):\")\nfor i, var in enumerate(pca.explained_variance_):\n    print(f\"  PC{i+1}: {var:.4f}\")\n\n# components_: autovectores (direcciones de los componentes)\n# Cada fila es un componente, cada columna es el peso de esa variable original\nprint(f\"\\nAutovectores (componentes_):\")\nprint(f\"Formato: [sepal_length, sepal_width, petal_length, petal_width]\")\nfor i, comp in enumerate(pca.components_):\n    print(f\"  PC{i+1}: {comp.round(4)}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZAR LOS DATOS REDUCIDOS\n# -------------------------------------------------------------\nplt.figure(figsize=(10, 8))\n\n# Crear scatter plot coloreado por especie\ncolors = ['navy', 'turquoise', 'darkorange']\nfor i, (color, name) in enumerate(zip(colors, target_names)):\n    mask = y == i\n    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],\n                c=color, label=name, alpha=0.7, edgecolors='w', s=60)\n\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% varianza)', fontsize=12)\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% varianza)', fontsize=12)\nplt.title('PCA del Dataset Iris (4D \u2192 2D)', fontsize=14)\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\n\n# A\u00f1adir vectores de carga (loadings) para interpretar los componentes\n# Los loadings muestran c\u00f3mo contribuye cada variable a cada componente\nfor i, (var, name) in enumerate(zip(pca.components_.T, feature_names)):\n    plt.annotate('', xy=(var[0]*3, var[1]*3), xytext=(0, 0),\n                 arrowprops=dict(arrowstyle='-&gt;', color='red', lw=2))\n    plt.text(var[0]*3.2, var[1]*3.2, name.replace(' (cm)', ''),\n             fontsize=10, color='red', ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 6. INTERPRETACI\u00d3N DE LOS COMPONENTES\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"INTERPRETACI\u00d3N DE LOS COMPONENTES\")\nprint(\"=\"*50)\n\n# Crear DataFrame de loadings para mejor visualizaci\u00f3n\nimport pandas as pd\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=feature_names\n)\nprint(\"\\nLoadings (contribuci\u00f3n de cada variable):\")\nprint(loadings.round(4))\n\nprint(\"\"\"\nInterpretaci\u00f3n:\n- PC1: Combinaci\u00f3n principalmente de petal_length y petal_width (tama\u00f1o del p\u00e9talo)\n       Variables con signos similares y magnitudes altas.\n- PC2: Contrasta sepal_width contra las dem\u00e1s (forma del s\u00e9palo vs resto)\n       sepal_width tiene signo opuesto a las otras variables.\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#55-ejemplo-avanzado-seleccion-de-componentes-y-reconstruccion","title":"5.5. Ejemplo Avanzado: Selecci\u00f3n de Componentes y Reconstrucci\u00f3n","text":"<p>Este ejemplo muestra c\u00f3mo elegir el n\u00famero \u00f3ptimo de componentes y c\u00f3mo reconstruir los datos.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: PCA completo con an\u00e1lisis de varianza\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import fetch_openml\n\n# -------------------------------------------------------------\n# 1. CARGAR DATOS DE ALTA DIMENSIONALIDAD (MNIST)\n# -------------------------------------------------------------\nprint(\"=\"*60)\nprint(\"PCA AVANZADO - DATASET MNIST (D\u00cdGITOS)\")\nprint(\"=\"*60)\nprint(\"\\nCargando datos... (puede tardar un momento)\")\n\n# Cargar un subconjunto de MNIST para eficiencia\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data     # 1797 muestras \u00d7 64 p\u00edxeles (8\u00d78)\ny = digits.target\n\nprint(f\"\\nDimensiones originales: {X.shape}\")\nprint(f\"Cada imagen es de {int(np.sqrt(X.shape[1]))}\u00d7{int(np.sqrt(X.shape[1]))} p\u00edxeles\")\n\n# -------------------------------------------------------------\n# 2. ESTANDARIZAR\n# -------------------------------------------------------------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. PCA COMPLETO PARA AN\u00c1LISIS DE VARIANZA\n# -------------------------------------------------------------\n# Primero, ajustar PCA con todos los componentes para analizar varianza\npca_full = PCA()  # n_components=None \u2192 todos los componentes\npca_full.fit(X_scaled)\n\n# Varianza explicada por cada componente\nvar_ratio = pca_full.explained_variance_ratio_\nvar_cumsum = np.cumsum(var_ratio)\n\nprint(f\"\\nVarianza explicada por los primeros 10 componentes:\")\nfor i in range(10):\n    print(f\"  PC{i+1}: {var_ratio[i]*100:.2f}% (acumulada: {var_cumsum[i]*100:.2f}%)\")\n\n# -------------------------------------------------------------\n# 4. VISUALIZAR VARIANZA EXPLICADA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Gr\u00e1fico de varianza individual\nax1 = axes[0]\nax1.bar(range(1, len(var_ratio)+1), var_ratio, alpha=0.6, label='Individual')\nax1.set_xlabel('Componente Principal')\nax1.set_ylabel('Proporci\u00f3n de Varianza Explicada')\nax1.set_title('Varianza Explicada por Componente')\nax1.set_xlim([0, 30])\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Gr\u00e1fico de varianza acumulada\nax2 = axes[1]\nax2.plot(range(1, len(var_cumsum)+1), var_cumsum, 'bo-', markersize=4)\nax2.axhline(y=0.90, color='r', linestyle='--', label='90% varianza')\nax2.axhline(y=0.95, color='g', linestyle='--', label='95% varianza')\nax2.axhline(y=0.99, color='orange', linestyle='--', label='99% varianza')\n\n# Encontrar n\u00famero de componentes para diferentes umbrales\nn_90 = np.argmax(var_cumsum &gt;= 0.90) + 1\nn_95 = np.argmax(var_cumsum &gt;= 0.95) + 1\nn_99 = np.argmax(var_cumsum &gt;= 0.99) + 1\n\nax2.axvline(x=n_90, color='r', linestyle=':', alpha=0.5)\nax2.axvline(x=n_95, color='g', linestyle=':', alpha=0.5)\nax2.axvline(x=n_99, color='orange', linestyle=':', alpha=0.5)\n\nax2.set_xlabel('N\u00famero de Componentes')\nax2.set_ylabel('Varianza Acumulada')\nax2.set_title('Varianza Acumulada')\nax2.legend(loc='lower right')\nax2.grid(True, alpha=0.3)\nax2.set_xlim([0, 64])\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nComponentes necesarios para:\")\nprint(f\"  90% varianza: {n_90} componentes\")\nprint(f\"  95% varianza: {n_95} componentes\")\nprint(f\"  99% varianza: {n_99} componentes\")\n\n# -------------------------------------------------------------\n# 5. REDUCCI\u00d3N A 2D PARA VISUALIZACI\u00d3N\n# -------------------------------------------------------------\npca_2d = PCA(n_components=2)\nX_2d = pca_2d.fit_transform(X_scaled)\n\nplt.figure(figsize=(12, 10))\nscatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10',\n                      alpha=0.6, edgecolors='w', s=20)\nplt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\nplt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\nplt.title('D\u00edgitos MNIST Proyectados a 2D con PCA')\nplt.colorbar(scatter, label='D\u00edgito')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# -------------------------------------------------------------\n# 6. RECONSTRUCCI\u00d3N DE IM\u00c1GENES\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECONSTRUCCI\u00d3N DE IM\u00c1GENES\")\nprint(\"=\"*60)\n\n# Funci\u00f3n para reconstruir y visualizar\ndef reconstruct_and_show(X, pca, scaler, n_components_list, sample_idx=0):\n    \"\"\"\n    Reconstruye una imagen con diferente n\u00famero de componentes.\n    \"\"\"\n    original = X[sample_idx].reshape(8, 8)\n\n    fig, axes = plt.subplots(1, len(n_components_list)+1, figsize=(15, 3))\n\n    # Imagen original\n    axes[0].imshow(original, cmap='gray')\n    axes[0].set_title(f'Original\\n(64 dims)')\n    axes[0].axis('off')\n\n    X_sample = X[sample_idx:sample_idx+1]\n    X_scaled = scaler.transform(X_sample)\n\n    for i, n_comp in enumerate(n_components_list):\n        # Ajustar PCA con n componentes\n        pca_temp = PCA(n_components=n_comp)\n        pca_temp.fit(scaler.transform(X))\n\n        # Transformar y reconstruir\n        X_reduced = pca_temp.transform(X_scaled)\n        X_reconstructed = pca_temp.inverse_transform(X_reduced)\n        X_reconstructed = scaler.inverse_transform(X_reconstructed)\n\n        # Calcular error de reconstrucci\u00f3n\n        mse = np.mean((X_sample - X_reconstructed)**2)\n\n        # Mostrar\n        axes[i+1].imshow(X_reconstructed.reshape(8, 8), cmap='gray')\n        axes[i+1].set_title(f'{n_comp} componentes\\nMSE: {mse:.2f}')\n        axes[i+1].axis('off')\n\n    plt.suptitle('Reconstrucci\u00f3n con Diferente N\u00famero de Componentes', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# Reconstruir el primer d\u00edgito con diferentes componentes\nn_components_list = [2, 5, 10, 20, 40]\nreconstruct_and_show(X, pca_full, scaler, n_components_list, sample_idx=0)\n\nprint(\"\\nObservaci\u00f3n: Con ~20 componentes ya se obtiene una buena reconstrucci\u00f3n\")\nprint(\"del d\u00edgito original, a pesar de reducir de 64 a 20 dimensiones (~70% compresi\u00f3n)\")\n\n# -------------------------------------------------------------\n# 7. VISUALIZAR LOS COMPONENTES PRINCIPALES (EIGENFACES)\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPONENTES PRINCIPALES COMO IM\u00c1GENES\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor i in range(10):\n    # Cada componente se puede visualizar como una imagen\n    component = pca_full.components_[i].reshape(8, 8)\n    axes[i].imshow(component, cmap='RdBu')\n    axes[i].set_title(f'PC{i+1}\\n({var_ratio[i]*100:.1f}%)')\n    axes[i].axis('off')\n\nplt.suptitle('Primeros 10 Componentes Principales (como im\u00e1genes 8\u00d78)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"Los primeros componentes capturan patrones globales (bordes, formas)\")\nprint(\"Los componentes posteriores capturan detalles m\u00e1s finos\")\n\n# -------------------------------------------------------------\n# 8. PCA COMO PREPROCESAMIENTO PARA CLASIFICACI\u00d3N\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"PCA COMO PREPROCESAMIENTO PARA ML\")\nprint(\"=\"*60)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.3, random_state=42\n)\n\n# Comparar rendimiento con diferentes niveles de reducci\u00f3n\nresults = []\n\nfor n_comp in [None, 40, 20, 10, 5, 2]:\n    if n_comp is None:\n        # Sin reducci\u00f3n\n        X_train_pca = X_train\n        X_test_pca = X_test\n        n_dims = X_train.shape[1]\n    else:\n        # Con PCA\n        pca = PCA(n_components=n_comp)\n        X_train_pca = pca.fit_transform(X_train)\n        X_test_pca = pca.transform(X_test)\n        n_dims = n_comp\n\n    # Entrenar clasificador\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    start_time = time.time()\n    knn.fit(X_train_pca, y_train)\n    y_pred = knn.predict(X_test_pca)\n    elapsed = time.time() - start_time\n\n    acc = accuracy_score(y_test, y_pred)\n\n    results.append({\n        'Dimensiones': n_dims,\n        'Accuracy': acc,\n        'Tiempo (s)': elapsed\n    })\n\ndf_results = pd.DataFrame(results)\nprint(\"\\nRendimiento de KNN con diferentes niveles de reducci\u00f3n:\")\nprint(df_results.to_string(index=False))\n\n# Visualizar\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(df_results['Dimensiones'], df_results['Accuracy'], 'bo-', markersize=10)\nax.set_xlabel('N\u00famero de Dimensiones (componentes)')\nax.set_ylabel('Accuracy')\nax.set_title('Accuracy vs Dimensionalidad (KNN con PCA)')\nax.invert_xaxis()  # Invertir eje x para mejor lectura\nax.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\"\"\nConclusi\u00f3n:\n- PCA puede reducir significativamente las dimensiones manteniendo buen rendimiento\n- Menos dimensiones = entrenamiento/predicci\u00f3n m\u00e1s r\u00e1pido\n- Hay un punto \u00f3ptimo donde reducimos complejidad sin perder mucha precisi\u00f3n\n\"\"\")\n\n# -------------------------------------------------------------\n# 9. PCA INCREMENTAL PARA DATASETS GRANDES\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"VARIANTES DE PCA\")\nprint(\"=\"*60)\n\nprint(\"\"\"\nscikit-learn ofrece varias implementaciones de PCA:\n\n1. PCA (est\u00e1ndar):\n   - Usa SVD completo\n   - Para datasets que caben en memoria\n   - from sklearn.decomposition import PCA\n\n2. IncrementalPCA:\n   - Procesa datos en batches\n   - Para datasets muy grandes que no caben en memoria\n   - from sklearn.decomposition import IncrementalPCA\n\n3. SparsePCA:\n   - Produce componentes con muchos ceros (sparse)\n   - Mejor interpretabilidad\n   - from sklearn.decomposition import SparsePCA\n\n4. KernelPCA:\n   - PCA no lineal usando kernels\n   - Para relaciones no lineales en los datos\n   - from sklearn.decomposition import KernelPCA\n\"\"\")\n\n# Ejemplo de KernelPCA\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.datasets import make_moons\n\n# Datos no linealmente separables\nX_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=42)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Original\naxes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis')\naxes[0].set_title('Datos Originales')\n\n# PCA lineal\npca_linear = PCA(n_components=2)\nX_pca_linear = pca_linear.fit_transform(X_moons)\naxes[1].scatter(X_pca_linear[:, 0], X_pca_linear[:, 1], c=y_moons, cmap='viridis')\naxes[1].set_title('PCA Lineal (igual que original)')\n\n# Kernel PCA (RBF)\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\nX_kpca = kpca.fit_transform(X_moons)\naxes[2].scatter(X_kpca[:, 0], X_kpca[:, 1], c=y_moons, cmap='viridis')\naxes[2].set_title('Kernel PCA (RBF)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"KernelPCA puede capturar relaciones no lineales que PCA est\u00e1ndar no puede\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#56-hiperparametros-de-pca-en-scikit-learn","title":"5.6. Hiperpar\u00e1metros de PCA en scikit-learn","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_components</code> N\u00famero de componentes a mantener int, float, 'mle', None Ver nota abajo <code>svd_solver</code> Algoritmo para calcular SVD 'auto', 'full', 'arpack', 'randomized' 'auto' para la mayor\u00eda de casos <code>whiten</code> Si blanquear los datos (varianza unitaria) True/False True si se usa para clasificaci\u00f3n <code>random_state</code> Semilla para svd_solver='randomized' int o None Fijar para reproducibilidad"},{"location":"aprendizaje-no-supervisado/05-pca/#especificar-n_components","title":"Especificar <code>n_components</code>","text":"<pre><code># Formas de especificar n_components:\n\n# 1. N\u00famero exacto de componentes\npca = PCA(n_components=10)  # Exactamente 10 componentes\n\n# 2. Proporci\u00f3n de varianza a preservar (0-1)\npca = PCA(n_components=0.95)  # Componentes para retener 95% de varianza\n\n# 3. Estimaci\u00f3n autom\u00e1tica con MLE\npca = PCA(n_components='mle')  # Estimaci\u00f3n de m\u00e1xima verosimilitud\n\n# 4. Todos los componentes\npca = PCA(n_components=None)  # min(n_samples, n_features) componentes\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#57-aplicaciones-reales-de-pca","title":"5.7. Aplicaciones Reales de PCA","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#1-compresion-de-imagenes","title":"1. Compresi\u00f3n de Im\u00e1genes","text":"<ul> <li>Eigenfaces: Reconocimiento facial con dimensionalidad reducida</li> <li>Tutorial: Eigenfaces</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#2-visualizacion-de-datos","title":"2. Visualizaci\u00f3n de Datos","text":"<p>Proyectar datos de alta dimensi\u00f3n a 2D/3D para exploraci\u00f3n.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#3-preprocesamiento-para-ml","title":"3. Preprocesamiento para ML","text":"<ul> <li>Reducir overfitting al eliminar features redundantes</li> <li>Acelerar el entrenamiento de modelos</li> <li>Eliminar multicolinealidad</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#4-analisis-de-expresion-genica","title":"4. An\u00e1lisis de Expresi\u00f3n G\u00e9nica","text":"<p>Identificar genes importantes y visualizar grupos de muestras.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#5-finanzas","title":"5. Finanzas","text":"<ul> <li>An\u00e1lisis de portafolio (factores de riesgo)</li> <li>Detecci\u00f3n de patrones en series financieras</li> <li>Tutorial: PCA in Finance</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#58-pca-vs-otras-tecnicas","title":"5.8. PCA vs Otras T\u00e9cnicas","text":"T\u00e9cnica Tipo Preserva Mejor para PCA Lineal Varianza global Datos lineales, preprocesamiento t-SNE No lineal Estructura local Visualizaci\u00f3n de clusters UMAP No lineal Estructura local + global Visualizaci\u00f3n + ML LDA Supervisado Separabilidad de clases Clasificaci\u00f3n Autoencoders No lineal Representaci\u00f3n aprendida Relaciones complejas"},{"location":"aprendizaje-no-supervisado/05-pca/#59-resumen-y-mejores-practicas","title":"5.9. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#checklist-para-usar-pca","title":"Checklist para usar PCA","text":"<ul> <li>[ ] Estandarizar los datos (StandardScaler) - \u00a1Obligatorio!</li> <li>[ ] Analizar la varianza explicada para elegir n_components</li> <li>[ ] Usar 95% de varianza como regla general</li> <li>[ ] Visualizar loadings para interpretar componentes</li> <li>[ ] Verificar que la reducci\u00f3n no afecta negativamente el modelo downstream</li> <li>[ ] Considerar alternativas no lineales si PCA no funciona bien</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#cuando-usar-pca","title":"\u00bfCu\u00e1ndo usar PCA?","text":"<p>\u2705 Usar PCA cuando: - Los datos tienen alta dimensionalidad - Hay multicolinealidad entre variables - Necesitas visualizar datos multidimensionales - Quieres preprocesar para acelerar ML - Las relaciones son aproximadamente lineales</p> <p>\u274c Considerar alternativas cuando: - Las relaciones son altamente no lineales \u2192 Kernel PCA, Autoencoders - Necesitas visualizaci\u00f3n con clusters claros \u2192 t-SNE, UMAP - Tienes un problema de clasificaci\u00f3n \u2192 LDA</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/","title":"\ud83d\udd2e Unidad 6. t-SNE - Visualizaci\u00f3n de Alta Dimensi\u00f3n","text":"<p>t-SNE (t-distributed Stochastic Neighbor Embedding) es una t\u00e9cnica de reducci\u00f3n de dimensionalidad no lineal dise\u00f1ada espec\u00edficamente para visualizaci\u00f3n de datos de alta dimensi\u00f3n. A diferencia de PCA que preserva la varianza global, t-SNE se enfoca en preservar la estructura local: los puntos que son similares en el espacio original permanecen cercanos en el espacio reducido. Esto lo hace excepcional para revelar clusters y patrones ocultos.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#61-por-que-t-sne","title":"6.1. \u00bfPor Qu\u00e9 t-SNE?","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#limitaciones-de-pca","title":"Limitaciones de PCA","text":"<p>PCA es una t\u00e9cnica lineal que preserva la varianza global. Sin embargo: - No captura relaciones no lineales - No preserva bien la estructura de clusters - Los datos proyectados pueden solaparse incluso si los clusters originales est\u00e1n bien separados</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#la-idea-de-t-sne","title":"La Idea de t-SNE","text":"<p>t-SNE se pregunta: \"\u00bfC\u00f3mo puedo proyectar los datos de manera que los vecinos cercanos en alta dimensi\u00f3n sigan siendo vecinos cercanos en baja dimensi\u00f3n?\"</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INTUICI\u00d3N DE t-SNE                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502 Espacio Original (Alta Dimensi\u00f3n)                           \u2502\n\u2502                                                             \u2502\n\u2502     A est\u00e1 cerca de B y C                                   \u2502\n\u2502     A est\u00e1 lejos de X e Y                                   \u2502\n\u2502                                                             \u2502\n\u2502              A \u25cf \u25cf B                                        \u2502\n\u2502                \u25cf C                                          \u2502\n\u2502                                                             \u2502\n\u2502                           X \u25cf \u25cf Y                           \u2502\n\u2502                                                             \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502                                                             \u2502\n\u2502 t-SNE preserva estas relaciones de vecindad:                \u2502\n\u2502                                                             \u2502\n\u2502 Espacio Reducido (2D)                                       \u2502\n\u2502                                                             \u2502\n\u2502         \u25cfA \u25cfB               \u25cfX                              \u2502\n\u2502          \u25cfC                  \u25cfY                             \u2502\n\u2502                                                             \u2502\n\u2502 \u2713 A sigue cerca de B y C                                   \u2502\n\u2502 \u2713 A sigue lejos de X e Y                                   \u2502\n\u2502 \u2713 Clusters visualmente separados                            \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/06-tsne/#62-explicacion-matematica","title":"6.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#paso-1-calcular-similitudes-en-alta-dimension","title":"Paso 1: Calcular Similitudes en Alta Dimensi\u00f3n","text":"<p>Para cada par de puntos \\((x_i, x_j)\\) en el espacio original, t-SNE calcula una probabilidad condicional \\(p_{j|i}\\) que representa qu\u00e9 tan probable es que \\(x_i\\) elija a \\(x_j\\) como su vecino si los vecinos se eligieran proporcionalmente a una distribuci\u00f3n Gaussiana centrada en \\(x_i\\):</p> \\[p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\\] <p>Donde \\(\\sigma_i\\) es la varianza de la Gaussiana centrada en \\(x_i\\) (se ajusta autom\u00e1ticamente seg\u00fan el par\u00e1metro perplexity).</p> <p>Las probabilidades se simetrizan: \\(\\(p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#paso-2-calcular-similitudes-en-baja-dimension","title":"Paso 2: Calcular Similitudes en Baja Dimensi\u00f3n","text":"<p>En el espacio reducido, t-SNE usa una distribuci\u00f3n t de Student (con 1 grado de libertad, es decir, distribuci\u00f3n de Cauchy) en lugar de Gaussiana:</p> \\[q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1 + ||y_k - y_l||^2)^{-1}}\\] <p>Donde \\(y_i\\) y \\(y_j\\) son las representaciones en baja dimensi\u00f3n.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#por-que-la-distribucion-t","title":"\u00bfPor Qu\u00e9 la Distribuci\u00f3n t?","text":"<p>La distribuci\u00f3n t tiene colas m\u00e1s pesadas que la Gaussiana:</p> <pre><code>          Gaussiana          Distribuci\u00f3n t\n             ___                  ___\n            /   \\               /     \\\n           /     \\             /       \\\n          /       \\           /         \\\n      ___/         \\___     _/           \\_\n           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500              \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n         Colas ligeras       Colas pesadas\n</code></pre> <p>Esto resuelve el problema del amontonamiento (crowding problem): - En alta dimensi\u00f3n hay mucho \"espacio\" para que los puntos se dispersen - En baja dimensi\u00f3n hay menos espacio - Las colas pesadas permiten que puntos moderadamente lejanos se separen m\u00e1s, dejando espacio para los clusters</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#paso-3-minimizar-la-divergencia-kl","title":"Paso 3: Minimizar la Divergencia KL","text":"<p>t-SNE minimiza la divergencia de Kullback-Leibler entre las distribuciones P (alta dim) y Q (baja dim):</p> \\[KL(P||Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\\] <p>Esta funci\u00f3n de costo penaliza fuertemente cuando: - Puntos cercanos en alta dimensi\u00f3n (\\(p_{ij}\\) alto) quedan lejos en baja dimensi\u00f3n (\\(q_{ij}\\) bajo)</p> <p>La minimizaci\u00f3n se hace mediante descenso de gradiente.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#el-parametro-perplexity","title":"El Par\u00e1metro Perplexity","text":"<p>La perplexity es el hiperpar\u00e1metro m\u00e1s importante de t-SNE. Intuitivamente, es una medida del n\u00famero efectivo de vecinos cercanos:</p> \\[Perplexity = 2^{H(P_i)}\\] <p>Donde \\(H(P_i)\\) es la entrop\u00eda de Shannon de la distribuci\u00f3n de probabilidad centrada en \\(x_i\\).</p> <ul> <li>Perplexity baja (5-10): Solo considera vecinos muy cercanos \u2192 estructura muy local</li> <li>Perplexity alta (30-50): Considera m\u00e1s vecinos \u2192 estructura m\u00e1s global</li> <li>Regla: Debe ser menor que el n\u00famero de puntos</li> </ul>"},{"location":"aprendizaje-no-supervisado/06-tsne/#63-pros-y-contras","title":"6.3. Pros y Contras","text":"Ventajas Desventajas Excelente para visualizaci\u00f3n: Revela clusters claramente Solo para visualizaci\u00f3n: No usar para preprocesamiento de ML Preserva estructura local: Vecinos cercanos permanecen juntos Lento: Complejidad \\(O(n^2)\\), aunque hay aproximaciones No lineal: Captura relaciones complejas No determin\u00edstico: Diferentes ejecuciones dan diferentes resultados Funciona bien con clusters: Separa grupos visualmente Distancias no interpretables: Las distancias entre clusters no tienen significado Hiperpar\u00e1metros simples: Principalmente perplexity Sensible a hiperpar\u00e1metros: Perplexity afecta mucho el resultado"},{"location":"aprendizaje-no-supervisado/06-tsne/#64-ejemplo-basico-en-python","title":"6.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico de t-SNE para visualizar el dataset de d\u00edgitos.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: t-SNE para visualizaci\u00f3n de d\u00edgitos\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.manifold import TSNE           # Algoritmo t-SNE\nfrom sklearn.preprocessing import StandardScaler  # Estandarizaci\u00f3n\nfrom sklearn.datasets import load_digits    # Dataset de d\u00edgitos\n\n# -------------------------------------------------------------\n# 1. CARGAR DATOS\n# -------------------------------------------------------------\ndigits = load_digits()\nX = digits.data     # 1797 muestras \u00d7 64 caracter\u00edsticas (8\u00d78 p\u00edxeles)\ny = digits.target   # Etiquetas de d\u00edgitos (0-9)\n\nprint(\"=\"*50)\nprint(\"t-SNE - EJEMPLO B\u00c1SICO CON D\u00cdGITOS\")\nprint(\"=\"*50)\nprint(f\"\\nDimensiones originales: {X.shape}\")\nprint(f\"Clases: {np.unique(y)}\")\n\n# -------------------------------------------------------------\n# 2. ESTANDARIZAR LOS DATOS\n# -------------------------------------------------------------\n# Aunque t-SNE es robusto a la escala, es buena pr\u00e1ctica estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. APLICAR t-SNE\n# -------------------------------------------------------------\n# Reducir de 64 dimensiones a 2 para visualizaci\u00f3n\nprint(\"\\nAplicando t-SNE (puede tardar un momento)...\")\n\ntsne = TSNE(\n    n_components=2,         # Reducir a 2 dimensiones\n    perplexity=30,          # N\u00famero efectivo de vecinos (t\u00edpico: 5-50)\n    random_state=42,        # Reproducibilidad\n    n_iter=1000,            # N\u00famero de iteraciones de optimizaci\u00f3n\n    learning_rate='auto'    # Tasa de aprendizaje autom\u00e1tica\n)\n\n# fit_transform: ajusta el modelo y transforma los datos\nX_tsne = tsne.fit_transform(X_scaled)\n\nprint(f\"Dimensiones despu\u00e9s de t-SNE: {X_tsne.shape}\")\nprint(f\"Divergencia KL final: {tsne.kl_divergence_:.4f}\")\n\n# -------------------------------------------------------------\n# 4. VISUALIZAR RESULTADOS\n# -------------------------------------------------------------\nplt.figure(figsize=(12, 10))\n\n# Crear scatter plot con colores por d\u00edgito\nscatter = plt.scatter(\n    X_tsne[:, 0], X_tsne[:, 1],\n    c=y,                    # Color seg\u00fan el d\u00edgito\n    cmap='tab10',           # Paleta de 10 colores\n    alpha=0.7,              # Transparencia\n    edgecolors='w',         # Borde blanco\n    s=30                    # Tama\u00f1o de puntos\n)\n\n# A\u00f1adir etiquetas en los centroides de cada cluster\nfor digit in range(10):\n    mask = y == digit\n    centroid = X_tsne[mask].mean(axis=0)\n    plt.annotate(\n        str(digit),\n        centroid,\n        fontsize=20,\n        fontweight='bold',\n        ha='center',\n        va='center',\n        color='black',\n        bbox=dict(boxstyle='circle', facecolor='white', alpha=0.8)\n    )\n\nplt.xlabel('t-SNE Dimensi\u00f3n 1', fontsize=12)\nplt.ylabel('t-SNE Dimensi\u00f3n 2', fontsize=12)\nplt.title('Visualizaci\u00f3n de D\u00edgitos con t-SNE (64D \u2192 2D)', fontsize=14)\nplt.colorbar(scatter, label='D\u00edgito')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# -------------------------------------------------------------\n# 5. COMPARAR CON PCA\n# -------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n\n# Aplicar PCA para comparaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Comparar visualizaciones\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# PCA\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10',\n                           alpha=0.7, edgecolors='w', s=20)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA (lineal) - D\u00edgitos')\nplt.colorbar(scatter1, ax=axes[0])\n\n# t-SNE\nscatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                           alpha=0.7, edgecolors='w', s=20)\naxes[1].set_xlabel('t-SNE 1')\naxes[1].set_ylabel('t-SNE 2')\naxes[1].set_title('t-SNE (no lineal) - D\u00edgitos')\nplt.colorbar(scatter2, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nObservaciones:\n- t-SNE separa claramente los clusters de d\u00edgitos\n- PCA muestra m\u00e1s solapamiento entre clases\n- t-SNE es superior para visualizar estructura de clusters\n- Las distancias en t-SNE no son interpretables (solo la estructura)\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/06-tsne/#65-ejemplo-avanzado-efecto-de-hiperparametros-y-buenas-practicas","title":"6.5. Ejemplo Avanzado: Efecto de Hiperpar\u00e1metros y Buenas Pr\u00e1cticas","text":"<p>Este ejemplo explora el efecto de la perplexity y otros par\u00e1metros.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: An\u00e1lisis de hiperpar\u00e1metros de t-SNE\n# ============================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_digits\nimport time\n\n# -------------------------------------------------------------\n# 1. CARGAR Y PREPARAR DATOS\n# -------------------------------------------------------------\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS DE HIPERPAR\u00c1METROS DE t-SNE\")\nprint(\"=\"*60)\n\n# -------------------------------------------------------------\n# 2. EFECTO DE LA PERPLEXITY\n# -------------------------------------------------------------\nprint(\"\\n[1] EFECTO DE LA PERPLEXITY\")\nprint(\"-\"*40)\n\nperplexities = [5, 15, 30, 50, 100]\n\nfig, axes = plt.subplots(1, len(perplexities), figsize=(20, 4))\n\nfor i, perp in enumerate(perplexities):\n    print(f\"  Calculando perplexity={perp}...\", end=\" \")\n    start = time.time()\n\n    tsne = TSNE(n_components=2, perplexity=perp, random_state=42,\n                n_iter=1000, learning_rate='auto')\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    elapsed = time.time() - start\n    print(f\"({elapsed:.1f}s)\")\n\n    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                    alpha=0.6, s=10, edgecolors='none')\n    axes[i].set_title(f'Perplexity = {perp}')\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n\nplt.suptitle('Efecto de la Perplexity en t-SNE', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nInterpretaci\u00f3n de Perplexity:\n- Perplexity baja (5-10): Estructura muy local, clusters peque\u00f1os\n- Perplexity media (30): Balance entre local y global (recomendado)\n- Perplexity alta (50-100): Estructura m\u00e1s global, clusters m\u00e1s grandes\n- Perplexity &gt; n_samples/3 puede causar problemas\n\"\"\")\n\n# -------------------------------------------------------------\n# 3. EFECTO DEL N\u00daMERO DE ITERACIONES\n# -------------------------------------------------------------\nprint(\"\\n[2] EFECTO DEL N\u00daMERO DE ITERACIONES\")\nprint(\"-\"*40)\n\nn_iters = [250, 500, 1000, 2000]\n\nfig, axes = plt.subplots(1, len(n_iters), figsize=(16, 4))\n\nfor i, n_iter in enumerate(n_iters):\n    print(f\"  Calculando n_iter={n_iter}...\", end=\" \")\n    start = time.time()\n\n    tsne = TSNE(n_components=2, perplexity=30, random_state=42,\n                n_iter=n_iter, learning_rate='auto')\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    elapsed = time.time() - start\n    print(f\"KL={tsne.kl_divergence_:.4f} ({elapsed:.1f}s)\")\n\n    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                    alpha=0.6, s=10, edgecolors='none')\n    axes[i].set_title(f'n_iter={n_iter}\\nKL={tsne.kl_divergence_:.3f}')\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n\nplt.suptitle('Efecto del N\u00famero de Iteraciones', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nInterpretaci\u00f3n:\n- Muy pocas iteraciones: t-SNE no converge (estructura incompleta)\n- 1000 iteraciones suele ser suficiente para la mayor\u00eda de casos\n- M\u00e1s iteraciones mejoran hasta un punto, luego estabilizan\n\"\"\")\n\n# -------------------------------------------------------------\n# 4. ESTABILIDAD: M\u00daLTIPLES EJECUCIONES\n# -------------------------------------------------------------\nprint(\"\\n[3] ESTABILIDAD DE t-SNE\")\nprint(\"-\"*40)\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\nfor i in range(4):\n    # Diferentes random_state\n    tsne = TSNE(n_components=2, perplexity=30, random_state=i*10,\n                n_iter=1000, learning_rate='auto')\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                    alpha=0.6, s=10, edgecolors='none')\n    axes[i].set_title(f'random_state={i*10}')\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n\nplt.suptitle('Diferentes Inicializaciones de t-SNE', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nObservaciones sobre estabilidad:\n- t-SNE NO es determin\u00edstico (diferente resultado cada vez)\n- Los CLUSTERS se preservan, pero su POSICI\u00d3N y ORIENTACI\u00d3N cambian\n- No comparar posiciones entre diferentes ejecuciones\n- Usar random_state fijo para reproducibilidad\n\"\"\")\n\n# -------------------------------------------------------------\n# 5. INICIALIZACI\u00d3N CON PCA\n# -------------------------------------------------------------\nprint(\"\\n[4] INICIALIZACI\u00d3N CON PCA (Recomendado)\")\nprint(\"-\"*40)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Sin inicializaci\u00f3n PCA\ntsne_random = TSNE(n_components=2, perplexity=30, random_state=42,\n                   n_iter=1000, init='random', learning_rate='auto')\nX_tsne_random = tsne_random.fit_transform(X_scaled)\n\n# Con inicializaci\u00f3n PCA\ntsne_pca = TSNE(n_components=2, perplexity=30, random_state=42,\n                n_iter=1000, init='pca', learning_rate='auto')\nX_tsne_pca = tsne_pca.fit_transform(X_scaled)\n\naxes[0].scatter(X_tsne_random[:, 0], X_tsne_random[:, 1], c=y, \n                cmap='tab10', alpha=0.6, s=15)\naxes[0].set_title(f\"init='random'\\nKL={tsne_random.kl_divergence_:.4f}\")\naxes[0].set_xticks([])\naxes[0].set_yticks([])\n\naxes[1].scatter(X_tsne_pca[:, 0], X_tsne_pca[:, 1], c=y,\n                cmap='tab10', alpha=0.6, s=15)\naxes[1].set_title(f\"init='pca' (recomendado)\\nKL={tsne_pca.kl_divergence_:.4f}\")\naxes[1].set_xticks([])\naxes[1].set_yticks([])\n\nplt.suptitle('Efecto de la Inicializaci\u00f3n', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\ninit='pca' es recomendado porque:\n- M\u00e1s reproducible\n- Convergencia m\u00e1s r\u00e1pida\n- Mejor preservaci\u00f3n de la estructura global\n\"\"\")\n\n# -------------------------------------------------------------\n# 6. t-SNE CON DATOS GRANDES (Barnes-Hut)\n# -------------------------------------------------------------\nprint(\"\\n[5] ESCALABILIDAD: Barnes-Hut vs Exact\")\nprint(\"-\"*40)\n\nprint(\"\"\"\nPara datasets grandes, usar method='barnes_hut':\n- Complejidad: O(n\u00b2) \u2192 O(n log n)\n- Aproximaci\u00f3n del algoritmo exacto\n- Por defecto cuando n_samples &gt; 10000\n\"\"\")\n\n# Ejemplo con datos m\u00e1s grandes\nfrom sklearn.datasets import make_blobs\nX_large, y_large = make_blobs(n_samples=5000, n_features=50, centers=10, random_state=42)\nX_large_scaled = StandardScaler().fit_transform(X_large)\n\n# Barnes-Hut (aproximado)\nprint(\"  Barnes-Hut (aproximado)...\", end=\" \")\nstart = time.time()\ntsne_bh = TSNE(n_components=2, perplexity=30, method='barnes_hut', \n               random_state=42, n_iter=1000)\nX_bh = tsne_bh.fit_transform(X_large_scaled)\nprint(f\"{time.time()-start:.1f}s\")\n\n# Exact (para comparaci\u00f3n - ser\u00e1 m\u00e1s lento)\nprint(\"  Exact...\", end=\" \")\nstart = time.time()\ntsne_exact = TSNE(n_components=2, perplexity=30, method='exact',\n                  random_state=42, n_iter=1000)\nX_exact = tsne_exact.fit_transform(X_large_scaled)\nprint(f\"{time.time()-start:.1f}s\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(X_bh[:, 0], X_bh[:, 1], c=y_large, cmap='tab10', alpha=0.5, s=5)\naxes[0].set_title(\"method='barnes_hut' (R\u00e1pido)\")\n\naxes[1].scatter(X_exact[:, 0], X_exact[:, 1], c=y_large, cmap='tab10', alpha=0.5, s=5)\naxes[1].set_title(\"method='exact' (Preciso)\")\n\nplt.suptitle('Comparaci\u00f3n de M\u00e9todos para n=5000', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 7. INTERPRETACI\u00d3N CORRECTA DE t-SNE\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"C\u00d3MO INTERPRETAR (Y NO INTERPRETAR) t-SNE\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n\u2705 LO QUE S\u00cd PUEDES INTERPRETAR:\n   - La existencia de clusters separados\n   - Puntos cercanos en t-SNE \u2192 similares en alta dimensi\u00f3n\n   - Estructura general de los datos\n\n\u274c LO QUE NO PUEDES INTERPRETAR:\n   - Tama\u00f1o de los clusters (distorsionado)\n   - Distancia entre clusters (no tiene significado)\n   - Densidad de los clusters\n   - Posici\u00f3n absoluta (rotaci\u00f3n/reflejo arbitrarios)\n\n\u26a0\ufe0f ERRORES COMUNES:\n   1. \"El cluster A es m\u00e1s grande que B\" \u2192 FALSO\n   2. \"Los clusters A y B est\u00e1n m\u00e1s cerca que A y C\" \u2192 PUEDE SER FALSO\n   3. \"Hay m\u00e1s densidad en esta regi\u00f3n\" \u2192 NO NECESARIAMENTE\n   4. Usar t-SNE como preprocesamiento para ML \u2192 NO RECOMENDADO\n\"\"\")\n\n# Demostraci\u00f3n del problema de distancias entre clusters\nprint(\"\\n[Demostraci\u00f3n: Distancias entre clusters NO son confiables]\")\n\n# Crear datos con distancias conocidas\nfrom sklearn.datasets import make_blobs\ncenters = [[0, 0], [10, 0], [100, 0]]  # Distancias 10 y 90\nX_demo, y_demo = make_blobs(n_samples=300, centers=centers, \n                            cluster_std=1, random_state=42)\n\n# Aplicar t-SNE\ntsne_demo = TSNE(n_components=2, perplexity=30, random_state=42)\nX_demo_tsne = tsne_demo.fit_transform(X_demo)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Espacio original\naxes[0].scatter(X_demo[:, 0], X_demo[:, 1], c=y_demo, cmap='tab10', alpha=0.7)\naxes[0].set_title('Espacio Original\\nDistancias: A-B=10, B-C=90')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\n\n# Espacio t-SNE\naxes[1].scatter(X_demo_tsne[:, 0], X_demo_tsne[:, 1], c=y_demo, cmap='tab10', alpha=0.7)\naxes[1].set_title('Espacio t-SNE\\n\u00bfSe preservan las distancias relativas?')\naxes[1].set_xlabel('t-SNE 1')\naxes[1].set_ylabel('t-SNE 2')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nConclusi\u00f3n: Las distancias relativas entre clusters NO se preservan en t-SNE\nEl cluster C que estaba 9x m\u00e1s lejos puede aparecer a distancia similar en t-SNE\n\"\"\")\n\n# -------------------------------------------------------------\n# 8. RESUMEN DE MEJORES PR\u00c1CTICAS\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEJORES PR\u00c1CTICAS PARA t-SNE\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n1. PREPROCESAMIENTO:\n   - Siempre estandarizar (StandardScaler)\n   - Considerar reducir con PCA primero si dim &gt; 50\n\n2. HIPERPAR\u00c1METROS:\n   - perplexity: 5-50, t\u00edpicamente 30\n   - n_iter: al menos 1000, verificar convergencia (KL divergence)\n   - learning_rate: 'auto' o n_samples/12\n   - init: 'pca' para mayor reproducibilidad\n\n3. VISUALIZACI\u00d3N:\n   - No confiar en tama\u00f1os de clusters\n   - No confiar en distancias entre clusters\n   - Ejecutar varias veces para verificar estabilidad\n\n4. NO USAR PARA:\n   - Preprocesamiento de ML\n   - Clustering (usar los datos originales)\n   - Comparar posiciones entre diferentes ejecuciones\n\"\"\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/06-tsne/#66-hiperparametros-de-t-sne-en-scikit-learn","title":"6.6. Hiperpar\u00e1metros de t-SNE en scikit-learn","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_components</code> Dimensiones de salida 2 o 3 2 para visualizaci\u00f3n <code>perplexity</code> N\u00famero efectivo de vecinos 5-50 30 es un buen inicio <code>learning_rate</code> Tasa de aprendizaje 'auto', 10-1000 'auto' (n_samples/12) <code>n_iter</code> N\u00famero de iteraciones int &gt; 0 1000 m\u00ednimo <code>init</code> Inicializaci\u00f3n 'random', 'pca' 'pca' para reproducibilidad <code>method</code> Algoritmo 'barnes_hut', 'exact' 'barnes_hut' si n &gt; 10000 <code>metric</code> M\u00e9trica de distancia 'euclidean', 'cosine', etc. 'euclidean' <code>random_state</code> Semilla int o None Fijar para reproducibilidad"},{"location":"aprendizaje-no-supervisado/06-tsne/#67-aplicaciones-reales","title":"6.7. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#1-visualizacion-de-word-embeddings","title":"1. Visualizaci\u00f3n de Word Embeddings","text":"<p>Visualizar relaciones sem\u00e1nticas entre palabras (Word2Vec, GloVe). * Tutorial: Visualizing Word Embeddings</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#2-analisis-de-imagenes","title":"2. An\u00e1lisis de Im\u00e1genes","text":"<p>Explorar similitud entre im\u00e1genes en datasets como MNIST, CIFAR. * Ejemplo: t-SNE on MNIST</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#3-bioinformatica","title":"3. Bioinform\u00e1tica","text":"<p>Visualizar expresi\u00f3n g\u00e9nica, scRNA-seq (single-cell RNA sequencing).</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#4-deteccion-de-fraude","title":"4. Detecci\u00f3n de Fraude","text":"<p>Visualizar transacciones para identificar patrones an\u00f3malos.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#68-t-sne-vs-otras-tecnicas-de-visualizacion","title":"6.8. t-SNE vs Otras T\u00e9cnicas de Visualizaci\u00f3n","text":"T\u00e9cnica Tipo Velocidad Estructura Mejor para PCA Lineal Muy r\u00e1pida Global Preprocesamiento, interpretaci\u00f3n t-SNE No lineal Lenta Local Visualizaci\u00f3n de clusters UMAP No lineal R\u00e1pida Local + Global Visualizaci\u00f3n + ML MDS No lineal Media Global Preservar distancias"},{"location":"aprendizaje-no-supervisado/06-tsne/#umap-la-alternativa-moderna","title":"UMAP: La Alternativa Moderna","text":"<p>UMAP (Uniform Manifold Approximation and Projection) es una alternativa m\u00e1s reciente a t-SNE:</p> <pre><code># pip install umap-learn\nimport umap\n\nreducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\nX_umap = reducer.fit_transform(X_scaled)\n</code></pre> <p>Ventajas de UMAP sobre t-SNE: - M\u00e1s r\u00e1pido - Mejor preservaci\u00f3n de estructura global - Se puede usar para ML (transformar nuevos datos)</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#69-resumen-y-mejores-practicas","title":"6.9. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#checklist-para-usar-t-sne","title":"Checklist para usar t-SNE","text":"<ul> <li>[ ] Estandarizar los datos</li> <li>[ ] Reducir dimensionalidad primero con PCA si dim &gt; 50</li> <li>[ ] Empezar con perplexity=30 y ajustar</li> <li>[ ] Usar n_iter &gt;= 1000 y verificar convergencia</li> <li>[ ] Usar init='pca' para reproducibilidad</li> <li>[ ] Ejecutar m\u00faltiples veces para verificar estabilidad</li> <li>[ ] NO interpretar tama\u00f1os ni distancias entre clusters</li> </ul>"},{"location":"aprendizaje-no-supervisado/06-tsne/#cuando-usar-t-sne","title":"\u00bfCu\u00e1ndo usar t-SNE?","text":"<p>\u2705 Usar t-SNE cuando: - Quieres visualizar datos de alta dimensi\u00f3n - Buscas identificar clusters visualmente - El dataset es de tama\u00f1o moderado (&lt; 50K puntos) - Solo necesitas visualizaci\u00f3n (no ML downstream)</p> <p>\u274c Considerar alternativas cuando: - Necesitas velocidad con datos grandes \u2192 UMAP - Quieres preservar distancias globales \u2192 PCA, MDS - Necesitas transformar nuevos datos \u2192 UMAP, PCA - Quieres interpretabilidad \u2192 PCA</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/","title":"\ud83d\udd0d Unidad 7. Isolation Forest - Detecci\u00f3n de Anomal\u00edas","text":"<p>Isolation Forest es un algoritmo de detecci\u00f3n de anomal\u00edas no supervisado basado en \u00e1rboles de decisi\u00f3n. A diferencia de otros m\u00e9todos que intentan modelar los datos normales, Isolation Forest se enfoca en aislar las anomal\u00edas. La idea clave es que las anomal\u00edas son m\u00e1s f\u00e1ciles de aislar porque son raras y tienen valores at\u00edpicos, lo que significa que requieren menos divisiones para separarlas del resto.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#71-como-funciona-isolation-forest","title":"7.1. \u00bfC\u00f3mo Funciona Isolation Forest?","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#la-intuicion","title":"La Intuici\u00f3n","text":"<p>Imagina que tienes un bosque de puntos de datos. Si eliges un punto al azar y empiezas a hacer divisiones aleatorias:</p> <ul> <li>Puntos normales: Est\u00e1n en regiones densas, rodeados de muchos puntos \u2192 necesitan muchas divisiones para ser aislados</li> <li>Anomal\u00edas: Est\u00e1n solos, lejos del resto \u2192 se a\u00edslan con pocas divisiones</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INTUICI\u00d3N DE ISOLATION FOREST                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502     Datos originales:                                       \u2502\n\u2502                                                             \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf             \u2190 Cluster denso                     \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf             (puntos normales)                   \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf                                                 \u2502\n\u2502                                                             \u2502\n\u2502                                         \u2297 \u2190 Anomal\u00eda       \u2502\n\u2502                                                             \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502                                                             \u2502\n\u2502     Una divisi\u00f3n aleatoria puede aislar la anomal\u00eda:       \u2502\n\u2502                                                             \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf \u2502                                               \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf \u2502                                               \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf \u2502                   \u2297 \u2190 \u00a1Aislada con 1 corte!   \u2502\n\u2502            \u2502                                               \u2502\n\u2502                                                             \u2502\n\u2502     Pero un punto normal necesita m\u00e1s cortes:              \u2502\n\u2502                                                             \u2502\n\u2502     \u25cf\u25cf\u2500\u252c\u2500\u25cf\u25cf \u2502                                              \u2502\n\u2502     \u25cf\u25cf\u2500\u253c\u2500\u25cf\u25cf \u2502    Necesita 3+ cortes para                   \u2502\n\u2502     \u25cf\u25cf\u2500\u2534\u2500\u25cf\u25cf \u2502    aislar un punto del cluster               \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#el-algoritmo","title":"El Algoritmo","text":"<ol> <li>Construir m\u00faltiples \u00e1rboles de aislamiento (iForest)</li> <li>Para cada \u00e1rbol:</li> <li>Seleccionar aleatoriamente un subconjunto de datos</li> <li>Seleccionar aleatoriamente una caracter\u00edstica</li> <li>Seleccionar aleatoriamente un valor de corte entre min y max</li> <li>Dividir los datos recursivamente hasta que cada punto quede aislado o se alcance un l\u00edmite</li> <li>Calcular la \"path length\" (longitud del camino) promedio para cada punto</li> <li>Puntos con path length corta = Anomal\u00edas</li> </ol>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#72-explicacion-matematica","title":"7.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#path-length-longitud-del-camino","title":"Path Length (Longitud del Camino)","text":"<p>La path length \\(h(x)\\) de un punto \\(x\\) es el n\u00famero de divisiones necesarias para aislar ese punto desde la ra\u00edz hasta el nodo terminal.</p> <p>Para un punto \\(x\\), calculamos el path length promedio sobre todos los \u00e1rboles: \\(\\(E[h(x)] = \\frac{1}{t} \\sum_{i=1}^{t} h_i(x)\\)\\)</p> <p>Donde \\(t\\) es el n\u00famero de \u00e1rboles y \\(h_i(x)\\) es la path length en el \u00e1rbol \\(i\\).</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#path-length-esperada","title":"Path Length Esperada","text":"<p>Para un \u00e1rbol binario construido con \\(n\\) puntos, la path length promedio esperada para un punto normal es aproximadamente:</p> \\[c(n) = 2H(n-1) - \\frac{2(n-1)}{n}\\] <p>Donde \\(H(i)\\) es el n\u00famero harm\u00f3nico: \\(H(i) = \\ln(i) + \\gamma\\) (\u03b3 \u2248 0.5772 es la constante de Euler-Mascheroni).</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#anomaly-score","title":"Anomaly Score","text":"<p>El anomaly score normaliza la path length:</p> \\[s(x, n) = 2^{-\\frac{E[h(x)]}{c(n)}}\\] <p>Interpretaci\u00f3n: - \\(s(x, n) \\approx 1\\): Punto es una anomal\u00eda (path length muy corta) - \\(s(x, n) \\approx 0.5\\): Punto normal (path length promedio) - \\(s(x, n) &lt; 0.5\\): Punto muy normal (path length larga)</p> <pre><code>      Anomaly Score\n\n      1.0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2297 Anomal\u00edas\n\n      0.5 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u25cf Puntos normales\n\n      0.0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#73-pros-y-contras","title":"7.3. Pros y Contras","text":"Ventajas Desventajas No necesita etiquetas: Completamente no supervisado Sensible a contaminaci\u00f3n: El par\u00e1metro contamination afecta mucho Muy eficiente: Complejidad \\(O(n \\log n)\\) No ideal para datos de alta dimensi\u00f3n: Pierde efectividad Escalable: Funciona bien con datasets grandes Asume anomal\u00edas son aislables: No funciona si las anomal\u00edas forman clusters Robusto: No asume distribuci\u00f3n de los datos No probabil\u00edstico: Solo da scores, no probabilidades Pocos hiperpar\u00e1metros: F\u00e1cil de configurar Puede perder anomal\u00edas sutiles: Si est\u00e1n cerca de datos normales"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#74-ejemplo-basico-en-python","title":"7.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico de Isolation Forest para detectar anomal\u00edas.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: Isolation Forest para detecci\u00f3n de anomal\u00edas\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.ensemble import IsolationForest  # Algoritmo principal\nfrom sklearn.datasets import make_blobs     # Generar datos sint\u00e9ticos\n\n# -------------------------------------------------------------\n# 1. CREAR DATOS CON ANOMAL\u00cdAS\n# -------------------------------------------------------------\n# Generar datos normales (cluster)\nnp.random.seed(42)\nX_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n\n# A\u00f1adir anomal\u00edas (puntos lejanos del cluster)\nn_anomalies = 20\nX_anomalies = np.random.uniform(low=-6, high=6, size=(n_anomalies, 2))\n\n# Combinar datos\nX = np.vstack([X_normal, X_anomalies])\n\n# Crear etiquetas verdaderas para evaluaci\u00f3n\n# 1 = normal, -1 = anomal\u00eda\ny_true = np.array([1] * len(X_normal) + [-1] * n_anomalies)\n\nprint(\"=\"*50)\nprint(\"ISOLATION FOREST - DETECCI\u00d3N DE ANOMAL\u00cdAS\")\nprint(\"=\"*50)\nprint(f\"\\nTotal de puntos: {len(X)}\")\nprint(f\"Puntos normales: {len(X_normal)}\")\nprint(f\"Anomal\u00edas verdaderas: {n_anomalies}\")\nprint(f\"Tasa de contaminaci\u00f3n real: {n_anomalies/len(X):.2%}\")\n\n# -------------------------------------------------------------\n# 2. VISUALIZAR DATOS ORIGINALES\n# -------------------------------------------------------------\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_normal[:, 0], X_normal[:, 1], c='blue', alpha=0.6, label='Normal', s=30)\nplt.scatter(X_anomalies[:, 0], X_anomalies[:, 1], c='red', marker='x', s=100, \n            label='Anomal\u00edas', linewidths=2)\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Datos Originales (con etiquetas verdaderas)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# -------------------------------------------------------------\n# 3. APLICAR ISOLATION FOREST\n# -------------------------------------------------------------\n# Crear y entrenar el modelo\n# contamination: proporci\u00f3n esperada de anomal\u00edas\niso_forest = IsolationForest(\n    n_estimators=100,           # N\u00famero de \u00e1rboles\n    contamination=0.1,          # Proporci\u00f3n esperada de anomal\u00edas (10%)\n    random_state=42,            # Reproducibilidad\n    max_samples='auto'          # Muestras por \u00e1rbol\n)\n\n# Entrenar y predecir\n# fit_predict devuelve: 1 (normal), -1 (anomal\u00eda)\ny_pred = iso_forest.fit_predict(X)\n\n# -------------------------------------------------------------\n# 4. ANALIZAR RESULTADOS\n# -------------------------------------------------------------\n# Contar predicciones\nn_pred_normal = np.sum(y_pred == 1)\nn_pred_anomaly = np.sum(y_pred == -1)\n\nprint(f\"\\n--- Resultados de Isolation Forest ---\")\nprint(f\"Predichos como normal: {n_pred_normal}\")\nprint(f\"Predichos como anomal\u00eda: {n_pred_anomaly}\")\n\n# Calcular m\u00e9tricas de rendimiento\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Convertir etiquetas para m\u00e9tricas\nprint(f\"\\n--- Reporte de Clasificaci\u00f3n ---\")\nprint(classification_report(y_true, y_pred, target_names=['Anomal\u00eda (-1)', 'Normal (1)']))\n\n# Matriz de confusi\u00f3n\ncm = confusion_matrix(y_true, y_pred)\nprint(f\"Matriz de Confusi\u00f3n:\")\nprint(f\"                 Predicho Anomal\u00eda | Predicho Normal\")\nprint(f\"Real Anomal\u00eda:          {cm[0,0]:4d}      |      {cm[0,1]:4d}\")\nprint(f\"Real Normal:            {cm[1,0]:4d}      |      {cm[1,1]:4d}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZAR PREDICCIONES\n# -------------------------------------------------------------\nplt.subplot(1, 2, 2)\n\n# Separar por predicci\u00f3n\nmask_pred_normal = y_pred == 1\nmask_pred_anomaly = y_pred == -1\n\nplt.scatter(X[mask_pred_normal, 0], X[mask_pred_normal, 1], \n            c='green', alpha=0.6, label='Predicho Normal', s=30)\nplt.scatter(X[mask_pred_anomaly, 0], X[mask_pred_anomaly, 1], \n            c='red', marker='x', s=100, label='Predicho Anomal\u00eda', linewidths=2)\n\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Predicciones de Isolation Forest')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 6. OBTENER ANOMALY SCORES\n# -------------------------------------------------------------\n# decision_function devuelve el score de anomal\u00eda\n# Valores m\u00e1s negativos = m\u00e1s an\u00f3malo\nscores = iso_forest.decision_function(X)\n\n# score_samples devuelve valores similares (opuesto de la depth)\n# Valores m\u00e1s negativos = m\u00e1s an\u00f3malo\n\nprint(f\"\\n--- Anomaly Scores ---\")\nprint(f\"Score medio (normales): {scores[y_true == 1].mean():.3f}\")\nprint(f\"Score medio (anomal\u00edas): {scores[y_true == -1].mean():.3f}\")\nprint(f\"Rango de scores: [{scores.min():.3f}, {scores.max():.3f}]\")\n\n# Visualizar distribuci\u00f3n de scores\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(scores[y_true == 1], bins=30, alpha=0.7, label='Normal', color='blue')\nplt.hist(scores[y_true == -1], bins=10, alpha=0.7, label='Anomal\u00eda', color='red')\nplt.xlabel('Anomaly Score (decision_function)')\nplt.ylabel('Frecuencia')\nplt.title('Distribuci\u00f3n de Anomaly Scores')\nplt.legend()\nplt.axvline(x=0, color='black', linestyle='--', label='Umbral (0)')\n\nplt.subplot(1, 2, 2)\n# Colorear puntos por score\nscatter = plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='RdYlGn', \n                      alpha=0.7, s=30, edgecolors='k', linewidths=0.5)\nplt.colorbar(scatter, label='Anomaly Score')\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Mapa de Anomaly Scores\\n(Verde=Normal, Rojo=Anomal\u00eda)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nInterpretaci\u00f3n de decision_function:\n- Valores positivos \u2192 punto probablemente normal\n- Valores negativos \u2192 punto probablemente an\u00f3malo\n- El umbral por defecto es 0 (controlado por contamination)\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#75-ejemplo-avanzado-analisis-de-hiperparametros-y-casos-de-uso","title":"7.5. Ejemplo Avanzado: An\u00e1lisis de Hiperpar\u00e1metros y Casos de Uso","text":"<p>Este ejemplo explora la configuraci\u00f3n \u00f3ptima y casos de uso reales.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: Isolation Forest - An\u00e1lisis profundo\n# ============================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# -------------------------------------------------------------\n# 1. CREAR DATASET COMPLEJO\n# -------------------------------------------------------------\nnp.random.seed(42)\n\n# Datos normales: dos clusters\nfrom sklearn.datasets import make_blobs\nX_normal1, _ = make_blobs(n_samples=400, centers=[[2, 2]], cluster_std=0.8)\nX_normal2, _ = make_blobs(n_samples=400, centers=[[-2, -2]], cluster_std=0.8)\nX_normal = np.vstack([X_normal1, X_normal2])\n\n# Anomal\u00edas de diferentes tipos\nanomalies_global = np.random.uniform(-6, 6, (15, 2))  # Aleatorias\nanomalies_local = np.array([[0, 0], [0.5, 0.5], [-0.5, -0.5]])  # Entre clusters\nanomalies_edge = np.array([[4, 2], [2, 4], [-4, -2]])  # En bordes\n\nX_anomalies = np.vstack([anomalies_global, anomalies_local, anomalies_edge])\n\nX = np.vstack([X_normal, X_anomalies])\ny_true = np.array([1] * len(X_normal) + [-1] * len(X_anomalies))\n\nprint(\"=\"*60)\nprint(\"ISOLATION FOREST - AN\u00c1LISIS AVANZADO\")\nprint(\"=\"*60)\nprint(f\"\\nDataset: {len(X)} puntos ({len(X_normal)} normales, {len(X_anomalies)} anomal\u00edas)\")\n\n# -------------------------------------------------------------\n# 2. EFECTO DEL N\u00daMERO DE ESTIMADORES\n# -------------------------------------------------------------\nprint(\"\\n[1] EFECTO DEL N\u00daMERO DE ESTIMADORES (n_estimators)\")\nprint(\"-\"*50)\n\nn_estimators_list = [10, 50, 100, 200, 500]\nresults_estimators = []\n\nfor n_est in n_estimators_list:\n    iso = IsolationForest(n_estimators=n_est, contamination=0.05, random_state=42)\n    y_pred = iso.fit_predict(X)\n    f1 = f1_score(y_true, y_pred, pos_label=-1)\n    results_estimators.append(f1)\n    print(f\"  n_estimators={n_est:3d}: F1={f1:.3f}\")\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators_list, results_estimators, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('N\u00famero de Estimadores')\nplt.ylabel('F1-Score (Anomal\u00edas)')\nplt.title('Efecto del N\u00famero de \u00c1rboles')\nplt.grid(True, alpha=0.3)\n\n# -------------------------------------------------------------\n# 3. EFECTO DE LA CONTAMINACI\u00d3N\n# -------------------------------------------------------------\nprint(\"\\n[2] EFECTO DE LA CONTAMINACI\u00d3N (contamination)\")\nprint(\"-\"*50)\n\ncontamination_list = [0.01, 0.03, 0.05, 0.1, 0.15, 0.2]\nresults_contamination = {'precision': [], 'recall': [], 'f1': []}\n\nfor cont in contamination_list:\n    iso = IsolationForest(n_estimators=100, contamination=cont, random_state=42)\n    y_pred = iso.fit_predict(X)\n\n    prec = precision_score(y_true, y_pred, pos_label=-1)\n    rec = recall_score(y_true, y_pred, pos_label=-1)\n    f1 = f1_score(y_true, y_pred, pos_label=-1)\n\n    results_contamination['precision'].append(prec)\n    results_contamination['recall'].append(rec)\n    results_contamination['f1'].append(f1)\n\n    print(f\"  contamination={cont:.2f}: Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}\")\n\nplt.subplot(1, 2, 2)\nplt.plot(contamination_list, results_contamination['precision'], 'g^-', label='Precision', linewidth=2)\nplt.plot(contamination_list, results_contamination['recall'], 'rs-', label='Recall', linewidth=2)\nplt.plot(contamination_list, results_contamination['f1'], 'bo-', label='F1-Score', linewidth=2)\nplt.axvline(x=len(X_anomalies)/len(X), color='k', linestyle='--', alpha=0.5, label='Contam. real')\nplt.xlabel('Contamination')\nplt.ylabel('Score')\nplt.title('Efecto del Par\u00e1metro Contamination')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nContaminaci\u00f3n real en los datos: {len(X_anomalies)/len(X):.3f}\")\n\n# -------------------------------------------------------------\n# 4. EFECTO DE max_samples\n# -------------------------------------------------------------\nprint(\"\\n[3] EFECTO DE max_samples\")\nprint(\"-\"*50)\n\nmax_samples_list = [32, 64, 128, 256, 'auto']\nresults_samples = []\n\nfor max_s in max_samples_list:\n    iso = IsolationForest(n_estimators=100, contamination=0.05, \n                          max_samples=max_s, random_state=42)\n    y_pred = iso.fit_predict(X)\n    f1 = f1_score(y_true, y_pred, pos_label=-1)\n    results_samples.append(f1)\n    print(f\"  max_samples={str(max_s):5s}: F1={f1:.3f}\")\n\n# -------------------------------------------------------------\n# 5. CURVA ROC CON DIFERENTES UMBRALES\n# -------------------------------------------------------------\nprint(\"\\n[4] AN\u00c1LISIS DE UMBRALES Y CURVA ROC\")\nprint(\"-\"*50)\n\n# Entrenar modelo\niso = IsolationForest(n_estimators=100, random_state=42)\niso.fit(X)\n\n# Obtener scores (invertir signo para ROC)\nscores = -iso.decision_function(X)  # M\u00e1s alto = m\u00e1s an\u00f3malo\n\n# Calcular ROC\nfpr, tpr, thresholds = roc_curve(y_true == -1, scores)\nroc_auc = auc(fpr, tpr)\n\nprint(f\"  AUC-ROC: {roc_auc:.3f}\")\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {roc_auc:.3f}')\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Curva ROC')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Visualizar diferentes umbrales\nplt.subplot(1, 3, 2)\n\n# Umbral autom\u00e1tico (contamination=0.05)\niso_auto = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\ny_pred_auto = iso_auto.fit_predict(X)\n\nmask_normal = y_pred_auto == 1\nmask_anomaly = y_pred_auto == -1\n\nplt.scatter(X[mask_normal, 0], X[mask_normal, 1], c='blue', alpha=0.5, s=20, label='Normal')\nplt.scatter(X[mask_anomaly, 0], X[mask_anomaly, 1], c='red', marker='x', s=80, \n            label='Anomal\u00eda', linewidths=2)\nplt.title('Contamination = 5%')\nplt.legend()\n\n# Umbral m\u00e1s estricto\nplt.subplot(1, 3, 3)\n\niso_strict = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\ny_pred_strict = iso_strict.fit_predict(X)\n\nmask_normal = y_pred_strict == 1\nmask_anomaly = y_pred_strict == -1\n\nplt.scatter(X[mask_normal, 0], X[mask_normal, 1], c='blue', alpha=0.5, s=20, label='Normal')\nplt.scatter(X[mask_anomaly, 0], X[mask_anomaly, 1], c='red', marker='x', s=80, \n            label='Anomal\u00eda', linewidths=2)\nplt.title('Contamination = 2%')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 6. VISUALIZAR REGIONES DE DECISI\u00d3N\n# -------------------------------------------------------------\nprint(\"\\n[5] VISUALIZACI\u00d3N DE REGIONES DE DECISI\u00d3N\")\nprint(\"-\"*50)\n\n# Crear grid para visualizaci\u00f3n\nxx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Entrenar modelo\niso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\niso.fit(X)\n\n# Obtener scores para el grid\nZ = iso.decision_function(grid)\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\n# Contour de scores\ncontour = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlGn', alpha=0.8)\nplt.colorbar(contour, label='Anomaly Score')\n\n# Puntos reales\nplt.scatter(X[y_true == 1, 0], X[y_true == 1, 1], c='blue', edgecolors='k', \n            s=20, alpha=0.6, label='Normal')\nplt.scatter(X[y_true == -1, 0], X[y_true == -1, 1], c='red', marker='x',\n            s=100, linewidths=2, label='Anomal\u00eda real')\n\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Mapa de Anomaly Scores\\n(Verde=Normal, Rojo=An\u00f3malo)')\nplt.legend()\n\nplt.subplot(1, 2, 2)\n# Frontera de decisi\u00f3n\nplt.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\nplt.contourf(xx, yy, Z, levels=[Z.min(), 0], colors=['red'], alpha=0.3)\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=['green'], alpha=0.3)\n\nplt.scatter(X[y_true == 1, 0], X[y_true == 1, 1], c='blue', edgecolors='k',\n            s=20, alpha=0.6, label='Normal')\nplt.scatter(X[y_true == -1, 0], X[y_true == -1, 1], c='red', marker='x',\n            s=100, linewidths=2, label='Anomal\u00eda real')\n\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Frontera de Decisi\u00f3n\\n(Rojo=Regi\u00f3n an\u00f3mala)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 7. CASO REAL: DETECCI\u00d3N DE FRAUDE (SIMULADO)\n# -------------------------------------------------------------\nprint(\"\\n[6] CASO DE USO: DETECCI\u00d3N DE FRAUDE\")\nprint(\"-\"*50)\n\n# Simular datos de transacciones\nnp.random.seed(42)\nn_normal = 10000\nn_fraud = 100  # 1% de fraude\n\n# Transacciones normales: monto bajo-medio, hora comercial\nnormal_amount = np.abs(np.random.normal(50, 30, n_normal))\nnormal_hour = np.random.normal(14, 4, n_normal)  # Centrado en 2pm\nnormal_hour = np.clip(normal_hour, 0, 24)\n\n# Transacciones fraudulentas: montos altos, horas inusuales\nfraud_amount = np.abs(np.random.normal(500, 200, n_fraud))\nfraud_hour = np.random.uniform(0, 6, n_fraud)  # Madrugada\n\n# Combinar\nX_fraud = np.column_stack([\n    np.concatenate([normal_amount, fraud_amount]),\n    np.concatenate([normal_hour, fraud_hour])\n])\ny_fraud = np.array([1] * n_normal + [-1] * n_fraud)\n\n# Estandarizar\nscaler = StandardScaler()\nX_fraud_scaled = scaler.fit_transform(X_fraud)\n\n# Entrenar Isolation Forest\niso_fraud = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\ny_pred_fraud = iso_fraud.fit_predict(X_fraud_scaled)\n\n# Resultados\nprint(f\"  Transacciones totales: {len(X_fraud):,}\")\nprint(f\"  Fraudes reales: {n_fraud}\")\nprint(f\"  Fraudes detectados: {np.sum(y_pred_fraud == -1)}\")\n\n# M\u00e9tricas\nprec = precision_score(y_fraud, y_pred_fraud, pos_label=-1)\nrec = recall_score(y_fraud, y_pred_fraud, pos_label=-1)\nf1 = f1_score(y_fraud, y_pred_fraud, pos_label=-1)\n\nprint(f\"\\n  Precision: {prec:.3f} (De los detectados, qu\u00e9 % son fraude)\")\nprint(f\"  Recall: {rec:.3f} (De los fraudes reales, qu\u00e9 % detectamos)\")\nprint(f\"  F1-Score: {f1:.3f}\")\n\n# Visualizar\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_fraud[y_fraud == 1, 0], X_fraud[y_fraud == 1, 1], \n            c='green', alpha=0.3, s=5, label='Normal')\nplt.scatter(X_fraud[y_fraud == -1, 0], X_fraud[y_fraud == -1, 1],\n            c='red', marker='x', s=50, label='Fraude real')\nplt.xlabel('Monto ($)')\nplt.ylabel('Hora del d\u00eda')\nplt.title('Datos Reales')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_fraud[y_pred_fraud == 1, 0], X_fraud[y_pred_fraud == 1, 1],\n            c='green', alpha=0.3, s=5, label='Predicho Normal')\nplt.scatter(X_fraud[y_pred_fraud == -1, 0], X_fraud[y_pred_fraud == -1, 1],\n            c='red', marker='x', s=50, label='Predicho Fraude')\nplt.xlabel('Monto ($)')\nplt.ylabel('Hora del d\u00eda')\nplt.title('Predicciones de Isolation Forest')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 8. MEJORES PR\u00c1CTICAS\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEJORES PR\u00c1CTICAS PARA ISOLATION FOREST\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n1. PREPROCESAMIENTO:\n   - Estandarizar/normalizar las caracter\u00edsticas\n   - Considerar encoding adecuado para categor\u00edas\n   - Manejar valores faltantes antes\n\n2. HIPERPAR\u00c1METROS:\n   - n_estimators: 100 suele ser suficiente\n   - contamination: Usar conocimiento del dominio si es posible\n   - max_samples: 'auto' o sqrt(n_samples)\n\n3. EVALUACI\u00d3N:\n   - Si hay etiquetas: Precision, Recall, F1, AUC-ROC\n   - Sin etiquetas: Inspecci\u00f3n manual de anomal\u00edas detectadas\n   - Analizar distribuci\u00f3n de scores\n\n4. CONSIDERACIONES:\n   - No asume distribuci\u00f3n de los datos\n   - Funciona mejor con anomal\u00edas globales/aisladas\n   - Puede fallar con anomal\u00edas en clusters\n   - Revisar puntos cerca del umbral manualmente\n\"\"\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#76-hiperparametros-de-isolation-forest-en-scikit-learn","title":"7.6. Hiperpar\u00e1metros de Isolation Forest en scikit-learn","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_estimators</code> N\u00famero de \u00e1rboles int &gt; 0 100 (m\u00e1s = m\u00e1s estable) <code>contamination</code> Proporci\u00f3n esperada de anomal\u00edas float (0, 0.5) o 'auto' Basado en conocimiento del dominio <code>max_samples</code> Muestras por \u00e1rbol int, float, 'auto' 'auto' (min(256, n_samples)) <code>max_features</code> Caracter\u00edsticas por \u00e1rbol int o float 1.0 (todas) <code>bootstrap</code> Muestreo con reemplazo bool False <code>random_state</code> Semilla int o None Fijar para reproducibilidad <code>n_jobs</code> Procesadores paralelos int -1 para usar todos <code>warm_start</code> A\u00f1adir \u00e1rboles incrementalmente bool False"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#77-aplicaciones-reales","title":"7.7. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#1-deteccion-de-fraude-financiero","title":"1. Detecci\u00f3n de Fraude Financiero","text":"<p>Identificar transacciones sospechosas en tarjetas de cr\u00e9dito. * Ejemplo: Credit Card Fraud Detection</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#2-deteccion-de-intrusiones-en-redes","title":"2. Detecci\u00f3n de Intrusiones en Redes","text":"<p>Identificar patrones de tr\u00e1fico an\u00f3malos que puedan indicar ataques.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#3-mantenimiento-predictivo","title":"3. Mantenimiento Predictivo","text":"<p>Detectar comportamientos an\u00f3malos en sensores de maquinaria industrial.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#4-control-de-calidad","title":"4. Control de Calidad","text":"<p>Identificar productos defectuosos en l\u00edneas de producci\u00f3n.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#5-salud","title":"5. Salud","text":"<p>Detectar patrones an\u00f3malos en datos m\u00e9dicos (ECG, diagn\u00f3sticos).</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#78-comparacion-con-otros-metodos-de-deteccion-de-anomalias","title":"7.8. Comparaci\u00f3n con Otros M\u00e9todos de Detecci\u00f3n de Anomal\u00edas","text":"M\u00e9todo Tipo Velocidad Escalabilidad Mejor para Isolation Forest Basado en \u00e1rboles R\u00e1pido Excelente Anomal\u00edas globales One-Class SVM Basado en kernel Lento Pobre Datasets peque\u00f1os LOF Basado en densidad Medio Media Anomal\u00edas locales DBSCAN Clustering Medio Buena Clusters + anomal\u00edas Autoencoder Deep Learning Lento (train) Buena Alta dimensionalidad"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#79-resumen-y-checklist","title":"7.9. Resumen y Checklist","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#checklist-para-usar-isolation-forest","title":"Checklist para usar Isolation Forest","text":"<ul> <li>[ ] Preprocesar los datos (estandarizar, manejar NaN)</li> <li>[ ] Estimar contamination si es posible</li> <li>[ ] Empezar con n_estimators=100</li> <li>[ ] Visualizar anomaly scores para entender la distribuci\u00f3n</li> <li>[ ] Ajustar contamination seg\u00fan resultados</li> <li>[ ] Validar con m\u00e9tricas si hay etiquetas disponibles</li> <li>[ ] Inspeccionar manualmente las anomal\u00edas detectadas</li> </ul>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#cuando-usar-isolation-forest","title":"\u00bfCu\u00e1ndo usar Isolation Forest?","text":"<p>\u2705 Usar Isolation Forest cuando: - Tienes un dataset grande - Las anomal\u00edas son raras y diferentes al resto - No tienes etiquetas de anomal\u00edas - Necesitas un m\u00e9todo r\u00e1pido y escalable</p> <p>\u274c Considerar alternativas cuando: - Las anomal\u00edas forman clusters \u2192 LOF o DBSCAN - Dataset muy peque\u00f1o \u2192 One-Class SVM - Datos de alta dimensi\u00f3n complejos \u2192 Autoencoders - Necesitas probabilidades \u2192 Gaussian Mixture</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/","title":"\ud83d\uded2 Unidad 8. Apriori - Reglas de Asociaci\u00f3n","text":"<p>Apriori es un algoritmo cl\u00e1sico de miner\u00eda de reglas de asociaci\u00f3n utilizado para descubrir patrones frecuentes y relaciones entre \u00edtems en grandes datasets transaccionales. Es famoso por su aplicaci\u00f3n en el an\u00e1lisis de la cesta de la compra (Market Basket Analysis), donde se identifican qu\u00e9 productos suelen comprarse juntos. A diferencia de otros algoritmos de aprendizaje no supervisado, Apriori trabaja con datos transaccionales discretos.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#81-que-son-las-reglas-de-asociacion","title":"8.1. \u00bfQu\u00e9 son las Reglas de Asociaci\u00f3n?","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#el-problema-de-la-cesta-de-la-compra","title":"El Problema de la Cesta de la Compra","text":"<p>Imagina que tienes datos de transacciones de un supermercado:</p> Transacci\u00f3n \u00cdtems Comprados T1 Pan, Leche, Mantequilla T2 Pan, Cerveza T3 Leche, Pa\u00f1ales, Cerveza T4 Pan, Leche, Pa\u00f1ales, Cerveza T5 Pan, Leche, Pa\u00f1ales <p>Pregunta: \u00bfQu\u00e9 productos tienden a comprarse juntos?</p> <p>Una regla de asociaci\u00f3n tiene la forma:</p> \\[\\text{Si } \\{A, B\\} \\rightarrow \\text{ entonces } \\{C\\}\\] <p>Por ejemplo: \"Si un cliente compra Pan y Leche, es probable que tambi\u00e9n compre Mantequilla\"</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 REGLA DE ASOCIACI\u00d3N                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   {Pan, Leche} \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 {Mantequilla}               \u2502\n\u2502                                                             \u2502\n\u2502   \u25b2                               \u25b2                        \u2502\n\u2502   \u2502                               \u2502                        \u2502\n\u2502   Antecedente (LHS)              Consecuente (RHS)         \u2502\n\u2502   \"Si compra esto...\"            \"...probablemente         \u2502\n\u2502                                   tambi\u00e9n compra esto\"      \u2502\n\u2502                                                             \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502                                                             \u2502\n\u2502   M\u00e9tricas de la regla:                                     \u2502\n\u2502                                                             \u2502\n\u2502   Support (Soporte): \u00bfQu\u00e9 tan frecuente es {Pan,Leche}?    \u2502\n\u2502   Confidence (Confianza): \u00bfCon qu\u00e9 frecuencia se cumple?   \u2502\n\u2502   Lift: \u00bfEs la asociaci\u00f3n significativa?                   \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#82-conceptos-fundamentales-y-metricas","title":"8.2. Conceptos Fundamentales y M\u00e9tricas","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#itemset","title":"Itemset","text":"<p>Un itemset es un conjunto de \u00edtems. Por ejemplo: {Pan, Leche, Mantequilla}</p> <p>Un itemset frecuente es uno que aparece en al menos un n\u00famero m\u00ednimo de transacciones.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#support-soporte","title":"Support (Soporte)","text":"<p>El soporte mide qu\u00e9 tan frecuente es un itemset en el dataset:</p> \\[Support(A) = \\frac{\\text{N\u00famero de transacciones que contienen } A}{\\text{N\u00famero total de transacciones}}\\] <p>Para una regla \\(A \\rightarrow B\\):</p> \\[Support(A \\rightarrow B) = \\frac{|\\{t : A \\cup B \\subseteq t\\}|}{|T|}\\] <p>Ejemplo: Si {Pan, Leche} aparece en 3 de 5 transacciones: \\(\\(Support(\\{Pan, Leche\\}) = \\frac{3}{5} = 0.6 = 60\\%\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#confidence-confianza","title":"Confidence (Confianza)","text":"<p>La confianza mide qu\u00e9 tan a menudo se cumple la regla cuando el antecedente est\u00e1 presente:</p> \\[Confidence(A \\rightarrow B) = \\frac{Support(A \\cup B)}{Support(A)} = P(B|A)\\] <p>Ejemplo: Si {Pan} aparece en 4 transacciones y {Pan, Leche} en 3: \\(\\(Confidence(\\{Pan\\} \\rightarrow \\{Leche\\}) = \\frac{3}{4} = 0.75 = 75\\%\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#lift","title":"Lift","text":"<p>El lift mide si la asociaci\u00f3n es significativa o simplemente debida al azar:</p> \\[Lift(A \\rightarrow B) = \\frac{Confidence(A \\rightarrow B)}{Support(B)} = \\frac{P(A \\cap B)}{P(A) \\cdot P(B)}\\] <p>Interpretaci\u00f3n: - Lift &gt; 1: A y B aparecen juntos m\u00e1s de lo esperado por azar \u2192 Asociaci\u00f3n positiva - Lift = 1: A y B son independientes - Lift &lt; 1: A y B aparecen juntos menos de lo esperado \u2192 Asociaci\u00f3n negativa</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#conviction","title":"Conviction","text":"<p>La conviction mide qu\u00e9 tan diferente es la regla de una asociaci\u00f3n aleatoria:</p> \\[Conviction(A \\rightarrow B) = \\frac{1 - Support(B)}{1 - Confidence(A \\rightarrow B)}\\] <p>Interpretaci\u00f3n: - Conviction alto: La regla es muy \u00fatil - Conviction = 1: A y B son independientes - Conviction = \u221e: La regla siempre se cumple</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#83-el-algoritmo-apriori","title":"8.3. El Algoritmo Apriori","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#la-propiedad-apriori-antimonotonia","title":"La Propiedad Apriori (Antimonoton\u00eda)","text":"<p>El algoritmo se basa en un principio clave:</p> <p>\"Si un itemset es infrecuente, todos sus superconjuntos tambi\u00e9n ser\u00e1n infrecuentes\"</p> <p>Esto permite podar el espacio de b\u00fasqueda eficientemente.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PRINCIPIO APRIORI                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   Si {A, B} es INFRECUENTE                                 \u2502\n\u2502                                                             \u2502\n\u2502   Entonces NO necesitamos verificar:                       \u2502\n\u2502   - {A, B, C}                                               \u2502\n\u2502   - {A, B, D}                                               \u2502\n\u2502   - {A, B, C, D}                                            \u2502\n\u2502   - ... (todos los superconjuntos)                         \u2502\n\u2502                                                             \u2502\n\u2502   \u00a1Esto ahorra much\u00edsimo c\u00f3mputo!                          \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#pasos-del-algoritmo","title":"Pasos del Algoritmo","text":"<ol> <li>Paso 1: Encontrar todos los itemsets de tama\u00f1o 1 que cumplan min_support</li> <li>Paso 2: Generar candidatos de tama\u00f1o k+1 combinando itemsets frecuentes de tama\u00f1o k</li> <li>Paso 3: Podar candidatos que contengan subconjuntos infrecuentes</li> <li>Paso 4: Calcular soporte de candidatos restantes</li> <li>Paso 5: Repetir hasta que no haya m\u00e1s itemsets frecuentes</li> <li>Paso 6: Generar reglas de los itemsets frecuentes que cumplan min_confidence</li> </ol>"},{"location":"aprendizaje-no-supervisado/08-apriori/#84-pros-y-contras","title":"8.4. Pros y Contras","text":"Ventajas Desventajas F\u00e1cil de entender e implementar: Algoritmo intuitivo Puede ser lento: M\u00faltiples escaneos del dataset Interpretable: Las reglas son f\u00e1ciles de explicar Genera muchos candidatos: Especialmente con min_support bajo Escalable con poda: La propiedad apriori reduce b\u00fasqueda Requiere datos discretos: No funciona con datos continuos directamente Ampliamente usado: Implementaciones optimizadas disponibles Sensible a par\u00e1metros: min_support y min_confidence afectan mucho Resultados accionables: \u00datil para decisiones de negocio No captura contexto: Solo frecuencia, no causa-efecto"},{"location":"aprendizaje-no-supervisado/08-apriori/#85-ejemplo-basico-en-python","title":"8.5. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo usa la biblioteca <code>mlxtend</code> para implementar Apriori y reglas de asociaci\u00f3n.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: Apriori para an\u00e1lisis de cesta de compra\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport pandas as pd                          # Manipulaci\u00f3n de datos\nimport numpy as np                           # Operaciones num\u00e9ricas\nfrom mlxtend.preprocessing import TransactionEncoder  # Codificar transacciones\nfrom mlxtend.frequent_patterns import apriori, association_rules  # Algoritmo Apriori\n\n# -------------------------------------------------------------\n# 1. CREAR DATASET DE TRANSACCIONES\n# -------------------------------------------------------------\n# Lista de transacciones (cada transacci\u00f3n es una lista de \u00edtems)\ntransactions = [\n    ['Pan', 'Leche', 'Mantequilla'],\n    ['Pan', 'Cerveza'],\n    ['Leche', 'Pa\u00f1ales', 'Cerveza'],\n    ['Pan', 'Leche', 'Pa\u00f1ales', 'Cerveza'],\n    ['Pan', 'Leche', 'Pa\u00f1ales'],\n    ['Leche', 'Pa\u00f1ales', 'Cerveza'],\n    ['Pan', 'Leche'],\n    ['Pan', 'Cerveza', 'Pa\u00f1ales'],\n    ['Pan', 'Leche', 'Cerveza', 'Pa\u00f1ales'],\n    ['Leche', 'Mantequilla']\n]\n\nprint(\"=\"*60)\nprint(\"APRIORI - AN\u00c1LISIS DE CESTA DE LA COMPRA\")\nprint(\"=\"*60)\n\nprint(f\"\\n--- Transacciones Originales ---\")\nfor i, t in enumerate(transactions, 1):\n    print(f\"T{i}: {t}\")\n\nprint(f\"\\nTotal de transacciones: {len(transactions)}\")\n\n# -------------------------------------------------------------\n# 2. PREPARAR DATOS PARA APRIORI\n# -------------------------------------------------------------\n# TransactionEncoder convierte listas a matriz binaria\nte = TransactionEncoder()\nte_array = te.fit_transform(transactions)\n\n# Crear DataFrame con nombres de columnas (\u00edtems)\ndf = pd.DataFrame(te_array, columns=te.columns_)\n\nprint(f\"\\n--- Matriz de Transacciones (One-Hot Encoded) ---\")\nprint(df)\nprint(f\"\\n\u00cdtems \u00fanicos: {list(te.columns_)}\")\n\n# -------------------------------------------------------------\n# 3. ENCONTRAR ITEMSETS FRECUENTES\n# -------------------------------------------------------------\nprint(f\"\\n--- Paso 1: Encontrar Itemsets Frecuentes ---\")\n\n# Aplicar algoritmo Apriori\n# min_support: soporte m\u00ednimo (% de transacciones)\nfrequent_itemsets = apriori(\n    df, \n    min_support=0.3,  # Itemset debe aparecer en al menos 30% de transacciones\n    use_colnames=True  # Usar nombres de \u00edtems en lugar de \u00edndices\n)\n\n# Ordenar por soporte\nfrequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)\n\nprint(f\"\\nItemsets frecuentes (min_support=30%):\")\nprint(frequent_itemsets.to_string(index=False))\n\nprint(f\"\\nTotal de itemsets frecuentes: {len(frequent_itemsets)}\")\n\n# -------------------------------------------------------------\n# 4. GENERAR REGLAS DE ASOCIACI\u00d3N\n# -------------------------------------------------------------\nprint(f\"\\n--- Paso 2: Generar Reglas de Asociaci\u00f3n ---\")\n\n# Generar reglas con confianza m\u00ednima\nrules = association_rules(\n    frequent_itemsets,\n    metric='confidence',  # M\u00e9trica para filtrar\n    min_threshold=0.5     # Confianza m\u00ednima del 50%\n)\n\n# Seleccionar columnas relevantes y ordenar\nrules_display = rules[['antecedents', 'consequents', 'support', \n                        'confidence', 'lift', 'conviction']]\nrules_display = rules_display.sort_values('lift', ascending=False)\n\nprint(f\"\\nReglas de asociaci\u00f3n (min_confidence=50%):\")\nprint(rules_display.to_string(index=False))\n\nprint(f\"\\nTotal de reglas: {len(rules)}\")\n\n# -------------------------------------------------------------\n# 5. INTERPRETAR REGLAS\n# -------------------------------------------------------------\nprint(f\"\\n--- Interpretaci\u00f3n de las Mejores Reglas ---\")\n\n# Top 3 reglas por Lift\ntop_rules = rules.nlargest(3, 'lift')\n\nfor idx, (_, rule) in enumerate(top_rules.iterrows(), 1):\n    ant = list(rule['antecedents'])\n    cons = list(rule['consequents'])\n    supp = rule['support']\n    conf = rule['confidence']\n    lift = rule['lift']\n\n    print(f\"\\n{idx}. {ant} \u2192 {cons}\")\n    print(f\"   Soporte: {supp:.1%} (aparece en {supp*10:.0f}/10 transacciones)\")\n    print(f\"   Confianza: {conf:.1%} (cuando se compra {ant}, {conf:.0%} compra {cons})\")\n    print(f\"   Lift: {lift:.2f} ({lift:.2f}x m\u00e1s probable que por azar)\")\n\n# -------------------------------------------------------------\n# 6. VISUALIZAR REGLAS\n# -------------------------------------------------------------\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Scatter: Support vs Confidence, coloreado por Lift\nscatter = axes[0].scatter(rules['support'], rules['confidence'], \n                          c=rules['lift'], cmap='RdYlGn', s=100, alpha=0.7)\naxes[0].set_xlabel('Support')\naxes[0].set_ylabel('Confidence')\naxes[0].set_title('Reglas: Support vs Confidence\\n(color=Lift)')\nplt.colorbar(scatter, ax=axes[0], label='Lift')\n\n# Histograma de Lift\naxes[1].hist(rules['lift'], bins=10, color='steelblue', edgecolor='black')\naxes[1].axvline(x=1, color='red', linestyle='--', label='Lift=1 (independencia)')\naxes[1].set_xlabel('Lift')\naxes[1].set_ylabel('Frecuencia')\naxes[1].set_title('Distribuci\u00f3n del Lift')\naxes[1].legend()\n\n# Top 5 reglas por Lift\ntop_5 = rules.nlargest(5, 'lift')\nlabels = [f\"{list(r['antecedents'])} \u2192 {list(r['consequents'])}\" \n          for _, r in top_5.iterrows()]\nlabels = [l[:30] + '...' if len(l) &gt; 30 else l for l in labels]  # Truncar\n\naxes[2].barh(labels, top_5['lift'], color='steelblue')\naxes[2].axvline(x=1, color='red', linestyle='--', alpha=0.5)\naxes[2].set_xlabel('Lift')\naxes[2].set_title('Top 5 Reglas por Lift')\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 7. FILTRAR REGLAS \u00daTILES\n# -------------------------------------------------------------\nprint(f\"\\n--- Reglas Recomendadas para el Negocio ---\")\n\n# Reglas con alto lift Y alta confianza\ngood_rules = rules[(rules['lift'] &gt; 1.0) &amp; (rules['confidence'] &gt; 0.5)]\ngood_rules = good_rules.sort_values('lift', ascending=False)\n\nprint(f\"\\nReglas con Lift &gt; 1 y Confidence &gt; 50%:\")\nfor _, rule in good_rules.head(5).iterrows():\n    ant = ', '.join(list(rule['antecedents']))\n    cons = ', '.join(list(rule['consequents']))\n    print(f\"  \u2022 Si compra [{ant}], es {rule['confidence']:.0%} probable que compre [{cons}]\")\n    print(f\"    (Lift: {rule['lift']:.2f})\")\n\nprint(\"\"\"\nRecomendaciones basadas en las reglas:\n1. Colocar productos asociados cerca en el supermercado\n2. Crear promociones de productos que se compran juntos\n3. Recomendar productos complementarios en el checkout\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#86-ejemplo-avanzado-dataset-real-y-optimizacion","title":"8.6. Ejemplo Avanzado: Dataset Real y Optimizaci\u00f3n","text":"<p>Este ejemplo trabaja con un dataset m\u00e1s grande y explora diferentes par\u00e1metros.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: Apriori con dataset de retail\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# -------------------------------------------------------------\n# 1. CREAR DATASET SINT\u00c9TICO REALISTA\n# -------------------------------------------------------------\nnp.random.seed(42)\n\n# Simular 1000 transacciones de supermercado\nproducts = {\n    'Desayuno': ['Pan', 'Leche', 'Cereales', 'Huevos', 'Yogurt', 'Caf\u00e9', 'Zumo'],\n    'Snacks': ['Galletas', 'Chocolate', 'Patatas', 'Frutos_secos'],\n    'Bebidas': ['Agua', 'Cerveza', 'Vino', 'Refresco'],\n    'Limpieza': ['Detergente', 'Jab\u00f3n', 'Papel_higienico'],\n    'Beb\u00e9s': ['Pa\u00f1ales', 'Toallitas', 'Papilla']\n}\n\nall_products = [p for sublist in products.values() for p in sublist]\n\ndef generate_transaction():\n    \"\"\"Genera una transacci\u00f3n con productos correlacionados\"\"\"\n    items = []\n\n    # Desayuno: alta probabilidad de combinar\n    if np.random.random() &lt; 0.6:\n        items.extend(np.random.choice(['Pan', 'Leche', 'Cereales'], \n                                      size=np.random.randint(2, 4), replace=False))\n        if np.random.random() &lt; 0.3:\n            items.append('Mantequilla')\n\n    # Cerveza y pa\u00f1ales (ejemplo cl\u00e1sico)\n    if np.random.random() &lt; 0.15:\n        items.extend(['Cerveza', 'Pa\u00f1ales'])\n\n    # Productos aleatorios adicionales\n    n_random = np.random.randint(1, 5)\n    items.extend(np.random.choice(all_products, size=n_random, replace=False))\n\n    return list(set(items))\n\n# Generar transacciones\nn_transactions = 1000\ntransactions = [generate_transaction() for _ in range(n_transactions)]\n\nprint(\"=\"*60)\nprint(\"APRIORI - AN\u00c1LISIS AVANZADO\")\nprint(\"=\"*60)\n\nprint(f\"\\nTransacciones generadas: {n_transactions}\")\nprint(f\"Productos \u00fanicos: {len(all_products)}\")\n\n# Estad\u00edsticas b\u00e1sicas\nsizes = [len(t) for t in transactions]\nprint(f\"Tama\u00f1o promedio de transacci\u00f3n: {np.mean(sizes):.1f} \u00edtems\")\nprint(f\"Rango: {min(sizes)} - {max(sizes)} \u00edtems\")\n\n# -------------------------------------------------------------\n# 2. PREPARAR DATOS\n# -------------------------------------------------------------\nte = TransactionEncoder()\nte_array = te.fit_transform(transactions)\ndf = pd.DataFrame(te_array, columns=te.columns_)\n\nprint(f\"\\nMatriz de transacciones: {df.shape}\")\n\n# Frecuencia de productos individuales\nproduct_freq = df.sum().sort_values(ascending=False)\nprint(f\"\\n--- Top 10 Productos m\u00e1s Frecuentes ---\")\nprint(product_freq.head(10))\n\n# -------------------------------------------------------------\n# 3. COMPARAR PAR\u00c1METROS DE SOPORTE M\u00cdNIMO\n# -------------------------------------------------------------\nprint(f\"\\n--- Efecto del Soporte M\u00ednimo ---\")\n\nmin_supports = [0.01, 0.05, 0.1, 0.2, 0.3]\nresults = []\n\nfor min_sup in min_supports:\n    freq_items = apriori(df, min_support=min_sup, use_colnames=True)\n    n_items = len(freq_items)\n    max_size = freq_items['itemsets'].apply(len).max() if n_items &gt; 0 else 0\n    results.append({\n        'min_support': min_sup,\n        'n_itemsets': n_items,\n        'max_size': max_size\n    })\n    print(f\"  min_support={min_sup:.0%}: {n_items:4d} itemsets, max tama\u00f1o={max_size}\")\n\n# -------------------------------------------------------------\n# 4. ENCONTRAR ITEMSETS FRECUENTES (APRIORI vs FP-GROWTH)\n# -------------------------------------------------------------\nprint(f\"\\n--- Comparaci\u00f3n: Apriori vs FP-Growth ---\")\n\nimport time\n\n# Apriori\nstart = time.time()\nfreq_apriori = apriori(df, min_support=0.05, use_colnames=True)\ntime_apriori = time.time() - start\n\n# FP-Growth (m\u00e1s eficiente)\nstart = time.time()\nfreq_fpgrowth = fpgrowth(df, min_support=0.05, use_colnames=True)\ntime_fpgrowth = time.time() - start\n\nprint(f\"  Apriori:   {len(freq_apriori):4d} itemsets en {time_apriori:.3f}s\")\nprint(f\"  FP-Growth: {len(freq_fpgrowth):4d} itemsets en {time_fpgrowth:.3f}s\")\nprint(f\"  Speedup FP-Growth: {time_apriori/time_fpgrowth:.2f}x\")\n\n# Usar FP-Growth para el resto del an\u00e1lisis\nfrequent_itemsets = freq_fpgrowth\n\n# -------------------------------------------------------------\n# 5. GENERAR Y ANALIZAR REGLAS\n# -------------------------------------------------------------\nprint(f\"\\n--- Generaci\u00f3n de Reglas de Asociaci\u00f3n ---\")\n\n# Generar reglas con diferentes m\u00e9tricas\nmetrics = ['confidence', 'lift', 'conviction']\n\nfor metric in metrics:\n    rules = association_rules(frequent_itemsets, metric=metric, \n                              min_threshold=0.5 if metric=='confidence' else 1.0)\n    print(f\"  M\u00e9trica '{metric}': {len(rules)} reglas\")\n\n# Usar confianza como m\u00e9trica principal\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\nrules = rules[rules['lift'] &gt; 1]  # Solo asociaciones positivas\n\nprint(f\"\\nReglas finales (confidence&gt;50%, lift&gt;1): {len(rules)}\")\n\n# -------------------------------------------------------------\n# 6. TOP REGLAS POR DIFERENTES CRITERIOS\n# -------------------------------------------------------------\nprint(f\"\\n--- Top Reglas por Diferentes Criterios ---\")\n\ndef format_rule(row):\n    ant = ', '.join(list(row['antecedents']))\n    cons = ', '.join(list(row['consequents']))\n    return f\"{ant} \u2192 {cons}\"\n\n# Por Lift (asociaci\u00f3n m\u00e1s fuerte)\nprint(f\"\\n[Top 5 por LIFT - Asociaci\u00f3n m\u00e1s fuerte]\")\ntop_lift = rules.nlargest(5, 'lift')\nfor _, r in top_lift.iterrows():\n    print(f\"  {format_rule(r)}\")\n    print(f\"    Lift: {r['lift']:.2f}, Conf: {r['confidence']:.1%}, Supp: {r['support']:.1%}\")\n\n# Por Confidence (m\u00e1s confiables)\nprint(f\"\\n[Top 5 por CONFIDENCE - M\u00e1s confiables]\")\ntop_conf = rules.nlargest(5, 'confidence')\nfor _, r in top_conf.iterrows():\n    print(f\"  {format_rule(r)}\")\n    print(f\"    Conf: {r['confidence']:.1%}, Lift: {r['lift']:.2f}, Supp: {r['support']:.1%}\")\n\n# Por Support (m\u00e1s frecuentes)\nprint(f\"\\n[Top 5 por SUPPORT - M\u00e1s frecuentes]\")\ntop_supp = rules.nlargest(5, 'support')\nfor _, r in top_supp.iterrows():\n    print(f\"  {format_rule(r)}\")\n    print(f\"    Supp: {r['support']:.1%}, Conf: {r['confidence']:.1%}, Lift: {r['lift']:.2f}\")\n\n# -------------------------------------------------------------\n# 7. VISUALIZACI\u00d3N AVANZADA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 1. Heatmap de m\u00e9tricas\nmetrics_df = rules[['support', 'confidence', 'lift', 'leverage', 'conviction']].copy()\nmetrics_df.index = [format_rule(rules.iloc[i])[:20] + '...' for i in range(len(rules))]\n# Tomar solo top 15 para visualizaci\u00f3n\nmetrics_sample = metrics_df.head(15)\n\nax1 = axes[0, 0]\nfrom matplotlib.colors import Normalize\n# Normalizar para heatmap\nnormalized = (metrics_sample - metrics_sample.min()) / (metrics_sample.max() - metrics_sample.min())\nim = ax1.imshow(normalized.values, cmap='YlOrRd', aspect='auto')\nax1.set_xticks(range(len(metrics_sample.columns)))\nax1.set_xticklabels(metrics_sample.columns, rotation=45)\nax1.set_yticks(range(len(metrics_sample.index)))\nax1.set_yticklabels(metrics_sample.index, fontsize=8)\nax1.set_title('M\u00e9tricas de Reglas (normalizado)')\nplt.colorbar(im, ax=ax1)\n\n# 2. Scatter 3D: Support, Confidence, Lift\nfrom mpl_toolkits.mplot3d import Axes3D\nax2 = fig.add_subplot(2, 2, 2, projection='3d')\nax2.scatter(rules['support'], rules['confidence'], rules['lift'],\n            c=rules['lift'], cmap='viridis', s=50)\nax2.set_xlabel('Support')\nax2.set_ylabel('Confidence')\nax2.set_zlabel('Lift')\nax2.set_title('Reglas en 3D')\n\n# 3. An\u00e1lisis de productos en reglas\nax3 = axes[1, 0]\n\n# Contar apariciones de cada producto en reglas\nproduct_counts = {}\nfor _, row in rules.iterrows():\n    for item in row['antecedents']:\n        product_counts[item] = product_counts.get(item, 0) + 1\n    for item in row['consequents']:\n        product_counts[item] = product_counts.get(item, 0) + 1\n\nproduct_counts_sorted = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)[:10]\nproducts_names, counts = zip(*product_counts_sorted)\n\nax3.barh(products_names, counts, color='steelblue')\nax3.set_xlabel('Apariciones en reglas')\nax3.set_title('Productos m\u00e1s frecuentes en reglas')\n\n# 4. Distribuci\u00f3n de tama\u00f1os de itemsets\nax4 = axes[1, 1]\n\n# Tama\u00f1o de antecedentes y consecuentes\nant_sizes = rules['antecedents'].apply(len)\ncons_sizes = rules['consequents'].apply(len)\n\nax4.hist(ant_sizes, bins=range(1, max(ant_sizes)+2), alpha=0.7, \n         label='Antecedentes', color='blue')\nax4.hist(cons_sizes, bins=range(1, max(cons_sizes)+2), alpha=0.7,\n         label='Consecuentes', color='green')\nax4.set_xlabel('N\u00famero de \u00edtems')\nax4.set_ylabel('Frecuencia')\nax4.set_title('Tama\u00f1o de Antecedentes y Consecuentes')\nax4.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 8. FILTRAR REGLAS ACCIONABLES\n# -------------------------------------------------------------\nprint(f\"\\n--- Reglas Accionables para el Negocio ---\")\n\n# Definir umbrales para reglas \u00fatiles\nactionable_rules = rules[\n    (rules['support'] &gt; 0.05) &amp;      # Al menos 5% de transacciones\n    (rules['confidence'] &gt; 0.6) &amp;     # Al menos 60% de confianza\n    (rules['lift'] &gt; 1.2)             # Al menos 20% mejor que azar\n].copy()\n\nactionable_rules = actionable_rules.sort_values('lift', ascending=False)\n\nprint(f\"\\nReglas accionables encontradas: {len(actionable_rules)}\")\nprint(\"\\n[Recomendaciones estrat\u00e9gicas]\")\n\nfor i, (_, rule) in enumerate(actionable_rules.head(5).iterrows(), 1):\n    ant = ', '.join(list(rule['antecedents']))\n    cons = ', '.join(list(rule['consequents']))\n\n    print(f\"\\n{i}. Cuando el cliente compra: {ant}\")\n    print(f\"   \u2192 Recomendar: {cons}\")\n    print(f\"   Efectividad: {rule['confidence']:.0%} de probabilidad\")\n    print(f\"   Impacto: {rule['lift']:.2f}x m\u00e1s probable que aleatorio\")\n\n# -------------------------------------------------------------\n# 9. AN\u00c1LISIS DE PATRONES ESPEC\u00cdFICOS\n# -------------------------------------------------------------\nprint(f\"\\n--- An\u00e1lisis de Patrones Espec\u00edficos ---\")\n\n# Buscar reglas que contengan un producto espec\u00edfico\ntarget_product = 'Cerveza'\n\nrules_with_target = rules[\n    rules['antecedents'].apply(lambda x: target_product in x) |\n    rules['consequents'].apply(lambda x: target_product in x)\n]\n\nprint(f\"\\nReglas relacionadas con '{target_product}': {len(rules_with_target)}\")\nfor _, r in rules_with_target.head(3).iterrows():\n    print(f\"  {format_rule(r)} (Lift: {r['lift']:.2f})\")\n\n# -------------------------------------------------------------\n# 10. RESUMEN\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESUMEN Y MEJORES PR\u00c1CTICAS\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nAn\u00e1lisis completado:\n- Transacciones analizadas: {n_transactions:,}\n- Itemsets frecuentes encontrados: {len(frequent_itemsets)}\n- Reglas generadas: {len(rules)}\n- Reglas accionables: {len(actionable_rules)}\n\nMejores pr\u00e1cticas para Apriori:\n1. min_support: Empezar alto (0.1) y bajar si hay pocos resultados\n2. min_confidence: 0.5-0.7 para reglas confiables\n3. Lift: Siempre filtrar por lift &gt; 1 para asociaciones reales\n4. FP-Growth: Usar en lugar de Apriori para datasets grandes\n5. Validaci\u00f3n: Revisar reglas con expertos del dominio\n\nUso empresarial:\n- Cross-selling: Recomendar productos complementarios\n- Layout de tienda: Colocar productos asociados juntos\n- Promociones: Crear bundles basados en asociaciones\n- Inventario: Mantener stock de productos asociados\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#87-hiperparametros-y-parametros","title":"8.7. Hiperpar\u00e1metros y Par\u00e1metros","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#parametros-de-apriori-mlxtend","title":"Par\u00e1metros de Apriori (mlxtend)","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>min_support</code> Soporte m\u00ednimo para itemsets 0.0-1.0 0.01-0.1 (depende del dataset) <code>use_colnames</code> Usar nombres de columnas bool True <code>max_len</code> Tama\u00f1o m\u00e1ximo de itemsets int None (sin l\u00edmite) <code>low_memory</code> Modo bajo memoria bool False"},{"location":"aprendizaje-no-supervisado/08-apriori/#parametros-de-association-rules-mlxtend","title":"Par\u00e1metros de Association Rules (mlxtend)","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>metric</code> M\u00e9trica para filtrar 'support', 'confidence', 'lift', etc. 'confidence' o 'lift' <code>min_threshold</code> Umbral m\u00ednimo de la m\u00e9trica float Depende de la m\u00e9trica"},{"location":"aprendizaje-no-supervisado/08-apriori/#88-alternativas-a-apriori","title":"8.8. Alternativas a Apriori","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#fp-growth","title":"FP-Growth","text":"<p>FP-Growth (Frequent Pattern Growth) es m\u00e1s eficiente que Apriori: - No genera candidatos expl\u00edcitamente - Usa una estructura de \u00e1rbol (FP-tree) - Solo requiere 2 escaneos del dataset</p> <pre><code>from mlxtend.frequent_patterns import fpgrowth\n\nfrequent_itemsets = fpgrowth(df, min_support=0.05, use_colnames=True)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#eclat","title":"Eclat","text":"<p>Eclat usa intersecci\u00f3n de conjuntos verticales: - Puede ser m\u00e1s r\u00e1pido para datasets densos</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#spmf","title":"SPMF","text":"<p>Para an\u00e1lisis m\u00e1s avanzado, la biblioteca SPMF ofrece muchos algoritmos de patrones secuenciales.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#89-aplicaciones-reales","title":"8.9. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#1-retail-y-e-commerce","title":"1. Retail y E-commerce","text":"<p>An\u00e1lisis de cesta de compra, recomendaciones de productos. * Amazon: \"Customers who bought this also bought...\"</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#2-banca-y-finanzas","title":"2. Banca y Finanzas","text":"<p>Detectar patrones de fraude, servicios complementarios.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#3-medicina","title":"3. Medicina","text":"<p>Asociaciones entre s\u00edntomas y diagn\u00f3sticos, efectos secundarios de medicamentos.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#4-web-mining","title":"4. Web Mining","text":"<p>Analizar patrones de navegaci\u00f3n, p\u00e1ginas visitadas juntas.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#5-telecomunicaciones","title":"5. Telecomunicaciones","text":"<p>An\u00e1lisis de servicios contratados juntos, patrones de uso.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#810-resumen-y-checklist","title":"8.10. Resumen y Checklist","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#checklist-para-usar-apriori","title":"Checklist para usar Apriori","text":"<ul> <li>[ ] Datos en formato transaccional (lista de \u00edtems por transacci\u00f3n)</li> <li>[ ] Codificar con TransactionEncoder para matriz binaria</li> <li>[ ] Empezar con min_support alto y ajustar</li> <li>[ ] Generar reglas con confidence razonable (&gt;50%)</li> <li>[ ] Filtrar por lift &gt; 1 para asociaciones reales</li> <li>[ ] Validar reglas con conocimiento del dominio</li> <li>[ ] Usar FP-Growth para datasets grandes</li> </ul>"},{"location":"aprendizaje-no-supervisado/08-apriori/#cuando-usar-apriori","title":"\u00bfCu\u00e1ndo usar Apriori?","text":"<p>\u2705 Usar Apriori cuando: - Tienes datos transaccionales discretos - Buscas patrones de co-ocurrencia - Necesitas reglas interpretables - Dataset de tama\u00f1o moderado (&lt;100K transacciones)</p> <p>\u274c Considerar alternativas cuando: - Datos continuos \u2192 Primero discretizar o usar clustering - Dataset muy grande \u2192 FP-Growth - Patrones secuenciales \u2192 GSP, PrefixSpan - Necesitas predicci\u00f3n \u2192 Modelos supervisados</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/","title":"Aprendizaje Supervisado en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 En este documento encontrar\u00e1s una introducci\u00f3n a los conceptos b\u00e1sicos del aprendizaje supervisado, una de las ramas fundamentales de la Inteligencia Artificial (IA) y del Machine Learning (ML).</p>"},{"location":"aprendizaje-supervisado/#que-es-el-aprendizaje-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje Supervisado?","text":"<p>El aprendizaje supervisado es un tipo de aprendizaje autom\u00e1tico en el que un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada ejemplo del conjunto de entrenamiento incluye tanto la entrada (X) como la salida deseada (Y).</p> <p>El objetivo del modelo es aprender la relaci\u00f3n entre las entradas y las salidas para poder predecir correctamente la salida de nuevos datos nunca vistos.</p>"},{"location":"aprendizaje-supervisado/#tipos-de-problemas-supervisados","title":"\ud83e\udde0 Tipos de Problemas Supervisados","text":"<ol> <li> <p>Clasificaci\u00f3n:    El modelo aprende a asignar una etiqueta o categor\u00eda a cada ejemplo.    \ud83d\udccd Ejemplo: Clasificar correos como \"spam\" o \"no spam\".</p> </li> <li> <p>Regresi\u00f3n:    El modelo aprende a predecir un valor num\u00e9rico continuo.    \ud83d\udccd Ejemplo: Predecir el precio de una vivienda seg\u00fan sus caracter\u00edsticas.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/#flujo-de-trabajo-del-aprendizaje-supervisado","title":"\u2699\ufe0f Flujo de Trabajo del Aprendizaje Supervisado","text":"<ol> <li>Recolecci\u00f3n de datos: Se obtiene un conjunto de datos con ejemplos representativos del problema.</li> <li>Preprocesamiento: Limpieza, normalizaci\u00f3n y divisi\u00f3n del conjunto de datos (entrenamiento y prueba).</li> <li>Selecci\u00f3n del modelo: Elegir el algoritmo m\u00e1s adecuado (por ejemplo, SVM, \u00c1rboles de decisi\u00f3n, Redes neuronales, etc.).</li> <li>Entrenamiento: El modelo aprende a partir de los datos etiquetados.</li> <li>Evaluaci\u00f3n: Se mide el rendimiento utilizando m\u00e9tricas como precisi\u00f3n, recall o error cuadr\u00e1tico medio.</li> <li>Predicci\u00f3n: Se aplican los conocimientos adquiridos a nuevos datos.</li> </ol>"},{"location":"aprendizaje-supervisado/#ejemplos-de-algoritmos-comunes","title":"\ud83d\udd0d Ejemplos de Algoritmos Comunes","text":"<ul> <li>Regresi\u00f3n lineal</li> <li>K-Nearest Neighbors (KNN)</li> <li>\u00c1rboles de decisi\u00f3n</li> <li>Random Forest</li> <li>M\u00e1quinas de Vectores de Soporte (SVM)</li> <li>Redes neuronales artificiales</li> </ul>"},{"location":"aprendizaje-supervisado/#evaluacion-del-modelo","title":"\ud83d\udcca Evaluaci\u00f3n del Modelo","text":"<p>Para medir la calidad del modelo se utilizan m\u00e9tricas que dependen del tipo de problema:</p> Tipo de problema M\u00e9tricas comunes Clasificaci\u00f3n Exactitud, Precisi\u00f3n, Recall, F1-score Regresi\u00f3n Error absoluto medio (MAE), Error cuadr\u00e1tico medio (MSE), R\u00b2"},{"location":"aprendizaje-supervisado/#consejos-practicos","title":"\ud83d\udca1 Consejos Pr\u00e1cticos","text":"<ul> <li>Siempre divide tus datos en entrenamiento y prueba (por ejemplo, 80% / 20%).</li> <li>Evita el sobreajuste (overfitting): si el modelo aprende demasiado bien los datos de entrenamiento, fallar\u00e1 en los nuevos.</li> <li>Usa validaci\u00f3n cruzada para estimar el rendimiento real del modelo.</li> </ul>"},{"location":"aprendizaje-supervisado/#conclusion","title":"\ud83e\udde9 Conclusi\u00f3n","text":"<p>El aprendizaje supervisado es la base de muchas aplicaciones modernas de IA, desde sistemas de recomendaci\u00f3n hasta diagn\u00f3stico m\u00e9dico. Dominar sus fundamentos te permitir\u00e1 avanzar hacia t\u00e9cnicas m\u00e1s complejas y poderosas.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 26/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/","title":"\ud83e\udd16 Unidad 1. Machine Learning Basado en el An\u00e1lisis de Datos","text":"<p>Esta unidad introduce los conceptos fundamentales del Machine Learning (ML), su flujo de trabajo, las herramientas clave de la biblioteca <code>scikit-learn</code> del lenguaje Python, y las metodolog\u00edas esenciales para la preparaci\u00f3n, divisi\u00f3n y preprocesamiento de datos.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#11-que-es-el-machine-learning","title":"1.1. \u00bfQu\u00e9 es el Machine Learning?","text":"<p>El Machine Learning (ML) se define como un campo de estudio que utiliza modelos estad\u00edsticos para aprender de los datos. Un aspecto clave es que modelos relativamente simples pueden realizar predicciones complejas.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#definiciones-clave","title":"Definiciones Clave","text":"<ul> <li>Definici\u00f3n temprana (Samuel, 1959): \"Programar computadoras para que aprendan de la experiencia deber\u00eda eliminar la necesidad de gran parte de este esfuerzo de programaci\u00f3n detallado\".</li> <li>Definici\u00f3n moderna (Mitchell, 1997): \"Se dice que un programa de computadora aprende de la experiencia E con respecto a alguna clase de tareas T y una medida de rendimiento P, si su rendimiento en las tareas T, medido por P, mejora con la experiencia E\".</li> <li>Definici\u00f3n matem\u00e1tica (Ej. Regresi\u00f3n Lineal): Un modelo matem\u00e1tico que intenta encontrar la relaci\u00f3n \u00f3ptima entre variables. Por ejemplo, predecir ventas (Target, \\(y\\)) bas\u00e1ndose en gastos de publicidad (Feature, \\(x\\)). El modelo \\(y = wx + b\\) aprende los par\u00e1metros \\(w\\) (peso) y \\(b\\) iterando desde valores arbitrarios (\\(f_1\\)) hasta un valor \u00f3ptimo (\\(f_3\\)) que minimiza el error.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#ml-y-otros-campos","title":"ML y Otros Campos","text":"<p>El Machine Learning est\u00e1 profundamente interconectado con otros campos: * Es un subcampo de la Inteligencia Artificial. * Deep Learning es un subcampo del Machine Learning. * Se solapa significativamente con Estad\u00edstica, Miner\u00eda de Datos y Reconocimiento de Patrones.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#tipos-de-machine-learning","title":"Tipos de Machine Learning","text":"<p>Seg\u00fan el m\u00e9todo de supervisi\u00f3n, el ML se divide en: 1.  Supervisado: Se proporciona un patr\u00f3n objetivo (datos etiquetados). El cap\u00edtulo se centra en este tipo, que incluye algoritmos como Regresi\u00f3n Lineal, Regresi\u00f3n Log\u00edstica, \u00c1rboles de Decisi\u00f3n, KNN, SVM y Redes Neuronales. 2.  No Supervisado: El patr\u00f3n objetivo debe ser descubierto (datos no etiquetados). Incluye Clustering, PCA y An\u00e1lisis de Asociaci\u00f3n. 3.  Refuerzo: Se aprende mediante la optimizaci\u00f3n de pol\u00edticas (recompensas y castigos).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#flujo-de-trabajo-del-machine-learning","title":"Flujo de Trabajo del Machine Learning","text":"<p>El proceso general para construir un modelo de ML es: 1.  Definici\u00f3n del Problema: Comprender el objetivo de negocio. 2.  Preparaci\u00f3n de Datos: Recolecci\u00f3n de datos brutos (Raw Data) y preprocesamiento. 3.  Machine Learning (Modelado): Se divide la data en conjuntos de Train (Entrenamiento), Validate (Validaci\u00f3n) y Test (Prueba). 4.  Entrenamiento y Evaluaci\u00f3n: Esta fase incluye Ingenier\u00eda de caracter\u00edsticas (Feature engineering), Modelado y optimizaci\u00f3n (entrenar el modelo con los datos), y Evaluaci\u00f3n de rendimiento (Performance metrics). 5.  Aplicaci\u00f3n: Aplicar el modelo en la vida real.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#parametros-vs-hiperparametros","title":"Par\u00e1metros vs. Hiperpar\u00e1metros","text":"<ul> <li>Par\u00e1metros: Se aprenden desde los datos durante el entrenamiento. Contienen el patr\u00f3n de los datos (ej. \\(w\\) y \\(b\\) en regresi\u00f3n lineal, pesos de una red neuronal).</li> <li>Hiperpar\u00e1metros: Se configuran manualmente por el practicante antes del entrenamiento. Se \"afinan\" (tunan) para optimizar el rendimiento (ej. el valor \\(k\\) en KNN, la tasa de aprendizaje, la profundidad m\u00e1xima de un \u00e1rbol).</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#12-biblioteca-python-scikit-learn","title":"1.2. Biblioteca Python scikit-learn","text":"<p><code>scikit-learn</code> es la biblioteca de ML m\u00e1s representativa de Python.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Proporciona una interfaz de biblioteca integrada y unificada.</li> <li>Incluye una amplia variedad de algoritmos de ML, funciones de preprocesamiento y selecci\u00f3n de modelos.</li> <li>Es simple, eficiente y est\u00e1 construida sobre NumPy, SciPy y matplotlib.</li> <li>Es de c\u00f3digo abierto y puede usarse comercialmente.</li> <li>No soporta GPU.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#mecanismo-de-scikit-learn","title":"Mecanismo de <code>scikit-learn</code>","text":"<p>El flujo de trabajo de la API de <code>scikit-learn</code> es intuitivo y sigue tres pasos: 1.  Instance: Crear una instancia del objeto del modelo (Estimator). 2.  Fit: Entrenar el modelo con los datos. 3.  Predict / transform: Usar el modelo entrenado para hacer predicciones o transformar datos.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#estimator-classifier-y-regressor","title":"Estimator, Classifier y Regressor","text":"<ul> <li><code>Estimator</code>: El objeto base. Aprende de los datos usando el m\u00e9todo <code>.fit()</code> y puede hacer predicciones usando <code>.predict()</code>.</li> <li><code>Classifier</code>: Un estimador para tareas de clasificaci\u00f3n (ej. <code>DecisionTreeClassifier</code>, <code>KNeighborsClassifier</code>).</li> <li><code>Regressor</code>: Un estimador para tareas de regresi\u00f3n (predicci\u00f3n num\u00e9rica) (ej. <code>LinearRegression</code>, <code>KNeighborsRegressor</code>).</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#sintaxis-basica-de-scikit-learn","title":"Sintaxis B\u00e1sica de <code>scikit-learn</code>","text":"<ul> <li>Importar un estimador: <code>from sklearn.linear_model import LinearRegression</code></li> <li>Importar un preprocesador: <code>from sklearn.preprocessing import StandardScaler</code></li> <li>Importar divisi\u00f3n de datos: <code>from sklearn.model_selection import train_test_split</code></li> <li> <p>Importar m\u00e9tricas: <code>from sklearn import metrics</code></p> </li> <li> <p>Instanciar (con hiperpar\u00e1metros): <code>myModel = KNeighborsClassifier(n_neighbors=10)</code></p> </li> <li>Dividir los datos (Hold-out): <code>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)</code></li> <li>Entrenar el modelo (Supervisado): <code>myModel.fit(X_train, Y_train)</code></li> <li>Hacer predicciones: <code>Y_pred = myModel.predict(X_test)</code></li> <li>Evaluar el rendimiento: <code>metrics.accuracy_score(Y_test, Y_pred)</code></li> <li>Afinar hiperpar\u00e1metros (con Cross-Validation): <code>myGridCV = GridSearchCV(estimator, parameter_grid, cv=5)</code></li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#ejemplo-practico-estandarizacion","title":"Ejemplo Pr\u00e1ctico: Estandarizaci\u00f3n","text":"<p>El preprocesamiento, como la estandarizaci\u00f3n, es crucial para mejorar el rendimiento. La estandarizaci\u00f3n (o z-transformation) convierte los datos para que sigan una distribuci\u00f3n normal est\u00e1ndar, usando la f\u00f3rmula \\(z = \\frac{x - m}{\\sigma}\\) (donde \\(m\\) es la media y \\(\\sigma\\) la desviaci\u00f3n est\u00e1ndar).</p> <p>En <code>scikit-learn</code>, se usa <code>StandardScaler</code>: 1.  Importar: <code>from sklearn.preprocessing import StandardScaler</code> 2.  Instanciar: <code>scaler = StandardScaler()</code> 3.  Ajustar (Fit): Se aprende la media (\\(m\\)) y la desviaci\u00f3n (\\(\\sigma\\)) solo de los datos de entrenamiento:     <code>scaler.fit(X_train)</code> 4.  Transformar: Se aplica la transformaci\u00f3n a los datos de entrenamiento y prueba:     <code>X_train = scaler.transform(X_train)</code> <code>X_test = scaler.transform(X_test)</code> 5.  <code>fit_transform</code>: Se pueden combinar los pasos 3 y 4 (solo para <code>X_train</code>):     <code>X_train = scaler.fit_transform(X_train)</code></p> <p>Antes de la estandarizaci\u00f3n, las columnas pueden tener rangos de valores muy diferentes. Despu\u00e9s, todos los valores est\u00e1n centrados alrededor de 0, lo que ayuda a muchos algoritmos a converger mejor.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#modulos-principales-de-scikit-learn","title":"M\u00f3dulos Principales de <code>scikit-learn</code>","text":"M\u00f3dulo Funci\u00f3n Principal Ejemplos <code>sklearn.datasets</code> Cargar datasets de ejemplo. <code>load_iris()</code>, <code>load_breast_cancer()</code> <code>sklearn.preprocessing</code> Preprocesamiento de datos (escalado, codificaci\u00f3n). <code>StandardScaler</code>, <code>LabelEncoder</code>, <code>OneHotEncoder</code> <code>sklearn.model_selection</code> Divisi\u00f3n de datos, validaci\u00f3n y afinado de hiperpar\u00e1metros. <code>train_test_split</code>, <code>GridSearchCV</code>, <code>KFold</code> <code>sklearn.metrics</code> Evaluaci\u00f3n de rendimiento del modelo. <code>accuracy_score</code>, <code>precision_score</code>, <code>recall_score</code>, <code>roc_auc_score</code> <code>sklearn.linear_model</code> Algoritmos lineales. <code>LinearRegression</code>, <code>LogisticRegression</code> <code>sklearn.tree</code> Algoritmos de \u00c1rboles de Decisi\u00f3n. <code>DecisionTreeClassifier</code> <code>sklearn.neighbors</code> Algoritmos de vecinos cercanos. <code>KNeighborsClassifier</code> (K-NN) <code>sklearn.svm</code> Support Vector Machine (M\u00e1quinas de Vectores de Soporte). <code>SVC</code> <code>sklearn.ensemble</code> Algoritmos de Ensamblado (Ensemble). <code>RandomForestClassifier</code>, <code>AdaBoostClassifier</code> <code>sklearn.cluster</code> Algoritmos de clustering (No supervisado). <code>KMeans</code>, <code>DBSCAN</code> <code>sklearn.pipeline</code> Herramienta para encadenar pasos de preprocesamiento y modelado. <code>Pipeline</code>"},{"location":"aprendizaje-supervisado/01-machine-learning/#13-preparacion-y-division-del-dataset","title":"1.3. Preparaci\u00f3n y Divisi\u00f3n del Dataset","text":"<p>La divisi\u00f3n de datos es fundamental para evaluar un modelo de ML. El conjunto de datos general se divide en un conjunto de entrenamiento y uno de evaluaci\u00f3n (prueba).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#overfitting-sobreajuste-y-generalizacion","title":"Overfitting (Sobreajuste) y Generalizaci\u00f3n","text":"<ul> <li>Generalizaci\u00f3n: Es la capacidad del modelo para predecir con precisi\u00f3n datos nuevos que no ha visto antes.</li> <li>Overfitting: Ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento, aprendiendo incluso el ruido.</li> <li>Underfitting (Subajuste): Ocurre cuando un modelo es demasiado simple (baja capacidad) y no puede capturar el patr\u00f3n subyacente de los datos.</li> </ul> <p>El Dilema: A medida que aumenta la complejidad (flexibilidad) del modelo: * El error en el conjunto de entrenamiento (Training set) siempre disminuye. * El error en el conjunto de prueba (Test set) disminuye al principio, pero luego comienza a aumentar. El punto donde el error de prueba empieza a subir es donde comienza el overfitting.</p> <p>El conjunto de prueba es esencial para detectar el overfitting y seleccionar un modelo que generalice bien.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#cross-validation-validacion-cruzada","title":"Cross-Validation (Validaci\u00f3n Cruzada)","text":"<p>El conjunto de prueba (Test set) debe usarse \u00a1solo una vez! al final, para la evaluaci\u00f3n final.</p> <p>Para evaluar el modelo durante el entrenamiento (por ejemplo, para afinar hiperpar\u00e1metros), necesitamos una forma de simular un \"conjunto de prueba\" sin tocar el real. Para esto, dividimos el conjunto de entrenamiento (Training Data) en dos partes m\u00e1s peque\u00f1as: un nuevo conjunto de <code>Train</code> y un conjunto de <code>Cross Validate</code> (Validaci\u00f3n).</p> <p>M\u00e9todo: k-Fold Cross-Validation Es el m\u00e9todo m\u00e1s com\u00fan: 1.  Se subdivide el conjunto de entrenamiento (original) en k partes iguales (folds). (Usualmente k=10). 2.  Se itera k veces (rondas). 3.  En cada ronda, se usa 1 fold como conjunto de validaci\u00f3n y los k-1 folds restantes como conjunto de entrenamiento. 4.  Se calcula la m\u00e9trica de rendimiento (ej. accuracy) en cada ronda. 5.  El rendimiento final del modelo es el promedio de las m\u00e9tricas de las k rondas.</p> <p>M\u00e9todo: Leave One Out (LOO) Es un caso extremo de k-Fold donde \\(k\\) es igual al n\u00famero total de muestras. Se entrena con todos los datos menos uno, y se valida con ese \u00fanico dato. Es computacionalmente muy costoso.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#14-preprocesamiento-de-datos","title":"1.4. Preprocesamiento de Datos","text":"<p>Preparar los datos es vital para un buen modelo. Esto incluye la limpieza (manejo de valores at\u00edpicos y faltantes) y la transformaci\u00f3n (escalado y codificaci\u00f3n).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#manejo-de-valores-faltantes-missing-values","title":"Manejo de Valores Faltantes (Missing Values)","text":"<p>Los valores faltantes (identificados en Python como <code>np.nan</code> o <code>NaN</code>) deben ser tratados.</p> <p>1. Identificaci\u00f3n: Se pueden contar usando <code>df.isnull().sum()</code>.</p> <p>2. Eliminaci\u00f3n (con Pandas <code>dropna()</code>): * <code>df.dropna()</code>: Elimina cualquier fila que contenga al menos un <code>NaN</code> (eje por defecto 0). * <code>df.dropna(axis=1)</code>: Elimina cualquier columna que contenga un <code>NaN</code>. * <code>df.dropna(how='all')</code>: Elimina filas/columnas donde todos los valores son <code>NaN</code>. * <code>df.dropna(thresh=N)</code>: Mantiene las filas que tienen al menos <code>N</code> valores no-<code>NaN</code>. * <code>df.dropna(subset=['col_name'])</code>: Elimina filas que tienen <code>NaN</code> espec\u00edficamente en la columna 'col_name'.</p> <p>3. Imputaci\u00f3n (Relleno): Se usa cuando eliminar datos resultar\u00eda en una p\u00e9rdida significativa de informaci\u00f3n. * M\u00e9todos simples: Rellenar con un valor (ej. 'unknown'), la media, la mediana o el valor m\u00e1s frecuente (moda) de la columna. * Con <code>scikit-learn</code> (<code>SimpleImputer</code>): Es el m\u00e9todo preferido.     * <code>from sklearn.impute import SimpleImputer</code>     * <code>impt = SimpleImputer(strategy='mean')</code> (Estrategias: 'mean', 'median', 'most_frequent').     * <code>impt.fit(X_train)</code>: Aprende la media (o mediana/moda) del set de entrenamiento.     * <code>X_train_imputed = impt.transform(X_train)</code>: Aplica la imputaci\u00f3n.     * <code>X_test_imputed = impt.transform(X_test)</code>: Aplica la misma imputaci\u00f3n (con la media de train) al set de prueba.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#manejo-de-datos-categoricos","title":"Manejo de Datos Categ\u00f3ricos","text":"<p>Los algoritmos de ML requieren entradas num\u00e9ricas. Los datos categ\u00f3ricos deben ser convertidos.</p> <p>1. Datos Ordinales (con orden): Ej. Tallas: 'M' &lt; 'L' &lt; 'XL'. Se deben mapear a enteros que respeten ese orden. * <code>size_mapping = {'M': 1, 'L': 2, 'XL': 3}</code> * <code>df['size'] = df['size'].map(size_mapping)</code></p> <p>2. Datos Nominales (sin orden) y Etiquetas de Clase: Ej. Colores: 'red', 'green', 'blue' o Etiquetas: 'setosa', 'versicolor'.</p> <ul> <li> <p>Codificaci\u00f3n de Etiquetas (Label Encoding):     Convierte cada etiqueta \u00fanica en un entero (ej. 'class1': 0, 'class2': 1). Se usa <code>LabelEncoder</code> de <code>scikit-learn</code>.</p> <ul> <li><code>from sklearn.preprocessing import LabelEncoder</code></li> <li><code>enc = LabelEncoder()</code></li> <li><code>y_encoded = enc.fit_transform(df['classlabel'])</code></li> </ul> </li> <li> <p>One-Hot Encoding (para caracter\u00edsticas nominales):     Usar <code>LabelEncoder</code> para caracter\u00edsticas (X) es incorrecto, ya que crea un orden artificial. Se debe usar One-Hot Encoding.     Crea nuevas columnas \"dummy\" (0 o 1) para cada categor\u00eda, indicando presencia (1) o ausencia (0).</p> <ul> <li>M\u00e9todo Pandas: <code>pd.get_dummies(df['species'])</code>.</li> <li>M\u00e9todo <code>scikit-learn</code>: <code>OneHotEncoder</code>. Este m\u00e9todo es preferido en pipelines y a menudo devuelve una matriz dispersa (sparse matrix) para ahorrar memoria, ya que la mayor\u00eda de los valores ser\u00e1n 0.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#division-de-datos-estratificada-stratify","title":"Divisi\u00f3n de Datos Estratificada (Stratify)","text":"<p>Al usar <code>train_test_split</code>, si el dataset est\u00e1 desbalanceado (ej. 90% clase A, 10% clase B), una divisi\u00f3n aleatoria simple podr\u00eda resultar en un set de prueba sin muestras de la clase B. * Soluci\u00f3n: Usar el par\u00e1metro <code>stratify=y</code>. * Esto asegura que la proporci\u00f3n de las clases (ej. 90/10) se mantenga id\u00e9ntica tanto en el conjunto de entrenamiento como en el de prueba, reflejando el dataset original.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#topicos-avanzados-tradeoff-de-sesgo-varianza-y-regularizacion","title":"T\u00f3picos Avanzados: Tradeoff de Sesgo-Varianza y Regularizaci\u00f3n","text":"<ul> <li>Tradeoff de Sesgo-Varianza:<ul> <li>Sesgo (Bias): Error por suposiciones incorrectas (Underfitting).</li> <li>Varianza (Variance): Error por sensibilidad excesiva a los datos de entrenamiento (Overfitting).</li> <li>Error Total \\(\\approx\\) Sesgo\u00b2 + Varianza. El objetivo es encontrar la complejidad \u00f3ptima que minimice este error total.</li> </ul> </li> <li>Regularizaci\u00f3n: T\u00e9cnica para prevenir el overfitting en modelos lineales penalizando coeficientes (pesos) grandes.<ul> <li>Ridge (L2): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{w_j^2}\\). Encoge los pesos, pero no los hace cero.</li> <li>Lasso (L1): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{|w_j|}\\). Puede forzar que algunos pesos sean exactamente cero, realizando una selecci\u00f3n de caracter\u00edsticas autom\u00e1tica.</li> <li>ElasticNet: Combina penalizaciones L1 y L2.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#15-practica-solucion-de-problemas-con-scikit-learn-ej-iris","title":"1.5. Pr\u00e1ctica: Soluci\u00f3n de Problemas con scikit-learn (Ej. Iris)","text":"<p>Esta secci\u00f3n aplica todos los conceptos anteriores en un caso pr\u00e1ctico completo usando el dataset \"Iris\".</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#1-entendimiento-del-problema-y-datos-eda","title":"1. Entendimiento del Problema y Datos (EDA)","text":"<ul> <li>Objetivo: Clasificar la especie de una flor Iris (Target).</li> <li>Clases (Target): 3 especies (Setosa, Versicolor, Virginica).</li> <li>Caracter\u00edsticas (Features): <code>sepal_length</code>, <code>sepal_width</code>, <code>petal_length</code>, <code>petal_width</code>.</li> <li>An\u00e1lisis de Datos:<ul> <li>Se cargan los datos y se convierten a un DataFrame de Pandas.</li> <li>Valores Faltantes: Se comprueba con <code>iris.isnull().sum()</code>. No se encontraron.</li> <li>Distribuci\u00f3n de Clases: Se comprueba con <code>iris.groupby('target').size()</code>. Hay 50 muestras de cada clase (33.3% cada una). Es un dataset balanceado.</li> <li>Estad\u00edsticas y Correlaci\u00f3n: <code>iris.describe()</code> y <code>iris.corr()</code>. Se observa que <code>petal_length</code> y <code>petal_width</code> est\u00e1n altamente correlacionados (0.96), sugiriendo un problema de multicolinealidad.</li> <li>Visualizaci\u00f3n: Se usan <code>pairplot</code> y <code>heatmap</code> para confirmar visualmente las relaciones y la alta correlaci\u00f3n.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#2-division-y-preparacion-de-datos","title":"2. Divisi\u00f3n y Preparaci\u00f3n de Datos","text":"<ul> <li>Separaci\u00f3n X/y: Se separan las caracter\u00edsticas (X) del objetivo (y).</li> <li>Divisi\u00f3n Train/Test: Se usa <code>train_test_split</code> (ej. 80% train, 20% test).</li> <li>Validaci\u00f3n Cruzada:<ul> <li>Se muestra c\u00f3mo usar <code>KFold</code> (CV est\u00e1ndar) y <code>StratifiedKFold</code> (CV estratificada).</li> <li><code>StratifiedKFold</code> es preferible porque mantiene la distribuci\u00f3n 33/33/33 de las clases en cada fold, asegurando que la validaci\u00f3n sea representativa.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#3-seleccion-y-evaluacion-del-modelo","title":"3. Selecci\u00f3n y Evaluaci\u00f3n del Modelo","text":"<ul> <li>Curva de Aprendizaje (<code>Learning Curve</code>):     Se usa para diagnosticar bias vs. variance. Muestra el rendimiento del modelo a medida que ve m\u00e1s datos de entrenamiento.</li> <li>Afinado de Hiperpar\u00e1metros (<code>GridSearchCV</code>):     Se utiliza para encontrar la mejor combinaci\u00f3n de hiperpar\u00e1metros (ej. <code>criterion</code>, <code>max_depth</code> para un <code>DecisionTreeClassifier</code>) probando todas las combinaciones posibles mediante validaci\u00f3n cruzada.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#4-metricas-de-evaluacion-clasificacion","title":"4. M\u00e9tricas de Evaluaci\u00f3n (Clasificaci\u00f3n)","text":"<p>Una vez que el modelo (<code>GridSearchCV</code>) est\u00e1 entrenado y se hacen predicciones sobre el <code>X_test</code>, se eval\u00faa el rendimiento.</p> <ul> <li> <p>Matriz de Confusi\u00f3n (<code>Confusion Matrix</code>):     Es la base para todas las m\u00e9tricas. Compara los valores reales (True label) con los predichos (Predicted label).</p> <ul> <li>TP (True Positive): Real = 1, Predicho = 1.</li> <li>FN (False Negative): Real = 1, Predicho = 0.</li> <li>FP (False Positive): Real = 0, Predicho = 1.</li> <li>TN (True Negative): Real = 0, Predicho = 0.</li> </ul> </li> <li> <p>M\u00e9tricas Clave:</p> <ul> <li>Accuracy (Exactitud): \\(\\frac{TP + TN}{Total}\\). Proporci\u00f3n de predicciones correctas. (Usar con cuidado en datasets desbalanceados).</li> <li>Precision (Precisi\u00f3n): \\(\\frac{TP}{TP + FP}\\). De los que dijimos que eran positivos, \u00bfcu\u00e1ntos acertamos?.</li> <li>Recall (Sensibilidad o TPR): \\(\\frac{TP}{TP + FN}\\). De todos los positivos reales, \u00bfcu\u00e1ntos encontramos?.</li> <li>F1-Score: La media arm\u00f3nica de Precision y Recall. Es una m\u00e9trica excelente para datasets desbalanceados. \\(F_1 = 2 \\frac{Precision \\times Recall}{Precision + Recall}\\).</li> <li>FPR (Tasa de Falsos Positivos): \\(\\frac{FP}{FP + TN}\\). Proporci\u00f3n de negativos reales que clasificamos incorrectamente como positivos.</li> </ul> </li> <li> <p>Curva ROC y AUC:</p> <ul> <li>Curva ROC: Gr\u00e1fica que muestra el rendimiento de un clasificador en todos los umbrales de clasificaci\u00f3n. Muestra TPR (Eje Y) vs. FPR (Eje X).</li> <li>AUC (Area Under the Curve): El \u00e1rea bajo la curva ROC. Es una m\u00e9trica \u00fanica que resume el rendimiento del modelo.<ul> <li>AUC = 1.0: Clasificador perfecto.</li> <li>AUC = 0.5: Clasificador in\u00fatil (aleatorio).</li> <li>Un AUC de 0.85 o m\u00e1s se considera bueno.</li> </ul> </li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#5-prediccion-final","title":"5. Predicci\u00f3n Final","text":"<ul> <li>Se carga el modelo final (el mejor <code>estimator_</code> encontrado por <code>GridSearchCV</code>).</li> <li>Se realizan las predicciones finales sobre el conjunto de prueba (<code>X_test</code>).</li> <li>Los resultados se guardan, por ejemplo, en un archivo CSV.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 27/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/","title":"\ud83e\udd16 Unidad 2. Regresi\u00f3n Lineal para la Inteligencia Artificial","text":"<p>La regresi\u00f3n lineal es uno de los modelos m\u00e1s simples y fundamentales en el campo de la inteligencia artificial y el aprendizaje autom\u00e1tico. A pesar de su simplicidad, proporciona una base s\u00f3lida para entender c\u00f3mo los algoritmos de regresi\u00f3n pueden ser usados para hacer predicciones. En este art\u00edculo exploraremos c\u00f3mo funciona la regresi\u00f3n lineal, sus aplicaciones, y la compararemos con otros modelos de regresi\u00f3n como Ridge y Lasso.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#que-es-la-regresion-lineal","title":"\u00bfQu\u00e9 es la Regresi\u00f3n Lineal?","text":"<p>La regresi\u00f3n lineal es un m\u00e9todo estad\u00edstico que intenta modelar la relaci\u00f3n entre una variable dependiente y una o m\u00e1s variables independientes mediante una l\u00ednea recta. La ecuaci\u00f3n que representa una regresi\u00f3n lineal simple tiene la siguiente forma:</p> \\[ y = b_0 + b_1X + \\epsilon \\] <ul> <li>y: Variable dependiente (la que se intenta predecir).</li> <li>X: Variable independiente (el predictor).</li> <li>b_0: Intercepto, valor de y cuando X es cero.</li> <li>b_1: Coeficiente que representa la pendiente de la l\u00ednea.</li> <li>\\(\\epsilon\\): Error o ruido, la diferencia entre la predicci\u00f3n y el valor real.</li> </ul> <p>La regresi\u00f3n lineal se utiliza principalmente para problemas de predicci\u00f3n num\u00e9rica, como el precio de una vivienda, el rendimiento de una acci\u00f3n o cualquier otra situaci\u00f3n en la que exista una relaci\u00f3n lineal entre las variables.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#aplicaciones-de-la-regresion-lineal","title":"Aplicaciones de la Regresi\u00f3n Lineal","text":"<p>La regresi\u00f3n lineal es ampliamente utilizada en una variedad de aplicaciones, como:</p> <ul> <li> <p>Econom\u00eda: Predicci\u00f3n de precios de bienes y servicios. Por ejemplo, podemos usar la regresi\u00f3n lineal para modelar la relaci\u00f3n entre la inflaci\u00f3n y el precio de los alimentos. En este caso, la regresi\u00f3n lineal simple puede ser suficiente si se trata de una relaci\u00f3n clara y directa. Ejemplo en Python</p> </li> <li> <p>Finanzas: Estimaci\u00f3n del rendimiento de acciones o bonos. La regresi\u00f3n lineal puede ayudar a estimar c\u00f3mo factores como las tasas de inter\u00e9s y el crecimiento econ\u00f3mico afectan los precios de las acciones. Si existen muchas variables correlacionadas, Ridge Regression ser\u00eda una mejor opci\u00f3n para estabilizar el modelo y evitar el sobreajuste. Ejemplo en Python</p> </li> <li> <p>Salud: Modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente. En este \u00e1mbito, se podr\u00eda usar la regresi\u00f3n lineal para entender c\u00f3mo var\u00eda la presi\u00f3n sangu\u00ednea en respuesta a diferentes dosis de un medicamento. Si existen m\u00faltiples factores (como edad, peso, y otras condiciones de salud), Lasso Regression podr\u00eda ayudar a identificar cu\u00e1les son las caracter\u00edsticas m\u00e1s relevantes. Ejemplo en Python</p> </li> <li> <p>Marketing: Determinaci\u00f3n de la relaci\u00f3n entre el gasto publicitario y las ventas. La regresi\u00f3n lineal se utiliza para estimar el impacto de diferentes estrategias publicitarias en las ventas. Si existen muchas campa\u00f1as publicitarias y se necesita identificar cu\u00e1les son las m\u00e1s efectivas, Lasso Regression podr\u00eda ayudar a eliminar las menos significativas y reducir la complejidad del modelo. </p> </li> <li> <p>Educaci\u00f3n: Predicci\u00f3n de calificaciones de estudiantes en funci\u00f3n de variables como el tiempo de estudio y la asistencia. Si el objetivo es identificar los factores que tienen mayor influencia en el rendimiento acad\u00e9mico, Lasso Regression ser\u00eda \u00fatil para seleccionar solo las caracter\u00edsticas m\u00e1s relevantes, como participaci\u00f3n en clase, tiempo de estudio, o participaci\u00f3n en actividades extracurriculares. </p> </li> <li> <p>Inmobiliaria: Predicci\u00f3n del valor de una propiedad con base en caracter\u00edsticas como la ubicaci\u00f3n, el tama\u00f1o y el n\u00famero de habitaciones. En este contexto, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que caracter\u00edsticas como la ubicaci\u00f3n y el tama\u00f1o de una propiedad suelen estar correlacionadas. Ridge ayuda a estabilizar los coeficientes y mejorar la capacidad predictiva del modelo. </p> </li> <li> <p>Agricultura: Estimaci\u00f3n del rendimiento de cultivos en funci\u00f3n de factores como el clima, la cantidad de fertilizante y el tipo de suelo. Ridge Regression es adecuada cuando hay m\u00faltiples factores que pueden estar correlacionados, como la temperatura y la precipitaci\u00f3n. Esto ayuda a manejar mejor la multicolinealidad y a mejorar la generalizaci\u00f3n del modelo. Ejemplo en Python</p> </li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#que-tecnica-es-mas-apropiada","title":"\u00bfQu\u00e9 t\u00e9cnica es m\u00e1s apropiada?","text":"<ul> <li>Econom\u00eda y Finanzas: En estos campos, la regresi\u00f3n lineal puede ser \u00fatil cuando se trata de problemas simples, como la predicci\u00f3n de precios basada en una o dos caracter\u00edsticas. Sin embargo, si hay muchas variables que est\u00e1n altamente correlacionadas, Ridge Regression ser\u00eda m\u00e1s apropiada para evitar el sobreajuste. Ejemplo en Python</li> <li>Salud: Para el modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente, Lasso Regression ser\u00eda adecuada si hay muchas caracter\u00edsticas potenciales, ya que podr\u00eda simplificar el modelo eliminando caracter\u00edsticas irrelevantes. Ejemplo en Python</li> <li>Marketing: Si hay muchas variables de marketing, como diferentes tipos de publicidad, Lasso Regression puede ayudar a identificar cu\u00e1les de ellas son las m\u00e1s importantes, eliminando las menos significativas. </li> <li>Inmobiliaria: En el caso de la predicci\u00f3n de precios de propiedades, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que a menudo las caracter\u00edsticas como ubicaci\u00f3n, tama\u00f1o y tipo de propiedad est\u00e1n correlacionadas. </li> <li>Educaci\u00f3n: Si queremos predecir las calificaciones de los estudiantes y hay muchas caracter\u00edsticas (como el historial acad\u00e9mico, asistencia, participaci\u00f3n en clase, etc.), Lasso ser\u00eda \u00fatil para identificar las variables m\u00e1s relevantes y eliminar las menos importantes. </li> <li>Agricultura: Para la estimaci\u00f3n del rendimiento de cultivos, Ridge Regression ser\u00eda adecuada si existen m\u00faltiples factores correlacionados, ya que permite manejar mejor la multicolinealidad. Ejemplo en Python</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#limitaciones-de-la-regresion-lineal","title":"Limitaciones de la Regresi\u00f3n Lineal","text":"<p>Aunque la regresi\u00f3n lineal es f\u00e1cil de entender y usar, presenta algunas limitaciones importantes que deben tenerse en cuenta al aplicar este modelo:</p> <ul> <li> <p>Supone una relaci\u00f3n lineal: La regresi\u00f3n lineal solo puede modelar relaciones lineales entre las variables. Si la relaci\u00f3n es no lineal, el modelo tendr\u00e1 un rendimiento pobre. Esto implica que, si los datos muestran una relaci\u00f3n m\u00e1s compleja (por ejemplo, cuadr\u00e1tica o exponencial), la regresi\u00f3n lineal no podr\u00e1 capturar dicha complejidad, resultando en predicciones inexactas. En estos casos, ser\u00eda mejor utilizar modelos que puedan capturar la no linealidad, como la regresi\u00f3n polin\u00f3mica o t\u00e9cnicas m\u00e1s avanzadas como redes neuronales.</p> </li> <li> <p>Sensibilidad a los outliers: La presencia de valores at\u00edpicos puede afectar significativamente el ajuste de la l\u00ednea, ya que la regresi\u00f3n lineal minimiza la suma de los errores al cuadrado. Los outliers, al tener errores m\u00e1s grandes, influyen desproporcionadamente en la l\u00ednea de ajuste, lo cual puede distorsionar el modelo. Para mitigar este problema, se pueden utilizar t\u00e9cnicas como la detecci\u00f3n y eliminaci\u00f3n de outliers, o emplear m\u00e9todos de regresi\u00f3n robusta que minimicen el impacto de estos valores extremos.</p> </li> <li> <p>Multicolinealidad: Cuando las variables independientes est\u00e1n altamente correlacionadas, el modelo puede producir resultados inestables. La multicolinealidad genera problemas en la estimaci\u00f3n de los coeficientes, haciendo que sean muy sensibles a peque\u00f1as variaciones en los datos y, por lo tanto, menos interpretables. Esto puede llevar a una disminuci\u00f3n en la precisi\u00f3n de las predicciones y a problemas en la generalizaci\u00f3n del modelo. En estos casos, se recomienda usar t\u00e9cnicas de regularizaci\u00f3n, como Ridge Regression, que penaliza los coeficientes grandes y ayuda a reducir los efectos de la multicolinealidad, estabilizando el modelo.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ridge-regression-y-lasso-regression","title":"Ridge Regression y Lasso Regression","text":"<p>Para superar algunas de las limitaciones de la regresi\u00f3n lineal est\u00e1ndar, se han desarrollado t\u00e9cnicas de regularizaci\u00f3n como Ridge y Lasso. Ambas t\u00e9cnicas son versiones modificadas de la regresi\u00f3n lineal que incluyen un t\u00e9rmino de penalizaci\u00f3n para mejorar el rendimiento y evitar el sobreajuste.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ridge-regression","title":"Ridge Regression","text":"<p>Ridge Regression, tambi\u00e9n conocida como regresi\u00f3n de cresta, a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L2 a la funci\u00f3n de p\u00e9rdida. Esto significa que el modelo penaliza los coeficientes grandes, haciendo que los valores de los par\u00e1metros sean m\u00e1s peque\u00f1os y estables. La ecuaci\u00f3n para Ridge es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum     heta_j^2 \\] <ul> <li>\\lambda: Par\u00e1metro de regularizaci\u00f3n que controla la cantidad de penalizaci\u00f3n.</li> <li>**    \\(heta_j\\)**: Coeficientes del modelo.</li> </ul> <p>El t\u00e9rmino de penalizaci\u00f3n ayuda a reducir la complejidad del modelo, lo cual resulta \u00fatil especialmente cuando existen m\u00faltiples variables independientes correlacionadas (multicolinealidad).</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ventajas-de-ridge-regression","title":"Ventajas de Ridge Regression","text":"<ul> <li>Reducci\u00f3n del sobreajuste: Ridge ayuda a reducir el riesgo de sobreajuste al penalizar los coeficientes grandes.</li> <li>Mejora la estabilidad: Especialmente en presencia de multicolinealidad, el modelo Ridge tiende a ser m\u00e1s estable.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#lasso-regression","title":"Lasso Regression","text":"<p>Lasso Regression a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L1 a la funci\u00f3n de p\u00e9rdida. Este t\u00e9rmino tiene la capacidad de hacer que algunos coeficientes sean exactamente cero, eliminando efectivamente ciertas caracter\u00edsticas del modelo. La ecuaci\u00f3n de Lasso es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum |   heta_j| \\] <ul> <li>\\(\\lambda\\): Par\u00e1metro de regularizaci\u00f3n que controla la penalizaci\u00f3n.</li> </ul> <p>Lasso es \u00fatil no solo para reducir el sobreajuste, sino tambi\u00e9n para la selecci\u00f3n de caracter\u00edsticas, ya que elimina autom\u00e1ticamente aquellas que no son \u00fatiles para la predicci\u00f3n.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ventajas-de-lasso-regression","title":"Ventajas de Lasso Regression","text":"<ul> <li>Selecci\u00f3n de caracter\u00edsticas: Lasso simplifica el modelo seleccionando solo las caracter\u00edsticas m\u00e1s relevantes.</li> <li>Reducci\u00f3n del sobreajuste: Similar a Ridge, Lasso ayuda a evitar el sobreajuste del modelo.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#comparacion-entre-regresion-lineal-ridge-y-lasso","title":"Comparaci\u00f3n entre Regresi\u00f3n Lineal, Ridge y Lasso","text":"Caracter\u00edstica Regresi\u00f3n Lineal Ridge Regression Lasso Regression Regularizaci\u00f3n No L2 L1 Penalizaci\u00f3n Ninguna Penaliza coeficientes grandes Algunos coeficientes se hacen cero Sobreajuste Alta posibilidad Baja Baja Multicolinealidad Problemas con multicolinealidad Mejor manejo Mejor manejo Selecci\u00f3n de caracter\u00edsticas No No S\u00ed <ul> <li>Regresi\u00f3n Lineal: Ideal para problemas simples y cuando existe una relaci\u00f3n lineal clara entre las variables. Sin embargo, es propensa al sobreajuste si no se maneja adecuadamente.</li> <li>Ridge Regression: \u00datil cuando existe multicolinealidad, ya que la regularizaci\u00f3n L2 ayuda a estabilizar el modelo. No elimina caracter\u00edsticas, pero hace que los coeficientes sean m\u00e1s peque\u00f1os.</li> <li>Lasso Regression: \u00datil para la selecci\u00f3n de caracter\u00edsticas, ya que fuerza algunos coeficientes a ser exactamente cero. Esto resulta en un modelo m\u00e1s sencillo y f\u00e1cil de interpretar.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#conclusiones","title":"Conclusiones","text":"<p>La regresi\u00f3n lineal es una excelente herramienta para comenzar a entender los modelos de regresi\u00f3n. Sin embargo, cuando nos enfrentamos a datos m\u00e1s complejos, con m\u00faltiples caracter\u00edsticas y posibles problemas de sobreajuste, Ridge y Lasso se presentan como mejores alternativas. Estos modelos ayudan a mejorar la capacidad de generalizaci\u00f3n del modelo y a reducir la complejidad, haciendo que la predicci\u00f3n sea m\u00e1s precisa y confiable.</p> <p>La elecci\u00f3n entre la regresi\u00f3n lineal, Ridge y Lasso depender\u00e1 de la naturaleza de los datos y los objetivos del an\u00e1lisis. Si se desea simplicidad y no hay riesgo de multicolinealidad, la regresi\u00f3n lineal puede ser suficiente. Si el modelo tiende a sobreajustarse o hay muchas caracter\u00edsticas correlacionadas, Ridge y Lasso son opciones a considerar, siendo Lasso ideal si se desea simplificar el modelo eliminando caracter\u00edsticas irrelevantes.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ejemplos-adicionales-de-uso","title":"Ejemplos Adicionales de Uso","text":"<ul> <li>Predicci\u00f3n de Ventas Minoristas: En un negocio minorista donde existen m\u00faltiples caracter\u00edsticas que afectan las ventas (promociones, temporadas, clima, ubicaci\u00f3n), Ridge Regression ser\u00eda \u00fatil para manejar la posible multicolinealidad entre estas caracter\u00edsticas. Ejemplo en Python</li> <li>Modelado de la Demanda Energ\u00e9tica: En la predicci\u00f3n del consumo de energ\u00eda el\u00e9ctrica, que depende de variables como temperatura, hora del d\u00eda, y tipo de d\u00eda (laboral o festivo), Ridge podr\u00eda ayudar a manejar la complejidad y multicolinealidad. </li> <li>An\u00e1lisis de Sentimientos: Al predecir la polaridad de una opini\u00f3n (positiva o negativa) en base a muchas palabras o frases, Lasso Regression ser\u00eda ideal para seleccionar las palabras m\u00e1s relevantes y reducir la dimensionalidad. </li> <li>Predicci\u00f3n de Costos de Seguros M\u00e9dicos: Para estimar los costos de seguros m\u00e9dicos en funci\u00f3n de caracter\u00edsticas como edad, estado de salud, h\u00e1bitos de vida y ubicaci\u00f3n geogr\u00e1fica, Lasso podr\u00eda ayudar a eliminar caracter\u00edsticas redundantes, haciendo el modelo m\u00e1s interpretable. </li> <li>Optimizaci\u00f3n de Cadenas de Suministro: Para predecir el tiempo de entrega de productos considerando m\u00faltiples variables (tr\u00e1fico, distancia, clima, inventario), Ridge Regression puede ser \u00fatil para manejar la correlaci\u00f3n entre factores como tr\u00e1fico y distancia. </li> <li>Reconocimiento de Actividad Humana: En la clasificaci\u00f3n de actividades humanas usando sensores port\u00e1tiles (como aceler\u00f3metros y giroscopios), Lasso podr\u00eda ayudar a identificar cu\u00e1les de las se\u00f1ales del sensor son m\u00e1s importantes para diferenciar entre actividades como caminar, correr o estar de pie. Ejemplo en Python</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/","title":"\ud83e\udd16 Unidad 3. Modelos de Aprendizaje Supervisado para Predicci\u00f3n Categ\u00f3rica","text":"<p>La clasificaci\u00f3n es una subcategor\u00eda del aprendizaje supervisado donde el objetivo es predecir una etiqueta de clase categ\u00f3rica (discreta) para una instancia de datos dada. A diferencia de la regresi\u00f3n, que predice valores continuos, la clasificaci\u00f3n asigna entradas a una de varias categor\u00edas predefinidas.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#31-entrenamiento-y-testing-en-clasificacion","title":"3.1. Entrenamiento y Testing en Clasificaci\u00f3n","text":"<p>El proceso de construcci\u00f3n de un modelo de clasificaci\u00f3n sigue el flujo est\u00e1ndar de Machine Learning:</p> <ol> <li>Divisi\u00f3n de Datos: Se divide el dataset en un conjunto de entrenamiento (para ajustar el modelo) y un conjunto de prueba (para evaluar su rendimiento en datos no vistos).</li> <li>Entrenamiento: El algoritmo aprende la frontera de decisi\u00f3n que separa las diferentes clases bas\u00e1ndose en las caracter\u00edsticas (features) de los datos de entrenamiento.</li> <li>Testing (Predicci\u00f3n): El modelo asigna etiquetas a los datos de prueba.</li> <li>Evaluaci\u00f3n: Se comparan las etiquetas predichas con las etiquetas reales para calcular m\u00e9tricas de rendimiento.</li> </ol>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#32-ejemplos-frecuentes-de-uso","title":"3.2. Ejemplos Frecuentes de Uso","text":"<p>La clasificaci\u00f3n est\u00e1 omnipresente en aplicaciones modernas:</p> <ul> <li>Detecci\u00f3n de Spam: Clasificar correos como \"Spam\" o \"No Spam\".</li> <li>Diagn\u00f3stico M\u00e9dico: Determinar si un paciente tiene una enfermedad (\"Positivo\") o no (\"Negativo\") bas\u00e1ndose en s\u00edntomas y an\u00e1lisis.</li> <li>Reconocimiento de Im\u00e1genes: Identificar si una imagen contiene un \"Gato\", \"Perro\" o \"Coche\".</li> <li>Aprobaci\u00f3n de Cr\u00e9ditos: Clasificar a un solicitante como de \"Alto Riesgo\" o \"Bajo Riesgo\".</li> <li>An\u00e1lisis de Sentimientos: Clasificar opiniones como \"Positivas\", \"Negativas\" o \"Neutrales\".</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#33-algoritmos-de-clasificacion-en-machine-learning","title":"3.3. Algoritmos de Clasificaci\u00f3n en Machine Learning","text":"<p>Existen diversos algoritmos para abordar problemas de clasificaci\u00f3n:</p> <ul> <li>Regresi\u00f3n Log\u00edstica: Simple, interpretable y base para redes neuronales.</li> <li>K-Nearest Neighbors (KNN): Basado en similitud y distancia.</li> <li>Support Vector Machines (SVM): Busca el hiperplano de separaci\u00f3n \u00f3ptimo.</li> <li>\u00c1rboles de Decisi\u00f3n y Random Forest: Basados en reglas de decisi\u00f3n jer\u00e1rquicas.</li> <li>Naive Bayes: Basado en probabilidad y el teorema de Bayes.</li> <li>Redes Neuronales: Para patrones complejos y datos no estructurados.</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#34-regresion-logistica","title":"3.4. Regresi\u00f3n Log\u00edstica","text":"<p>A pesar de su nombre, la Regresi\u00f3n Log\u00edstica es un algoritmo de clasificaci\u00f3n, no de regresi\u00f3n. Se utiliza para estimar la probabilidad de que una instancia pertenezca a una clase particular (por ejemplo, probabilidad de que un correo sea spam).</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#conceptos-basicos-y-matematicos","title":"Conceptos B\u00e1sicos y Matem\u00e1ticos","text":"<p>La regresi\u00f3n log\u00edstica utiliza la funci\u00f3n sigmoide (o log\u00edstica) para transformar la salida de una ecuaci\u00f3n lineal en un valor de probabilidad entre 0 y 1.</p> <ol> <li>Funci\u00f3n Lineal: \\(z = w \\cdot x + b\\) (donde \\(w\\) son los pesos y \\(x\\) las caracter\u00edsticas).</li> <li>Funci\u00f3n Sigmoide: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</li> </ol> <p>Si la probabilidad estimada \\(\\hat{p} = \\sigma(z)\\) es mayor o igual a 0.5, el modelo predice la clase 1; de lo contrario, predice la clase 0.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#algoritmo-del-gradiente-descendente","title":"Algoritmo del Gradiente Descendente","text":"<p>Para entrenar el modelo, necesitamos encontrar los pesos \\(w\\) y el sesgo \\(b\\) que minimicen el error. La funci\u00f3n de costo utilizada es la Log Loss (P\u00e9rdida Logar\u00edtmica), ya que el error cuadr\u00e1tico medio no es convexo para esta funci\u00f3n.</p> <p>El Gradiente Descendente es un algoritmo de optimizaci\u00f3n iterativo: 1.  Inicializa los pesos aleatoriamente. 2.  Calcula el gradiente de la funci\u00f3n de costo (la direcci\u00f3n en la que el error aumenta m\u00e1s r\u00e1pido). 3.  Actualiza los pesos movi\u00e9ndose en la direcci\u00f3n opuesta al gradiente para reducir el error.     \\(\\(w_{nuevo} = w_{viejo} - \\eta \\cdot \\nabla Costo\\)\\)     (Donde \\(\\eta\\) es la tasa de aprendizaje).</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#ejemplo-en-python","title":"Ejemplo en Python","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar datos\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Dividir y Escalar (Importante para Gradiente Descendente)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Entrenar modelo\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n\n# Predecir\ny_pred = log_reg.predict(X_test)\n</code></pre>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#35-metricas-de-rendimiento","title":"3.5. M\u00e9tricas de Rendimiento","text":"<p>Evaluar un clasificador va m\u00e1s all\u00e1 de simplemente contar cu\u00e1ntos aciertos tuvo.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#matriz-de-confusion","title":"Matriz de Confusi\u00f3n","text":"<p>Es una tabla que resume el rendimiento del modelo comparando las clases reales con las predichas.</p> Predicho Negativo (0) Predicho Positivo (1) Real Negativo (0) TN (True Negative) FP (False Positive) Real Positivo (1) FN (False Negative) TP (True Positive) <ul> <li>TP: Enfermos detectados correctamente.</li> <li>TN: Sanos detectados correctamente.</li> <li>FP (Error Tipo I): Sanos detectados err\u00f3neamente como enfermos (\"Falsa Alarma\").</li> <li>FN (Error Tipo II): Enfermos no detectados (\"Peligroso\").</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#metricas-derivadas","title":"M\u00e9tricas Derivadas","text":"<ol> <li> <p>Accuracy (Exactitud): Proporci\u00f3n total de predicciones correctas.     \\(\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\\)</p> </li> <li> <p>Error Rate (Tasa de Error): Proporci\u00f3n de predicciones incorrectas.     \\(\\(Error Rate = 1 - Accuracy = \\frac{FP + FN}{Total}\\)\\)</p> </li> <li> <p>Sensitivity / Recall / TPR (Tasa de Verdaderos Positivos): Capacidad para detectar la clase positiva.     \\(\\(Sensitivity = \\frac{TP}{TP + FN}\\)\\)</p> </li> <li> <p>Specificity / TNR (Tasa de Verdaderos Negativos): Capacidad para detectar la clase negativa.     \\(\\(Specificity = \\frac{TN}{TN + FP}\\)\\)</p> </li> <li> <p>False Positive Rate (FPR): \\(\\(FPR = 1 - Specificity = \\frac{FP}{TN + FP}\\)\\)</p> </li> <li> <p>Precision (Precisi\u00f3n): De los que predije positivos, \u00bfcu\u00e1ntos lo son realmente?     \\(\\(Precision = \\frac{TP}{TP + FP}\\)\\)</p> </li> <li> <p>F1-Score (F-Measure): Media arm\u00f3nica de Precision y Recall. \u00datil cuando las clases est\u00e1n desbalanceadas.     \\(\\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\\)</p> </li> <li> <p>Kappa Statistic (Cohen's Kappa): Mide la concordancia entre la predicci\u00f3n y la realidad, ajustada por el azar. Un valor de 1 es concordancia perfecta, 0 es igual al azar.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#ejemplo-en-python_1","title":"Ejemplo en Python","text":"<pre><code>from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n\nprint(\"Matriz de Confusi\u00f3n:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nReporte de Clasificaci\u00f3n:\\n\", classification_report(y_test, y_pred))\nprint(f\"Kappa Score: {cohen_kappa_score(y_test, y_pred):.4f}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#36-curva-roc-y-auc","title":"3.6. Curva ROC y AUC","text":"<p>La Curva ROC (Receiver Operating Characteristic) es un gr\u00e1fico que ilustra el rendimiento de un clasificador binario a medida que var\u00eda el umbral de discriminaci\u00f3n. *   Eje X: False Positive Rate (1 - Specificity). *   Eje Y: True Positive Rate (Sensitivity).</p> <p>Un modelo ideal se acerca a la esquina superior izquierda (TPR=1, FPR=0). La l\u00ednea diagonal representa un clasificador aleatorio.</p> <p>AUC (Area Under Curve): Es el \u00e1rea bajo la curva ROC. Resume el rendimiento en un solo n\u00famero. *   AUC = 0.5: Aleatorio. *   AUC = 1.0: Perfecto.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#37-sensibilidad-especificidad-y-el-teorema-de-bayes","title":"3.7. Sensibilidad, Especificidad y el Teorema de Bayes","text":"<p>Estos conceptos est\u00e1n \u00edntimamente ligados al Teorema de Bayes cuando queremos calcular la probabilidad real de tener una condici\u00f3n dado un resultado positivo en un test (Probabilidad a Posteriori).</p> <p>Supongamos un test m\u00e9dico para una enfermedad rara: *   \\(P(E)\\): Probabilidad a priori de tener la enfermedad (Prevalencia). *   \\(P(+|E)\\): Sensibilidad del test. *   \\(P(-|No E)\\): Especificidad del test.</p> <p>Si un paciente da positivo, \u00bfcu\u00e1l es la probabilidad de que realmente tenga la enfermedad \\(P(E|+)\\)?</p> \\[P(E|+) = \\frac{P(+|E) \\cdot P(E)}{P(+|E) \\cdot P(E) + P(+|No E) \\cdot P(No E)}\\] <p>Donde \\(P(+|No E)\\) es el False Positive Rate (\\(1 - Especificidad\\)). Este c\u00e1lculo demuestra que si la prevalencia de la enfermedad es muy baja, incluso un test con alta sensibilidad y especificidad puede generar muchos falsos positivos, haciendo que la probabilidad real de estar enfermo sea baja a pesar del resultado positivo.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/","title":"\ud83e\udd16 Unidad 4. \u00c1rbol de Decisi\u00f3n en Inteligencia Artificial: Explicaci\u00f3n Detallada","text":"<p>El algoritmo de \u00c1rbol de Decisi\u00f3n (Decision Tree) es un modelo de aprendizaje supervisado que se utiliza tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n. Su objetivo es dividir el espacio de datos en subconjuntos homog\u00e9neos bas\u00e1ndose en una serie de reglas, de modo que cada subconjunto sea lo m\u00e1s puro posible con respecto a la variable objetivo. Los \u00e1rboles de decisi\u00f3n son f\u00e1ciles de interpretar, muy \u00fatiles para entender las relaciones en los datos, y se aplican ampliamente en una variedad de campos.</p> <p>A continuaci\u00f3n, exploraremos la teor\u00eda detr\u00e1s de los \u00e1rboles de decisi\u00f3n, c\u00f3mo se construyen, y daremos ejemplos que ilustran c\u00f3mo funciona este algoritmo en la pr\u00e1ctica. Tambi\u00e9n incluiremos casos reales en los que este algoritmo ha demostrado ser \u00fatil, as\u00ed como c\u00f3mo encontrar los mejores valores para los metapar\u00e1metros del modelo.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#1-estructura-de-un-arbol-de-decision","title":"1. Estructura de un \u00c1rbol de Decisi\u00f3n","text":"<p>Un \u00e1rbol de decisi\u00f3n est\u00e1 compuesto por varios elementos fundamentales: - Nodos de Decisi\u00f3n: Representan la divisi\u00f3n de los datos seg\u00fan una caracter\u00edstica espec\u00edfica. Aqu\u00ed se toma una decisi\u00f3n sobre qu\u00e9 atributo se usa para dividir el conjunto de datos. - Ramas: Las conexiones entre nodos representan el resultado de una decisi\u00f3n. Cada rama lleva a un nuevo nodo o a un nodo hoja. - Nodos Hoja: Son los puntos finales del \u00e1rbol. Representan la categor\u00eda final o el valor predicho para una determinada observaci\u00f3n.</p> <p>Cada divisi\u00f3n en un \u00e1rbol de decisi\u00f3n intenta dividir los datos de manera que maximice la pureza de los subconjuntos resultantes, es decir, que agrupe datos similares juntos. Este proceso contin\u00faa hasta que se cumplen ciertas condiciones, como alcanzar un n\u00famero m\u00ednimo de muestras en un nodo o una profundidad m\u00e1xima del \u00e1rbol.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#2-criterios-de-division-y-formulas-matematicas","title":"2. Criterios de Divisi\u00f3n y F\u00f3rmulas Matem\u00e1ticas","text":"<p>Los \u00e1rboles de decisi\u00f3n se construyen utilizando una serie de divisiones, cada una de las cuales se elige bas\u00e1ndose en un criterio que mide la calidad de la divisi\u00f3n. Existen varias m\u00e9tricas para seleccionar la caracter\u00edstica que mejor divide los datos:</p> <ul> <li>Entrop\u00eda e \u00cdndice de Ganancia de Informaci\u00f3n</li> <li>\u00cdndice Gini</li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#21-entropia-e-indice-de-ganancia-de-informacion","title":"2.1. Entrop\u00eda e \u00cdndice de Ganancia de Informaci\u00f3n","text":"<p>La entrop\u00eda mide la pureza de un nodo. Se define de la siguiente manera para un nodo que tiene dos clases (positiva y negativa):</p> \\[ H(S) = -p_+ \\cdot \\log_2(p_+) - p_- \\cdot \\log_2(p_-) \\] <p>Donde: - \\( p_+ \\) y \\( p_- \\) son las proporciones de ejemplos positivos y negativos en el nodo.</p> <p>El objetivo es minimizar la entrop\u00eda en cada nodo, lo que equivale a hacer los nodos lo m\u00e1s homog\u00e9neos posible.</p> <p>La ganancia de informaci\u00f3n mide la reducci\u00f3n de la entrop\u00eda despu\u00e9s de dividir un nodo. La f\u00f3rmula para la ganancia de informaci\u00f3n (\\( IG \\)) es:</p> \\[ IG(S, A) = H(S) - \\sum_{v \\in Valores(A)} \\frac{|S_v|}{|S|} H(S_v) \\] <p>Donde: - S es el conjunto de datos original. - A es el atributo por el cual se est\u00e1 dividiendo. - S_v son los subconjuntos de S resultantes de la divisi\u00f3n por el valor v de la caracter\u00edstica A.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#22-indice-gini","title":"2.2. \u00cdndice Gini","text":"<p>El \u00edndice Gini es otra medida utilizada para evaluar la calidad de una divisi\u00f3n. Representa la probabilidad de que una observaci\u00f3n seleccionada aleatoriamente sea clasificada incorrectamente si se realiza una predicci\u00f3n aleatoria basada en la distribuci\u00f3n de clases del nodo. La f\u00f3rmula para el \u00edndice Gini es:</p> \\[ Gini(S) = 1 - \\sum_{i=1}^C p_i^2 \\] <p>Donde \\( p_i \\) es la proporci\u00f3n de elementos pertenecientes a la clase \\( i \\) en el conjunto \\( S \\), y \\( C \\) es el n\u00famero de clases.</p> <p>La idea detr\u00e1s del \u00edndice Gini es minimizar el valor de \\( Gini(S) \\) en cada divisi\u00f3n, buscando nodos lo m\u00e1s puros posible.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#23-calculo-de-ejemplos-con-entropia-e-indice-gini","title":"2.3. C\u00e1lculo de Ejemplos con Entrop\u00eda e \u00cdndice Gini","text":"<p>Veamos un ejemplo detallado de c\u00f3mo calcular la entrop\u00eda y el \u00edndice Gini para una divisi\u00f3n espec\u00edfica de datos.</p> <p>Supongamos que tenemos un conjunto de datos con la siguiente distribuci\u00f3n para la variable objetivo (S\u00ed/No):</p> Caracter\u00edstica Clase: S\u00ed Clase: No A 4 2 B 1 3"},{"location":"aprendizaje-supervisado/04-arboles-decision/#231-calculo-de-la-entropia","title":"2.3.1. C\u00e1lculo de la Entrop\u00eda","text":"<p>Primero calculamos la entrop\u00eda para cada uno de los nodos resultantes de dividir el conjunto de datos seg\u00fan la caracter\u00edstica \"A\":</p> <p>Para A: - Total de ejemplos: \\( 4 + 2 = 6 \\) - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{4}{6})\\) - Proporci\u00f3n de clase No \\(( p_- ): (\\frac{2}{6})\\)</p> <p>La entrop\u00eda para la caracter\u00edstica A es: $$ H(A) = -\\left( \\frac{4}{6} \\right) \\log_2\\left( \\frac{4}{6} \\right) - \\left( \\frac{2}{6} \\right) \\log_2\\left( \\frac{2}{6} \\right) = 0.918 $$</p> <p>Para B: - Total de ejemplos: \\( 1 + 3 = 4 \\) - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{1}{4})\\) - Proporci\u00f3n de clase No \\(( p_+ ): (\\frac{3}{4})\\)</p> <p>La entrop\u00eda para la caracter\u00edstica B es: $$ H(B) = -\\left( \\frac{1}{4} \\right) \\log_2\\left( \\frac{1}{4} \\right) - \\left( \\frac{3}{4} \\right) \\log_2\\left( \\frac{3}{4} \\right) = 0.811 $$</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#232-calculo-del-indice-gini","title":"2.3.2. C\u00e1lculo del \u00cdndice Gini","text":"<p>Ahora calculamos el \u00edndice Gini para la misma divisi\u00f3n:</p> <p>Para A: - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{4}{6})\\) - Proporci\u00f3n de clase No \\(( p_- ): (\\frac{2}{6})\\)</p> <p>El \u00edndice Gini para la caracter\u00edstica A es: $$ Gini(A) = 1 - \\left( \\frac{4}{6} \\right)^2 - \\left( \\frac{2}{6} \\right)^2 = 0.444 $$</p> <p>Para B: - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{1}{4})\\) - Proporci\u00f3n de clase No \\(( p_+ ): (\\frac{3}{4})\\)</p> <p>El \u00edndice Gini para la caracter\u00edstica B es: $$ Gini(B) = 1 - \\left( \\frac{1}{4} \\right)^2 - \\left( \\frac{3}{4} \\right)^2 = 0.375 $$</p> <p>Con estos valores, podemos comparar las caracter\u00edsticas y elegir cu\u00e1l proporciona una mejor divisi\u00f3n de los datos seg\u00fan el criterio seleccionado (en este caso, el que minimice la entrop\u00eda o el \u00edndice Gini).</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#3-construccion-de-un-arbol-de-decision","title":"3. Construcci\u00f3n de un \u00c1rbol de Decisi\u00f3n","text":"<p>La construcci\u00f3n de un \u00e1rbol de decisi\u00f3n se realiza de manera recursiva, siguiendo estos pasos:</p> <ol> <li>Seleccionar el Mejor Atributo: Se elige el atributo que maximiza la ganancia de informaci\u00f3n o minimiza el \u00edndice Gini.</li> <li>Dividir el Conjunto de Datos: Se divide el conjunto de datos en funci\u00f3n del atributo seleccionado.</li> <li>Repetir el Proceso: Se repiten los pasos anteriores para cada subconjunto resultante hasta alcanzar un criterio de parada.</li> </ol> <p>Criterios de Parada pueden ser, por ejemplo, que todos los datos del nodo sean de la misma clase, que el nodo contenga muy pocas instancias (por debajo de un umbral m\u00ednimo), o que se haya alcanzado una profundidad m\u00e1xima predefinida.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#4-ejemplo-de-arbol-de-decision","title":"4. Ejemplo de \u00c1rbol de Decisi\u00f3n","text":"<p>Supongamos que queremos predecir si una persona har\u00e1 ejercicio al aire libre en funci\u00f3n de dos variables: tiempo (soleado, nublado, lluvioso) y temperatura (alta, baja).</p> <ol> <li>Ra\u00edz del \u00c1rbol: Elegimos la primera divisi\u00f3n. Si utilizamos la ganancia de informaci\u00f3n, tal vez encontremos que la variable tiempo tiene la mayor ganancia.</li> <li>Nodo Ra\u00edz: Tiempo</li> <li> <p>Ramas: Soleado, Nublado, Lluvioso</p> </li> <li> <p>Divisiones Subsiguientes: Para cada valor del tiempo, examinamos la temperatura.</p> </li> <li> <p>Para tiempo = Soleado, podemos tener otra divisi\u00f3n por temperatura.</p> </li> <li> <p>Nodos Hoja: Al final de las ramas, llegamos a los nodos hoja, que pueden ser \"S\u00ed\" o \"No\" indicando si la persona har\u00e1 ejercicio o no.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#5-ventajas-y-limitaciones-de-los-arboles-de-decision","title":"5. Ventajas y Limitaciones de los \u00c1rboles de Decisi\u00f3n","text":"<ul> <li>Ventajas:</li> <li>F\u00e1cil Interpretaci\u00f3n: Los \u00e1rboles de decisi\u00f3n son f\u00e1ciles de interpretar, ya que se asemejan a c\u00f3mo los humanos toman decisiones.</li> <li>Pocos Supuestos sobre los Datos: No necesitan normalizaci\u00f3n de datos ni que las caracter\u00edsticas sean escaladas.</li> <li> <p>Manejo de Datos Categ\u00f3ricos y Num\u00e9ricos: Los \u00e1rboles de decisi\u00f3n pueden trabajar con ambos tipos de datos.</p> </li> <li> <p>Limitaciones:</p> </li> <li>Sobreajuste: Los \u00e1rboles de decisi\u00f3n tienden a sobreajustarse si no se limitan adecuadamente (por ejemplo, estableciendo una profundidad m\u00e1xima).</li> <li>Inestabilidad: Los \u00e1rboles de decisi\u00f3n son sensibles a peque\u00f1as variaciones en los datos, lo cual puede generar \u00e1rboles diferentes para conjuntos de datos similares.</li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#6-optimizacion-de-los-metaparametros","title":"6. Optimizaci\u00f3n de los Metapar\u00e1metros","text":"<p>La calidad de un \u00e1rbol de decisi\u00f3n depende en gran medida de los metapar\u00e1metros que se elijan. Algunos de los metapar\u00e1metros clave para un \u00e1rbol de decisi\u00f3n son:</p> <ol> <li> <p>Profundidad M\u00e1xima (<code>max_depth</code>): Limitar la profundidad del \u00e1rbol ayuda a evitar el sobreajuste. La profundidad m\u00e1xima determina cu\u00e1ntos niveles puede tener el \u00e1rbol. Una profundidad muy alta puede llevar al sobreajuste, mientras que una profundidad muy baja puede causar un subajuste.</p> </li> <li> <p>N\u00famero M\u00ednimo de Muestras por Hoja (<code>min_samples_leaf</code>): Controla el n\u00famero m\u00ednimo de muestras que debe haber en un nodo hoja. Un valor m\u00e1s alto reduce el sobreajuste, ya que asegura que las hojas tengan un n\u00famero significativo de ejemplos.</p> </li> <li> <p>N\u00famero M\u00ednimo de Muestras para Dividir (<code>min_samples_split</code>): Especifica el n\u00famero m\u00ednimo de muestras requerido para dividir un nodo. Un valor m\u00e1s alto evita divisiones innecesarias, lo cual ayuda a mantener el \u00e1rbol m\u00e1s simple y reducir el riesgo de sobreajuste.</p> </li> <li> <p>Criterio de Divisi\u00f3n (<code>criterion</code>): Define la funci\u00f3n que se usa para medir la calidad de una divisi\u00f3n. Los criterios comunes son <code>gini</code> e <code>entropy</code>. La elecci\u00f3n del criterio puede influir en la estructura del \u00e1rbol y su capacidad de generalizaci\u00f3n.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#busqueda-de-los-mejores-valores-de-los-metaparametros","title":"B\u00fasqueda de los Mejores Valores de los Metapar\u00e1metros","text":"<p>Para encontrar los valores \u00f3ptimos de estos metapar\u00e1metros, se suelen usar t\u00e9cnicas como la b\u00fasqueda en cuadr\u00edcula (Grid Search) o la b\u00fasqueda aleatoria (Random Search), combinadas con la validaci\u00f3n cruzada.</p> <ul> <li> <p>Grid Search: Busca de manera exhaustiva entre una lista predefinida de valores para cada metapar\u00e1metro. Es eficaz pero puede ser computacionalmente costosa si hay muchos par\u00e1metros y valores posibles.</p> </li> <li> <p>Random Search: Busca valores de metapar\u00e1metros de manera aleatoria dentro de un rango definido. Es m\u00e1s eficiente que Grid Search cuando se trabaja con un gran n\u00famero de combinaciones posibles.</p> </li> <li> <p>Validaci\u00f3n Cruzada: Tanto en Grid Search como en Random Search, se utiliza validaci\u00f3n cruzada para evaluar el rendimiento del modelo para cada combinaci\u00f3n de metapar\u00e1metros y seleccionar aquella que maximice la m\u00e9trica de rendimiento, como la precisi\u00f3n o el F1-score.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#7-aplicaciones-reales-de-los-arboles-de-decision","title":"7. Aplicaciones Reales de los \u00c1rboles de Decisi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n se aplican en una amplia gama de problemas reales debido a su versatilidad y facilidad de interpretaci\u00f3n. Algunos ejemplos son:</p> <ol> <li> <p>Diagn\u00f3stico M\u00e9dico: En la medicina, los \u00e1rboles de decisi\u00f3n se utilizan para ayudar a los m\u00e9dicos a diagnosticar enfermedades bas\u00e1ndose en s\u00edntomas y pruebas de laboratorio. Por ejemplo, un \u00e1rbol de decisi\u00f3n puede ayudar a predecir si un paciente tiene diabetes en funci\u00f3n de caracter\u00edsticas como nivel de glucosa, presi\u00f3n arterial y edad.</p> </li> <li> <p>Cr\u00e9dito y Riesgo Financiero: En el sector financiero, los \u00e1rboles de decisi\u00f3n se usan para evaluar la probabilidad de que un cliente incumpla un pr\u00e9stamo. Las caracter\u00edsticas utilizadas pueden incluir el historial crediticio, los ingresos mensuales y el monto del pr\u00e9stamo solicitado.</p> </li> <li> <p>M\u00e1rketing y Segmentaci\u00f3n de Clientes: En el marketing, los \u00e1rboles de decisi\u00f3n ayudan a segmentar a los clientes y a predecir si un cliente potencial realizar\u00e1 una compra. Los datos analizados pueden incluir el historial de compras, la interacci\u00f3n con campa\u00f1as publicitarias y la demograf\u00eda del cliente.</p> </li> <li> <p>Control de Calidad en Manufactura: En el sector manufacturero, los \u00e1rboles de decisi\u00f3n pueden ayudar a detectar productos defectuosos durante el proceso de producci\u00f3n, bas\u00e1ndose en caracter\u00edsticas como la temperatura, el tiempo de producci\u00f3n, y otras m\u00e9tricas de calidad.</p> </li> <li> <p>Predicci\u00f3n de Deserci\u00f3n Escolar: En educaci\u00f3n, los \u00e1rboles de decisi\u00f3n se usan para predecir la probabilidad de que un estudiante abandone sus estudios, bas\u00e1ndose en factores como la asistencia, las calificaciones y el apoyo familiar.</p> </li> <li> <p>Clasificaci\u00f3n de Especies: En la biolog\u00eda, se utilizan para clasificar especies de plantas o animales seg\u00fan caracter\u00edsticas observadas. Un ejemplo cl\u00e1sico es el conjunto de datos Iris, donde se clasifica una flor en una de tres especies seg\u00fan el largo y ancho de los p\u00e9talos y s\u00e9palos.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#8-conclusion","title":"8. Conclusi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n son una herramienta fundamental en el aprendizaje autom\u00e1tico debido a su capacidad para dividir los datos de manera iterativa y sencilla, maximizando la pureza de los nodos en cada divisi\u00f3n. Aunque presentan ciertas limitaciones, como el riesgo de sobreajuste, son particularmente valiosos cuando se necesita una explicaci\u00f3n clara y comprensible del proceso de decisi\u00f3n. Los \u00e1rboles de decisi\u00f3n se utilizan ampliamente en muchos sectores, y sus aplicaciones van desde el diagn\u00f3stico m\u00e9dico hasta la predicci\u00f3n del comportamiento de los clientes. Son una excelente elecci\u00f3n cuando la interpretabilidad y la facilidad de uso son factores importantes a considerar.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/","title":"\ud83e\udd16 Unidad 5. Algoritmo de Bayes y Naive Bayes en Inteligencia Artificial","text":"<p>El algoritmo de Bayes, tambi\u00e9n conocido como Teorema de Bayes, es un enfoque probabil\u00edstico utilizado para la clasificaci\u00f3n y el an\u00e1lisis en inteligencia artificial y aprendizaje autom\u00e1tico. Este algoritmo se basa en la probabilidad condicional, lo cual permite actualizar la probabilidad de un evento en funci\u00f3n de nueva evidencia.</p> <p>El algoritmo Naive Bayes simplifica el Teorema de Bayes haciendo una suposici\u00f3n fundamental: que todas las caracter\u00edsticas (o atributos) son independientes entre s\u00ed. Esta simplificaci\u00f3n permite construir modelos de clasificaci\u00f3n r\u00e1pidos y eficientes, especialmente \u00fatiles en aplicaciones de clasificaci\u00f3n de texto, como la clasificaci\u00f3n de correos electr\u00f3nicos o el an\u00e1lisis de sentimientos.</p> <p>A continuaci\u00f3n, veremos en detalle c\u00f3mo se deriva el modelo Naive Bayes a partir del Teorema de Bayes y c\u00f3mo funciona, junto con ejemplos y las f\u00f3rmulas matem\u00e1ticas correspondientes.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#1-teorema-de-bayes","title":"1. Teorema de Bayes","text":"<p>El Teorema de Bayes describe la probabilidad de que ocurra un evento \\( A \\) dado que ya ha ocurrido otro evento \\( B \\). La f\u00f3rmula se expresa de la siguiente manera:</p> \\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)} \\] <p>Donde: - \\( P(A|B) \\): Probabilidad de que ocurra el evento \\( A \\) dado que \\( B \\) ha ocurrido (probabilidad posterior). - \\( P(B|A) \\): Probabilidad de que ocurra el evento \\( B \\) dado que \\( A \\) ha ocurrido (verosimilitud). - \\( P(A) \\): Probabilidad a priori del evento \\( A \\). - \\( P(B) \\): Probabilidad del evento \\( B \\).</p> <p>El Teorema de Bayes permite actualizar la probabilidad a priori de un evento a partir de nueva informaci\u00f3n (evidencia).</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#2-naive-bayes","title":"2. Naive Bayes","text":"<p>El clasificador Naive Bayes se deriva del Teorema de Bayes con la suposici\u00f3n de independencia entre caracter\u00edsticas. En lugar de considerar todas las relaciones posibles entre los atributos, se asume que cada caracter\u00edstica es independiente de las dem\u00e1s, dado el resultado. Esto simplifica el c\u00e1lculo de la probabilidad conjunta.</p> <p>La probabilidad de que un ejemplo  $$   x = (x_1, x_2, \\dots, x_n) $$  pertenezca a una clase \\( C_k \\) se puede calcular como:</p> \\[ P(C_k | x) = \\frac{P(C_k) \\prod_{i=1}^n P(x_i | C_k)}{P(x)} \\] <p>Dado que \\( P(x) \\) es constante para todas las clases, podemos simplificar la f\u00f3rmula a:</p> \\[ P(C_k | x) \\propto P(C_k) \\prod_{i=1}^n P(x_i | C_k) \\] <p>Donde: - P(C_k | x): Probabilidad posterior de que el ejemplo pertenezca a la clase C_k . - P(C_k): Probabilidad a priori de la clase C_k. - P(x_i | C_k): Probabilidad condicional de la caracter\u00edstica x_i dada la clase C_k.</p> <p>El clasificador Naive Bayes elige la clase que maximiza esta probabilidad posterior.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#3-tipos-de-clasificadores-naive-bayes","title":"3. Tipos de Clasificadores Naive Bayes","text":"<p>Existen diferentes tipos de clasificadores Naive Bayes, dependiendo del tipo de datos y de c\u00f3mo se calcula la probabilidad condicional:</p> <ul> <li>Naive Bayes Gaussiano: Se utiliza cuando las caracter\u00edsticas tienen una distribuci\u00f3n continua que se puede aproximar a una distribuci\u00f3n normal (gaussiana).</li> <li>Naive Bayes Multinomial: Es adecuado para datos discretos, como el conteo de palabras en un documento. Es ampliamente utilizado en clasificaci\u00f3n de texto.</li> <li>Naive Bayes Bernoulli: Se utiliza para caracter\u00edsticas binarias. Es \u00fatil cuando cada caracter\u00edstica es booleana (por ejemplo, si una palabra aparece o no en un documento).</li> </ul>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#4-ejemplo-completo-de-naive-bayes","title":"4. Ejemplo Completo de Naive Bayes","text":"<p>Vamos a clasificar correos electr\u00f3nicos como \"spam\" o \"no spam\" usando el algoritmo de Naive Bayes. Para este ejemplo, supongamos que tenemos los siguientes datos de entrenamiento, con algunas palabras clave y la clase correspondiente (\"spam\" o \"no spam\"):</p> Correo ID Contenido Clase 1 Oferta barata, gana dinero Spam 2 Proyecto pendiente de trabajo No Spam 3 Oferta especial gratis Spam 4 Reuni\u00f3n de equipo ma\u00f1ana No Spam 5 Gana premios y dinero ahora Spam 6 Informe mensual adjunto No Spam <p>Vamos a suponer que queremos clasificar un nuevo correo con el contenido: \"Oferta gratis y premios\". Para esto, usaremos el clasificador de Naive Bayes, asumiendo que todas las palabras son independientes (el supuesto \"naive\").</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-1-calcular-las-probabilidades-previas","title":"Paso 1: Calcular las Probabilidades Previas","text":"<p>Primero calculamos la probabilidad previa de cada clase.</p> <ul> <li>Probabilidad de Spam (\u03c0(spam)):</li> </ul> <p>$$   P(Spam) = \\frac{N_{Spam}}{N_{Total}} = \\frac{3}{6} = 0.5   $$</p> <ul> <li>Probabilidad de No Spam (\u03c0(no_spam)):</li> </ul> <p>$$   P(No\\,Spam) = \\frac{N_{No\\,Spam}}{N_{Total}} = \\frac{3}{6} = 0.5   $$</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-2-calcular-la-probabilidad-de-cada-palabra","title":"Paso 2: Calcular la Probabilidad de Cada Palabra","text":"<p>Ahora, necesitamos calcular la probabilidad de cada palabra en el contexto de cada clase (es decir, \"spam\" y \"no spam\"). Las palabras \u00fanicas en nuestro conjunto de entrenamiento son:</p> <ul> <li>\"oferta\", \"barata\", \"gana\", \"dinero\", \"proyecto\", \"pendiente\", \"trabajo\", \"especial\", \"gratis\", \"reuni\u00f3n\", \"equipo\", \"ma\u00f1ana\", \"premios\", \"ahora\", \"informe\", \"mensual\", \"adjunto\".</li> </ul> <p>Vamos a usar suavizado de Laplace (adicionando 1 a cada recuento) para evitar probabilidades de cero.</p> <p>Por ejemplo, calculamos la probabilidad de cada palabra para spam:</p> <ul> <li>P(oferta | Spam):</li> </ul> <p>La palabra \"oferta\" aparece en 2 de los 3 correos spam.</p> <p>$$   P(oferta \\mid Spam) = \\frac{2 + 1}{N_{Spam} + V} = \\frac{2 + 1}{3 + 17} = \\frac{3}{20} = 0.15   $$</p> <ul> <li>P(gratis | Spam) y P(premios | Spam) tambi\u00e9n se calculan de manera similar.</li> </ul> <p>Donde: - \\(N_{Spam}\\) : N\u00famero de correos spam. - V : N\u00famero de palabras \u00fanicas en el vocabulario.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-3-clasificar-el-nuevo-correo","title":"Paso 3: Clasificar el Nuevo Correo","text":"<p>El nuevo correo es: \"Oferta gratis y premios\". Queremos calcular la probabilidad de que sea spam o no spam.</p> <p>Para calcular esto, usamos la f\u00f3rmula de Naive Bayes:</p> \\[ P(Clase \\mid X) \\propto P(X \\mid Clase) \\cdot P(Clase) \\] <p>Primero calculamos la probabilidad de que el correo sea spam:</p> \\[ P(Spam \\mid X) \\propto P(oferta \\mid Spam) \\cdot P(gratis \\mid Spam) \\cdot P(premios \\mid Spam) \\cdot P(Spam) \\] <p>Sustituimos los valores y multiplicamos:</p> \\[ P(Spam \\mid X) \\propto 0.15 \\times 0.15 \\times 0.1 \\times 0.5 \\] <p>Hacemos el mismo c\u00e1lculo para No Spam y comparamos ambas probabilidades.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-4-decision","title":"Paso 4: Decisi\u00f3n","text":"<p>Finalmente, elegimos la clase que tiene la probabilidad mayor. Si $ P(Spam \\mid X) $ es mayor que $ P(No\\,Spam \\mid X) $, clasificamos el correo como spam; de lo contrario, como no spam.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#5-aplicaciones-reales-de-naive-bayes","title":"5. Aplicaciones Reales de Naive Bayes","text":"<p>Debido a su eficiencia y capacidad para manejar grandes vol\u00famenes de datos, Naive Bayes se utiliza en una amplia variedad de aplicaciones del mundo real:</p> <ul> <li>Filtrado de Spam: Es el uso m\u00e1s cl\u00e1sico. Servicios como Gmail o Outlook utilizan variantes de Naive Bayes para clasificar correos entrantes como deseados o no deseados bas\u00e1ndose en la frecuencia de ciertas palabras.<ul> <li>Ejemplo de implementaci\u00f3n de filtro Spam</li> </ul> </li> <li>An\u00e1lisis de Sentimientos: Determinar si una opini\u00f3n en redes sociales (Twitter, rese\u00f1as de productos) es positiva, negativa o neutral. Es muy usado en marketing para monitorear la reputaci\u00f3n de marca.<ul> <li>An\u00e1lisis de sentimientos en Twitter</li> </ul> </li> <li>Clasificaci\u00f3n de Documentos: Organizar noticias en categor\u00edas (Deportes, Pol\u00edtica, Tecnolog\u00eda) o clasificar documentos legales y m\u00e9dicos.</li> <li>Sistemas de Recomendaci\u00f3n: Filtrado colaborativo para predecir si a un usuario le gustar\u00e1 un recurso dado.</li> </ul>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#6-ventajas-y-limitaciones-de-naive-bayes","title":"6. Ventajas y Limitaciones de Naive Bayes","text":"<ul> <li>Ventajas:</li> <li>Simplicidad y rapidez: Naive Bayes es simple de implementar y muy r\u00e1pido para entrenar y hacer predicciones, incluso con grandes vol\u00famenes de datos.</li> <li>Escalabilidad: Funciona bien con datos de alta dimensionalidad, como texto.</li> <li> <p>Robustez frente al ruido: A pesar de la suposici\u00f3n de independencia, suele funcionar sorprendentemente bien en muchos problemas reales.</p> </li> <li> <p>Limitaciones:</p> </li> <li>Suposici\u00f3n de independencia: La suposici\u00f3n de independencia rara vez se cumple en problemas reales. Esto puede llevar a predicciones menos precisas cuando las caracter\u00edsticas est\u00e1n altamente correlacionadas.</li> <li>Problemas con datos cero: Si una caracter\u00edstica no se presenta en los datos de entrenamiento, la probabilidad condicional se convierte en cero, lo que hace que la probabilidad posterior tambi\u00e9n sea cero. Esto se suele solucionar con la suavizaci\u00f3n de Laplace.</li> </ul> <p>El clasificador Naive Bayes sigue siendo una herramienta poderosa para muchas aplicaciones de inteligencia artificial y aprendizaje autom\u00e1tico, especialmente en la clasificaci\u00f3n de texto y otros problemas donde la independencia de las caracter\u00edsticas no afecta significativamente el rendimiento del modelo.</p>"},{"location":"aprendizaje-supervisado/06-knn/","title":"\ud83e\udd16 Unidad 6. Algoritmo K-Nearest Neighbors (KNN)","text":"<p>El algoritmo K-Nearest Neighbors (K-Vecinos M\u00e1s Cercanos) es uno de los m\u00e9todos m\u00e1s simples y efectivos en Machine Learning supervisado. Se utiliza tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n. Es un algoritmo no param\u00e9trico (no hace suposiciones sobre la distribuci\u00f3n de los datos subyacentes) y de aprendizaje perezoso (lazy learning), lo que significa que no \"aprende\" un modelo discriminativo durante la fase de entrenamiento, sino que memoriza los datos de entrenamiento para realizar predicciones en el momento necesario.</p>"},{"location":"aprendizaje-supervisado/06-knn/#61-como-funciona-el-algoritmo","title":"6.1. \u00bfC\u00f3mo funciona el algoritmo?","text":"<p>La intuici\u00f3n detr\u00e1s de KNN es sencilla y se basa en la proximidad: \"Dime con qui\u00e9n andas y te dir\u00e9 qui\u00e9n eres\". Para clasificar un nuevo punto de datos, el algoritmo busca en todo el conjunto de datos de entrenamiento los 'k' puntos m\u00e1s cercanos (vecinos) a ese nuevo punto.</p> <ol> <li>Calcular distancias: Se calcula la distancia matem\u00e1tica entre el punto nuevo que queremos predecir y todos los puntos existentes en el dataset.</li> <li>Buscar vecinos: Se seleccionan los \\(k\\) puntos con las distancias m\u00e1s cortas.</li> <li>Votaci\u00f3n (Clasificaci\u00f3n): La clase del nuevo punto se determina por mayor\u00eda de votos de sus vecinos. La clase m\u00e1s frecuente entre los \\(k\\) vecinos se asigna al nuevo punto.</li> <li>Promedio (Regresi\u00f3n): El valor del nuevo punto es el promedio (o media ponderada) de los valores num\u00e9ricos de sus vecinos.</li> </ol>"},{"location":"aprendizaje-supervisado/06-knn/#62-explicacion-matematica","title":"6.2. Explicaci\u00f3n Matem\u00e1tica","text":"<p>El n\u00facleo de KNN es la medici\u00f3n de la distancia para determinar la similitud. La m\u00e9trica m\u00e1s com\u00fan es la Distancia Euclidiana, aunque existen otras dependiendo del tipo de datos y el problema.</p> <p>Dados dos puntos \\(P\\) y \\(Q\\) en un espacio n-dimensional (donde \\(n\\) es el n\u00famero de caracter\u00edsticas): \\(P = (p_1, p_2, ..., p_n)\\) \\(Q = (q_1, q_2, ..., q_n)\\)</p> <ul> <li> <p>Distancia Euclidiana (L2): Es la distancia en l\u00ednea recta \"a vuelo de p\u00e1jaro\". Es la m\u00e1s utilizada por defecto.     \\(\\(d(P, Q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}\\)\\)</p> </li> <li> <p>Distancia Manhattan (L1): Suma de las diferencias absolutas. Es \u00fatil en sistemas tipo cuadr\u00edcula (como manzanas de una ciudad).     \\(\\(d(P, Q) = \\sum_{i=1}^{n} |q_i - p_i|\\)\\)</p> </li> <li> <p>Distancia Minkowski: Una generalizaci\u00f3n matem\u00e1tica de las anteriores.     \\(\\(d(P, Q) = (\\sum_{i=1}^{n} |q_i - p_i|^p)^{1/p}\\)\\)     (Si \\(p=1\\) es Manhattan, si \\(p=2\\) es Euclidiana).</p> </li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#63-pros-y-contras","title":"6.3. Pros y Contras","text":"Ventajas Desventajas Simplicidad: Es extremadamente f\u00e1cil de entender, explicar e implementar. Costo Computacional: Es lento en la fase de predicci\u00f3n con grandes datasets, ya que debe calcular distancias con todos los puntos cada vez. Sin Entrenamiento: La fase de entrenamiento es casi instant\u00e1nea (solo almacena datos), ideal si los datos cambian constantemente. Sensible a Outliers: Los valores at\u00edpicos o ruido pueden afectar significativamente la predicci\u00f3n si \\(k\\) es peque\u00f1o. Versatilidad: Sirve tanto para tareas de clasificaci\u00f3n como de regresi\u00f3n. Sensible a la Escala: Requiere estrictamente que los datos est\u00e9n normalizados o estandarizados. No Lineal: Se adapta bien a fronteras de decisi\u00f3n irregulares y complejas. Maldici\u00f3n de la Dimensionalidad: Su rendimiento decae dr\u00e1sticamente cuando hay muchas dimensiones (features) irrelevantes."},{"location":"aprendizaje-supervisado/06-knn/#64-ejemplo-en-python-con-scikit-learn","title":"6.4. Ejemplo en Python con <code>scikit-learn</code>","text":"<p>A continuaci\u00f3n, un ejemplo b\u00e1sico de clasificaci\u00f3n usando el dataset Iris y la clase <code>KNeighborsClassifier</code>.</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# 1. Cargar datos\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 2. Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 3. Escalar datos (CRUCIAL para KNN)\n# Como KNN usa distancias, las variables con rangos grandes dominar\u00e1n a las peque\u00f1as.\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 4. Instanciar el modelo (k=3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# 5. Entrenar (En KNN esto es solo almacenar los datos)\nknn.fit(X_train, y_train)\n\n# 6. Predecir y Evaluar\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"aprendizaje-supervisado/06-knn/#65-ejemplos-comunes-de-uso","title":"6.5. Ejemplos Comunes de Uso","text":"<ul> <li>Sistemas de Recomendaci\u00f3n: Sugerir productos, pel\u00edculas o m\u00fasica bas\u00e1ndose en las preferencias de usuarios \"vecinos\" con gustos similares (Filtrado Colaborativo).</li> <li>Reconocimiento de Patrones: Reconocimiento de caracteres escritos a mano (OCR) o clasificaci\u00f3n de im\u00e1genes simples bas\u00e1ndose en la similitud de p\u00edxeles.</li> <li>Detecci\u00f3n de Anomal\u00edas: Identificar fraudes bancarios o intrusiones en redes detectando eventos que est\u00e1n \"lejos\" de los grupos de vecinos normales.</li> <li>Imputaci\u00f3n de Datos Faltantes: Rellenar valores nulos en un dataset bas\u00e1ndose en los valores de los vecinos m\u00e1s cercanos (<code>KNNImputer</code>).</li> <li>Medicina: Clasificaci\u00f3n de pacientes con perfiles similares para predecir riesgos de enfermedades.</li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#66-aplicaciones-reales-de-knn","title":"6.6. Aplicaciones Reales de KNN","text":"<p>Aunque es un algoritmo simple, KNN se utiliza en sistemas donde la interpretabilidad y la simplicidad son clave:</p> <ul> <li>Sistemas de Recomendaci\u00f3n (Retail): Empresas como Amazon o Netflix utilizan variantes de algoritmos basados en vecindad para recomendar productos (\"Los usuarios que compraron X tambi\u00e9n compraron Y\").<ul> <li>Sistemas de recomendaci\u00f3n con KNN</li> </ul> </li> <li>Reconocimiento de Escritura a Mano: El servicio postal de EE.UU. (USPS) utiliz\u00f3 m\u00e9todos basados en vecindad para reconocer d\u00edgitos escritos a mano en c\u00f3digos postales.<ul> <li>Dataset MNIST y KNN</li> </ul> </li> <li>Detecci\u00f3n de Intrusiones (Ciberseguridad): Clasificar actividades de red como normales o sospechosas bas\u00e1ndose en su similitud con patrones de ataques conocidos.</li> <li>Bioinform\u00e1tica: Clasificaci\u00f3n de muestras de genes o prote\u00ednas bas\u00e1ndose en su similitud con perfiles conocidos para el diagn\u00f3stico de enfermedades.</li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#67-consideraciones-finales","title":"6.7. Consideraciones Finales","text":"<ol> <li> <p>Elecci\u00f3n de 'k' (Hiperpar\u00e1metro clave):</p> <ul> <li>Un \\(k\\) muy peque\u00f1o (ej. \\(k=1\\)) hace que el modelo sea muy sensible al ruido (Overfitting).</li> <li>Un \\(k\\) muy grande suaviza demasiado la frontera de decisi\u00f3n y puede incluir vecinos de otras clases lejanas (Underfitting).</li> <li>Se suele elegir un \\(k\\) impar para evitar empates en clasificaci\u00f3n binaria.</li> <li>El valor \u00f3ptimo se encuentra usualmente mediante validaci\u00f3n cruzada (t\u00e9cnica del codo o Elbow Method).</li> </ul> </li> <li> <p>Escalado de Caracter\u00edsticas:</p> <ul> <li>Dado que KNN se basa puramente en distancias, es obligatorio escalar las variables (usando <code>StandardScaler</code> o <code>MinMaxScaler</code>). Si una variable tiene una magnitud mucho mayor que otra (ej. Salario [1000-5000] vs Edad [20-60]), la variable de mayor magnitud dominar\u00e1 completamente el c\u00e1lculo de la distancia, haciendo que la otra sea irrelevante.</li> </ul> </li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/07-svm/","title":"\ud83e\udd16 Unidad 7. M\u00e1quinas de Vectores de Soporte (SVM)","text":"<p>Las M\u00e1quinas de Vectores de Soporte (Support Vector Machines o SVM) son un conjunto de algoritmos de aprendizaje supervisado potentes y vers\u00e1tiles, utilizados tanto para clasificaci\u00f3n (SVC) como para regresi\u00f3n (SVR). Su objetivo principal es encontrar el hiperplano \u00f3ptimo que mejor separe las clases en el espacio de caracter\u00edsticas.</p>"},{"location":"aprendizaje-supervisado/07-svm/#71-como-funciona-el-algoritmo","title":"7.1. \u00bfC\u00f3mo funciona el algoritmo?","text":"<p>La idea central de SVM es encontrar una l\u00ednea (en 2D), un plano (en 3D) o un hiperplano (en m\u00e1s dimensiones) que divida los datos en clases distintas. Pero no cualquier separaci\u00f3n sirve; SVM busca la separaci\u00f3n que tenga el mayor margen posible.</p> <ol> <li>Hiperplano: Es la frontera de decisi\u00f3n que separa las clases.</li> <li>Vectores de Soporte: Son los puntos de datos m\u00e1s cercanos al hiperplano. Estos puntos son los m\u00e1s \"dif\u00edciles\" de clasificar y son los \u00fanicos que importan para definir la posici\u00f3n del hiperplano.</li> <li>Margen: Es la distancia entre el hiperplano y los vectores de soporte m\u00e1s cercanos de cada clase. SVM intenta maximizar este margen para mejorar la generalizaci\u00f3n del modelo.</li> </ol>"},{"location":"aprendizaje-supervisado/07-svm/#72-explicacion-matematica-y-el-kernel-trick","title":"7.2. Explicaci\u00f3n Matem\u00e1tica y el \"Kernel Trick\"","text":"<p>Matem\u00e1ticamente, para un problema linealmente separable, buscamos los par\u00e1metros \\(w\\) (vector de pesos) y \\(b\\) (sesgo) tal que el hiperplano se defina como: \\(\\(w \\cdot x + b = 0\\)\\)</p> <p>El objetivo es minimizar \\(||w||\\) (lo que equivale a maximizar el margen) sujeto a que todas las muestras est\u00e9n correctamente clasificadas fuera del margen.</p>"},{"location":"aprendizaje-supervisado/07-svm/#el-truco-del-kernel-kernel-trick","title":"El Truco del Kernel (Kernel Trick)","text":"<p>Cuando los datos no son separables linealmente (ej. un c\u00edrculo dentro de otro), SVM utiliza una t\u00e9cnica llamada Kernel Trick. Esta t\u00e9cnica proyecta los datos originales a un espacio de mayor dimensi\u00f3n donde s\u00ed son linealmente separables, sin necesidad de calcular expl\u00edcitamente las coordenadas en ese espacio complejo (lo cual ser\u00eda computacionalmente costoso).</p> <p>Kernels comunes: *   Lineal: Para datos linealmente separables. *   Polin\u00f3mico: Mapea a espacios de dimensiones polin\u00f3micas. *   RBF (Radial Basis Function): El m\u00e1s popular. Mapea a un espacio de dimensi\u00f3n infinita. Es muy efectivo para fronteras de decisi\u00f3n complejas y curvas.</p>"},{"location":"aprendizaje-supervisado/07-svm/#73-pros-y-contras","title":"7.3. Pros y Contras","text":"Ventajas Desventajas Alta Dimensionalidad: Es muy efectivo en espacios con muchas dimensiones (incluso si hay m\u00e1s dimensiones que muestras). Grandes Datasets: No escala bien con datasets muy grandes (el tiempo de entrenamiento crece c\u00fabicamente). Eficiencia de Memoria: Solo usa un subconjunto de puntos de entrenamiento (los vectores de soporte) para definir el modelo. Ruido: Es sensible al ruido y a clases que se solapan mucho (si no se ajustan bien los par\u00e1metros). Versatilidad: Gracias a los Kernels, puede modelar relaciones lineales y no lineales complejas. Probabilidades: No proporciona estimaciones de probabilidad directas (se calculan mediante validaci\u00f3n cruzada costosa). Robustez: Maximizar el margen ayuda a reducir el riesgo de overfitting. Ajuste de Par\u00e1metros: Requiere un ajuste cuidadoso de hiperpar\u00e1metros clave (\\(C\\), \\(\\gamma\\), Kernel)."},{"location":"aprendizaje-supervisado/07-svm/#74-ejemplo-en-python-con-scikit-learn","title":"7.4. Ejemplo en Python con <code>scikit-learn</code>","text":"<p>Ejemplo de clasificaci\u00f3n usando <code>SVC</code> (Support Vector Classification) con un kernel RBF.</p> <pre><code>from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# 1. Cargar datos\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 2. Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 3. Escalar datos (IMPORTANTE para SVM)\n# SVM es sensible a la escala porque intenta maximizar la distancia (margen).\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 4. Instanciar el modelo\n# kernel='rbf' es el valor por defecto. C es el par\u00e1metro de regularizaci\u00f3n.\nclf = svm.SVC(kernel='rbf', C=1.0, gamma='scale')\n\n# 5. Entrenar\nclf.fit(X_train, y_train)\n\n# 6. Predecir y Evaluar\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"aprendizaje-supervisado/07-svm/#75-ejemplos-comunes-de-uso","title":"7.5. Ejemplos Comunes de Uso","text":"<ul> <li>Clasificaci\u00f3n de Texto: Categorizaci\u00f3n de noticias, detecci\u00f3n de spam y an\u00e1lisis de sentimientos. SVM maneja muy bien la alta dimensionalidad de los vectores de texto (Bag of Words).</li> <li>Reconocimiento de Im\u00e1genes: Clasificaci\u00f3n de im\u00e1genes, reconocimiento facial y reconocimiento de escritura a mano (OCR).</li> <li>Bioinform\u00e1tica: Clasificaci\u00f3n de prote\u00ednas y genes, donde los datos suelen tener muchas caracter\u00edsticas y pocas muestras.</li> <li>Detecci\u00f3n de Intrusos: Identificar actividad maliciosa en redes bas\u00e1ndose en patrones de tr\u00e1fico.</li> </ul>"},{"location":"aprendizaje-supervisado/07-svm/#76-aplicaciones-reales-de-svm","title":"7.6. Aplicaciones Reales de SVM","text":"<p>SVM ha sido uno de los algoritmos m\u00e1s exitosos antes del auge del Deep Learning y sigue siendo muy relevante:</p> <ul> <li>Clasificaci\u00f3n de Im\u00e1genes (Hist\u00f3rico): Antes de las redes neuronales convolucionales (CNN), SVM era el est\u00e1ndar para clasificaci\u00f3n de im\u00e1genes y detecci\u00f3n de objetos (ej. detecci\u00f3n de peatones).</li> <li>Bioinform\u00e1tica (Clasificaci\u00f3n de Prote\u00ednas): Se utiliza para clasificar prote\u00ednas en familias funcionales y predecir la estructura secundaria de las prote\u00ednas, dado que maneja muy bien la alta dimensionalidad de los datos gen\u00f3micos.<ul> <li>SVM en Bioinform\u00e1tica</li> </ul> </li> <li>Reconocimiento de Escritura: SVM ha demostrado ser muy eficaz en el reconocimiento de caracteres manuscritos (OCR), compitiendo con redes neuronales en datasets como MNIST.</li> <li>Geolog\u00eda y Miner\u00eda: Clasificaci\u00f3n de tipos de suelo y rocas a partir de datos s\u00edsmicos o im\u00e1genes satelitales.</li> </ul>"},{"location":"aprendizaje-supervisado/07-svm/#77-consideraciones-finales","title":"7.7. Consideraciones Finales","text":"<ol> <li> <p>Par\u00e1metro C (Regularizaci\u00f3n):</p> <ul> <li>Controla el equilibrio entre tener un margen amplio y clasificar correctamente los puntos de entrenamiento.</li> <li>C alto: Intenta clasificar todo correctamente (riesgo de Overfitting, margen estrecho).</li> <li>C bajo: Permite algunos errores para obtener un margen m\u00e1s amplio (mejor generalizaci\u00f3n, margen suave).</li> </ul> </li> <li> <p>Par\u00e1metro Gamma (\\(\\gamma\\)) (Solo para kernels RBF/Poly):</p> <ul> <li>Define qu\u00e9 tan lejos llega la influencia de un solo ejemplo de entrenamiento.</li> <li>Gamma alto: Solo los puntos muy cercanos influyen. Puede llevar a fronteras de decisi\u00f3n muy ajustadas e irregulares (Overfitting).</li> <li>Gamma bajo: La influencia llega lejos. La frontera de decisi\u00f3n es m\u00e1s suave (Underfitting si es muy bajo).</li> </ul> </li> <li> <p>Escalado: Al igual que KNN, SVM se basa en distancias. Es cr\u00edtico estandarizar los datos antes de entrenar.</p> </li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/","title":"\ud83e\udd16 Unidad 8. Algoritmos de Ensamblado (Ensemble Learning)","text":"<p>Los Algoritmos de Ensamblado (Ensemble Methods) son una t\u00e9cnica de Machine Learning que combina las predicciones de m\u00faltiples modelos base (conocidos como weak learners o aprendices d\u00e9biles) para construir un modelo final m\u00e1s robusto y preciso (strong learner).</p> <p>La intuici\u00f3n detr\u00e1s de esto es la \"Sabidur\u00eda de las Masas\": as\u00ed como la opini\u00f3n colectiva de un grupo de expertos suele ser mejor que la de un solo experto, un grupo de modelos predictivos suele superar el rendimiento de un modelo individual.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#81-conceptos-clave-y-categorias","title":"8.1. Conceptos Clave y Categor\u00edas","text":"<p>El objetivo principal es reducir el sesgo (bias) o la varianza (variance), o ambos. Los m\u00e9todos de ensamblado se dividen principalmente en tres categor\u00edas seg\u00fan c\u00f3mo combinan los modelos:</p> <ol> <li>Voting (Votaci\u00f3n): Se entrenan varios modelos diferentes (ej. KNN, SVM, \u00c1rbol) y se \"vota\" para decidir la clase final.</li> <li>Bagging (Bootstrap Aggregating): Se entrena el mismo algoritmo muchas veces en paralelo, pero con diferentes subconjuntos aleatorios de los datos de entrenamiento. Su objetivo es reducir la varianza (evitar overfitting). El ejemplo cl\u00e1sico es Random Forest.</li> <li>Boosting: Se entrena el mismo algoritmo de forma secuencial. Cada nuevo modelo intenta corregir los errores cometidos por el modelo anterior. Su objetivo es reducir el sesgo (evitar underfitting). Ejemplos: AdaBoost, XGBoost.</li> </ol>"},{"location":"aprendizaje-supervisado/08-ensamblado/#82-voting-classifiers-votacion","title":"8.2. Voting Classifiers (Votaci\u00f3n)","text":"<p>Es la forma m\u00e1s simple de ensamblado. Consiste en agregar las predicciones de clasificadores totalmente diferentes.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#tipos-de-votacion","title":"Tipos de Votaci\u00f3n","text":"<ul> <li>Hard Voting (Votaci\u00f3n Dura): Cada clasificador vota por una clase. La clase con la mayor\u00eda de votos gana (moda).</li> <li>Soft Voting (Votaci\u00f3n Suave): Si los clasificadores pueden estimar probabilidades (tienen m\u00e9todo <code>predict_proba</code>), se promedian las probabilidades de cada clase. La clase con el promedio de probabilidad m\u00e1s alto gana. El Soft Voting suele funcionar mejor porque da m\u00e1s peso a los votos con \"alta confianza\".</li> </ul>"},{"location":"aprendizaje-supervisado/08-ensamblado/#ejemplo-en-python-voting","title":"Ejemplo en Python (Voting)","text":"<pre><code>from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Datos de ejemplo\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Modelos individuales\nlog_clf = LogisticRegression(random_state=42)\nrnd_clf = DecisionTreeClassifier(random_state=42)\nsvm_clf = SVC(probability=True, random_state=42) # probability=True necesario para Soft Voting\n\n# Ensamblado por Votaci\u00f3n (Soft)\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='soft'\n)\n\n# Entrenamiento y Comparaci\u00f3n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(f\"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/08-ensamblado/#83-bagging-y-random-forest","title":"8.3. Bagging y Random Forest","text":"<p>Bagging (Bootstrap Aggregating) implica entrenar el mismo algoritmo en diferentes subconjuntos aleatorios del dataset de entrenamiento. *   Bootstrap: El muestreo se hace con reemplazo (una misma muestra puede aparecer varias veces en el mismo subconjunto). *   Pasting: El muestreo se hace sin reemplazo.</p> <p>Una vez entrenados, los modelos agregan sus predicciones (moda para clasificaci\u00f3n, promedio para regresi\u00f3n).</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#random-forest-bosques-aleatorios","title":"Random Forest (Bosques Aleatorios)","text":"<p>Es una implementaci\u00f3n espec\u00edfica y optimizada de Bagging usando \u00c1rboles de Decisi\u00f3n. Introduce aleatoriedad extra: al dividir un nodo en el \u00e1rbol, no busca la mejor caracter\u00edstica de todas las disponibles, sino la mejor caracter\u00edstica dentro de un subconjunto aleatorio de caracter\u00edsticas. Esto hace que los \u00e1rboles sean m\u00e1s diversos (descorrelacionados), lo que reduce dr\u00e1sticamente la varianza.</p> <p>Hiperpar\u00e1metros Clave: *   <code>n_estimators</code>: N\u00famero de \u00e1rboles (m\u00e1s es mejor, pero m\u00e1s lento). *   <code>max_features</code>: N\u00famero m\u00e1ximo de caracter\u00edsticas a considerar en cada divisi\u00f3n. *   <code>bootstrap</code>: Si usar muestreo con reemplazo (True por defecto). *   <code>n_jobs</code>: N\u00famero de n\u00facleos de CPU a usar (-1 para todos).</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#ejemplo-en-python-random-forest","title":"Ejemplo en Python (Random Forest)","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Instanciar Random Forest\n# 500 \u00e1rboles, usando todos los n\u00facleos de CPU\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n\n# Importancia de Caracter\u00edsticas\n# Random Forest permite ver qu\u00e9 variables son m\u00e1s \u00fatiles\nfor name, score in zip([\"Feature 1\", \"Feature 2\"], rnd_clf.feature_importances_):\n    print(f\"{name}: {score}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/08-ensamblado/#84-boosting-impulso","title":"8.4. Boosting (Impulso)","text":"<p>El Boosting entrena predictores secuencialmente, cada uno intentando corregir a su predecesor.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#841-adaboost-adaptive-boosting","title":"8.4.1. AdaBoost (Adaptive Boosting)","text":"<p>El algoritmo presta m\u00e1s atenci\u00f3n a las instancias de entrenamiento que el predecesor clasific\u00f3 incorrectamente. 1.  Entrena un clasificador base. 2.  Aumenta el peso relativo de las instancias mal clasificadas. 3.  Entrena un segundo clasificador con los pesos actualizados. 4.  Repite el proceso.</p> <p>Hiperpar\u00e1metros Clave: *   <code>n_estimators</code>: N\u00famero de iteraciones. *   <code>learning_rate</code>: Cu\u00e1nto contribuye cada modelo. Un valor bajo requiere m\u00e1s estimadores.</p> <p>Ejemplo Python (AdaBoost): <pre><code>from sklearn.ensemble import AdaBoostClassifier\n\n# AdaBoost usando \u00c1rboles de Decisi\u00f3n muy simples (stumps)\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42\n)\nada_clf.fit(X_train, y_train)\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#842-gradient-boosting-machine-gbm","title":"8.4.2. Gradient Boosting Machine (GBM)","text":"<p>En lugar de ajustar los pesos de las instancias, GBM intenta ajustar el nuevo predictor a los errores residuales (la diferencia entre el valor real y el predicho) del predictor anterior.</p> <p>Ejemplo Python (GradientBoosting de sklearn): <pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\ngbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\ngbrt.fit(X_train, y_train)\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#843-xgboost-extreme-gradient-boosting","title":"8.4.3. XGBoost (Extreme Gradient Boosting)","text":"<p>Es una versi\u00f3n optimizada de Gradient Boosting dise\u00f1ada para ser altamente eficiente, flexible y port\u00e1til. Es el algoritmo dominante en competiciones de Machine Learning (Kaggle). *   Regularizaci\u00f3n: Incluye regularizaci\u00f3n L1 y L2 para evitar overfitting. *   Paralelizaci\u00f3n: Construcci\u00f3n de \u00e1rboles en paralelo. *   Manejo de nulos: Aprende autom\u00e1ticamente la mejor direcci\u00f3n para valores faltantes.</p> <p>Hiperpar\u00e1metros Clave: *   <code>eta</code> (learning_rate): Paso de reducci\u00f3n de pesos para prevenir overfitting. *   <code>max_depth</code>: Profundidad m\u00e1xima del \u00e1rbol. *   <code>subsample</code>: Ratio de muestras de entrenamiento usadas. *   <code>colsample_bytree</code>: Ratio de columnas usadas por \u00e1rbol.</p> <p>Ejemplo Python (XGBoost): <pre><code>import xgboost as xgb\n\n# XGBoost tiene su propia estructura de datos optimizada (DMatrix), pero es compatible con sklearn\nxgb_clf = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=3,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    n_jobs=-1,\n    random_state=42\n)\n\nxgb_clf.fit(X_train, y_train)\nprint(f\"XGBoost Accuracy: {accuracy_score(y_test, xgb_clf.predict(X_test)):.4f}\")\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#844-lightgbm-light-gradient-boosting-machine","title":"8.4.4. LightGBM (Light Gradient Boosting Machine)","text":"<p>Desarrollado por Microsoft. A diferencia de otros que crecen el \u00e1rbol por niveles (level-wise), LightGBM crece por hojas (leaf-wise). Elige la hoja con mayor p\u00e9rdida para crecer. *   Ventajas: Mucho m\u00e1s r\u00e1pido que XGBoost en grandes datasets y consume menos memoria. *   Desventajas: Puede hacer overfitting f\u00e1cilmente en datasets peque\u00f1os (&lt; 10,000 filas).</p> <p>Hiperpar\u00e1metros Clave: *   <code>num_leaves</code>: Par\u00e1metro principal para controlar la complejidad (en lugar de max_depth). *   <code>min_data_in_leaf</code>: Importante para evitar overfitting.</p> <p>Ejemplo Python (LightGBM): <pre><code>import lightgbm as lgb\n\nlgb_clf = lgb.LGBMClassifier(\n    num_leaves=31,\n    learning_rate=0.05,\n    n_estimators=100,\n    random_state=42\n)\n\nlgb_clf.fit(X_train, y_train)\nprint(f\"LightGBM Accuracy: {accuracy_score(y_test, lgb_clf.predict(X_test)):.4f}\")\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#85-resumen-comparativo","title":"8.5. Resumen Comparativo","text":"T\u00e9cnica Algoritmo Principal Estrategia Objetivo Paralelizable Voting VotingClassifier Promedio de modelos distintos Robustez general S\u00ed Bagging Random Forest Modelos iguales, datos aleatorios (independientes) Reducir Varianza S\u00ed (Muy r\u00e1pido) Boosting AdaBoost, XGBoost Modelos iguales, secuenciales (dependientes) Reducir Sesgo No (Secuencial)* <p>* Nota: XGBoost y LightGBM paralelizan la construcci\u00f3n dentro del \u00e1rbol, pero los \u00e1rboles se crean secuencialmente.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#86-aplicaciones-reales-de-algoritmos-de-ensamblado","title":"8.6. Aplicaciones Reales de Algoritmos de Ensamblado","text":"<p>Los m\u00e9todos de ensamblado dominan actualmente las competiciones de ciencia de datos y las aplicaciones industriales en datos estructurados:</p> <ul> <li>Detecci\u00f3n de Fraude (Banca): Algoritmos como XGBoost y Random Forest son el est\u00e1ndar en la industria financiera para detectar transacciones fraudulentas en tiempo real debido a su alta precisi\u00f3n y velocidad.<ul> <li>Detecci\u00f3n de fraude con XGBoost</li> </ul> </li> <li>Diagn\u00f3stico M\u00e9dico: Random Forest se utiliza para diagnosticar enfermedades (como la retinopat\u00eda diab\u00e9tica) analizando m\u00faltiples variables de pacientes, ya que proporciona una medida de qu\u00e9 s\u00edntomas son m\u00e1s relevantes.</li> <li>Ranking de B\u00fasqueda (Search Engines): Motores de b\u00fasqueda utilizan Gradient Boosting para ordenar los resultados de b\u00fasqueda (Learning to Rank), optimizando la relevancia para el usuario.<ul> <li>Learning to Rank con LightGBM</li> </ul> </li> <li>Predicci\u00f3n de Demanda (Retail): Cadenas de suministro usan estos modelos para predecir la demanda futura de productos, optimizando el inventario y reduciendo desperdicios.</li> </ul>"},{"location":"aprendizaje-supervisado/08-ensamblado/#87-consideraciones-finales","title":"8.7. Consideraciones Finales","text":"<ol> <li>Random Forest es una excelente \"primera opci\u00f3n\". Es robusto, requiere poco ajuste de hiperpar\u00e1metros y nos da la importancia de las caracter\u00edsticas.</li> <li>XGBoost / LightGBM suelen ofrecer el mejor rendimiento (Accuracy) en datos tabulares estructurados, pero requieren m\u00e1s ajuste de hiperpar\u00e1metros y cuidado con el overfitting.</li> <li>Escalado: Los algoritmos basados en \u00e1rboles (Random Forest, Boosting) NO requieren escalado de caracter\u00edsticas (StandardScaler), lo cual es una gran ventaja pr\u00e1ctica.</li> <li>Interpretabilidad: Los modelos de ensamblado son \"Cajas Negras\". Perdemos la interpretabilidad simple de un solo \u00c1rbol de Decisi\u00f3n o una Regresi\u00f3n Lineal, aunque podemos usar la \"Importancia de Caracter\u00edsticas\" para entender qu\u00e9 variables pesan m\u00e1s.</li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/","title":"\ud83e\udde0 Deep Learning","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Deep Learning! \ud83c\udf89</p>"},{"location":"deep-learning/#que-es-el-deep-learning","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Deep Learning?","text":"<p>El Deep Learning (Aprendizaje Profundo) es un subcampo del Machine Learning que utiliza redes neuronales artificiales con m\u00faltiples capas (de ah\u00ed el t\u00e9rmino \"profundo\") para aprender representaciones jer\u00e1rquicas de los datos.</p> <p>A diferencia de los algoritmos tradicionales de ML que requieren ingenier\u00eda de caracter\u00edsticas manual, el Deep Learning puede aprender autom\u00e1ticamente las caracter\u00edsticas relevantes directamente de los datos en bruto.</p>"},{"location":"deep-learning/#arquitecturas-principales","title":"\ud83e\udde0 Arquitecturas Principales","text":"<ol> <li> <p>Redes Neuronales Artificiales (ANN):    La base del Deep Learning, inspirada en el cerebro humano.    \ud83d\udccd Uso: Clasificaci\u00f3n, regresi\u00f3n.</p> </li> <li> <p>Redes Neuronales Convolucionales (CNN):    Especializadas en procesar datos con estructura de cuadr\u00edcula (im\u00e1genes).    \ud83d\udccd Uso: Visi\u00f3n por computadora, clasificaci\u00f3n de im\u00e1genes, detecci\u00f3n de objetos.</p> </li> <li> <p>Redes Neuronales Recurrentes (RNN):    Dise\u00f1adas para datos secuenciales con memoria de estados anteriores.    \ud83d\udccd Uso: Series temporales, texto.</p> </li> <li> <p>LSTM y GRU:    Variantes de RNN que solucionan el problema del gradiente desvaneciente.    \ud83d\udccd Uso: Secuencias largas, traducci\u00f3n.</p> </li> <li> <p>Transformers:    Arquitectura basada en mecanismos de atenci\u00f3n, revolucion\u00f3 el NLP y m\u00e1s.    \ud83d\udccd Uso: GPT, BERT, modelos de lenguaje grandes (LLMs).</p> </li> <li> <p>Autoencoders:    Redes que aprenden representaciones comprimidas de los datos.    \ud83d\udccd Uso: Reducci\u00f3n de dimensionalidad, generaci\u00f3n.</p> </li> <li> <p>GANs (Redes Generativas Adversarias):    Dos redes que compiten para generar datos realistas.    \ud83d\udccd Uso: Generaci\u00f3n de im\u00e1genes, arte, deepfakes.</p> </li> </ol>"},{"location":"deep-learning/#conceptos-fundamentales","title":"\u2699\ufe0f Conceptos Fundamentales","text":"<ul> <li>Neurona artificial: Unidad b\u00e1sica que aplica pesos, bias y funci\u00f3n de activaci\u00f3n</li> <li>Capas: Entrada, ocultas y salida</li> <li>Funciones de activaci\u00f3n: ReLU, Sigmoid, Tanh, Softmax</li> <li>Backpropagation: Algoritmo para calcular gradientes</li> <li>Optimizadores: SGD, Adam, RMSprop</li> <li>Regularizaci\u00f3n: Dropout, L1/L2, Batch Normalization</li> <li>Transfer Learning: Reutilizar modelos preentrenados</li> </ul>"},{"location":"deep-learning/#frameworks-populares","title":"\ud83d\udd0d Frameworks Populares","text":"<ul> <li>TensorFlow - Framework de Google</li> <li>Keras - API de alto nivel (integrada en TensorFlow)</li> <li>PyTorch - Framework de Meta, muy usado en investigaci\u00f3n</li> <li>JAX - Computaci\u00f3n num\u00e9rica de alto rendimiento</li> </ul>"},{"location":"deep-learning/#contenido-en-construccion","title":"\ud83d\udea7 Contenido en construcci\u00f3n","text":"<p>Esta secci\u00f3n est\u00e1 siendo desarrollada. Pr\u00f3ximamente encontrar\u00e1s:</p> <ul> <li>[ ] Fundamentos de redes neuronales</li> <li>[ ] Redes convolucionales (CNN)</li> <li>[ ] Redes recurrentes (RNN, LSTM)</li> <li>[ ] Transformers y atenci\u00f3n</li> <li>[ ] Ejemplos pr\u00e1cticos con TensorFlow/PyTorch</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/","title":"\ud83d\udcac Procesamiento de Lenguaje Natural (NLP)","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Procesamiento de Lenguaje Natural! \ud83c\udf89</p>"},{"location":"procesamiento-lenguaje-natural/#que-es-el-procesamiento-de-lenguaje-natural","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Procesamiento de Lenguaje Natural?","text":"<p>El Procesamiento de Lenguaje Natural (NLP), del ingl\u00e9s Natural Language Processing, es una rama de la Inteligencia Artificial que se centra en la interacci\u00f3n entre las computadoras y el lenguaje humano.</p> <p>El objetivo del NLP es permitir que las m\u00e1quinas comprendan, interpreten y generen lenguaje humano de manera \u00fatil y significativa.</p>"},{"location":"procesamiento-lenguaje-natural/#aplicaciones-del-nlp","title":"\ud83e\udde0 Aplicaciones del NLP","text":"<ol> <li> <p>An\u00e1lisis de Sentimientos:    Determinar si un texto expresa una opini\u00f3n positiva, negativa o neutral.    \ud83d\udccd Ejemplo: Analizar rese\u00f1as de productos.</p> </li> <li> <p>Chatbots y Asistentes Virtuales:    Sistemas que interact\u00faan con usuarios mediante lenguaje natural.    \ud83d\udccd Ejemplo: ChatGPT, Alexa, Siri.</p> </li> <li> <p>Traducci\u00f3n Autom\u00e1tica:    Traducir texto de un idioma a otro.    \ud83d\udccd Ejemplo: Google Translate.</p> </li> <li> <p>Extracci\u00f3n de Informaci\u00f3n:    Identificar entidades y relaciones en textos.    \ud83d\udccd Ejemplo: Reconocimiento de nombres, fechas, lugares.</p> </li> <li> <p>Generaci\u00f3n de Texto:    Crear texto coherente y relevante autom\u00e1ticamente.    \ud83d\udccd Ejemplo: Generaci\u00f3n de res\u00famenes, escritura asistida.</p> </li> </ol>"},{"location":"procesamiento-lenguaje-natural/#conceptos-fundamentales","title":"\u2699\ufe0f Conceptos Fundamentales","text":"<ul> <li>Tokenizaci\u00f3n: Divisi\u00f3n del texto en unidades (palabras, subpalabras, caracteres)</li> <li>Stemming y Lematizaci\u00f3n: Reducci\u00f3n de palabras a su ra\u00edz</li> <li>Stop Words: Palabras comunes que suelen filtrarse</li> <li>Bag of Words (BoW): Representaci\u00f3n de texto como frecuencia de palabras</li> <li>TF-IDF: Medida de importancia de palabras en documentos</li> <li>Word Embeddings: Representaciones vectoriales de palabras (Word2Vec, GloVe)</li> <li>Transformers: Arquitectura base de modelos modernos (BERT, GPT)</li> </ul>"},{"location":"procesamiento-lenguaje-natural/#bibliotecas-populares","title":"\ud83d\udd0d Bibliotecas Populares","text":"<ul> <li>NLTK - Natural Language Toolkit</li> <li>spaCy - Procesamiento industrial de NLP</li> <li>Hugging Face Transformers - Modelos preentrenados</li> <li>Gensim - Modelado de t\u00f3picos y word embeddings</li> </ul>"},{"location":"procesamiento-lenguaje-natural/#contenido-en-construccion","title":"\ud83d\udea7 Contenido en construcci\u00f3n","text":"<p>Esta secci\u00f3n est\u00e1 siendo desarrollada. Pr\u00f3ximamente encontrar\u00e1s:</p> <ul> <li>[ ] Preprocesamiento de texto</li> <li>[ ] Representaci\u00f3n de texto</li> <li>[ ] Modelos de clasificaci\u00f3n de texto</li> <li>[ ] Transformers y modelos de lenguaje</li> <li>[ ] Ejemplos pr\u00e1cticos con Python</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"}]}