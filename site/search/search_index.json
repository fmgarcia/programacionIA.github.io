{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd16 Programaci\u00f3n en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 Este sitio web recopila documentaci\u00f3n y recursos sobre diferentes \u00e1reas de la Inteligencia Artificial (IA) y el Machine Learning (ML).</p>"},{"location":"#contenidos","title":"\ud83d\udcda Contenidos","text":"<p>Explora las diferentes secciones disponibles:</p>"},{"location":"#aprendizaje-supervisado","title":"\ud83c\udfaf Aprendizaje Supervisado","text":"<p>Aprende sobre algoritmos que utilizan datos etiquetados para realizar predicciones. Incluye t\u00e9cnicas de clasificaci\u00f3n y regresi\u00f3n como: - Machine Learning para an\u00e1lisis de datos - \u00c1rboles de decisi\u00f3n - Naive Bayes - K-Nearest Neighbors (KNN) - Support Vector Machines (SVM) - Algoritmos de ensamblado</p>"},{"location":"#aprendizaje-no-supervisado","title":"\ud83d\udd0d Aprendizaje No Supervisado","text":"<p>Descubre t\u00e9cnicas para encontrar patrones en datos no etiquetados: - Clustering (K-Means, DBSCAN, Jer\u00e1rquico) - Reducci\u00f3n de dimensionalidad (PCA, t-SNE) - An\u00e1lisis de asociaci\u00f3n</p>"},{"location":"#procesamiento-de-lenguaje-natural","title":"\ud83d\udcac Procesamiento de Lenguaje Natural","text":"<p>Explora c\u00f3mo las m\u00e1quinas comprenden y generan lenguaje humano: - Tokenizaci\u00f3n y preprocesamiento de texto - Modelos de lenguaje - An\u00e1lisis de sentimientos - Transformers y atenci\u00f3n</p>"},{"location":"#deep-learning","title":"\ud83e\udde0 Deep Learning","text":"<p>Profundiza en redes neuronales y arquitecturas avanzadas: - Redes neuronales artificiales - Redes convolucionales (CNN) - Redes recurrentes (RNN, LSTM) - Arquitecturas modernas</p>"},{"location":"#sobre-este-sitio","title":"\ud83c\udf93 Sobre este sitio","text":"<p>Este recurso est\u00e1 dise\u00f1ado como material de apoyo para el aprendizaje de t\u00e9cnicas de Inteligencia Artificial y Machine Learning, con ejemplos pr\u00e1cticos en Python.</p> <p>\ud83d\udcc5 \u00daltima actualizaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/","title":"\ud83d\udd0d Aprendizaje No Supervisado","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Aprendizaje No Supervisado! \ud83c\udf89</p>"},{"location":"aprendizaje-no-supervisado/#que-es-el-aprendizaje-no-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje No Supervisado?","text":"<p>El aprendizaje no supervisado es un tipo de aprendizaje autom\u00e1tico en el que el modelo trabaja con datos sin etiquetas. El objetivo es descubrir patrones ocultos, estructuras o agrupaciones en los datos sin tener una respuesta correcta predefinida.</p> <p>A diferencia del aprendizaje supervisado, aqu\u00ed no hay una variable objetivo que predecir; el algoritmo debe encontrar por s\u00ed mismo las relaciones entre los datos.</p>"},{"location":"aprendizaje-no-supervisado/#tipos-de-problemas-no-supervisados","title":"\ud83e\udde0 Tipos de Problemas No Supervisados","text":"<ol> <li> <p>Clustering (Agrupamiento):    Agrupa datos similares en clusters o grupos.    \ud83d\udccd Ejemplo: Segmentaci\u00f3n de clientes seg\u00fan su comportamiento de compra.</p> </li> <li> <p>Reducci\u00f3n de Dimensionalidad:    Reduce el n\u00famero de variables manteniendo la informaci\u00f3n m\u00e1s relevante.    \ud83d\udccd Ejemplo: Visualizar datos de alta dimensi\u00f3n en 2D o 3D.</p> </li> <li> <p>Detecci\u00f3n de Anomal\u00edas:    Identifica datos que se desv\u00edan significativamente del patr\u00f3n normal.    \ud83d\udccd Ejemplo: Detectar transacciones fraudulentas.</p> </li> <li> <p>Reglas de Asociaci\u00f3n:    Encuentra relaciones entre variables en grandes conjuntos de datos.    \ud83d\udccd Ejemplo: An\u00e1lisis de cesta de compra (\u00bfqu\u00e9 productos se compran juntos?).</p> </li> </ol>"},{"location":"aprendizaje-no-supervisado/#algoritmos-comunes","title":"\ud83d\udd0d Algoritmos Comunes","text":"<ul> <li>K-Means</li> <li>DBSCAN</li> <li>Clustering Jer\u00e1rquico</li> <li>PCA (An\u00e1lisis de Componentes Principales)</li> <li>t-SNE</li> <li>Isolation Forest</li> <li>Apriori</li> </ul>"},{"location":"aprendizaje-no-supervisado/#contenido-en-construccion","title":"\ud83d\udea7 Contenido en construcci\u00f3n","text":"<p>Esta secci\u00f3n est\u00e1 siendo desarrollada. Pr\u00f3ximamente encontrar\u00e1s:</p> <ul> <li>[ ] Algoritmos de Clustering</li> <li>[ ] Reducci\u00f3n de dimensionalidad</li> <li>[ ] Detecci\u00f3n de anomal\u00edas</li> <li>[ ] Ejemplos pr\u00e1cticos con Python</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/","title":"Aprendizaje Supervisado en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 En este documento encontrar\u00e1s una introducci\u00f3n a los conceptos b\u00e1sicos del aprendizaje supervisado, una de las ramas fundamentales de la Inteligencia Artificial (IA) y del Machine Learning (ML).</p>"},{"location":"aprendizaje-supervisado/#que-es-el-aprendizaje-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje Supervisado?","text":"<p>El aprendizaje supervisado es un tipo de aprendizaje autom\u00e1tico en el que un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada ejemplo del conjunto de entrenamiento incluye tanto la entrada (X) como la salida deseada (Y).</p> <p>El objetivo del modelo es aprender la relaci\u00f3n entre las entradas y las salidas para poder predecir correctamente la salida de nuevos datos nunca vistos.</p>"},{"location":"aprendizaje-supervisado/#tipos-de-problemas-supervisados","title":"\ud83e\udde0 Tipos de Problemas Supervisados","text":"<ol> <li> <p>Clasificaci\u00f3n:    El modelo aprende a asignar una etiqueta o categor\u00eda a cada ejemplo.    \ud83d\udccd Ejemplo: Clasificar correos como \"spam\" o \"no spam\".</p> </li> <li> <p>Regresi\u00f3n:    El modelo aprende a predecir un valor num\u00e9rico continuo.    \ud83d\udccd Ejemplo: Predecir el precio de una vivienda seg\u00fan sus caracter\u00edsticas.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/#flujo-de-trabajo-del-aprendizaje-supervisado","title":"\u2699\ufe0f Flujo de Trabajo del Aprendizaje Supervisado","text":"<ol> <li>Recolecci\u00f3n de datos: Se obtiene un conjunto de datos con ejemplos representativos del problema.</li> <li>Preprocesamiento: Limpieza, normalizaci\u00f3n y divisi\u00f3n del conjunto de datos (entrenamiento y prueba).</li> <li>Selecci\u00f3n del modelo: Elegir el algoritmo m\u00e1s adecuado (por ejemplo, SVM, \u00c1rboles de decisi\u00f3n, Redes neuronales, etc.).</li> <li>Entrenamiento: El modelo aprende a partir de los datos etiquetados.</li> <li>Evaluaci\u00f3n: Se mide el rendimiento utilizando m\u00e9tricas como precisi\u00f3n, recall o error cuadr\u00e1tico medio.</li> <li>Predicci\u00f3n: Se aplican los conocimientos adquiridos a nuevos datos.</li> </ol>"},{"location":"aprendizaje-supervisado/#ejemplos-de-algoritmos-comunes","title":"\ud83d\udd0d Ejemplos de Algoritmos Comunes","text":"<ul> <li>Regresi\u00f3n lineal</li> <li>K-Nearest Neighbors (KNN)</li> <li>\u00c1rboles de decisi\u00f3n</li> <li>Random Forest</li> <li>M\u00e1quinas de Vectores de Soporte (SVM)</li> <li>Redes neuronales artificiales</li> </ul>"},{"location":"aprendizaje-supervisado/#evaluacion-del-modelo","title":"\ud83d\udcca Evaluaci\u00f3n del Modelo","text":"<p>Para medir la calidad del modelo se utilizan m\u00e9tricas que dependen del tipo de problema:</p> Tipo de problema M\u00e9tricas comunes Clasificaci\u00f3n Exactitud, Precisi\u00f3n, Recall, F1-score Regresi\u00f3n Error absoluto medio (MAE), Error cuadr\u00e1tico medio (MSE), R\u00b2"},{"location":"aprendizaje-supervisado/#consejos-practicos","title":"\ud83d\udca1 Consejos Pr\u00e1cticos","text":"<ul> <li>Siempre divide tus datos en entrenamiento y prueba (por ejemplo, 80% / 20%).</li> <li>Evita el sobreajuste (overfitting): si el modelo aprende demasiado bien los datos de entrenamiento, fallar\u00e1 en los nuevos.</li> <li>Usa validaci\u00f3n cruzada para estimar el rendimiento real del modelo.</li> </ul>"},{"location":"aprendizaje-supervisado/#conclusion","title":"\ud83e\udde9 Conclusi\u00f3n","text":"<p>El aprendizaje supervisado es la base de muchas aplicaciones modernas de IA, desde sistemas de recomendaci\u00f3n hasta diagn\u00f3stico m\u00e9dico. Dominar sus fundamentos te permitir\u00e1 avanzar hacia t\u00e9cnicas m\u00e1s complejas y poderosas.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 26/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/","title":"\ud83e\udd16 Unidad 1. Machine Learning Basado en el An\u00e1lisis de Datos","text":"<p>Esta unidad introduce los conceptos fundamentales del Machine Learning (ML), su flujo de trabajo, las herramientas clave de la biblioteca <code>scikit-learn</code> del lenguaje Python, y las metodolog\u00edas esenciales para la preparaci\u00f3n, divisi\u00f3n y preprocesamiento de datos.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#11-que-es-el-machine-learning","title":"1.1. \u00bfQu\u00e9 es el Machine Learning?","text":"<p>El Machine Learning (ML) se define como un campo de estudio que utiliza modelos estad\u00edsticos para aprender de los datos. Un aspecto clave es que modelos relativamente simples pueden realizar predicciones complejas.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#definiciones-clave","title":"Definiciones Clave","text":"<ul> <li>Definici\u00f3n temprana (Samuel, 1959): \"Programar computadoras para que aprendan de la experiencia deber\u00eda eliminar la necesidad de gran parte de este esfuerzo de programaci\u00f3n detallado\".</li> <li>Definici\u00f3n moderna (Mitchell, 1997): \"Se dice que un programa de computadora aprende de la experiencia E con respecto a alguna clase de tareas T y una medida de rendimiento P, si su rendimiento en las tareas T, medido por P, mejora con la experiencia E\".</li> <li>Definici\u00f3n matem\u00e1tica (Ej. Regresi\u00f3n Lineal): Un modelo matem\u00e1tico que intenta encontrar la relaci\u00f3n \u00f3ptima entre variables. Por ejemplo, predecir ventas (Target, \\(y\\)) bas\u00e1ndose en gastos de publicidad (Feature, \\(x\\)). El modelo \\(y = wx + b\\) aprende los par\u00e1metros \\(w\\) (peso) y \\(b\\) iterando desde valores arbitrarios (\\(f_1\\)) hasta un valor \u00f3ptimo (\\(f_3\\)) que minimiza el error.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#ml-y-otros-campos","title":"ML y Otros Campos","text":"<p>El Machine Learning est\u00e1 profundamente interconectado con otros campos: * Es un subcampo de la Inteligencia Artificial. * Deep Learning es un subcampo del Machine Learning. * Se solapa significativamente con Estad\u00edstica, Miner\u00eda de Datos y Reconocimiento de Patrones.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#tipos-de-machine-learning","title":"Tipos de Machine Learning","text":"<p>Seg\u00fan el m\u00e9todo de supervisi\u00f3n, el ML se divide en: 1.  Supervisado: Se proporciona un patr\u00f3n objetivo (datos etiquetados). El cap\u00edtulo se centra en este tipo, que incluye algoritmos como Regresi\u00f3n Lineal, Regresi\u00f3n Log\u00edstica, \u00c1rboles de Decisi\u00f3n, KNN, SVM y Redes Neuronales. 2.  No Supervisado: El patr\u00f3n objetivo debe ser descubierto (datos no etiquetados). Incluye Clustering, PCA y An\u00e1lisis de Asociaci\u00f3n. 3.  Refuerzo: Se aprende mediante la optimizaci\u00f3n de pol\u00edticas (recompensas y castigos).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#flujo-de-trabajo-del-machine-learning","title":"Flujo de Trabajo del Machine Learning","text":"<p>El proceso general para construir un modelo de ML es: 1.  Definici\u00f3n del Problema: Comprender el objetivo de negocio. 2.  Preparaci\u00f3n de Datos: Recolecci\u00f3n de datos brutos (Raw Data) y preprocesamiento. 3.  Machine Learning (Modelado): Se divide la data en conjuntos de Train (Entrenamiento), Validate (Validaci\u00f3n) y Test (Prueba). 4.  Entrenamiento y Evaluaci\u00f3n: Esta fase incluye Ingenier\u00eda de caracter\u00edsticas (Feature engineering), Modelado y optimizaci\u00f3n (entrenar el modelo con los datos), y Evaluaci\u00f3n de rendimiento (Performance metrics). 5.  Aplicaci\u00f3n: Aplicar el modelo en la vida real.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#parametros-vs-hiperparametros","title":"Par\u00e1metros vs. Hiperpar\u00e1metros","text":"<ul> <li>Par\u00e1metros: Se aprenden desde los datos durante el entrenamiento. Contienen el patr\u00f3n de los datos (ej. \\(w\\) y \\(b\\) en regresi\u00f3n lineal, pesos de una red neuronal).</li> <li>Hiperpar\u00e1metros: Se configuran manualmente por el practicante antes del entrenamiento. Se \"afinan\" (tunan) para optimizar el rendimiento (ej. el valor \\(k\\) en KNN, la tasa de aprendizaje, la profundidad m\u00e1xima de un \u00e1rbol).</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#12-biblioteca-python-scikit-learn","title":"1.2. Biblioteca Python scikit-learn","text":"<p><code>scikit-learn</code> es la biblioteca de ML m\u00e1s representativa de Python.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Proporciona una interfaz de biblioteca integrada y unificada.</li> <li>Incluye una amplia variedad de algoritmos de ML, funciones de preprocesamiento y selecci\u00f3n de modelos.</li> <li>Es simple, eficiente y est\u00e1 construida sobre NumPy, SciPy y matplotlib.</li> <li>Es de c\u00f3digo abierto y puede usarse comercialmente.</li> <li>No soporta GPU.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#mecanismo-de-scikit-learn","title":"Mecanismo de <code>scikit-learn</code>","text":"<p>El flujo de trabajo de la API de <code>scikit-learn</code> es intuitivo y sigue tres pasos: 1.  Instance: Crear una instancia del objeto del modelo (Estimator). 2.  Fit: Entrenar el modelo con los datos. 3.  Predict / transform: Usar el modelo entrenado para hacer predicciones o transformar datos.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#estimator-classifier-y-regressor","title":"Estimator, Classifier y Regressor","text":"<ul> <li><code>Estimator</code>: El objeto base. Aprende de los datos usando el m\u00e9todo <code>.fit()</code> y puede hacer predicciones usando <code>.predict()</code>.</li> <li><code>Classifier</code>: Un estimador para tareas de clasificaci\u00f3n (ej. <code>DecisionTreeClassifier</code>, <code>KNeighborsClassifier</code>).</li> <li><code>Regressor</code>: Un estimador para tareas de regresi\u00f3n (predicci\u00f3n num\u00e9rica) (ej. <code>LinearRegression</code>, <code>KNeighborsRegressor</code>).</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#sintaxis-basica-de-scikit-learn","title":"Sintaxis B\u00e1sica de <code>scikit-learn</code>","text":"<ul> <li>Importar un estimador: <code>from sklearn.linear_model import LinearRegression</code></li> <li>Importar un preprocesador: <code>from sklearn.preprocessing import StandardScaler</code></li> <li>Importar divisi\u00f3n de datos: <code>from sklearn.model_selection import train_test_split</code></li> <li> <p>Importar m\u00e9tricas: <code>from sklearn import metrics</code></p> </li> <li> <p>Instanciar (con hiperpar\u00e1metros): <code>myModel = KNeighborsClassifier(n_neighbors=10)</code></p> </li> <li>Dividir los datos (Hold-out): <code>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)</code></li> <li>Entrenar el modelo (Supervisado): <code>myModel.fit(X_train, Y_train)</code></li> <li>Hacer predicciones: <code>Y_pred = myModel.predict(X_test)</code></li> <li>Evaluar el rendimiento: <code>metrics.accuracy_score(Y_test, Y_pred)</code></li> <li>Afinar hiperpar\u00e1metros (con Cross-Validation): <code>myGridCV = GridSearchCV(estimator, parameter_grid, cv=5)</code></li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#ejemplo-practico-estandarizacion","title":"Ejemplo Pr\u00e1ctico: Estandarizaci\u00f3n","text":"<p>El preprocesamiento, como la estandarizaci\u00f3n, es crucial para mejorar el rendimiento. La estandarizaci\u00f3n (o z-transformation) convierte los datos para que sigan una distribuci\u00f3n normal est\u00e1ndar, usando la f\u00f3rmula \\(z = \\frac{x - m}{\\sigma}\\) (donde \\(m\\) es la media y \\(\\sigma\\) la desviaci\u00f3n est\u00e1ndar).</p> <p>En <code>scikit-learn</code>, se usa <code>StandardScaler</code>: 1.  Importar: <code>from sklearn.preprocessing import StandardScaler</code> 2.  Instanciar: <code>scaler = StandardScaler()</code> 3.  Ajustar (Fit): Se aprende la media (\\(m\\)) y la desviaci\u00f3n (\\(\\sigma\\)) solo de los datos de entrenamiento:     <code>scaler.fit(X_train)</code> 4.  Transformar: Se aplica la transformaci\u00f3n a los datos de entrenamiento y prueba:     <code>X_train = scaler.transform(X_train)</code> <code>X_test = scaler.transform(X_test)</code> 5.  <code>fit_transform</code>: Se pueden combinar los pasos 3 y 4 (solo para <code>X_train</code>):     <code>X_train = scaler.fit_transform(X_train)</code></p> <p>Antes de la estandarizaci\u00f3n, las columnas pueden tener rangos de valores muy diferentes. Despu\u00e9s, todos los valores est\u00e1n centrados alrededor de 0, lo que ayuda a muchos algoritmos a converger mejor.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#modulos-principales-de-scikit-learn","title":"M\u00f3dulos Principales de <code>scikit-learn</code>","text":"M\u00f3dulo Funci\u00f3n Principal Ejemplos <code>sklearn.datasets</code> Cargar datasets de ejemplo. <code>load_iris()</code>, <code>load_breast_cancer()</code> <code>sklearn.preprocessing</code> Preprocesamiento de datos (escalado, codificaci\u00f3n). <code>StandardScaler</code>, <code>LabelEncoder</code>, <code>OneHotEncoder</code> <code>sklearn.model_selection</code> Divisi\u00f3n de datos, validaci\u00f3n y afinado de hiperpar\u00e1metros. <code>train_test_split</code>, <code>GridSearchCV</code>, <code>KFold</code> <code>sklearn.metrics</code> Evaluaci\u00f3n de rendimiento del modelo. <code>accuracy_score</code>, <code>precision_score</code>, <code>recall_score</code>, <code>roc_auc_score</code> <code>sklearn.linear_model</code> Algoritmos lineales. <code>LinearRegression</code>, <code>LogisticRegression</code> <code>sklearn.tree</code> Algoritmos de \u00c1rboles de Decisi\u00f3n. <code>DecisionTreeClassifier</code> <code>sklearn.neighbors</code> Algoritmos de vecinos cercanos. <code>KNeighborsClassifier</code> (K-NN) <code>sklearn.svm</code> Support Vector Machine (M\u00e1quinas de Vectores de Soporte). <code>SVC</code> <code>sklearn.ensemble</code> Algoritmos de Ensamblado (Ensemble). <code>RandomForestClassifier</code>, <code>AdaBoostClassifier</code> <code>sklearn.cluster</code> Algoritmos de clustering (No supervisado). <code>KMeans</code>, <code>DBSCAN</code> <code>sklearn.pipeline</code> Herramienta para encadenar pasos de preprocesamiento y modelado. <code>Pipeline</code>"},{"location":"aprendizaje-supervisado/01-machine-learning/#13-preparacion-y-division-del-dataset","title":"1.3. Preparaci\u00f3n y Divisi\u00f3n del Dataset","text":"<p>La divisi\u00f3n de datos es fundamental para evaluar un modelo de ML. El conjunto de datos general se divide en un conjunto de entrenamiento y uno de evaluaci\u00f3n (prueba).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#overfitting-sobreajuste-y-generalizacion","title":"Overfitting (Sobreajuste) y Generalizaci\u00f3n","text":"<ul> <li>Generalizaci\u00f3n: Es la capacidad del modelo para predecir con precisi\u00f3n datos nuevos que no ha visto antes.</li> <li>Overfitting: Ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento, aprendiendo incluso el ruido.</li> <li>Underfitting (Subajuste): Ocurre cuando un modelo es demasiado simple (baja capacidad) y no puede capturar el patr\u00f3n subyacente de los datos.</li> </ul> <p>El Dilema: A medida que aumenta la complejidad (flexibilidad) del modelo: * El error en el conjunto de entrenamiento (Training set) siempre disminuye. * El error en el conjunto de prueba (Test set) disminuye al principio, pero luego comienza a aumentar. El punto donde el error de prueba empieza a subir es donde comienza el overfitting.</p> <p>El conjunto de prueba es esencial para detectar el overfitting y seleccionar un modelo que generalice bien.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#cross-validation-validacion-cruzada","title":"Cross-Validation (Validaci\u00f3n Cruzada)","text":"<p>El conjunto de prueba (Test set) debe usarse \u00a1solo una vez! al final, para la evaluaci\u00f3n final.</p> <p>Para evaluar el modelo durante el entrenamiento (por ejemplo, para afinar hiperpar\u00e1metros), necesitamos una forma de simular un \"conjunto de prueba\" sin tocar el real. Para esto, dividimos el conjunto de entrenamiento (Training Data) en dos partes m\u00e1s peque\u00f1as: un nuevo conjunto de <code>Train</code> y un conjunto de <code>Cross Validate</code> (Validaci\u00f3n).</p> <p>M\u00e9todo: k-Fold Cross-Validation Es el m\u00e9todo m\u00e1s com\u00fan: 1.  Se subdivide el conjunto de entrenamiento (original) en k partes iguales (folds). (Usualmente k=10). 2.  Se itera k veces (rondas). 3.  En cada ronda, se usa 1 fold como conjunto de validaci\u00f3n y los k-1 folds restantes como conjunto de entrenamiento. 4.  Se calcula la m\u00e9trica de rendimiento (ej. accuracy) en cada ronda. 5.  El rendimiento final del modelo es el promedio de las m\u00e9tricas de las k rondas.</p> <p>M\u00e9todo: Leave One Out (LOO) Es un caso extremo de k-Fold donde \\(k\\) es igual al n\u00famero total de muestras. Se entrena con todos los datos menos uno, y se valida con ese \u00fanico dato. Es computacionalmente muy costoso.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#14-preprocesamiento-de-datos","title":"1.4. Preprocesamiento de Datos","text":"<p>Preparar los datos es vital para un buen modelo. Esto incluye la limpieza (manejo de valores at\u00edpicos y faltantes) y la transformaci\u00f3n (escalado y codificaci\u00f3n).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#manejo-de-valores-faltantes-missing-values","title":"Manejo de Valores Faltantes (Missing Values)","text":"<p>Los valores faltantes (identificados en Python como <code>np.nan</code> o <code>NaN</code>) deben ser tratados.</p> <p>1. Identificaci\u00f3n: Se pueden contar usando <code>df.isnull().sum()</code>.</p> <p>2. Eliminaci\u00f3n (con Pandas <code>dropna()</code>): * <code>df.dropna()</code>: Elimina cualquier fila que contenga al menos un <code>NaN</code> (eje por defecto 0). * <code>df.dropna(axis=1)</code>: Elimina cualquier columna que contenga un <code>NaN</code>. * <code>df.dropna(how='all')</code>: Elimina filas/columnas donde todos los valores son <code>NaN</code>. * <code>df.dropna(thresh=N)</code>: Mantiene las filas que tienen al menos <code>N</code> valores no-<code>NaN</code>. * <code>df.dropna(subset=['col_name'])</code>: Elimina filas que tienen <code>NaN</code> espec\u00edficamente en la columna 'col_name'.</p> <p>3. Imputaci\u00f3n (Relleno): Se usa cuando eliminar datos resultar\u00eda en una p\u00e9rdida significativa de informaci\u00f3n. * M\u00e9todos simples: Rellenar con un valor (ej. 'unknown'), la media, la mediana o el valor m\u00e1s frecuente (moda) de la columna. * Con <code>scikit-learn</code> (<code>SimpleImputer</code>): Es el m\u00e9todo preferido.     * <code>from sklearn.impute import SimpleImputer</code>     * <code>impt = SimpleImputer(strategy='mean')</code> (Estrategias: 'mean', 'median', 'most_frequent').     * <code>impt.fit(X_train)</code>: Aprende la media (o mediana/moda) del set de entrenamiento.     * <code>X_train_imputed = impt.transform(X_train)</code>: Aplica la imputaci\u00f3n.     * <code>X_test_imputed = impt.transform(X_test)</code>: Aplica la misma imputaci\u00f3n (con la media de train) al set de prueba.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#manejo-de-datos-categoricos","title":"Manejo de Datos Categ\u00f3ricos","text":"<p>Los algoritmos de ML requieren entradas num\u00e9ricas. Los datos categ\u00f3ricos deben ser convertidos.</p> <p>1. Datos Ordinales (con orden): Ej. Tallas: 'M' &lt; 'L' &lt; 'XL'. Se deben mapear a enteros que respeten ese orden. * <code>size_mapping = {'M': 1, 'L': 2, 'XL': 3}</code> * <code>df['size'] = df['size'].map(size_mapping)</code></p> <p>2. Datos Nominales (sin orden) y Etiquetas de Clase: Ej. Colores: 'red', 'green', 'blue' o Etiquetas: 'setosa', 'versicolor'.</p> <ul> <li> <p>Codificaci\u00f3n de Etiquetas (Label Encoding):     Convierte cada etiqueta \u00fanica en un entero (ej. 'class1': 0, 'class2': 1). Se usa <code>LabelEncoder</code> de <code>scikit-learn</code>.</p> <ul> <li><code>from sklearn.preprocessing import LabelEncoder</code></li> <li><code>enc = LabelEncoder()</code></li> <li><code>y_encoded = enc.fit_transform(df['classlabel'])</code></li> </ul> </li> <li> <p>One-Hot Encoding (para caracter\u00edsticas nominales):     Usar <code>LabelEncoder</code> para caracter\u00edsticas (X) es incorrecto, ya que crea un orden artificial. Se debe usar One-Hot Encoding.     Crea nuevas columnas \"dummy\" (0 o 1) para cada categor\u00eda, indicando presencia (1) o ausencia (0).</p> <ul> <li>M\u00e9todo Pandas: <code>pd.get_dummies(df['species'])</code>.</li> <li>M\u00e9todo <code>scikit-learn</code>: <code>OneHotEncoder</code>. Este m\u00e9todo es preferido en pipelines y a menudo devuelve una matriz dispersa (sparse matrix) para ahorrar memoria, ya que la mayor\u00eda de los valores ser\u00e1n 0.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#division-de-datos-estratificada-stratify","title":"Divisi\u00f3n de Datos Estratificada (Stratify)","text":"<p>Al usar <code>train_test_split</code>, si el dataset est\u00e1 desbalanceado (ej. 90% clase A, 10% clase B), una divisi\u00f3n aleatoria simple podr\u00eda resultar en un set de prueba sin muestras de la clase B. * Soluci\u00f3n: Usar el par\u00e1metro <code>stratify=y</code>. * Esto asegura que la proporci\u00f3n de las clases (ej. 90/10) se mantenga id\u00e9ntica tanto en el conjunto de entrenamiento como en el de prueba, reflejando el dataset original.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#topicos-avanzados-tradeoff-de-sesgo-varianza-y-regularizacion","title":"T\u00f3picos Avanzados: Tradeoff de Sesgo-Varianza y Regularizaci\u00f3n","text":"<ul> <li>Tradeoff de Sesgo-Varianza:<ul> <li>Sesgo (Bias): Error por suposiciones incorrectas (Underfitting).</li> <li>Varianza (Variance): Error por sensibilidad excesiva a los datos de entrenamiento (Overfitting).</li> <li>Error Total \\(\\approx\\) Sesgo\u00b2 + Varianza. El objetivo es encontrar la complejidad \u00f3ptima que minimice este error total.</li> </ul> </li> <li>Regularizaci\u00f3n: T\u00e9cnica para prevenir el overfitting en modelos lineales penalizando coeficientes (pesos) grandes.<ul> <li>Ridge (L2): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{w_j^2}\\). Encoge los pesos, pero no los hace cero.</li> <li>Lasso (L1): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{|w_j|}\\). Puede forzar que algunos pesos sean exactamente cero, realizando una selecci\u00f3n de caracter\u00edsticas autom\u00e1tica.</li> <li>ElasticNet: Combina penalizaciones L1 y L2.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#15-practica-solucion-de-problemas-con-scikit-learn-ej-iris","title":"1.5. Pr\u00e1ctica: Soluci\u00f3n de Problemas con scikit-learn (Ej. Iris)","text":"<p>Esta secci\u00f3n aplica todos los conceptos anteriores en un caso pr\u00e1ctico completo usando el dataset \"Iris\".</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#1-entendimiento-del-problema-y-datos-eda","title":"1. Entendimiento del Problema y Datos (EDA)","text":"<ul> <li>Objetivo: Clasificar la especie de una flor Iris (Target).</li> <li>Clases (Target): 3 especies (Setosa, Versicolor, Virginica).</li> <li>Caracter\u00edsticas (Features): <code>sepal_length</code>, <code>sepal_width</code>, <code>petal_length</code>, <code>petal_width</code>.</li> <li>An\u00e1lisis de Datos:<ul> <li>Se cargan los datos y se convierten a un DataFrame de Pandas.</li> <li>Valores Faltantes: Se comprueba con <code>iris.isnull().sum()</code>. No se encontraron.</li> <li>Distribuci\u00f3n de Clases: Se comprueba con <code>iris.groupby('target').size()</code>. Hay 50 muestras de cada clase (33.3% cada una). Es un dataset balanceado.</li> <li>Estad\u00edsticas y Correlaci\u00f3n: <code>iris.describe()</code> y <code>iris.corr()</code>. Se observa que <code>petal_length</code> y <code>petal_width</code> est\u00e1n altamente correlacionados (0.96), sugiriendo un problema de multicolinealidad.</li> <li>Visualizaci\u00f3n: Se usan <code>pairplot</code> y <code>heatmap</code> para confirmar visualmente las relaciones y la alta correlaci\u00f3n.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#2-division-y-preparacion-de-datos","title":"2. Divisi\u00f3n y Preparaci\u00f3n de Datos","text":"<ul> <li>Separaci\u00f3n X/y: Se separan las caracter\u00edsticas (X) del objetivo (y).</li> <li>Divisi\u00f3n Train/Test: Se usa <code>train_test_split</code> (ej. 80% train, 20% test).</li> <li>Validaci\u00f3n Cruzada:<ul> <li>Se muestra c\u00f3mo usar <code>KFold</code> (CV est\u00e1ndar) y <code>StratifiedKFold</code> (CV estratificada).</li> <li><code>StratifiedKFold</code> es preferible porque mantiene la distribuci\u00f3n 33/33/33 de las clases en cada fold, asegurando que la validaci\u00f3n sea representativa.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#3-seleccion-y-evaluacion-del-modelo","title":"3. Selecci\u00f3n y Evaluaci\u00f3n del Modelo","text":"<ul> <li>Curva de Aprendizaje (<code>Learning Curve</code>):     Se usa para diagnosticar bias vs. variance. Muestra el rendimiento del modelo a medida que ve m\u00e1s datos de entrenamiento.</li> <li>Afinado de Hiperpar\u00e1metros (<code>GridSearchCV</code>):     Se utiliza para encontrar la mejor combinaci\u00f3n de hiperpar\u00e1metros (ej. <code>criterion</code>, <code>max_depth</code> para un <code>DecisionTreeClassifier</code>) probando todas las combinaciones posibles mediante validaci\u00f3n cruzada.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#4-metricas-de-evaluacion-clasificacion","title":"4. M\u00e9tricas de Evaluaci\u00f3n (Clasificaci\u00f3n)","text":"<p>Una vez que el modelo (<code>GridSearchCV</code>) est\u00e1 entrenado y se hacen predicciones sobre el <code>X_test</code>, se eval\u00faa el rendimiento.</p> <ul> <li> <p>Matriz de Confusi\u00f3n (<code>Confusion Matrix</code>):     Es la base para todas las m\u00e9tricas. Compara los valores reales (True label) con los predichos (Predicted label).</p> <ul> <li>TP (True Positive): Real = 1, Predicho = 1.</li> <li>FN (False Negative): Real = 1, Predicho = 0.</li> <li>FP (False Positive): Real = 0, Predicho = 1.</li> <li>TN (True Negative): Real = 0, Predicho = 0.</li> </ul> </li> <li> <p>M\u00e9tricas Clave:</p> <ul> <li>Accuracy (Exactitud): \\(\\frac{TP + TN}{Total}\\). Proporci\u00f3n de predicciones correctas. (Usar con cuidado en datasets desbalanceados).</li> <li>Precision (Precisi\u00f3n): \\(\\frac{TP}{TP + FP}\\). De los que dijimos que eran positivos, \u00bfcu\u00e1ntos acertamos?.</li> <li>Recall (Sensibilidad o TPR): \\(\\frac{TP}{TP + FN}\\). De todos los positivos reales, \u00bfcu\u00e1ntos encontramos?.</li> <li>F1-Score: La media arm\u00f3nica de Precision y Recall. Es una m\u00e9trica excelente para datasets desbalanceados. \\(F_1 = 2 \\frac{Precision \\times Recall}{Precision + Recall}\\).</li> <li>FPR (Tasa de Falsos Positivos): \\(\\frac{FP}{FP + TN}\\). Proporci\u00f3n de negativos reales que clasificamos incorrectamente como positivos.</li> </ul> </li> <li> <p>Curva ROC y AUC:</p> <ul> <li>Curva ROC: Gr\u00e1fica que muestra el rendimiento de un clasificador en todos los umbrales de clasificaci\u00f3n. Muestra TPR (Eje Y) vs. FPR (Eje X).</li> <li>AUC (Area Under the Curve): El \u00e1rea bajo la curva ROC. Es una m\u00e9trica \u00fanica que resume el rendimiento del modelo.<ul> <li>AUC = 1.0: Clasificador perfecto.</li> <li>AUC = 0.5: Clasificador in\u00fatil (aleatorio).</li> <li>Un AUC de 0.85 o m\u00e1s se considera bueno.</li> </ul> </li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#5-prediccion-final","title":"5. Predicci\u00f3n Final","text":"<ul> <li>Se carga el modelo final (el mejor <code>estimator_</code> encontrado por <code>GridSearchCV</code>).</li> <li>Se realizan las predicciones finales sobre el conjunto de prueba (<code>X_test</code>).</li> <li>Los resultados se guardan, por ejemplo, en un archivo CSV.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 27/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/","title":"\ud83e\udd16 Unidad 2. Regresi\u00f3n Lineal para la Inteligencia Artificial","text":"<p>La regresi\u00f3n lineal es uno de los modelos m\u00e1s simples y fundamentales en el campo de la inteligencia artificial y el aprendizaje autom\u00e1tico. A pesar de su simplicidad, proporciona una base s\u00f3lida para entender c\u00f3mo los algoritmos de regresi\u00f3n pueden ser usados para hacer predicciones. En este art\u00edculo exploraremos c\u00f3mo funciona la regresi\u00f3n lineal, sus aplicaciones, y la compararemos con otros modelos de regresi\u00f3n como Ridge y Lasso.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#que-es-la-regresion-lineal","title":"\u00bfQu\u00e9 es la Regresi\u00f3n Lineal?","text":"<p>La regresi\u00f3n lineal es un m\u00e9todo estad\u00edstico que intenta modelar la relaci\u00f3n entre una variable dependiente y una o m\u00e1s variables independientes mediante una l\u00ednea recta. La ecuaci\u00f3n que representa una regresi\u00f3n lineal simple tiene la siguiente forma:</p> \\[ y = b_0 + b_1X + \\epsilon \\] <ul> <li>y: Variable dependiente (la que se intenta predecir).</li> <li>X: Variable independiente (el predictor).</li> <li>b_0: Intercepto, valor de y cuando X es cero.</li> <li>b_1: Coeficiente que representa la pendiente de la l\u00ednea.</li> <li>\\(\\epsilon\\): Error o ruido, la diferencia entre la predicci\u00f3n y el valor real.</li> </ul> <p>La regresi\u00f3n lineal se utiliza principalmente para problemas de predicci\u00f3n num\u00e9rica, como el precio de una vivienda, el rendimiento de una acci\u00f3n o cualquier otra situaci\u00f3n en la que exista una relaci\u00f3n lineal entre las variables.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#aplicaciones-de-la-regresion-lineal","title":"Aplicaciones de la Regresi\u00f3n Lineal","text":"<p>La regresi\u00f3n lineal es ampliamente utilizada en una variedad de aplicaciones, como:</p> <ul> <li> <p>Econom\u00eda: Predicci\u00f3n de precios de bienes y servicios. Por ejemplo, podemos usar la regresi\u00f3n lineal para modelar la relaci\u00f3n entre la inflaci\u00f3n y el precio de los alimentos. En este caso, la regresi\u00f3n lineal simple puede ser suficiente si se trata de una relaci\u00f3n clara y directa. Ejemplo en Python</p> </li> <li> <p>Finanzas: Estimaci\u00f3n del rendimiento de acciones o bonos. La regresi\u00f3n lineal puede ayudar a estimar c\u00f3mo factores como las tasas de inter\u00e9s y el crecimiento econ\u00f3mico afectan los precios de las acciones. Si existen muchas variables correlacionadas, Ridge Regression ser\u00eda una mejor opci\u00f3n para estabilizar el modelo y evitar el sobreajuste. Ejemplo en Python</p> </li> <li> <p>Salud: Modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente. En este \u00e1mbito, se podr\u00eda usar la regresi\u00f3n lineal para entender c\u00f3mo var\u00eda la presi\u00f3n sangu\u00ednea en respuesta a diferentes dosis de un medicamento. Si existen m\u00faltiples factores (como edad, peso, y otras condiciones de salud), Lasso Regression podr\u00eda ayudar a identificar cu\u00e1les son las caracter\u00edsticas m\u00e1s relevantes. Ejemplo en Python</p> </li> <li> <p>Marketing: Determinaci\u00f3n de la relaci\u00f3n entre el gasto publicitario y las ventas. La regresi\u00f3n lineal se utiliza para estimar el impacto de diferentes estrategias publicitarias en las ventas. Si existen muchas campa\u00f1as publicitarias y se necesita identificar cu\u00e1les son las m\u00e1s efectivas, Lasso Regression podr\u00eda ayudar a eliminar las menos significativas y reducir la complejidad del modelo. </p> </li> <li> <p>Educaci\u00f3n: Predicci\u00f3n de calificaciones de estudiantes en funci\u00f3n de variables como el tiempo de estudio y la asistencia. Si el objetivo es identificar los factores que tienen mayor influencia en el rendimiento acad\u00e9mico, Lasso Regression ser\u00eda \u00fatil para seleccionar solo las caracter\u00edsticas m\u00e1s relevantes, como participaci\u00f3n en clase, tiempo de estudio, o participaci\u00f3n en actividades extracurriculares. </p> </li> <li> <p>Inmobiliaria: Predicci\u00f3n del valor de una propiedad con base en caracter\u00edsticas como la ubicaci\u00f3n, el tama\u00f1o y el n\u00famero de habitaciones. En este contexto, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que caracter\u00edsticas como la ubicaci\u00f3n y el tama\u00f1o de una propiedad suelen estar correlacionadas. Ridge ayuda a estabilizar los coeficientes y mejorar la capacidad predictiva del modelo. </p> </li> <li> <p>Agricultura: Estimaci\u00f3n del rendimiento de cultivos en funci\u00f3n de factores como el clima, la cantidad de fertilizante y el tipo de suelo. Ridge Regression es adecuada cuando hay m\u00faltiples factores que pueden estar correlacionados, como la temperatura y la precipitaci\u00f3n. Esto ayuda a manejar mejor la multicolinealidad y a mejorar la generalizaci\u00f3n del modelo. Ejemplo en Python</p> </li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#que-tecnica-es-mas-apropiada","title":"\u00bfQu\u00e9 t\u00e9cnica es m\u00e1s apropiada?","text":"<ul> <li>Econom\u00eda y Finanzas: En estos campos, la regresi\u00f3n lineal puede ser \u00fatil cuando se trata de problemas simples, como la predicci\u00f3n de precios basada en una o dos caracter\u00edsticas. Sin embargo, si hay muchas variables que est\u00e1n altamente correlacionadas, Ridge Regression ser\u00eda m\u00e1s apropiada para evitar el sobreajuste. Ejemplo en Python</li> <li>Salud: Para el modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente, Lasso Regression ser\u00eda adecuada si hay muchas caracter\u00edsticas potenciales, ya que podr\u00eda simplificar el modelo eliminando caracter\u00edsticas irrelevantes. Ejemplo en Python</li> <li>Marketing: Si hay muchas variables de marketing, como diferentes tipos de publicidad, Lasso Regression puede ayudar a identificar cu\u00e1les de ellas son las m\u00e1s importantes, eliminando las menos significativas. </li> <li>Inmobiliaria: En el caso de la predicci\u00f3n de precios de propiedades, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que a menudo las caracter\u00edsticas como ubicaci\u00f3n, tama\u00f1o y tipo de propiedad est\u00e1n correlacionadas. </li> <li>Educaci\u00f3n: Si queremos predecir las calificaciones de los estudiantes y hay muchas caracter\u00edsticas (como el historial acad\u00e9mico, asistencia, participaci\u00f3n en clase, etc.), Lasso ser\u00eda \u00fatil para identificar las variables m\u00e1s relevantes y eliminar las menos importantes. </li> <li>Agricultura: Para la estimaci\u00f3n del rendimiento de cultivos, Ridge Regression ser\u00eda adecuada si existen m\u00faltiples factores correlacionados, ya que permite manejar mejor la multicolinealidad. Ejemplo en Python</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#limitaciones-de-la-regresion-lineal","title":"Limitaciones de la Regresi\u00f3n Lineal","text":"<p>Aunque la regresi\u00f3n lineal es f\u00e1cil de entender y usar, presenta algunas limitaciones importantes que deben tenerse en cuenta al aplicar este modelo:</p> <ul> <li> <p>Supone una relaci\u00f3n lineal: La regresi\u00f3n lineal solo puede modelar relaciones lineales entre las variables. Si la relaci\u00f3n es no lineal, el modelo tendr\u00e1 un rendimiento pobre. Esto implica que, si los datos muestran una relaci\u00f3n m\u00e1s compleja (por ejemplo, cuadr\u00e1tica o exponencial), la regresi\u00f3n lineal no podr\u00e1 capturar dicha complejidad, resultando en predicciones inexactas. En estos casos, ser\u00eda mejor utilizar modelos que puedan capturar la no linealidad, como la regresi\u00f3n polin\u00f3mica o t\u00e9cnicas m\u00e1s avanzadas como redes neuronales.</p> </li> <li> <p>Sensibilidad a los outliers: La presencia de valores at\u00edpicos puede afectar significativamente el ajuste de la l\u00ednea, ya que la regresi\u00f3n lineal minimiza la suma de los errores al cuadrado. Los outliers, al tener errores m\u00e1s grandes, influyen desproporcionadamente en la l\u00ednea de ajuste, lo cual puede distorsionar el modelo. Para mitigar este problema, se pueden utilizar t\u00e9cnicas como la detecci\u00f3n y eliminaci\u00f3n de outliers, o emplear m\u00e9todos de regresi\u00f3n robusta que minimicen el impacto de estos valores extremos.</p> </li> <li> <p>Multicolinealidad: Cuando las variables independientes est\u00e1n altamente correlacionadas, el modelo puede producir resultados inestables. La multicolinealidad genera problemas en la estimaci\u00f3n de los coeficientes, haciendo que sean muy sensibles a peque\u00f1as variaciones en los datos y, por lo tanto, menos interpretables. Esto puede llevar a una disminuci\u00f3n en la precisi\u00f3n de las predicciones y a problemas en la generalizaci\u00f3n del modelo. En estos casos, se recomienda usar t\u00e9cnicas de regularizaci\u00f3n, como Ridge Regression, que penaliza los coeficientes grandes y ayuda a reducir los efectos de la multicolinealidad, estabilizando el modelo.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ridge-regression-y-lasso-regression","title":"Ridge Regression y Lasso Regression","text":"<p>Para superar algunas de las limitaciones de la regresi\u00f3n lineal est\u00e1ndar, se han desarrollado t\u00e9cnicas de regularizaci\u00f3n como Ridge y Lasso. Ambas t\u00e9cnicas son versiones modificadas de la regresi\u00f3n lineal que incluyen un t\u00e9rmino de penalizaci\u00f3n para mejorar el rendimiento y evitar el sobreajuste.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ridge-regression","title":"Ridge Regression","text":"<p>Ridge Regression, tambi\u00e9n conocida como regresi\u00f3n de cresta, a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L2 a la funci\u00f3n de p\u00e9rdida. Esto significa que el modelo penaliza los coeficientes grandes, haciendo que los valores de los par\u00e1metros sean m\u00e1s peque\u00f1os y estables. La ecuaci\u00f3n para Ridge es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum     heta_j^2 \\] <ul> <li>\\lambda: Par\u00e1metro de regularizaci\u00f3n que controla la cantidad de penalizaci\u00f3n.</li> <li>**    \\(heta_j\\)**: Coeficientes del modelo.</li> </ul> <p>El t\u00e9rmino de penalizaci\u00f3n ayuda a reducir la complejidad del modelo, lo cual resulta \u00fatil especialmente cuando existen m\u00faltiples variables independientes correlacionadas (multicolinealidad).</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ventajas-de-ridge-regression","title":"Ventajas de Ridge Regression","text":"<ul> <li>Reducci\u00f3n del sobreajuste: Ridge ayuda a reducir el riesgo de sobreajuste al penalizar los coeficientes grandes.</li> <li>Mejora la estabilidad: Especialmente en presencia de multicolinealidad, el modelo Ridge tiende a ser m\u00e1s estable.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#lasso-regression","title":"Lasso Regression","text":"<p>Lasso Regression a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L1 a la funci\u00f3n de p\u00e9rdida. Este t\u00e9rmino tiene la capacidad de hacer que algunos coeficientes sean exactamente cero, eliminando efectivamente ciertas caracter\u00edsticas del modelo. La ecuaci\u00f3n de Lasso es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum |   heta_j| \\] <ul> <li>\\(\\lambda\\): Par\u00e1metro de regularizaci\u00f3n que controla la penalizaci\u00f3n.</li> </ul> <p>Lasso es \u00fatil no solo para reducir el sobreajuste, sino tambi\u00e9n para la selecci\u00f3n de caracter\u00edsticas, ya que elimina autom\u00e1ticamente aquellas que no son \u00fatiles para la predicci\u00f3n.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ventajas-de-lasso-regression","title":"Ventajas de Lasso Regression","text":"<ul> <li>Selecci\u00f3n de caracter\u00edsticas: Lasso simplifica el modelo seleccionando solo las caracter\u00edsticas m\u00e1s relevantes.</li> <li>Reducci\u00f3n del sobreajuste: Similar a Ridge, Lasso ayuda a evitar el sobreajuste del modelo.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#comparacion-entre-regresion-lineal-ridge-y-lasso","title":"Comparaci\u00f3n entre Regresi\u00f3n Lineal, Ridge y Lasso","text":"Caracter\u00edstica Regresi\u00f3n Lineal Ridge Regression Lasso Regression Regularizaci\u00f3n No L2 L1 Penalizaci\u00f3n Ninguna Penaliza coeficientes grandes Algunos coeficientes se hacen cero Sobreajuste Alta posibilidad Baja Baja Multicolinealidad Problemas con multicolinealidad Mejor manejo Mejor manejo Selecci\u00f3n de caracter\u00edsticas No No S\u00ed <ul> <li>Regresi\u00f3n Lineal: Ideal para problemas simples y cuando existe una relaci\u00f3n lineal clara entre las variables. Sin embargo, es propensa al sobreajuste si no se maneja adecuadamente.</li> <li>Ridge Regression: \u00datil cuando existe multicolinealidad, ya que la regularizaci\u00f3n L2 ayuda a estabilizar el modelo. No elimina caracter\u00edsticas, pero hace que los coeficientes sean m\u00e1s peque\u00f1os.</li> <li>Lasso Regression: \u00datil para la selecci\u00f3n de caracter\u00edsticas, ya que fuerza algunos coeficientes a ser exactamente cero. Esto resulta en un modelo m\u00e1s sencillo y f\u00e1cil de interpretar.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#conclusiones","title":"Conclusiones","text":"<p>La regresi\u00f3n lineal es una excelente herramienta para comenzar a entender los modelos de regresi\u00f3n. Sin embargo, cuando nos enfrentamos a datos m\u00e1s complejos, con m\u00faltiples caracter\u00edsticas y posibles problemas de sobreajuste, Ridge y Lasso se presentan como mejores alternativas. Estos modelos ayudan a mejorar la capacidad de generalizaci\u00f3n del modelo y a reducir la complejidad, haciendo que la predicci\u00f3n sea m\u00e1s precisa y confiable.</p> <p>La elecci\u00f3n entre la regresi\u00f3n lineal, Ridge y Lasso depender\u00e1 de la naturaleza de los datos y los objetivos del an\u00e1lisis. Si se desea simplicidad y no hay riesgo de multicolinealidad, la regresi\u00f3n lineal puede ser suficiente. Si el modelo tiende a sobreajustarse o hay muchas caracter\u00edsticas correlacionadas, Ridge y Lasso son opciones a considerar, siendo Lasso ideal si se desea simplificar el modelo eliminando caracter\u00edsticas irrelevantes.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ejemplos-adicionales-de-uso","title":"Ejemplos Adicionales de Uso","text":"<ul> <li>Predicci\u00f3n de Ventas Minoristas: En un negocio minorista donde existen m\u00faltiples caracter\u00edsticas que afectan las ventas (promociones, temporadas, clima, ubicaci\u00f3n), Ridge Regression ser\u00eda \u00fatil para manejar la posible multicolinealidad entre estas caracter\u00edsticas. Ejemplo en Python</li> <li>Modelado de la Demanda Energ\u00e9tica: En la predicci\u00f3n del consumo de energ\u00eda el\u00e9ctrica, que depende de variables como temperatura, hora del d\u00eda, y tipo de d\u00eda (laboral o festivo), Ridge podr\u00eda ayudar a manejar la complejidad y multicolinealidad. </li> <li>An\u00e1lisis de Sentimientos: Al predecir la polaridad de una opini\u00f3n (positiva o negativa) en base a muchas palabras o frases, Lasso Regression ser\u00eda ideal para seleccionar las palabras m\u00e1s relevantes y reducir la dimensionalidad. </li> <li>Predicci\u00f3n de Costos de Seguros M\u00e9dicos: Para estimar los costos de seguros m\u00e9dicos en funci\u00f3n de caracter\u00edsticas como edad, estado de salud, h\u00e1bitos de vida y ubicaci\u00f3n geogr\u00e1fica, Lasso podr\u00eda ayudar a eliminar caracter\u00edsticas redundantes, haciendo el modelo m\u00e1s interpretable. </li> <li>Optimizaci\u00f3n de Cadenas de Suministro: Para predecir el tiempo de entrega de productos considerando m\u00faltiples variables (tr\u00e1fico, distancia, clima, inventario), Ridge Regression puede ser \u00fatil para manejar la correlaci\u00f3n entre factores como tr\u00e1fico y distancia. </li> <li>Reconocimiento de Actividad Humana: En la clasificaci\u00f3n de actividades humanas usando sensores port\u00e1tiles (como aceler\u00f3metros y giroscopios), Lasso podr\u00eda ayudar a identificar cu\u00e1les de las se\u00f1ales del sensor son m\u00e1s importantes para diferenciar entre actividades como caminar, correr o estar de pie. Ejemplo en Python</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/","title":"\ud83e\udd16 Unidad 3. Modelos de Aprendizaje Supervisado para Predicci\u00f3n Categ\u00f3rica","text":"<p>La clasificaci\u00f3n es una subcategor\u00eda del aprendizaje supervisado donde el objetivo es predecir una etiqueta de clase categ\u00f3rica (discreta) para una instancia de datos dada. A diferencia de la regresi\u00f3n, que predice valores continuos, la clasificaci\u00f3n asigna entradas a una de varias categor\u00edas predefinidas.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#31-entrenamiento-y-testing-en-clasificacion","title":"3.1. Entrenamiento y Testing en Clasificaci\u00f3n","text":"<p>El proceso de construcci\u00f3n de un modelo de clasificaci\u00f3n sigue el flujo est\u00e1ndar de Machine Learning:</p> <ol> <li>Divisi\u00f3n de Datos: Se divide el dataset en un conjunto de entrenamiento (para ajustar el modelo) y un conjunto de prueba (para evaluar su rendimiento en datos no vistos).</li> <li>Entrenamiento: El algoritmo aprende la frontera de decisi\u00f3n que separa las diferentes clases bas\u00e1ndose en las caracter\u00edsticas (features) de los datos de entrenamiento.</li> <li>Testing (Predicci\u00f3n): El modelo asigna etiquetas a los datos de prueba.</li> <li>Evaluaci\u00f3n: Se comparan las etiquetas predichas con las etiquetas reales para calcular m\u00e9tricas de rendimiento.</li> </ol>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#32-ejemplos-frecuentes-de-uso","title":"3.2. Ejemplos Frecuentes de Uso","text":"<p>La clasificaci\u00f3n est\u00e1 omnipresente en aplicaciones modernas:</p> <ul> <li>Detecci\u00f3n de Spam: Clasificar correos como \"Spam\" o \"No Spam\".</li> <li>Diagn\u00f3stico M\u00e9dico: Determinar si un paciente tiene una enfermedad (\"Positivo\") o no (\"Negativo\") bas\u00e1ndose en s\u00edntomas y an\u00e1lisis.</li> <li>Reconocimiento de Im\u00e1genes: Identificar si una imagen contiene un \"Gato\", \"Perro\" o \"Coche\".</li> <li>Aprobaci\u00f3n de Cr\u00e9ditos: Clasificar a un solicitante como de \"Alto Riesgo\" o \"Bajo Riesgo\".</li> <li>An\u00e1lisis de Sentimientos: Clasificar opiniones como \"Positivas\", \"Negativas\" o \"Neutrales\".</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#33-algoritmos-de-clasificacion-en-machine-learning","title":"3.3. Algoritmos de Clasificaci\u00f3n en Machine Learning","text":"<p>Existen diversos algoritmos para abordar problemas de clasificaci\u00f3n:</p> <ul> <li>Regresi\u00f3n Log\u00edstica: Simple, interpretable y base para redes neuronales.</li> <li>K-Nearest Neighbors (KNN): Basado en similitud y distancia.</li> <li>Support Vector Machines (SVM): Busca el hiperplano de separaci\u00f3n \u00f3ptimo.</li> <li>\u00c1rboles de Decisi\u00f3n y Random Forest: Basados en reglas de decisi\u00f3n jer\u00e1rquicas.</li> <li>Naive Bayes: Basado en probabilidad y el teorema de Bayes.</li> <li>Redes Neuronales: Para patrones complejos y datos no estructurados.</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#34-regresion-logistica","title":"3.4. Regresi\u00f3n Log\u00edstica","text":"<p>A pesar de su nombre, la Regresi\u00f3n Log\u00edstica es un algoritmo de clasificaci\u00f3n, no de regresi\u00f3n. Se utiliza para estimar la probabilidad de que una instancia pertenezca a una clase particular (por ejemplo, probabilidad de que un correo sea spam).</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#conceptos-basicos-y-matematicos","title":"Conceptos B\u00e1sicos y Matem\u00e1ticos","text":"<p>La regresi\u00f3n log\u00edstica utiliza la funci\u00f3n sigmoide (o log\u00edstica) para transformar la salida de una ecuaci\u00f3n lineal en un valor de probabilidad entre 0 y 1.</p> <ol> <li>Funci\u00f3n Lineal: \\(z = w \\cdot x + b\\) (donde \\(w\\) son los pesos y \\(x\\) las caracter\u00edsticas).</li> <li>Funci\u00f3n Sigmoide: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</li> </ol> <p>Si la probabilidad estimada \\(\\hat{p} = \\sigma(z)\\) es mayor o igual a 0.5, el modelo predice la clase 1; de lo contrario, predice la clase 0.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#algoritmo-del-gradiente-descendente","title":"Algoritmo del Gradiente Descendente","text":"<p>Para entrenar el modelo, necesitamos encontrar los pesos \\(w\\) y el sesgo \\(b\\) que minimicen el error. La funci\u00f3n de costo utilizada es la Log Loss (P\u00e9rdida Logar\u00edtmica), ya que el error cuadr\u00e1tico medio no es convexo para esta funci\u00f3n.</p> <p>El Gradiente Descendente es un algoritmo de optimizaci\u00f3n iterativo: 1.  Inicializa los pesos aleatoriamente. 2.  Calcula el gradiente de la funci\u00f3n de costo (la direcci\u00f3n en la que el error aumenta m\u00e1s r\u00e1pido). 3.  Actualiza los pesos movi\u00e9ndose en la direcci\u00f3n opuesta al gradiente para reducir el error.     \\(\\(w_{nuevo} = w_{viejo} - \\eta \\cdot \\nabla Costo\\)\\)     (Donde \\(\\eta\\) es la tasa de aprendizaje).</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#ejemplo-en-python","title":"Ejemplo en Python","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar datos\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Dividir y Escalar (Importante para Gradiente Descendente)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Entrenar modelo\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n\n# Predecir\ny_pred = log_reg.predict(X_test)\n</code></pre>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#35-metricas-de-rendimiento","title":"3.5. M\u00e9tricas de Rendimiento","text":"<p>Evaluar un clasificador va m\u00e1s all\u00e1 de simplemente contar cu\u00e1ntos aciertos tuvo.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#matriz-de-confusion","title":"Matriz de Confusi\u00f3n","text":"<p>Es una tabla que resume el rendimiento del modelo comparando las clases reales con las predichas.</p> Predicho Negativo (0) Predicho Positivo (1) Real Negativo (0) TN (True Negative) FP (False Positive) Real Positivo (1) FN (False Negative) TP (True Positive) <ul> <li>TP: Enfermos detectados correctamente.</li> <li>TN: Sanos detectados correctamente.</li> <li>FP (Error Tipo I): Sanos detectados err\u00f3neamente como enfermos (\"Falsa Alarma\").</li> <li>FN (Error Tipo II): Enfermos no detectados (\"Peligroso\").</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#metricas-derivadas","title":"M\u00e9tricas Derivadas","text":"<ol> <li> <p>Accuracy (Exactitud): Proporci\u00f3n total de predicciones correctas.     \\(\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\\)</p> </li> <li> <p>Error Rate (Tasa de Error): Proporci\u00f3n de predicciones incorrectas.     \\(\\(Error Rate = 1 - Accuracy = \\frac{FP + FN}{Total}\\)\\)</p> </li> <li> <p>Sensitivity / Recall / TPR (Tasa de Verdaderos Positivos): Capacidad para detectar la clase positiva.     \\(\\(Sensitivity = \\frac{TP}{TP + FN}\\)\\)</p> </li> <li> <p>Specificity / TNR (Tasa de Verdaderos Negativos): Capacidad para detectar la clase negativa.     \\(\\(Specificity = \\frac{TN}{TN + FP}\\)\\)</p> </li> <li> <p>False Positive Rate (FPR): \\(\\(FPR = 1 - Specificity = \\frac{FP}{TN + FP}\\)\\)</p> </li> <li> <p>Precision (Precisi\u00f3n): De los que predije positivos, \u00bfcu\u00e1ntos lo son realmente?     \\(\\(Precision = \\frac{TP}{TP + FP}\\)\\)</p> </li> <li> <p>F1-Score (F-Measure): Media arm\u00f3nica de Precision y Recall. \u00datil cuando las clases est\u00e1n desbalanceadas.     \\(\\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\\)</p> </li> <li> <p>Kappa Statistic (Cohen's Kappa): Mide la concordancia entre la predicci\u00f3n y la realidad, ajustada por el azar. Un valor de 1 es concordancia perfecta, 0 es igual al azar.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#ejemplo-en-python_1","title":"Ejemplo en Python","text":"<pre><code>from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n\nprint(\"Matriz de Confusi\u00f3n:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nReporte de Clasificaci\u00f3n:\\n\", classification_report(y_test, y_pred))\nprint(f\"Kappa Score: {cohen_kappa_score(y_test, y_pred):.4f}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#36-curva-roc-y-auc","title":"3.6. Curva ROC y AUC","text":"<p>La Curva ROC (Receiver Operating Characteristic) es un gr\u00e1fico que ilustra el rendimiento de un clasificador binario a medida que var\u00eda el umbral de discriminaci\u00f3n. *   Eje X: False Positive Rate (1 - Specificity). *   Eje Y: True Positive Rate (Sensitivity).</p> <p>Un modelo ideal se acerca a la esquina superior izquierda (TPR=1, FPR=0). La l\u00ednea diagonal representa un clasificador aleatorio.</p> <p>AUC (Area Under Curve): Es el \u00e1rea bajo la curva ROC. Resume el rendimiento en un solo n\u00famero. *   AUC = 0.5: Aleatorio. *   AUC = 1.0: Perfecto.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#37-sensibilidad-especificidad-y-el-teorema-de-bayes","title":"3.7. Sensibilidad, Especificidad y el Teorema de Bayes","text":"<p>Estos conceptos est\u00e1n \u00edntimamente ligados al Teorema de Bayes cuando queremos calcular la probabilidad real de tener una condici\u00f3n dado un resultado positivo en un test (Probabilidad a Posteriori).</p> <p>Supongamos un test m\u00e9dico para una enfermedad rara: *   \\(P(E)\\): Probabilidad a priori de tener la enfermedad (Prevalencia). *   \\(P(+|E)\\): Sensibilidad del test. *   \\(P(-|No E)\\): Especificidad del test.</p> <p>Si un paciente da positivo, \u00bfcu\u00e1l es la probabilidad de que realmente tenga la enfermedad \\(P(E|+)\\)?</p> \\[P(E|+) = \\frac{P(+|E) \\cdot P(E)}{P(+|E) \\cdot P(E) + P(+|No E) \\cdot P(No E)}\\] <p>Donde \\(P(+|No E)\\) es el False Positive Rate (\\(1 - Especificidad\\)). Este c\u00e1lculo demuestra que si la prevalencia de la enfermedad es muy baja, incluso un test con alta sensibilidad y especificidad puede generar muchos falsos positivos, haciendo que la probabilidad real de estar enfermo sea baja a pesar del resultado positivo.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/","title":"\ud83e\udd16 Unidad 4. \u00c1rbol de Decisi\u00f3n en Inteligencia Artificial: Explicaci\u00f3n Detallada","text":"<p>El algoritmo de \u00c1rbol de Decisi\u00f3n (Decision Tree) es un modelo de aprendizaje supervisado que se utiliza tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n. Su objetivo es dividir el espacio de datos en subconjuntos homog\u00e9neos bas\u00e1ndose en una serie de reglas, de modo que cada subconjunto sea lo m\u00e1s puro posible con respecto a la variable objetivo. Los \u00e1rboles de decisi\u00f3n son f\u00e1ciles de interpretar, muy \u00fatiles para entender las relaciones en los datos, y se aplican ampliamente en una variedad de campos.</p> <p>A continuaci\u00f3n, exploraremos la teor\u00eda detr\u00e1s de los \u00e1rboles de decisi\u00f3n, c\u00f3mo se construyen, y daremos ejemplos que ilustran c\u00f3mo funciona este algoritmo en la pr\u00e1ctica. Tambi\u00e9n incluiremos casos reales en los que este algoritmo ha demostrado ser \u00fatil, as\u00ed como c\u00f3mo encontrar los mejores valores para los metapar\u00e1metros del modelo.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#1-estructura-de-un-arbol-de-decision","title":"1. Estructura de un \u00c1rbol de Decisi\u00f3n","text":"<p>Un \u00e1rbol de decisi\u00f3n est\u00e1 compuesto por varios elementos fundamentales: - Nodos de Decisi\u00f3n: Representan la divisi\u00f3n de los datos seg\u00fan una caracter\u00edstica espec\u00edfica. Aqu\u00ed se toma una decisi\u00f3n sobre qu\u00e9 atributo se usa para dividir el conjunto de datos. - Ramas: Las conexiones entre nodos representan el resultado de una decisi\u00f3n. Cada rama lleva a un nuevo nodo o a un nodo hoja. - Nodos Hoja: Son los puntos finales del \u00e1rbol. Representan la categor\u00eda final o el valor predicho para una determinada observaci\u00f3n.</p> <p>Cada divisi\u00f3n en un \u00e1rbol de decisi\u00f3n intenta dividir los datos de manera que maximice la pureza de los subconjuntos resultantes, es decir, que agrupe datos similares juntos. Este proceso contin\u00faa hasta que se cumplen ciertas condiciones, como alcanzar un n\u00famero m\u00ednimo de muestras en un nodo o una profundidad m\u00e1xima del \u00e1rbol.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#2-criterios-de-division-y-formulas-matematicas","title":"2. Criterios de Divisi\u00f3n y F\u00f3rmulas Matem\u00e1ticas","text":"<p>Los \u00e1rboles de decisi\u00f3n se construyen utilizando una serie de divisiones, cada una de las cuales se elige bas\u00e1ndose en un criterio que mide la calidad de la divisi\u00f3n. Existen varias m\u00e9tricas para seleccionar la caracter\u00edstica que mejor divide los datos:</p> <ul> <li>Entrop\u00eda e \u00cdndice de Ganancia de Informaci\u00f3n</li> <li>\u00cdndice Gini</li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#21-entropia-e-indice-de-ganancia-de-informacion","title":"2.1. Entrop\u00eda e \u00cdndice de Ganancia de Informaci\u00f3n","text":"<p>La entrop\u00eda mide la pureza de un nodo. Se define de la siguiente manera para un nodo que tiene dos clases (positiva y negativa):</p> \\[ H(S) = -p_+ \\cdot \\log_2(p_+) - p_- \\cdot \\log_2(p_-) \\] <p>Donde: - \\( p_+ \\) y \\( p_- \\) son las proporciones de ejemplos positivos y negativos en el nodo.</p> <p>El objetivo es minimizar la entrop\u00eda en cada nodo, lo que equivale a hacer los nodos lo m\u00e1s homog\u00e9neos posible.</p> <p>La ganancia de informaci\u00f3n mide la reducci\u00f3n de la entrop\u00eda despu\u00e9s de dividir un nodo. La f\u00f3rmula para la ganancia de informaci\u00f3n (\\( IG \\)) es:</p> \\[ IG(S, A) = H(S) - \\sum_{v \\in Valores(A)} \\frac{|S_v|}{|S|} H(S_v) \\] <p>Donde: - S es el conjunto de datos original. - A es el atributo por el cual se est\u00e1 dividiendo. - S_v son los subconjuntos de S resultantes de la divisi\u00f3n por el valor v de la caracter\u00edstica A.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#22-indice-gini","title":"2.2. \u00cdndice Gini","text":"<p>El \u00edndice Gini es otra medida utilizada para evaluar la calidad de una divisi\u00f3n. Representa la probabilidad de que una observaci\u00f3n seleccionada aleatoriamente sea clasificada incorrectamente si se realiza una predicci\u00f3n aleatoria basada en la distribuci\u00f3n de clases del nodo. La f\u00f3rmula para el \u00edndice Gini es:</p> \\[ Gini(S) = 1 - \\sum_{i=1}^C p_i^2 \\] <p>Donde \\( p_i \\) es la proporci\u00f3n de elementos pertenecientes a la clase \\( i \\) en el conjunto \\( S \\), y \\( C \\) es el n\u00famero de clases.</p> <p>La idea detr\u00e1s del \u00edndice Gini es minimizar el valor de \\( Gini(S) \\) en cada divisi\u00f3n, buscando nodos lo m\u00e1s puros posible.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#23-calculo-de-ejemplos-con-entropia-e-indice-gini","title":"2.3. C\u00e1lculo de Ejemplos con Entrop\u00eda e \u00cdndice Gini","text":"<p>Veamos un ejemplo detallado de c\u00f3mo calcular la entrop\u00eda y el \u00edndice Gini para una divisi\u00f3n espec\u00edfica de datos.</p> <p>Supongamos que tenemos un conjunto de datos con la siguiente distribuci\u00f3n para la variable objetivo (S\u00ed/No):</p> Caracter\u00edstica Clase: S\u00ed Clase: No A 4 2 B 1 3"},{"location":"aprendizaje-supervisado/04-arboles-decision/#231-calculo-de-la-entropia","title":"2.3.1. C\u00e1lculo de la Entrop\u00eda","text":"<p>Primero calculamos la entrop\u00eda para cada uno de los nodos resultantes de dividir el conjunto de datos seg\u00fan la caracter\u00edstica \"A\":</p> <p>Para A: - Total de ejemplos: \\( 4 + 2 = 6 \\) - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{4}{6})\\) - Proporci\u00f3n de clase No \\(( p_- ): (\\frac{2}{6})\\)</p> <p>La entrop\u00eda para la caracter\u00edstica A es: $$ H(A) = -\\left( \\frac{4}{6} \\right) \\log_2\\left( \\frac{4}{6} \\right) - \\left( \\frac{2}{6} \\right) \\log_2\\left( \\frac{2}{6} \\right) = 0.918 $$</p> <p>Para B: - Total de ejemplos: \\( 1 + 3 = 4 \\) - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{1}{4})\\) - Proporci\u00f3n de clase No \\(( p_+ ): (\\frac{3}{4})\\)</p> <p>La entrop\u00eda para la caracter\u00edstica B es: $$ H(B) = -\\left( \\frac{1}{4} \\right) \\log_2\\left( \\frac{1}{4} \\right) - \\left( \\frac{3}{4} \\right) \\log_2\\left( \\frac{3}{4} \\right) = 0.811 $$</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#232-calculo-del-indice-gini","title":"2.3.2. C\u00e1lculo del \u00cdndice Gini","text":"<p>Ahora calculamos el \u00edndice Gini para la misma divisi\u00f3n:</p> <p>Para A: - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{4}{6})\\) - Proporci\u00f3n de clase No \\(( p_- ): (\\frac{2}{6})\\)</p> <p>El \u00edndice Gini para la caracter\u00edstica A es: $$ Gini(A) = 1 - \\left( \\frac{4}{6} \\right)^2 - \\left( \\frac{2}{6} \\right)^2 = 0.444 $$</p> <p>Para B: - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{1}{4})\\) - Proporci\u00f3n de clase No \\(( p_+ ): (\\frac{3}{4})\\)</p> <p>El \u00edndice Gini para la caracter\u00edstica B es: $$ Gini(B) = 1 - \\left( \\frac{1}{4} \\right)^2 - \\left( \\frac{3}{4} \\right)^2 = 0.375 $$</p> <p>Con estos valores, podemos comparar las caracter\u00edsticas y elegir cu\u00e1l proporciona una mejor divisi\u00f3n de los datos seg\u00fan el criterio seleccionado (en este caso, el que minimice la entrop\u00eda o el \u00edndice Gini).</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#3-construccion-de-un-arbol-de-decision","title":"3. Construcci\u00f3n de un \u00c1rbol de Decisi\u00f3n","text":"<p>La construcci\u00f3n de un \u00e1rbol de decisi\u00f3n se realiza de manera recursiva, siguiendo estos pasos:</p> <ol> <li>Seleccionar el Mejor Atributo: Se elige el atributo que maximiza la ganancia de informaci\u00f3n o minimiza el \u00edndice Gini.</li> <li>Dividir el Conjunto de Datos: Se divide el conjunto de datos en funci\u00f3n del atributo seleccionado.</li> <li>Repetir el Proceso: Se repiten los pasos anteriores para cada subconjunto resultante hasta alcanzar un criterio de parada.</li> </ol> <p>Criterios de Parada pueden ser, por ejemplo, que todos los datos del nodo sean de la misma clase, que el nodo contenga muy pocas instancias (por debajo de un umbral m\u00ednimo), o que se haya alcanzado una profundidad m\u00e1xima predefinida.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#4-ejemplo-de-arbol-de-decision","title":"4. Ejemplo de \u00c1rbol de Decisi\u00f3n","text":"<p>Supongamos que queremos predecir si una persona har\u00e1 ejercicio al aire libre en funci\u00f3n de dos variables: tiempo (soleado, nublado, lluvioso) y temperatura (alta, baja).</p> <ol> <li>Ra\u00edz del \u00c1rbol: Elegimos la primera divisi\u00f3n. Si utilizamos la ganancia de informaci\u00f3n, tal vez encontremos que la variable tiempo tiene la mayor ganancia.</li> <li>Nodo Ra\u00edz: Tiempo</li> <li> <p>Ramas: Soleado, Nublado, Lluvioso</p> </li> <li> <p>Divisiones Subsiguientes: Para cada valor del tiempo, examinamos la temperatura.</p> </li> <li> <p>Para tiempo = Soleado, podemos tener otra divisi\u00f3n por temperatura.</p> </li> <li> <p>Nodos Hoja: Al final de las ramas, llegamos a los nodos hoja, que pueden ser \"S\u00ed\" o \"No\" indicando si la persona har\u00e1 ejercicio o no.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#5-ventajas-y-limitaciones-de-los-arboles-de-decision","title":"5. Ventajas y Limitaciones de los \u00c1rboles de Decisi\u00f3n","text":"<ul> <li>Ventajas:</li> <li>F\u00e1cil Interpretaci\u00f3n: Los \u00e1rboles de decisi\u00f3n son f\u00e1ciles de interpretar, ya que se asemejan a c\u00f3mo los humanos toman decisiones.</li> <li>Pocos Supuestos sobre los Datos: No necesitan normalizaci\u00f3n de datos ni que las caracter\u00edsticas sean escaladas.</li> <li> <p>Manejo de Datos Categ\u00f3ricos y Num\u00e9ricos: Los \u00e1rboles de decisi\u00f3n pueden trabajar con ambos tipos de datos.</p> </li> <li> <p>Limitaciones:</p> </li> <li>Sobreajuste: Los \u00e1rboles de decisi\u00f3n tienden a sobreajustarse si no se limitan adecuadamente (por ejemplo, estableciendo una profundidad m\u00e1xima).</li> <li>Inestabilidad: Los \u00e1rboles de decisi\u00f3n son sensibles a peque\u00f1as variaciones en los datos, lo cual puede generar \u00e1rboles diferentes para conjuntos de datos similares.</li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#6-optimizacion-de-los-metaparametros","title":"6. Optimizaci\u00f3n de los Metapar\u00e1metros","text":"<p>La calidad de un \u00e1rbol de decisi\u00f3n depende en gran medida de los metapar\u00e1metros que se elijan. Algunos de los metapar\u00e1metros clave para un \u00e1rbol de decisi\u00f3n son:</p> <ol> <li> <p>Profundidad M\u00e1xima (<code>max_depth</code>): Limitar la profundidad del \u00e1rbol ayuda a evitar el sobreajuste. La profundidad m\u00e1xima determina cu\u00e1ntos niveles puede tener el \u00e1rbol. Una profundidad muy alta puede llevar al sobreajuste, mientras que una profundidad muy baja puede causar un subajuste.</p> </li> <li> <p>N\u00famero M\u00ednimo de Muestras por Hoja (<code>min_samples_leaf</code>): Controla el n\u00famero m\u00ednimo de muestras que debe haber en un nodo hoja. Un valor m\u00e1s alto reduce el sobreajuste, ya que asegura que las hojas tengan un n\u00famero significativo de ejemplos.</p> </li> <li> <p>N\u00famero M\u00ednimo de Muestras para Dividir (<code>min_samples_split</code>): Especifica el n\u00famero m\u00ednimo de muestras requerido para dividir un nodo. Un valor m\u00e1s alto evita divisiones innecesarias, lo cual ayuda a mantener el \u00e1rbol m\u00e1s simple y reducir el riesgo de sobreajuste.</p> </li> <li> <p>Criterio de Divisi\u00f3n (<code>criterion</code>): Define la funci\u00f3n que se usa para medir la calidad de una divisi\u00f3n. Los criterios comunes son <code>gini</code> e <code>entropy</code>. La elecci\u00f3n del criterio puede influir en la estructura del \u00e1rbol y su capacidad de generalizaci\u00f3n.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#busqueda-de-los-mejores-valores-de-los-metaparametros","title":"B\u00fasqueda de los Mejores Valores de los Metapar\u00e1metros","text":"<p>Para encontrar los valores \u00f3ptimos de estos metapar\u00e1metros, se suelen usar t\u00e9cnicas como la b\u00fasqueda en cuadr\u00edcula (Grid Search) o la b\u00fasqueda aleatoria (Random Search), combinadas con la validaci\u00f3n cruzada.</p> <ul> <li> <p>Grid Search: Busca de manera exhaustiva entre una lista predefinida de valores para cada metapar\u00e1metro. Es eficaz pero puede ser computacionalmente costosa si hay muchos par\u00e1metros y valores posibles.</p> </li> <li> <p>Random Search: Busca valores de metapar\u00e1metros de manera aleatoria dentro de un rango definido. Es m\u00e1s eficiente que Grid Search cuando se trabaja con un gran n\u00famero de combinaciones posibles.</p> </li> <li> <p>Validaci\u00f3n Cruzada: Tanto en Grid Search como en Random Search, se utiliza validaci\u00f3n cruzada para evaluar el rendimiento del modelo para cada combinaci\u00f3n de metapar\u00e1metros y seleccionar aquella que maximice la m\u00e9trica de rendimiento, como la precisi\u00f3n o el F1-score.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#7-aplicaciones-reales-de-los-arboles-de-decision","title":"7. Aplicaciones Reales de los \u00c1rboles de Decisi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n se aplican en una amplia gama de problemas reales debido a su versatilidad y facilidad de interpretaci\u00f3n. Algunos ejemplos son:</p> <ol> <li> <p>Diagn\u00f3stico M\u00e9dico: En la medicina, los \u00e1rboles de decisi\u00f3n se utilizan para ayudar a los m\u00e9dicos a diagnosticar enfermedades bas\u00e1ndose en s\u00edntomas y pruebas de laboratorio. Por ejemplo, un \u00e1rbol de decisi\u00f3n puede ayudar a predecir si un paciente tiene diabetes en funci\u00f3n de caracter\u00edsticas como nivel de glucosa, presi\u00f3n arterial y edad.</p> </li> <li> <p>Cr\u00e9dito y Riesgo Financiero: En el sector financiero, los \u00e1rboles de decisi\u00f3n se usan para evaluar la probabilidad de que un cliente incumpla un pr\u00e9stamo. Las caracter\u00edsticas utilizadas pueden incluir el historial crediticio, los ingresos mensuales y el monto del pr\u00e9stamo solicitado.</p> </li> <li> <p>M\u00e1rketing y Segmentaci\u00f3n de Clientes: En el marketing, los \u00e1rboles de decisi\u00f3n ayudan a segmentar a los clientes y a predecir si un cliente potencial realizar\u00e1 una compra. Los datos analizados pueden incluir el historial de compras, la interacci\u00f3n con campa\u00f1as publicitarias y la demograf\u00eda del cliente.</p> </li> <li> <p>Control de Calidad en Manufactura: En el sector manufacturero, los \u00e1rboles de decisi\u00f3n pueden ayudar a detectar productos defectuosos durante el proceso de producci\u00f3n, bas\u00e1ndose en caracter\u00edsticas como la temperatura, el tiempo de producci\u00f3n, y otras m\u00e9tricas de calidad.</p> </li> <li> <p>Predicci\u00f3n de Deserci\u00f3n Escolar: En educaci\u00f3n, los \u00e1rboles de decisi\u00f3n se usan para predecir la probabilidad de que un estudiante abandone sus estudios, bas\u00e1ndose en factores como la asistencia, las calificaciones y el apoyo familiar.</p> </li> <li> <p>Clasificaci\u00f3n de Especies: En la biolog\u00eda, se utilizan para clasificar especies de plantas o animales seg\u00fan caracter\u00edsticas observadas. Un ejemplo cl\u00e1sico es el conjunto de datos Iris, donde se clasifica una flor en una de tres especies seg\u00fan el largo y ancho de los p\u00e9talos y s\u00e9palos.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#8-conclusion","title":"8. Conclusi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n son una herramienta fundamental en el aprendizaje autom\u00e1tico debido a su capacidad para dividir los datos de manera iterativa y sencilla, maximizando la pureza de los nodos en cada divisi\u00f3n. Aunque presentan ciertas limitaciones, como el riesgo de sobreajuste, son particularmente valiosos cuando se necesita una explicaci\u00f3n clara y comprensible del proceso de decisi\u00f3n. Los \u00e1rboles de decisi\u00f3n se utilizan ampliamente en muchos sectores, y sus aplicaciones van desde el diagn\u00f3stico m\u00e9dico hasta la predicci\u00f3n del comportamiento de los clientes. Son una excelente elecci\u00f3n cuando la interpretabilidad y la facilidad de uso son factores importantes a considerar.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/","title":"\ud83e\udd16 Unidad 5. Algoritmo de Bayes y Naive Bayes en Inteligencia Artificial","text":"<p>El algoritmo de Bayes, tambi\u00e9n conocido como Teorema de Bayes, es un enfoque probabil\u00edstico utilizado para la clasificaci\u00f3n y el an\u00e1lisis en inteligencia artificial y aprendizaje autom\u00e1tico. Este algoritmo se basa en la probabilidad condicional, lo cual permite actualizar la probabilidad de un evento en funci\u00f3n de nueva evidencia.</p> <p>El algoritmo Naive Bayes simplifica el Teorema de Bayes haciendo una suposici\u00f3n fundamental: que todas las caracter\u00edsticas (o atributos) son independientes entre s\u00ed. Esta simplificaci\u00f3n permite construir modelos de clasificaci\u00f3n r\u00e1pidos y eficientes, especialmente \u00fatiles en aplicaciones de clasificaci\u00f3n de texto, como la clasificaci\u00f3n de correos electr\u00f3nicos o el an\u00e1lisis de sentimientos.</p> <p>A continuaci\u00f3n, veremos en detalle c\u00f3mo se deriva el modelo Naive Bayes a partir del Teorema de Bayes y c\u00f3mo funciona, junto con ejemplos y las f\u00f3rmulas matem\u00e1ticas correspondientes.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#1-teorema-de-bayes","title":"1. Teorema de Bayes","text":"<p>El Teorema de Bayes describe la probabilidad de que ocurra un evento \\( A \\) dado que ya ha ocurrido otro evento \\( B \\). La f\u00f3rmula se expresa de la siguiente manera:</p> \\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)} \\] <p>Donde: - \\( P(A|B) \\): Probabilidad de que ocurra el evento \\( A \\) dado que \\( B \\) ha ocurrido (probabilidad posterior). - \\( P(B|A) \\): Probabilidad de que ocurra el evento \\( B \\) dado que \\( A \\) ha ocurrido (verosimilitud). - \\( P(A) \\): Probabilidad a priori del evento \\( A \\). - \\( P(B) \\): Probabilidad del evento \\( B \\).</p> <p>El Teorema de Bayes permite actualizar la probabilidad a priori de un evento a partir de nueva informaci\u00f3n (evidencia).</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#2-naive-bayes","title":"2. Naive Bayes","text":"<p>El clasificador Naive Bayes se deriva del Teorema de Bayes con la suposici\u00f3n de independencia entre caracter\u00edsticas. En lugar de considerar todas las relaciones posibles entre los atributos, se asume que cada caracter\u00edstica es independiente de las dem\u00e1s, dado el resultado. Esto simplifica el c\u00e1lculo de la probabilidad conjunta.</p> <p>La probabilidad de que un ejemplo  $$   x = (x_1, x_2, \\dots, x_n) $$  pertenezca a una clase \\( C_k \\) se puede calcular como:</p> \\[ P(C_k | x) = \\frac{P(C_k) \\prod_{i=1}^n P(x_i | C_k)}{P(x)} \\] <p>Dado que \\( P(x) \\) es constante para todas las clases, podemos simplificar la f\u00f3rmula a:</p> \\[ P(C_k | x) \\propto P(C_k) \\prod_{i=1}^n P(x_i | C_k) \\] <p>Donde: - P(C_k | x): Probabilidad posterior de que el ejemplo pertenezca a la clase C_k . - P(C_k): Probabilidad a priori de la clase C_k. - P(x_i | C_k): Probabilidad condicional de la caracter\u00edstica x_i dada la clase C_k.</p> <p>El clasificador Naive Bayes elige la clase que maximiza esta probabilidad posterior.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#3-tipos-de-clasificadores-naive-bayes","title":"3. Tipos de Clasificadores Naive Bayes","text":"<p>Existen diferentes tipos de clasificadores Naive Bayes, dependiendo del tipo de datos y de c\u00f3mo se calcula la probabilidad condicional:</p> <ul> <li>Naive Bayes Gaussiano: Se utiliza cuando las caracter\u00edsticas tienen una distribuci\u00f3n continua que se puede aproximar a una distribuci\u00f3n normal (gaussiana).</li> <li>Naive Bayes Multinomial: Es adecuado para datos discretos, como el conteo de palabras en un documento. Es ampliamente utilizado en clasificaci\u00f3n de texto.</li> <li>Naive Bayes Bernoulli: Se utiliza para caracter\u00edsticas binarias. Es \u00fatil cuando cada caracter\u00edstica es booleana (por ejemplo, si una palabra aparece o no en un documento).</li> </ul>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#4-ejemplo-completo-de-naive-bayes","title":"4. Ejemplo Completo de Naive Bayes","text":"<p>Vamos a clasificar correos electr\u00f3nicos como \"spam\" o \"no spam\" usando el algoritmo de Naive Bayes. Para este ejemplo, supongamos que tenemos los siguientes datos de entrenamiento, con algunas palabras clave y la clase correspondiente (\"spam\" o \"no spam\"):</p> Correo ID Contenido Clase 1 Oferta barata, gana dinero Spam 2 Proyecto pendiente de trabajo No Spam 3 Oferta especial gratis Spam 4 Reuni\u00f3n de equipo ma\u00f1ana No Spam 5 Gana premios y dinero ahora Spam 6 Informe mensual adjunto No Spam <p>Vamos a suponer que queremos clasificar un nuevo correo con el contenido: \"Oferta gratis y premios\". Para esto, usaremos el clasificador de Naive Bayes, asumiendo que todas las palabras son independientes (el supuesto \"naive\").</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-1-calcular-las-probabilidades-previas","title":"Paso 1: Calcular las Probabilidades Previas","text":"<p>Primero calculamos la probabilidad previa de cada clase.</p> <ul> <li>Probabilidad de Spam (\u03c0(spam)):</li> </ul> <p>$$   P(Spam) = \\frac{N_{Spam}}{N_{Total}} = \\frac{3}{6} = 0.5   $$</p> <ul> <li>Probabilidad de No Spam (\u03c0(no_spam)):</li> </ul> <p>$$   P(No\\,Spam) = \\frac{N_{No\\,Spam}}{N_{Total}} = \\frac{3}{6} = 0.5   $$</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-2-calcular-la-probabilidad-de-cada-palabra","title":"Paso 2: Calcular la Probabilidad de Cada Palabra","text":"<p>Ahora, necesitamos calcular la probabilidad de cada palabra en el contexto de cada clase (es decir, \"spam\" y \"no spam\"). Las palabras \u00fanicas en nuestro conjunto de entrenamiento son:</p> <ul> <li>\"oferta\", \"barata\", \"gana\", \"dinero\", \"proyecto\", \"pendiente\", \"trabajo\", \"especial\", \"gratis\", \"reuni\u00f3n\", \"equipo\", \"ma\u00f1ana\", \"premios\", \"ahora\", \"informe\", \"mensual\", \"adjunto\".</li> </ul> <p>Vamos a usar suavizado de Laplace (adicionando 1 a cada recuento) para evitar probabilidades de cero.</p> <p>Por ejemplo, calculamos la probabilidad de cada palabra para spam:</p> <ul> <li>P(oferta | Spam):</li> </ul> <p>La palabra \"oferta\" aparece en 2 de los 3 correos spam.</p> <p>$$   P(oferta \\mid Spam) = \\frac{2 + 1}{N_{Spam} + V} = \\frac{2 + 1}{3 + 17} = \\frac{3}{20} = 0.15   $$</p> <ul> <li>P(gratis | Spam) y P(premios | Spam) tambi\u00e9n se calculan de manera similar.</li> </ul> <p>Donde: - \\(N_{Spam}\\) : N\u00famero de correos spam. - V : N\u00famero de palabras \u00fanicas en el vocabulario.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-3-clasificar-el-nuevo-correo","title":"Paso 3: Clasificar el Nuevo Correo","text":"<p>El nuevo correo es: \"Oferta gratis y premios\". Queremos calcular la probabilidad de que sea spam o no spam.</p> <p>Para calcular esto, usamos la f\u00f3rmula de Naive Bayes:</p> \\[ P(Clase \\mid X) \\propto P(X \\mid Clase) \\cdot P(Clase) \\] <p>Primero calculamos la probabilidad de que el correo sea spam:</p> \\[ P(Spam \\mid X) \\propto P(oferta \\mid Spam) \\cdot P(gratis \\mid Spam) \\cdot P(premios \\mid Spam) \\cdot P(Spam) \\] <p>Sustituimos los valores y multiplicamos:</p> \\[ P(Spam \\mid X) \\propto 0.15 \\times 0.15 \\times 0.1 \\times 0.5 \\] <p>Hacemos el mismo c\u00e1lculo para No Spam y comparamos ambas probabilidades.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-4-decision","title":"Paso 4: Decisi\u00f3n","text":"<p>Finalmente, elegimos la clase que tiene la probabilidad mayor. Si $ P(Spam \\mid X) $ es mayor que $ P(No\\,Spam \\mid X) $, clasificamos el correo como spam; de lo contrario, como no spam.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#5-aplicaciones-reales-de-naive-bayes","title":"5. Aplicaciones Reales de Naive Bayes","text":"<p>Debido a su eficiencia y capacidad para manejar grandes vol\u00famenes de datos, Naive Bayes se utiliza en una amplia variedad de aplicaciones del mundo real:</p> <ul> <li>Filtrado de Spam: Es el uso m\u00e1s cl\u00e1sico. Servicios como Gmail o Outlook utilizan variantes de Naive Bayes para clasificar correos entrantes como deseados o no deseados bas\u00e1ndose en la frecuencia de ciertas palabras.<ul> <li>Ejemplo de implementaci\u00f3n de filtro Spam</li> </ul> </li> <li>An\u00e1lisis de Sentimientos: Determinar si una opini\u00f3n en redes sociales (Twitter, rese\u00f1as de productos) es positiva, negativa o neutral. Es muy usado en marketing para monitorear la reputaci\u00f3n de marca.<ul> <li>An\u00e1lisis de sentimientos en Twitter</li> </ul> </li> <li>Clasificaci\u00f3n de Documentos: Organizar noticias en categor\u00edas (Deportes, Pol\u00edtica, Tecnolog\u00eda) o clasificar documentos legales y m\u00e9dicos.</li> <li>Sistemas de Recomendaci\u00f3n: Filtrado colaborativo para predecir si a un usuario le gustar\u00e1 un recurso dado.</li> </ul>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#6-ventajas-y-limitaciones-de-naive-bayes","title":"6. Ventajas y Limitaciones de Naive Bayes","text":"<ul> <li>Ventajas:</li> <li>Simplicidad y rapidez: Naive Bayes es simple de implementar y muy r\u00e1pido para entrenar y hacer predicciones, incluso con grandes vol\u00famenes de datos.</li> <li>Escalabilidad: Funciona bien con datos de alta dimensionalidad, como texto.</li> <li> <p>Robustez frente al ruido: A pesar de la suposici\u00f3n de independencia, suele funcionar sorprendentemente bien en muchos problemas reales.</p> </li> <li> <p>Limitaciones:</p> </li> <li>Suposici\u00f3n de independencia: La suposici\u00f3n de independencia rara vez se cumple en problemas reales. Esto puede llevar a predicciones menos precisas cuando las caracter\u00edsticas est\u00e1n altamente correlacionadas.</li> <li>Problemas con datos cero: Si una caracter\u00edstica no se presenta en los datos de entrenamiento, la probabilidad condicional se convierte en cero, lo que hace que la probabilidad posterior tambi\u00e9n sea cero. Esto se suele solucionar con la suavizaci\u00f3n de Laplace.</li> </ul> <p>El clasificador Naive Bayes sigue siendo una herramienta poderosa para muchas aplicaciones de inteligencia artificial y aprendizaje autom\u00e1tico, especialmente en la clasificaci\u00f3n de texto y otros problemas donde la independencia de las caracter\u00edsticas no afecta significativamente el rendimiento del modelo.</p>"},{"location":"aprendizaje-supervisado/06-knn/","title":"\ud83e\udd16 Unidad 6. Algoritmo K-Nearest Neighbors (KNN)","text":"<p>El algoritmo K-Nearest Neighbors (K-Vecinos M\u00e1s Cercanos) es uno de los m\u00e9todos m\u00e1s simples y efectivos en Machine Learning supervisado. Se utiliza tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n. Es un algoritmo no param\u00e9trico (no hace suposiciones sobre la distribuci\u00f3n de los datos subyacentes) y de aprendizaje perezoso (lazy learning), lo que significa que no \"aprende\" un modelo discriminativo durante la fase de entrenamiento, sino que memoriza los datos de entrenamiento para realizar predicciones en el momento necesario.</p>"},{"location":"aprendizaje-supervisado/06-knn/#61-como-funciona-el-algoritmo","title":"6.1. \u00bfC\u00f3mo funciona el algoritmo?","text":"<p>La intuici\u00f3n detr\u00e1s de KNN es sencilla y se basa en la proximidad: \"Dime con qui\u00e9n andas y te dir\u00e9 qui\u00e9n eres\". Para clasificar un nuevo punto de datos, el algoritmo busca en todo el conjunto de datos de entrenamiento los 'k' puntos m\u00e1s cercanos (vecinos) a ese nuevo punto.</p> <ol> <li>Calcular distancias: Se calcula la distancia matem\u00e1tica entre el punto nuevo que queremos predecir y todos los puntos existentes en el dataset.</li> <li>Buscar vecinos: Se seleccionan los \\(k\\) puntos con las distancias m\u00e1s cortas.</li> <li>Votaci\u00f3n (Clasificaci\u00f3n): La clase del nuevo punto se determina por mayor\u00eda de votos de sus vecinos. La clase m\u00e1s frecuente entre los \\(k\\) vecinos se asigna al nuevo punto.</li> <li>Promedio (Regresi\u00f3n): El valor del nuevo punto es el promedio (o media ponderada) de los valores num\u00e9ricos de sus vecinos.</li> </ol>"},{"location":"aprendizaje-supervisado/06-knn/#62-explicacion-matematica","title":"6.2. Explicaci\u00f3n Matem\u00e1tica","text":"<p>El n\u00facleo de KNN es la medici\u00f3n de la distancia para determinar la similitud. La m\u00e9trica m\u00e1s com\u00fan es la Distancia Euclidiana, aunque existen otras dependiendo del tipo de datos y el problema.</p> <p>Dados dos puntos \\(P\\) y \\(Q\\) en un espacio n-dimensional (donde \\(n\\) es el n\u00famero de caracter\u00edsticas): \\(P = (p_1, p_2, ..., p_n)\\) \\(Q = (q_1, q_2, ..., q_n)\\)</p> <ul> <li> <p>Distancia Euclidiana (L2): Es la distancia en l\u00ednea recta \"a vuelo de p\u00e1jaro\". Es la m\u00e1s utilizada por defecto.     \\(\\(d(P, Q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}\\)\\)</p> </li> <li> <p>Distancia Manhattan (L1): Suma de las diferencias absolutas. Es \u00fatil en sistemas tipo cuadr\u00edcula (como manzanas de una ciudad).     \\(\\(d(P, Q) = \\sum_{i=1}^{n} |q_i - p_i|\\)\\)</p> </li> <li> <p>Distancia Minkowski: Una generalizaci\u00f3n matem\u00e1tica de las anteriores.     \\(\\(d(P, Q) = (\\sum_{i=1}^{n} |q_i - p_i|^p)^{1/p}\\)\\)     (Si \\(p=1\\) es Manhattan, si \\(p=2\\) es Euclidiana).</p> </li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#63-pros-y-contras","title":"6.3. Pros y Contras","text":"Ventajas Desventajas Simplicidad: Es extremadamente f\u00e1cil de entender, explicar e implementar. Costo Computacional: Es lento en la fase de predicci\u00f3n con grandes datasets, ya que debe calcular distancias con todos los puntos cada vez. Sin Entrenamiento: La fase de entrenamiento es casi instant\u00e1nea (solo almacena datos), ideal si los datos cambian constantemente. Sensible a Outliers: Los valores at\u00edpicos o ruido pueden afectar significativamente la predicci\u00f3n si \\(k\\) es peque\u00f1o. Versatilidad: Sirve tanto para tareas de clasificaci\u00f3n como de regresi\u00f3n. Sensible a la Escala: Requiere estrictamente que los datos est\u00e9n normalizados o estandarizados. No Lineal: Se adapta bien a fronteras de decisi\u00f3n irregulares y complejas. Maldici\u00f3n de la Dimensionalidad: Su rendimiento decae dr\u00e1sticamente cuando hay muchas dimensiones (features) irrelevantes."},{"location":"aprendizaje-supervisado/06-knn/#64-ejemplo-en-python-con-scikit-learn","title":"6.4. Ejemplo en Python con <code>scikit-learn</code>","text":"<p>A continuaci\u00f3n, un ejemplo b\u00e1sico de clasificaci\u00f3n usando el dataset Iris y la clase <code>KNeighborsClassifier</code>.</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# 1. Cargar datos\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 2. Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 3. Escalar datos (CRUCIAL para KNN)\n# Como KNN usa distancias, las variables con rangos grandes dominar\u00e1n a las peque\u00f1as.\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 4. Instanciar el modelo (k=3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# 5. Entrenar (En KNN esto es solo almacenar los datos)\nknn.fit(X_train, y_train)\n\n# 6. Predecir y Evaluar\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"aprendizaje-supervisado/06-knn/#65-ejemplos-comunes-de-uso","title":"6.5. Ejemplos Comunes de Uso","text":"<ul> <li>Sistemas de Recomendaci\u00f3n: Sugerir productos, pel\u00edculas o m\u00fasica bas\u00e1ndose en las preferencias de usuarios \"vecinos\" con gustos similares (Filtrado Colaborativo).</li> <li>Reconocimiento de Patrones: Reconocimiento de caracteres escritos a mano (OCR) o clasificaci\u00f3n de im\u00e1genes simples bas\u00e1ndose en la similitud de p\u00edxeles.</li> <li>Detecci\u00f3n de Anomal\u00edas: Identificar fraudes bancarios o intrusiones en redes detectando eventos que est\u00e1n \"lejos\" de los grupos de vecinos normales.</li> <li>Imputaci\u00f3n de Datos Faltantes: Rellenar valores nulos en un dataset bas\u00e1ndose en los valores de los vecinos m\u00e1s cercanos (<code>KNNImputer</code>).</li> <li>Medicina: Clasificaci\u00f3n de pacientes con perfiles similares para predecir riesgos de enfermedades.</li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#66-aplicaciones-reales-de-knn","title":"6.6. Aplicaciones Reales de KNN","text":"<p>Aunque es un algoritmo simple, KNN se utiliza en sistemas donde la interpretabilidad y la simplicidad son clave:</p> <ul> <li>Sistemas de Recomendaci\u00f3n (Retail): Empresas como Amazon o Netflix utilizan variantes de algoritmos basados en vecindad para recomendar productos (\"Los usuarios que compraron X tambi\u00e9n compraron Y\").<ul> <li>Sistemas de recomendaci\u00f3n con KNN</li> </ul> </li> <li>Reconocimiento de Escritura a Mano: El servicio postal de EE.UU. (USPS) utiliz\u00f3 m\u00e9todos basados en vecindad para reconocer d\u00edgitos escritos a mano en c\u00f3digos postales.<ul> <li>Dataset MNIST y KNN</li> </ul> </li> <li>Detecci\u00f3n de Intrusiones (Ciberseguridad): Clasificar actividades de red como normales o sospechosas bas\u00e1ndose en su similitud con patrones de ataques conocidos.</li> <li>Bioinform\u00e1tica: Clasificaci\u00f3n de muestras de genes o prote\u00ednas bas\u00e1ndose en su similitud con perfiles conocidos para el diagn\u00f3stico de enfermedades.</li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#67-consideraciones-finales","title":"6.7. Consideraciones Finales","text":"<ol> <li> <p>Elecci\u00f3n de 'k' (Hiperpar\u00e1metro clave):</p> <ul> <li>Un \\(k\\) muy peque\u00f1o (ej. \\(k=1\\)) hace que el modelo sea muy sensible al ruido (Overfitting).</li> <li>Un \\(k\\) muy grande suaviza demasiado la frontera de decisi\u00f3n y puede incluir vecinos de otras clases lejanas (Underfitting).</li> <li>Se suele elegir un \\(k\\) impar para evitar empates en clasificaci\u00f3n binaria.</li> <li>El valor \u00f3ptimo se encuentra usualmente mediante validaci\u00f3n cruzada (t\u00e9cnica del codo o Elbow Method).</li> </ul> </li> <li> <p>Escalado de Caracter\u00edsticas:</p> <ul> <li>Dado que KNN se basa puramente en distancias, es obligatorio escalar las variables (usando <code>StandardScaler</code> o <code>MinMaxScaler</code>). Si una variable tiene una magnitud mucho mayor que otra (ej. Salario [1000-5000] vs Edad [20-60]), la variable de mayor magnitud dominar\u00e1 completamente el c\u00e1lculo de la distancia, haciendo que la otra sea irrelevante.</li> </ul> </li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/07-svm/","title":"\ud83e\udd16 Unidad 7. M\u00e1quinas de Vectores de Soporte (SVM)","text":"<p>Las M\u00e1quinas de Vectores de Soporte (Support Vector Machines o SVM) son un conjunto de algoritmos de aprendizaje supervisado potentes y vers\u00e1tiles, utilizados tanto para clasificaci\u00f3n (SVC) como para regresi\u00f3n (SVR). Su objetivo principal es encontrar el hiperplano \u00f3ptimo que mejor separe las clases en el espacio de caracter\u00edsticas.</p>"},{"location":"aprendizaje-supervisado/07-svm/#71-como-funciona-el-algoritmo","title":"7.1. \u00bfC\u00f3mo funciona el algoritmo?","text":"<p>La idea central de SVM es encontrar una l\u00ednea (en 2D), un plano (en 3D) o un hiperplano (en m\u00e1s dimensiones) que divida los datos en clases distintas. Pero no cualquier separaci\u00f3n sirve; SVM busca la separaci\u00f3n que tenga el mayor margen posible.</p> <ol> <li>Hiperplano: Es la frontera de decisi\u00f3n que separa las clases.</li> <li>Vectores de Soporte: Son los puntos de datos m\u00e1s cercanos al hiperplano. Estos puntos son los m\u00e1s \"dif\u00edciles\" de clasificar y son los \u00fanicos que importan para definir la posici\u00f3n del hiperplano.</li> <li>Margen: Es la distancia entre el hiperplano y los vectores de soporte m\u00e1s cercanos de cada clase. SVM intenta maximizar este margen para mejorar la generalizaci\u00f3n del modelo.</li> </ol>"},{"location":"aprendizaje-supervisado/07-svm/#72-explicacion-matematica-y-el-kernel-trick","title":"7.2. Explicaci\u00f3n Matem\u00e1tica y el \"Kernel Trick\"","text":"<p>Matem\u00e1ticamente, para un problema linealmente separable, buscamos los par\u00e1metros \\(w\\) (vector de pesos) y \\(b\\) (sesgo) tal que el hiperplano se defina como: \\(\\(w \\cdot x + b = 0\\)\\)</p> <p>El objetivo es minimizar \\(||w||\\) (lo que equivale a maximizar el margen) sujeto a que todas las muestras est\u00e9n correctamente clasificadas fuera del margen.</p>"},{"location":"aprendizaje-supervisado/07-svm/#el-truco-del-kernel-kernel-trick","title":"El Truco del Kernel (Kernel Trick)","text":"<p>Cuando los datos no son separables linealmente (ej. un c\u00edrculo dentro de otro), SVM utiliza una t\u00e9cnica llamada Kernel Trick. Esta t\u00e9cnica proyecta los datos originales a un espacio de mayor dimensi\u00f3n donde s\u00ed son linealmente separables, sin necesidad de calcular expl\u00edcitamente las coordenadas en ese espacio complejo (lo cual ser\u00eda computacionalmente costoso).</p> <p>Kernels comunes: *   Lineal: Para datos linealmente separables. *   Polin\u00f3mico: Mapea a espacios de dimensiones polin\u00f3micas. *   RBF (Radial Basis Function): El m\u00e1s popular. Mapea a un espacio de dimensi\u00f3n infinita. Es muy efectivo para fronteras de decisi\u00f3n complejas y curvas.</p>"},{"location":"aprendizaje-supervisado/07-svm/#73-pros-y-contras","title":"7.3. Pros y Contras","text":"Ventajas Desventajas Alta Dimensionalidad: Es muy efectivo en espacios con muchas dimensiones (incluso si hay m\u00e1s dimensiones que muestras). Grandes Datasets: No escala bien con datasets muy grandes (el tiempo de entrenamiento crece c\u00fabicamente). Eficiencia de Memoria: Solo usa un subconjunto de puntos de entrenamiento (los vectores de soporte) para definir el modelo. Ruido: Es sensible al ruido y a clases que se solapan mucho (si no se ajustan bien los par\u00e1metros). Versatilidad: Gracias a los Kernels, puede modelar relaciones lineales y no lineales complejas. Probabilidades: No proporciona estimaciones de probabilidad directas (se calculan mediante validaci\u00f3n cruzada costosa). Robustez: Maximizar el margen ayuda a reducir el riesgo de overfitting. Ajuste de Par\u00e1metros: Requiere un ajuste cuidadoso de hiperpar\u00e1metros clave (\\(C\\), \\(\\gamma\\), Kernel)."},{"location":"aprendizaje-supervisado/07-svm/#74-ejemplo-en-python-con-scikit-learn","title":"7.4. Ejemplo en Python con <code>scikit-learn</code>","text":"<p>Ejemplo de clasificaci\u00f3n usando <code>SVC</code> (Support Vector Classification) con un kernel RBF.</p> <pre><code>from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# 1. Cargar datos\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 2. Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 3. Escalar datos (IMPORTANTE para SVM)\n# SVM es sensible a la escala porque intenta maximizar la distancia (margen).\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 4. Instanciar el modelo\n# kernel='rbf' es el valor por defecto. C es el par\u00e1metro de regularizaci\u00f3n.\nclf = svm.SVC(kernel='rbf', C=1.0, gamma='scale')\n\n# 5. Entrenar\nclf.fit(X_train, y_train)\n\n# 6. Predecir y Evaluar\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"aprendizaje-supervisado/07-svm/#75-ejemplos-comunes-de-uso","title":"7.5. Ejemplos Comunes de Uso","text":"<ul> <li>Clasificaci\u00f3n de Texto: Categorizaci\u00f3n de noticias, detecci\u00f3n de spam y an\u00e1lisis de sentimientos. SVM maneja muy bien la alta dimensionalidad de los vectores de texto (Bag of Words).</li> <li>Reconocimiento de Im\u00e1genes: Clasificaci\u00f3n de im\u00e1genes, reconocimiento facial y reconocimiento de escritura a mano (OCR).</li> <li>Bioinform\u00e1tica: Clasificaci\u00f3n de prote\u00ednas y genes, donde los datos suelen tener muchas caracter\u00edsticas y pocas muestras.</li> <li>Detecci\u00f3n de Intrusos: Identificar actividad maliciosa en redes bas\u00e1ndose en patrones de tr\u00e1fico.</li> </ul>"},{"location":"aprendizaje-supervisado/07-svm/#76-aplicaciones-reales-de-svm","title":"7.6. Aplicaciones Reales de SVM","text":"<p>SVM ha sido uno de los algoritmos m\u00e1s exitosos antes del auge del Deep Learning y sigue siendo muy relevante:</p> <ul> <li>Clasificaci\u00f3n de Im\u00e1genes (Hist\u00f3rico): Antes de las redes neuronales convolucionales (CNN), SVM era el est\u00e1ndar para clasificaci\u00f3n de im\u00e1genes y detecci\u00f3n de objetos (ej. detecci\u00f3n de peatones).</li> <li>Bioinform\u00e1tica (Clasificaci\u00f3n de Prote\u00ednas): Se utiliza para clasificar prote\u00ednas en familias funcionales y predecir la estructura secundaria de las prote\u00ednas, dado que maneja muy bien la alta dimensionalidad de los datos gen\u00f3micos.<ul> <li>SVM en Bioinform\u00e1tica</li> </ul> </li> <li>Reconocimiento de Escritura: SVM ha demostrado ser muy eficaz en el reconocimiento de caracteres manuscritos (OCR), compitiendo con redes neuronales en datasets como MNIST.</li> <li>Geolog\u00eda y Miner\u00eda: Clasificaci\u00f3n de tipos de suelo y rocas a partir de datos s\u00edsmicos o im\u00e1genes satelitales.</li> </ul>"},{"location":"aprendizaje-supervisado/07-svm/#77-consideraciones-finales","title":"7.7. Consideraciones Finales","text":"<ol> <li> <p>Par\u00e1metro C (Regularizaci\u00f3n):</p> <ul> <li>Controla el equilibrio entre tener un margen amplio y clasificar correctamente los puntos de entrenamiento.</li> <li>C alto: Intenta clasificar todo correctamente (riesgo de Overfitting, margen estrecho).</li> <li>C bajo: Permite algunos errores para obtener un margen m\u00e1s amplio (mejor generalizaci\u00f3n, margen suave).</li> </ul> </li> <li> <p>Par\u00e1metro Gamma (\\(\\gamma\\)) (Solo para kernels RBF/Poly):</p> <ul> <li>Define qu\u00e9 tan lejos llega la influencia de un solo ejemplo de entrenamiento.</li> <li>Gamma alto: Solo los puntos muy cercanos influyen. Puede llevar a fronteras de decisi\u00f3n muy ajustadas e irregulares (Overfitting).</li> <li>Gamma bajo: La influencia llega lejos. La frontera de decisi\u00f3n es m\u00e1s suave (Underfitting si es muy bajo).</li> </ul> </li> <li> <p>Escalado: Al igual que KNN, SVM se basa en distancias. Es cr\u00edtico estandarizar los datos antes de entrenar.</p> </li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/","title":"\ud83e\udd16 Unidad 8. Algoritmos de Ensamblado (Ensemble Learning)","text":"<p>Los Algoritmos de Ensamblado (Ensemble Methods) son una t\u00e9cnica de Machine Learning que combina las predicciones de m\u00faltiples modelos base (conocidos como weak learners o aprendices d\u00e9biles) para construir un modelo final m\u00e1s robusto y preciso (strong learner).</p> <p>La intuici\u00f3n detr\u00e1s de esto es la \"Sabidur\u00eda de las Masas\": as\u00ed como la opini\u00f3n colectiva de un grupo de expertos suele ser mejor que la de un solo experto, un grupo de modelos predictivos suele superar el rendimiento de un modelo individual.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#81-conceptos-clave-y-categorias","title":"8.1. Conceptos Clave y Categor\u00edas","text":"<p>El objetivo principal es reducir el sesgo (bias) o la varianza (variance), o ambos. Los m\u00e9todos de ensamblado se dividen principalmente en tres categor\u00edas seg\u00fan c\u00f3mo combinan los modelos:</p> <ol> <li>Voting (Votaci\u00f3n): Se entrenan varios modelos diferentes (ej. KNN, SVM, \u00c1rbol) y se \"vota\" para decidir la clase final.</li> <li>Bagging (Bootstrap Aggregating): Se entrena el mismo algoritmo muchas veces en paralelo, pero con diferentes subconjuntos aleatorios de los datos de entrenamiento. Su objetivo es reducir la varianza (evitar overfitting). El ejemplo cl\u00e1sico es Random Forest.</li> <li>Boosting: Se entrena el mismo algoritmo de forma secuencial. Cada nuevo modelo intenta corregir los errores cometidos por el modelo anterior. Su objetivo es reducir el sesgo (evitar underfitting). Ejemplos: AdaBoost, XGBoost.</li> </ol>"},{"location":"aprendizaje-supervisado/08-ensamblado/#82-voting-classifiers-votacion","title":"8.2. Voting Classifiers (Votaci\u00f3n)","text":"<p>Es la forma m\u00e1s simple de ensamblado. Consiste en agregar las predicciones de clasificadores totalmente diferentes.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#tipos-de-votacion","title":"Tipos de Votaci\u00f3n","text":"<ul> <li>Hard Voting (Votaci\u00f3n Dura): Cada clasificador vota por una clase. La clase con la mayor\u00eda de votos gana (moda).</li> <li>Soft Voting (Votaci\u00f3n Suave): Si los clasificadores pueden estimar probabilidades (tienen m\u00e9todo <code>predict_proba</code>), se promedian las probabilidades de cada clase. La clase con el promedio de probabilidad m\u00e1s alto gana. El Soft Voting suele funcionar mejor porque da m\u00e1s peso a los votos con \"alta confianza\".</li> </ul>"},{"location":"aprendizaje-supervisado/08-ensamblado/#ejemplo-en-python-voting","title":"Ejemplo en Python (Voting)","text":"<pre><code>from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Datos de ejemplo\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Modelos individuales\nlog_clf = LogisticRegression(random_state=42)\nrnd_clf = DecisionTreeClassifier(random_state=42)\nsvm_clf = SVC(probability=True, random_state=42) # probability=True necesario para Soft Voting\n\n# Ensamblado por Votaci\u00f3n (Soft)\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='soft'\n)\n\n# Entrenamiento y Comparaci\u00f3n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(f\"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/08-ensamblado/#83-bagging-y-random-forest","title":"8.3. Bagging y Random Forest","text":"<p>Bagging (Bootstrap Aggregating) implica entrenar el mismo algoritmo en diferentes subconjuntos aleatorios del dataset de entrenamiento. *   Bootstrap: El muestreo se hace con reemplazo (una misma muestra puede aparecer varias veces en el mismo subconjunto). *   Pasting: El muestreo se hace sin reemplazo.</p> <p>Una vez entrenados, los modelos agregan sus predicciones (moda para clasificaci\u00f3n, promedio para regresi\u00f3n).</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#random-forest-bosques-aleatorios","title":"Random Forest (Bosques Aleatorios)","text":"<p>Es una implementaci\u00f3n espec\u00edfica y optimizada de Bagging usando \u00c1rboles de Decisi\u00f3n. Introduce aleatoriedad extra: al dividir un nodo en el \u00e1rbol, no busca la mejor caracter\u00edstica de todas las disponibles, sino la mejor caracter\u00edstica dentro de un subconjunto aleatorio de caracter\u00edsticas. Esto hace que los \u00e1rboles sean m\u00e1s diversos (descorrelacionados), lo que reduce dr\u00e1sticamente la varianza.</p> <p>Hiperpar\u00e1metros Clave: *   <code>n_estimators</code>: N\u00famero de \u00e1rboles (m\u00e1s es mejor, pero m\u00e1s lento). *   <code>max_features</code>: N\u00famero m\u00e1ximo de caracter\u00edsticas a considerar en cada divisi\u00f3n. *   <code>bootstrap</code>: Si usar muestreo con reemplazo (True por defecto). *   <code>n_jobs</code>: N\u00famero de n\u00facleos de CPU a usar (-1 para todos).</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#ejemplo-en-python-random-forest","title":"Ejemplo en Python (Random Forest)","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Instanciar Random Forest\n# 500 \u00e1rboles, usando todos los n\u00facleos de CPU\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n\n# Importancia de Caracter\u00edsticas\n# Random Forest permite ver qu\u00e9 variables son m\u00e1s \u00fatiles\nfor name, score in zip([\"Feature 1\", \"Feature 2\"], rnd_clf.feature_importances_):\n    print(f\"{name}: {score}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/08-ensamblado/#84-boosting-impulso","title":"8.4. Boosting (Impulso)","text":"<p>El Boosting entrena predictores secuencialmente, cada uno intentando corregir a su predecesor.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#841-adaboost-adaptive-boosting","title":"8.4.1. AdaBoost (Adaptive Boosting)","text":"<p>El algoritmo presta m\u00e1s atenci\u00f3n a las instancias de entrenamiento que el predecesor clasific\u00f3 incorrectamente. 1.  Entrena un clasificador base. 2.  Aumenta el peso relativo de las instancias mal clasificadas. 3.  Entrena un segundo clasificador con los pesos actualizados. 4.  Repite el proceso.</p> <p>Hiperpar\u00e1metros Clave: *   <code>n_estimators</code>: N\u00famero de iteraciones. *   <code>learning_rate</code>: Cu\u00e1nto contribuye cada modelo. Un valor bajo requiere m\u00e1s estimadores.</p> <p>Ejemplo Python (AdaBoost): <pre><code>from sklearn.ensemble import AdaBoostClassifier\n\n# AdaBoost usando \u00c1rboles de Decisi\u00f3n muy simples (stumps)\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42\n)\nada_clf.fit(X_train, y_train)\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#842-gradient-boosting-machine-gbm","title":"8.4.2. Gradient Boosting Machine (GBM)","text":"<p>En lugar de ajustar los pesos de las instancias, GBM intenta ajustar el nuevo predictor a los errores residuales (la diferencia entre el valor real y el predicho) del predictor anterior.</p> <p>Ejemplo Python (GradientBoosting de sklearn): <pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\ngbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\ngbrt.fit(X_train, y_train)\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#843-xgboost-extreme-gradient-boosting","title":"8.4.3. XGBoost (Extreme Gradient Boosting)","text":"<p>Es una versi\u00f3n optimizada de Gradient Boosting dise\u00f1ada para ser altamente eficiente, flexible y port\u00e1til. Es el algoritmo dominante en competiciones de Machine Learning (Kaggle). *   Regularizaci\u00f3n: Incluye regularizaci\u00f3n L1 y L2 para evitar overfitting. *   Paralelizaci\u00f3n: Construcci\u00f3n de \u00e1rboles en paralelo. *   Manejo de nulos: Aprende autom\u00e1ticamente la mejor direcci\u00f3n para valores faltantes.</p> <p>Hiperpar\u00e1metros Clave: *   <code>eta</code> (learning_rate): Paso de reducci\u00f3n de pesos para prevenir overfitting. *   <code>max_depth</code>: Profundidad m\u00e1xima del \u00e1rbol. *   <code>subsample</code>: Ratio de muestras de entrenamiento usadas. *   <code>colsample_bytree</code>: Ratio de columnas usadas por \u00e1rbol.</p> <p>Ejemplo Python (XGBoost): <pre><code>import xgboost as xgb\n\n# XGBoost tiene su propia estructura de datos optimizada (DMatrix), pero es compatible con sklearn\nxgb_clf = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=3,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    n_jobs=-1,\n    random_state=42\n)\n\nxgb_clf.fit(X_train, y_train)\nprint(f\"XGBoost Accuracy: {accuracy_score(y_test, xgb_clf.predict(X_test)):.4f}\")\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#844-lightgbm-light-gradient-boosting-machine","title":"8.4.4. LightGBM (Light Gradient Boosting Machine)","text":"<p>Desarrollado por Microsoft. A diferencia de otros que crecen el \u00e1rbol por niveles (level-wise), LightGBM crece por hojas (leaf-wise). Elige la hoja con mayor p\u00e9rdida para crecer. *   Ventajas: Mucho m\u00e1s r\u00e1pido que XGBoost en grandes datasets y consume menos memoria. *   Desventajas: Puede hacer overfitting f\u00e1cilmente en datasets peque\u00f1os (&lt; 10,000 filas).</p> <p>Hiperpar\u00e1metros Clave: *   <code>num_leaves</code>: Par\u00e1metro principal para controlar la complejidad (en lugar de max_depth). *   <code>min_data_in_leaf</code>: Importante para evitar overfitting.</p> <p>Ejemplo Python (LightGBM): <pre><code>import lightgbm as lgb\n\nlgb_clf = lgb.LGBMClassifier(\n    num_leaves=31,\n    learning_rate=0.05,\n    n_estimators=100,\n    random_state=42\n)\n\nlgb_clf.fit(X_train, y_train)\nprint(f\"LightGBM Accuracy: {accuracy_score(y_test, lgb_clf.predict(X_test)):.4f}\")\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#85-resumen-comparativo","title":"8.5. Resumen Comparativo","text":"T\u00e9cnica Algoritmo Principal Estrategia Objetivo Paralelizable Voting VotingClassifier Promedio de modelos distintos Robustez general S\u00ed Bagging Random Forest Modelos iguales, datos aleatorios (independientes) Reducir Varianza S\u00ed (Muy r\u00e1pido) Boosting AdaBoost, XGBoost Modelos iguales, secuenciales (dependientes) Reducir Sesgo No (Secuencial)* <p>* Nota: XGBoost y LightGBM paralelizan la construcci\u00f3n dentro del \u00e1rbol, pero los \u00e1rboles se crean secuencialmente.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#86-aplicaciones-reales-de-algoritmos-de-ensamblado","title":"8.6. Aplicaciones Reales de Algoritmos de Ensamblado","text":"<p>Los m\u00e9todos de ensamblado dominan actualmente las competiciones de ciencia de datos y las aplicaciones industriales en datos estructurados:</p> <ul> <li>Detecci\u00f3n de Fraude (Banca): Algoritmos como XGBoost y Random Forest son el est\u00e1ndar en la industria financiera para detectar transacciones fraudulentas en tiempo real debido a su alta precisi\u00f3n y velocidad.<ul> <li>Detecci\u00f3n de fraude con XGBoost</li> </ul> </li> <li>Diagn\u00f3stico M\u00e9dico: Random Forest se utiliza para diagnosticar enfermedades (como la retinopat\u00eda diab\u00e9tica) analizando m\u00faltiples variables de pacientes, ya que proporciona una medida de qu\u00e9 s\u00edntomas son m\u00e1s relevantes.</li> <li>Ranking de B\u00fasqueda (Search Engines): Motores de b\u00fasqueda utilizan Gradient Boosting para ordenar los resultados de b\u00fasqueda (Learning to Rank), optimizando la relevancia para el usuario.<ul> <li>Learning to Rank con LightGBM</li> </ul> </li> <li>Predicci\u00f3n de Demanda (Retail): Cadenas de suministro usan estos modelos para predecir la demanda futura de productos, optimizando el inventario y reduciendo desperdicios.</li> </ul>"},{"location":"aprendizaje-supervisado/08-ensamblado/#87-consideraciones-finales","title":"8.7. Consideraciones Finales","text":"<ol> <li>Random Forest es una excelente \"primera opci\u00f3n\". Es robusto, requiere poco ajuste de hiperpar\u00e1metros y nos da la importancia de las caracter\u00edsticas.</li> <li>XGBoost / LightGBM suelen ofrecer el mejor rendimiento (Accuracy) en datos tabulares estructurados, pero requieren m\u00e1s ajuste de hiperpar\u00e1metros y cuidado con el overfitting.</li> <li>Escalado: Los algoritmos basados en \u00e1rboles (Random Forest, Boosting) NO requieren escalado de caracter\u00edsticas (StandardScaler), lo cual es una gran ventaja pr\u00e1ctica.</li> <li>Interpretabilidad: Los modelos de ensamblado son \"Cajas Negras\". Perdemos la interpretabilidad simple de un solo \u00c1rbol de Decisi\u00f3n o una Regresi\u00f3n Lineal, aunque podemos usar la \"Importancia de Caracter\u00edsticas\" para entender qu\u00e9 variables pesan m\u00e1s.</li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/","title":"\ud83e\udde0 Deep Learning","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Deep Learning! \ud83c\udf89</p>"},{"location":"deep-learning/#que-es-el-deep-learning","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Deep Learning?","text":"<p>El Deep Learning (Aprendizaje Profundo) es un subcampo del Machine Learning que utiliza redes neuronales artificiales con m\u00faltiples capas (de ah\u00ed el t\u00e9rmino \"profundo\") para aprender representaciones jer\u00e1rquicas de los datos.</p> <p>A diferencia de los algoritmos tradicionales de ML que requieren ingenier\u00eda de caracter\u00edsticas manual, el Deep Learning puede aprender autom\u00e1ticamente las caracter\u00edsticas relevantes directamente de los datos en bruto.</p>"},{"location":"deep-learning/#arquitecturas-principales","title":"\ud83e\udde0 Arquitecturas Principales","text":"<ol> <li> <p>Redes Neuronales Artificiales (ANN):    La base del Deep Learning, inspirada en el cerebro humano.    \ud83d\udccd Uso: Clasificaci\u00f3n, regresi\u00f3n.</p> </li> <li> <p>Redes Neuronales Convolucionales (CNN):    Especializadas en procesar datos con estructura de cuadr\u00edcula (im\u00e1genes).    \ud83d\udccd Uso: Visi\u00f3n por computadora, clasificaci\u00f3n de im\u00e1genes, detecci\u00f3n de objetos.</p> </li> <li> <p>Redes Neuronales Recurrentes (RNN):    Dise\u00f1adas para datos secuenciales con memoria de estados anteriores.    \ud83d\udccd Uso: Series temporales, texto.</p> </li> <li> <p>LSTM y GRU:    Variantes de RNN que solucionan el problema del gradiente desvaneciente.    \ud83d\udccd Uso: Secuencias largas, traducci\u00f3n.</p> </li> <li> <p>Transformers:    Arquitectura basada en mecanismos de atenci\u00f3n, revolucion\u00f3 el NLP y m\u00e1s.    \ud83d\udccd Uso: GPT, BERT, modelos de lenguaje grandes (LLMs).</p> </li> <li> <p>Autoencoders:    Redes que aprenden representaciones comprimidas de los datos.    \ud83d\udccd Uso: Reducci\u00f3n de dimensionalidad, generaci\u00f3n.</p> </li> <li> <p>GANs (Redes Generativas Adversarias):    Dos redes que compiten para generar datos realistas.    \ud83d\udccd Uso: Generaci\u00f3n de im\u00e1genes, arte, deepfakes.</p> </li> </ol>"},{"location":"deep-learning/#conceptos-fundamentales","title":"\u2699\ufe0f Conceptos Fundamentales","text":"<ul> <li>Neurona artificial: Unidad b\u00e1sica que aplica pesos, bias y funci\u00f3n de activaci\u00f3n</li> <li>Capas: Entrada, ocultas y salida</li> <li>Funciones de activaci\u00f3n: ReLU, Sigmoid, Tanh, Softmax</li> <li>Backpropagation: Algoritmo para calcular gradientes</li> <li>Optimizadores: SGD, Adam, RMSprop</li> <li>Regularizaci\u00f3n: Dropout, L1/L2, Batch Normalization</li> <li>Transfer Learning: Reutilizar modelos preentrenados</li> </ul>"},{"location":"deep-learning/#frameworks-populares","title":"\ud83d\udd0d Frameworks Populares","text":"<ul> <li>TensorFlow - Framework de Google</li> <li>Keras - API de alto nivel (integrada en TensorFlow)</li> <li>PyTorch - Framework de Meta, muy usado en investigaci\u00f3n</li> <li>JAX - Computaci\u00f3n num\u00e9rica de alto rendimiento</li> </ul>"},{"location":"deep-learning/#contenido-en-construccion","title":"\ud83d\udea7 Contenido en construcci\u00f3n","text":"<p>Esta secci\u00f3n est\u00e1 siendo desarrollada. Pr\u00f3ximamente encontrar\u00e1s:</p> <ul> <li>[ ] Fundamentos de redes neuronales</li> <li>[ ] Redes convolucionales (CNN)</li> <li>[ ] Redes recurrentes (RNN, LSTM)</li> <li>[ ] Transformers y atenci\u00f3n</li> <li>[ ] Ejemplos pr\u00e1cticos con TensorFlow/PyTorch</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/","title":"\ud83d\udcac Procesamiento de Lenguaje Natural (NLP)","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Procesamiento de Lenguaje Natural! \ud83c\udf89</p>"},{"location":"procesamiento-lenguaje-natural/#que-es-el-procesamiento-de-lenguaje-natural","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Procesamiento de Lenguaje Natural?","text":"<p>El Procesamiento de Lenguaje Natural (NLP), del ingl\u00e9s Natural Language Processing, es una rama de la Inteligencia Artificial que se centra en la interacci\u00f3n entre las computadoras y el lenguaje humano.</p> <p>El objetivo del NLP es permitir que las m\u00e1quinas comprendan, interpreten y generen lenguaje humano de manera \u00fatil y significativa.</p>"},{"location":"procesamiento-lenguaje-natural/#aplicaciones-del-nlp","title":"\ud83e\udde0 Aplicaciones del NLP","text":"<ol> <li> <p>An\u00e1lisis de Sentimientos:    Determinar si un texto expresa una opini\u00f3n positiva, negativa o neutral.    \ud83d\udccd Ejemplo: Analizar rese\u00f1as de productos.</p> </li> <li> <p>Chatbots y Asistentes Virtuales:    Sistemas que interact\u00faan con usuarios mediante lenguaje natural.    \ud83d\udccd Ejemplo: ChatGPT, Alexa, Siri.</p> </li> <li> <p>Traducci\u00f3n Autom\u00e1tica:    Traducir texto de un idioma a otro.    \ud83d\udccd Ejemplo: Google Translate.</p> </li> <li> <p>Extracci\u00f3n de Informaci\u00f3n:    Identificar entidades y relaciones en textos.    \ud83d\udccd Ejemplo: Reconocimiento de nombres, fechas, lugares.</p> </li> <li> <p>Generaci\u00f3n de Texto:    Crear texto coherente y relevante autom\u00e1ticamente.    \ud83d\udccd Ejemplo: Generaci\u00f3n de res\u00famenes, escritura asistida.</p> </li> </ol>"},{"location":"procesamiento-lenguaje-natural/#conceptos-fundamentales","title":"\u2699\ufe0f Conceptos Fundamentales","text":"<ul> <li>Tokenizaci\u00f3n: Divisi\u00f3n del texto en unidades (palabras, subpalabras, caracteres)</li> <li>Stemming y Lematizaci\u00f3n: Reducci\u00f3n de palabras a su ra\u00edz</li> <li>Stop Words: Palabras comunes que suelen filtrarse</li> <li>Bag of Words (BoW): Representaci\u00f3n de texto como frecuencia de palabras</li> <li>TF-IDF: Medida de importancia de palabras en documentos</li> <li>Word Embeddings: Representaciones vectoriales de palabras (Word2Vec, GloVe)</li> <li>Transformers: Arquitectura base de modelos modernos (BERT, GPT)</li> </ul>"},{"location":"procesamiento-lenguaje-natural/#bibliotecas-populares","title":"\ud83d\udd0d Bibliotecas Populares","text":"<ul> <li>NLTK - Natural Language Toolkit</li> <li>spaCy - Procesamiento industrial de NLP</li> <li>Hugging Face Transformers - Modelos preentrenados</li> <li>Gensim - Modelado de t\u00f3picos y word embeddings</li> </ul>"},{"location":"procesamiento-lenguaje-natural/#contenido-en-construccion","title":"\ud83d\udea7 Contenido en construcci\u00f3n","text":"<p>Esta secci\u00f3n est\u00e1 siendo desarrollada. Pr\u00f3ximamente encontrar\u00e1s:</p> <ul> <li>[ ] Preprocesamiento de texto</li> <li>[ ] Representaci\u00f3n de texto</li> <li>[ ] Modelos de clasificaci\u00f3n de texto</li> <li>[ ] Transformers y modelos de lenguaje</li> <li>[ ] Ejemplos pr\u00e1cticos con Python</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"}]}