{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd16 Programaci\u00f3n en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 Este sitio web recopila documentaci\u00f3n y recursos sobre diferentes \u00e1reas de la Inteligencia Artificial (IA) y el Machine Learning (ML).</p>"},{"location":"#contenidos","title":"\ud83d\udcda Contenidos","text":"<p>Explora las diferentes secciones disponibles:</p>"},{"location":"#conocimientos-basicos","title":"\ud83d\udcda Conocimientos B\u00e1sicos","text":"<p>Ad\u00e9ntrate en el universo de la programaci\u00f3n con Python, empezando desde lo m\u00e1s b\u00e1sico y llegando a conocer las librer\u00edas m\u00e1s importantes de la Inteligencia Artificial.</p> <ul> <li>Variables y Tipos de datos</li> <li>Operadores</li> <li>Estructuras de control</li> <li>Estructuras de datos</li> <li>Funciones, m\u00f3dulos y paquetes</li> <li>Manejo de archivos y excepciones</li> <li>POO</li> <li>Librer\u00edas b\u00e1sicas como Numpy, Pandas, Matplotlib, Seaborn y Scikit-Learn</li> </ul>"},{"location":"#aprendizaje-supervisado","title":"\ud83c\udfaf Aprendizaje Supervisado","text":"<p>Aprende sobre algoritmos que utilizan datos etiquetados para realizar predicciones. Incluye t\u00e9cnicas de clasificaci\u00f3n y regresi\u00f3n como:</p> <ul> <li>Machine Learning para an\u00e1lisis de datos</li> <li>\u00c1rboles de decisi\u00f3n</li> <li>Naive Bayes</li> <li>K-Nearest Neighbors (KNN)</li> <li>Support Vector Machines (SVM)</li> <li>Algoritmos de ensamblado</li> </ul>"},{"location":"#aprendizaje-no-supervisado","title":"\ud83d\udd0d Aprendizaje No Supervisado","text":"<p>Descubre t\u00e9cnicas para encontrar patrones en datos no etiquetados:</p> <ul> <li>Clustering (K-Means, DBSCAN, Jer\u00e1rquico)</li> <li>Reducci\u00f3n de dimensionalidad (PCA, t-SNE)</li> <li>An\u00e1lisis de asociaci\u00f3n</li> </ul>"},{"location":"#procesamiento-de-lenguaje-natural","title":"\ud83d\udcac Procesamiento de Lenguaje Natural","text":"<p>Explora c\u00f3mo las m\u00e1quinas comprenden y generan lenguaje humano:</p> <ul> <li>Tokenizaci\u00f3n y preprocesamiento de texto</li> <li>Modelos de lenguaje</li> <li>An\u00e1lisis de sentimientos</li> <li>Transformers y atenci\u00f3n</li> </ul>"},{"location":"#deep-learning","title":"\ud83e\udde0 Deep Learning","text":"<p>Profundiza en redes neuronales y arquitecturas avanzadas:</p> <ul> <li>Redes neuronales artificiales</li> <li>Redes convolucionales (CNN)</li> <li>Redes recurrentes (RNN, LSTM)</li> <li>Arquitecturas modernas</li> </ul>"},{"location":"#sobre-este-sitio","title":"\ud83c\udf93 Sobre este sitio","text":"<p>Este recurso est\u00e1 dise\u00f1ado como material de apoyo para el aprendizaje de t\u00e9cnicas de Inteligencia Artificial y Machine Learning, con ejemplos pr\u00e1cticos en Python.</p> <p>\ud83d\udcc5 \u00daltima actualizaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/","title":"\ud83d\udd0d Aprendizaje No Supervisado","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Aprendizaje No Supervisado! \ud83c\udf89</p>"},{"location":"aprendizaje-no-supervisado/#que-es-el-aprendizaje-no-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje No Supervisado?","text":"<p>El aprendizaje no supervisado es un tipo de aprendizaje autom\u00e1tico en el que el modelo trabaja con datos sin etiquetas. El objetivo es descubrir patrones ocultos, estructuras o agrupaciones en los datos sin tener una respuesta correcta predefinida.</p> <p>A diferencia del aprendizaje supervisado, aqu\u00ed no hay una variable objetivo que predecir; el algoritmo debe encontrar por s\u00ed mismo las relaciones entre los datos.</p>"},{"location":"aprendizaje-no-supervisado/#tipos-de-problemas-no-supervisados","title":"\ud83e\udde0 Tipos de Problemas No Supervisados","text":"<ol> <li> <p>Clustering (Agrupamiento):    Agrupa datos similares en clusters o grupos.    \ud83d\udccd Ejemplo: Segmentaci\u00f3n de clientes seg\u00fan su comportamiento de compra.</p> </li> <li> <p>Reducci\u00f3n de Dimensionalidad:    Reduce el n\u00famero de variables manteniendo la informaci\u00f3n m\u00e1s relevante.    \ud83d\udccd Ejemplo: Visualizar datos de alta dimensi\u00f3n en 2D o 3D.</p> </li> <li> <p>Detecci\u00f3n de Anomal\u00edas:    Identifica datos que se desv\u00edan significativamente del patr\u00f3n normal.    \ud83d\udccd Ejemplo: Detectar transacciones fraudulentas.</p> </li> <li> <p>Reglas de Asociaci\u00f3n:    Encuentra relaciones entre variables en grandes conjuntos de datos.    \ud83d\udccd Ejemplo: An\u00e1lisis de cesta de compra (\u00bfqu\u00e9 productos se compran juntos?).</p> </li> </ol>"},{"location":"aprendizaje-no-supervisado/#algoritmos-comunes","title":"\ud83d\udd0d Algoritmos Comunes","text":"<ul> <li>K-Means</li> <li>DBSCAN</li> <li>Clustering Jer\u00e1rquico</li> <li>PCA (An\u00e1lisis de Componentes Principales)</li> <li>t-SNE</li> <li>Isolation Forest</li> <li>Apriori</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/","title":"\ud83d\udd0d Unidad 1. Fundamentos del Aprendizaje No Supervisado","text":"<p>Esta unidad introduce los conceptos fundamentales del Aprendizaje No Supervisado, sus diferencias con el aprendizaje supervisado, las bibliotecas Python necesarias, el flujo de trabajo t\u00edpico, y las metodolog\u00edas esenciales para preparar datos y evaluar resultados en ausencia de etiquetas.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#11-que-es-el-aprendizaje-no-supervisado","title":"1.1. \u00bfQu\u00e9 es el Aprendizaje No Supervisado?","text":"<p>El Aprendizaje No Supervisado es una rama del Machine Learning donde los algoritmos trabajan con datos sin etiquetas. A diferencia del aprendizaje supervisado, no existe una \"respuesta correcta\" predefinida que gu\u00ede el entrenamiento.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#definiciones-clave","title":"Definiciones Clave","text":"<ul> <li> <p>Definici\u00f3n formal: \"El aprendizaje no supervisado es el entrenamiento de un modelo usando informaci\u00f3n que no est\u00e1 clasificada ni etiquetada, permitiendo al algoritmo actuar sobre esa informaci\u00f3n sin gu\u00eda.\"</p> </li> <li> <p>Objetivo principal: Descubrir estructuras ocultas, patrones o agrupaciones inherentes en los datos que no son evidentes a simple vista.</p> </li> <li> <p>Analog\u00eda: Imagina que te dan una caja con miles de fotograf\u00edas sin ninguna descripci\u00f3n. El aprendizaje no supervisado ser\u00eda como organizarlas autom\u00e1ticamente en grupos (paisajes, retratos, animales, etc.) bas\u00e1ndose \u00fanicamente en las similitudes visuales entre ellas.</p> </li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#diferencias-con-el-aprendizaje-supervisado","title":"Diferencias con el Aprendizaje Supervisado","text":"Aspecto Supervisado No Supervisado Datos Etiquetados (X, y) Sin etiquetas (solo X) Objetivo Predecir una variable objetivo Descubrir estructura en los datos Evaluaci\u00f3n M\u00e9tricas claras (accuracy, F1, MSE) M\u00e9tricas indirectas (silueta, inercia) Ejemplos Clasificaci\u00f3n, Regresi\u00f3n Clustering, Reducci\u00f3n de dimensionalidad Feedback Conocemos si la predicci\u00f3n es correcta No hay \"respuesta correcta\""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#tipos-de-problemas-no-supervisados","title":"Tipos de Problemas No Supervisados","text":"<p>El aprendizaje no supervisado abarca principalmente cuatro tipos de problemas:</p> <ol> <li>Clustering (Agrupamiento):</li> <li>Objetivo: Dividir los datos en grupos (clusters) donde los elementos dentro de un grupo son similares entre s\u00ed y diferentes a los de otros grupos.</li> <li>Algoritmos: K-Means, DBSCAN, Clustering Jer\u00e1rquico, OPTICS, Mean Shift.</li> <li> <p>Aplicaci\u00f3n: Segmentaci\u00f3n de clientes, agrupaci\u00f3n de documentos.</p> </li> <li> <p>Reducci\u00f3n de Dimensionalidad:</p> </li> <li>Objetivo: Reducir el n\u00famero de variables (features) manteniendo la mayor cantidad de informaci\u00f3n posible.</li> <li>Algoritmos: PCA, t-SNE, UMAP, LDA, Autoencoders.</li> <li> <p>Aplicaci\u00f3n: Visualizaci\u00f3n de datos, compresi\u00f3n, preprocesamiento.</p> </li> <li> <p>Detecci\u00f3n de Anomal\u00edas:</p> </li> <li>Objetivo: Identificar puntos de datos que se desv\u00edan significativamente del comportamiento normal.</li> <li>Algoritmos: Isolation Forest, One-Class SVM, LOF (Local Outlier Factor).</li> <li> <p>Aplicaci\u00f3n: Detecci\u00f3n de fraudes, mantenimiento predictivo.</p> </li> <li> <p>Reglas de Asociaci\u00f3n:</p> </li> <li>Objetivo: Descubrir relaciones interesantes entre variables en grandes conjuntos de datos.</li> <li>Algoritmos: Apriori, FP-Growth, Eclat.</li> <li>Aplicaci\u00f3n: An\u00e1lisis de cesta de compra, sistemas de recomendaci\u00f3n.</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#12-flujo-de-trabajo-del-aprendizaje-no-supervisado","title":"1.2. Flujo de Trabajo del Aprendizaje No Supervisado","text":"<p>El proceso general para aplicar t\u00e9cnicas no supervisadas sigue estos pasos:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Definici\u00f3n  \u2502\u2500\u2500\u2500\u25b6\u2502  2. Preparaci\u00f3n \u2502\u2500\u2500\u2500\u25b6\u2502  3. Selecci\u00f3n   \u2502\n\u2502  del Problema   \u2502    \u2502    de Datos     \u2502    \u2502  del Algoritmo  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                      \u2502\n                                                      \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 6. Aplicaci\u00f3n   \u2502\u25c0\u2500\u2500\u2500\u2502 5. Validaci\u00f3n   \u2502\u25c0\u2500\u2500\u2500\u2502 4. Entrenamiento\u2502\n\u2502 e Interpretaci\u00f3n\u2502    \u2502   y Evaluaci\u00f3n  \u2502    \u2502    del Modelo   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#1-definicion-del-problema","title":"1. Definici\u00f3n del Problema","text":"<ul> <li>\u00bfQu\u00e9 queremos descubrir? \u00bfGrupos de clientes? \u00bfPatrones an\u00f3malos?</li> <li>\u00bfCu\u00e1ntas dimensiones tienen los datos? \u00bfSon visualizables?</li> <li>\u00bfHay conocimiento del dominio que pueda guiar la interpretaci\u00f3n?</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#2-preparacion-de-datos","title":"2. Preparaci\u00f3n de Datos","text":"<ul> <li>Limpieza: Manejo de valores faltantes y outliers.</li> <li>Escalado: Crucial para algoritmos basados en distancias (K-Means, DBSCAN).</li> <li>Selecci\u00f3n de caracter\u00edsticas: Eliminar features irrelevantes o redundantes.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#3-seleccion-del-algoritmo","title":"3. Selecci\u00f3n del Algoritmo","text":"<ul> <li>Depende del tipo de problema y las caracter\u00edsticas de los datos.</li> <li>Considerar: tama\u00f1o del dataset, n\u00famero de clusters esperado, forma de los clusters.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#4-entrenamiento-del-modelo","title":"4. Entrenamiento del Modelo","text":"<ul> <li>No hay etiquetas, por lo que no hay conjunto de \"validaci\u00f3n\" tradicional.</li> <li>Se ajustan hiperpar\u00e1metros mediante t\u00e9cnicas espec\u00edficas (m\u00e9todo del codo, silueta).</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#5-validacion-y-evaluacion","title":"5. Validaci\u00f3n y Evaluaci\u00f3n","text":"<ul> <li>M\u00e9tricas internas: Silueta, Inercia, Davies-Bouldin.</li> <li>Validaci\u00f3n visual: Gr\u00e1ficos de clusters, dendrogramas.</li> <li>Validaci\u00f3n externa (si hay etiquetas disponibles): NMI, ARI.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#6-aplicacion-e-interpretacion","title":"6. Aplicaci\u00f3n e Interpretaci\u00f3n","text":"<ul> <li>Asignar significado a los clusters descubiertos.</li> <li>Integrar resultados en procesos de negocio o an\u00e1lisis posteriores.</li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#13-bibliotecas-python-para-aprendizaje-no-supervisado","title":"1.3. Bibliotecas Python para Aprendizaje No Supervisado","text":""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#instalacion-de-bibliotecas-esenciales","title":"Instalaci\u00f3n de Bibliotecas Esenciales","text":"<pre><code># Instalaci\u00f3n con pip\npip install numpy pandas matplotlib seaborn scikit-learn\n\n# Bibliotecas adicionales espec\u00edficas\npip install mlxtend          # Para reglas de asociaci\u00f3n (Apriori)\npip install umap-learn       # Para UMAP (reducci\u00f3n de dimensionalidad)\npip install hdbscan          # Para HDBSCAN (clustering avanzado)\npip install yellowbrick      # Para visualizaci\u00f3n de ML\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#bibliotecas-principales-y-sus-modulos","title":"Bibliotecas Principales y sus M\u00f3dulos","text":"Biblioteca M\u00f3dulo Funcionalidad Algoritmos/Funciones <code>scikit-learn</code> <code>sklearn.cluster</code> Clustering <code>KMeans</code>, <code>DBSCAN</code>, <code>AgglomerativeClustering</code> <code>scikit-learn</code> <code>sklearn.decomposition</code> Reducci\u00f3n de dimensionalidad <code>PCA</code>, <code>TruncatedSVD</code>, <code>NMF</code> <code>scikit-learn</code> <code>sklearn.manifold</code> Embedding no lineal <code>TSNE</code>, <code>MDS</code>, <code>Isomap</code> <code>scikit-learn</code> <code>sklearn.ensemble</code> Detecci\u00f3n de anomal\u00edas <code>IsolationForest</code> <code>scikit-learn</code> <code>sklearn.neighbors</code> Detecci\u00f3n de outliers <code>LocalOutlierFactor</code> <code>scikit-learn</code> <code>sklearn.metrics</code> M\u00e9tricas de evaluaci\u00f3n <code>silhouette_score</code>, <code>calinski_harabasz_score</code> <code>mlxtend</code> <code>mlxtend.frequent_patterns</code> Reglas de asociaci\u00f3n <code>apriori</code>, <code>association_rules</code> <code>scipy</code> <code>scipy.cluster.hierarchy</code> Clustering jer\u00e1rquico <code>linkage</code>, <code>dendrogram</code>, <code>fcluster</code>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#imports-tipicos-para-aprendizaje-no-supervisado","title":"Imports T\u00edpicos para Aprendizaje No Supervisado","text":"<pre><code># Bibliotecas b\u00e1sicas\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocesamiento\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Algoritmos de Clustering\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\n\n# Reducci\u00f3n de Dimensionalidad\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Detecci\u00f3n de Anomal\u00edas\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# Reglas de Asociaci\u00f3n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# M\u00e9tricas de Evaluaci\u00f3n\nfrom sklearn.metrics import (\n    silhouette_score,\n    silhouette_samples,\n    calinski_harabasz_score,\n    davies_bouldin_score\n)\n\n# Clustering Jer\u00e1rquico (scipy)\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#14-preprocesamiento-de-datos-para-aprendizaje-no-supervisado","title":"1.4. Preprocesamiento de Datos para Aprendizaje No Supervisado","text":"<p>El preprocesamiento es a\u00fan m\u00e1s cr\u00edtico en el aprendizaje no supervisado que en el supervisado, ya que los algoritmos son muy sensibles a la escala y calidad de los datos.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#141-escalado-de-caracteristicas","title":"1.4.1. Escalado de Caracter\u00edsticas","text":"<p>\u00bfPor qu\u00e9 es obligatorio?</p> <p>La mayor\u00eda de algoritmos no supervisados se basan en medidas de distancia (Euclidiana, Manhattan, etc.). Si las variables tienen escalas muy diferentes, las de mayor magnitud dominar\u00e1n completamente el c\u00e1lculo.</p> <p>Ejemplo del problema: <pre><code>Cliente A: Edad=25, Salario=50000\nCliente B: Edad=35, Salario=51000\nCliente C: Edad=26, Salario=80000\n</code></pre> Sin escalado, la diferencia de salario (miles) dominar\u00e1 sobre la edad (decenas).</p> <p>M\u00e9todos de Escalado:</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# 1. Estandarizaci\u00f3n (Z-score) - El m\u00e1s com\u00fan\n# Transforma datos para tener media=0 y desviaci\u00f3n est\u00e1ndar=1\n# F\u00f3rmula: z = (x - \u03bc) / \u03c3\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2. Normalizaci\u00f3n Min-Max\n# Escala valores al rango [0, 1]\n# F\u00f3rmula: x_norm = (x - x_min) / (x_max - x_min)\nscaler = MinMaxScaler()\nX_normalized = scaler.fit_transform(X)\n\n# 3. RobustScaler - Resistente a outliers\n# Usa mediana y rango intercuart\u00edlico en lugar de media y std\nscaler = RobustScaler()\nX_robust = scaler.fit_transform(X)\n</code></pre> <p>\u00bfCu\u00e1ndo usar cada uno?</p> M\u00e9todo Cu\u00e1ndo usar <code>StandardScaler</code> Datos aproximadamente normales, sin muchos outliers <code>MinMaxScaler</code> Cuando necesitas valores acotados [0,1], ej. para redes neuronales <code>RobustScaler</code> Cuando hay outliers significativos en los datos"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#142-manejo-de-valores-faltantes","title":"1.4.2. Manejo de Valores Faltantes","text":"<p>En aprendizaje no supervisado, los valores faltantes son problem\u00e1ticos porque: - Muchos algoritmos no los aceptan directamente - Pueden distorsionar las medidas de distancia</p> <pre><code>from sklearn.impute import SimpleImputer, KNNImputer\n\n# 1. Imputaci\u00f3n simple (media, mediana, moda)\nimputer = SimpleImputer(strategy='median')\nX_imputed = imputer.fit_transform(X)\n\n# 2. Imputaci\u00f3n basada en KNN (m\u00e1s sofisticada)\n# Imputa usando los k vecinos m\u00e1s cercanos\nimputer = KNNImputer(n_neighbors=5)\nX_imputed = imputer.fit_transform(X)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#143-manejo-de-datos-categoricos","title":"1.4.3. Manejo de Datos Categ\u00f3ricos","text":"<p>Los algoritmos de clustering generalmente requieren datos num\u00e9ricos:</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# Para variables nominales: One-Hot Encoding\ndf_encoded = pd.get_dummies(df, columns=['categoria'])\n\n# Alternativa: OneHotEncoder de sklearn\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nX_encoded = encoder.fit_transform(df[['categoria']])\n</code></pre> <p>Nota: Para clustering con variables categ\u00f3ricas puras, considerar algoritmos especializados como K-Modes o K-Prototypes (biblioteca <code>kmodes</code>).</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#15-metricas-de-evaluacion-en-aprendizaje-no-supervisado","title":"1.5. M\u00e9tricas de Evaluaci\u00f3n en Aprendizaje No Supervisado","text":"<p>Evaluar modelos no supervisados es m\u00e1s complejo porque no hay etiquetas de referencia. Existen dos tipos de m\u00e9tricas:</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#151-metricas-internas-sin-etiquetas-reales","title":"1.5.1. M\u00e9tricas Internas (sin etiquetas reales)","text":"<p>Eval\u00faan la calidad del clustering bas\u00e1ndose \u00fanicamente en los datos y las asignaciones de cluster.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#coeficiente-de-silueta-silhouette-score","title":"Coeficiente de Silueta (Silhouette Score)","text":"<p>Mide qu\u00e9 tan similar es un punto a su propio cluster comparado con otros clusters.</p> <p>F\u00f3rmula para un punto \\(i\\): \\(\\(s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\\)\\)</p> <p>Donde: - \\(a(i)\\) = distancia media de \\(i\\) a los otros puntos de su mismo cluster (cohesi\u00f3n) - \\(b(i)\\) = distancia media m\u00ednima de \\(i\\) a los puntos del cluster m\u00e1s cercano (separaci\u00f3n)</p> <p>Interpretaci\u00f3n: - \\(s(i) \\approx 1\\): El punto est\u00e1 bien asignado a su cluster - \\(s(i) \\approx 0\\): El punto est\u00e1 en la frontera entre clusters - \\(s(i) &lt; 0\\): El punto probablemente est\u00e1 mal asignado</p> <pre><code>from sklearn.metrics import silhouette_score, silhouette_samples\n\n# Silueta promedio del clustering\nscore = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {score:.3f}\")\n\n# Silueta por cada muestra (para an\u00e1lisis detallado)\nsample_scores = silhouette_samples(X, labels)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#inercia-within-cluster-sum-of-squares-wcss","title":"Inercia (Within-Cluster Sum of Squares - WCSS)","text":"<p>Suma de las distancias al cuadrado de cada punto al centroide de su cluster. Solo para K-Means.</p> \\[WCSS = \\sum_{i=1}^{k}\\sum_{x \\in C_i} ||x - \\mu_i||^2\\] <p>Interpretaci\u00f3n: - Menor inercia = clusters m\u00e1s compactos - Se usa en el M\u00e9todo del Codo para encontrar el n\u00famero \u00f3ptimo de clusters</p> <pre><code># La inercia se obtiene directamente del modelo KMeans\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nprint(f\"Inercia: {kmeans.inertia_}\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#indice-calinski-harabasz-variance-ratio-criterion","title":"\u00cdndice Calinski-Harabasz (Variance Ratio Criterion)","text":"<p>Ratio entre la dispersi\u00f3n entre clusters y la dispersi\u00f3n dentro de clusters.</p> \\[CH = \\frac{SS_B / (k-1)}{SS_W / (n-k)}\\] <p>Donde: - \\(SS_B\\) = dispersi\u00f3n entre clusters - \\(SS_W\\) = dispersi\u00f3n dentro de clusters - \\(k\\) = n\u00famero de clusters - \\(n\\) = n\u00famero de muestras</p> <p>Interpretaci\u00f3n: Mayor valor = mejor clustering</p> <pre><code>from sklearn.metrics import calinski_harabasz_score\n\nscore = calinski_harabasz_score(X, labels)\nprint(f\"Calinski-Harabasz Score: {score:.3f}\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#indice-davies-bouldin","title":"\u00cdndice Davies-Bouldin","text":"<p>Mide la similitud promedio entre cada cluster y su cluster m\u00e1s similar.</p> \\[DB = \\frac{1}{k}\\sum_{i=1}^{k}\\max_{j \\neq i}\\left(\\frac{s_i + s_j}{d_{ij}}\\right)\\] <p>Donde: - \\(s_i\\) = dispersi\u00f3n media del cluster \\(i\\) - \\(d_{ij}\\) = distancia entre centroides de clusters \\(i\\) y \\(j\\)</p> <p>Interpretaci\u00f3n: Menor valor = mejor clustering (clusters m\u00e1s separados y compactos)</p> <pre><code>from sklearn.metrics import davies_bouldin_score\n\nscore = davies_bouldin_score(X, labels)\nprint(f\"Davies-Bouldin Score: {score:.3f}\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#152-metricas-externas-con-etiquetas-reales","title":"1.5.2. M\u00e9tricas Externas (con etiquetas reales)","text":"<p>Cuando disponemos de etiquetas reales (ground truth), podemos comparar los clusters descubiertos con las clases verdaderas.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#adjusted-rand-index-ari","title":"Adjusted Rand Index (ARI)","text":"<p>Mide la similitud entre dos asignaciones de clusters, ajustada por azar.</p> <pre><code>from sklearn.metrics import adjusted_rand_score\n\nari = adjusted_rand_score(y_true, labels_pred)\n# Rango: [-1, 1], donde 1 = asignaci\u00f3n perfecta\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#normalized-mutual-information-nmi","title":"Normalized Mutual Information (NMI)","text":"<p>Mide la informaci\u00f3n mutua entre las asignaciones, normalizada.</p> <pre><code>from sklearn.metrics import normalized_mutual_info_score\n\nnmi = normalized_mutual_info_score(y_true, labels_pred)\n# Rango: [0, 1], donde 1 = asignaci\u00f3n perfecta\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#153-tabla-resumen-de-metricas","title":"1.5.3. Tabla Resumen de M\u00e9tricas","text":"M\u00e9trica Tipo Rango Mejor valor Uso principal Silhouette Interna [-1, 1] Cercano a 1 Evaluar calidad general Inercia (WCSS) Interna [0, \u221e) Menor M\u00e9todo del codo Calinski-Harabasz Interna [0, \u221e) Mayor Comparar configuraciones Davies-Bouldin Interna [0, \u221e) Menor Comparar configuraciones ARI Externa [-1, 1] Cercano a 1 Validar con ground truth NMI Externa [0, 1] Cercano a 1 Validar con ground truth"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#16-el-metodo-del-codo-elbow-method","title":"1.6. El M\u00e9todo del Codo (Elbow Method)","text":"<p>Es la t\u00e9cnica m\u00e1s popular para determinar el n\u00famero \u00f3ptimo de clusters en K-Means.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#concepto","title":"Concepto","text":"<ol> <li>Ejecutar K-Means con diferentes valores de \\(k\\) (n\u00famero de clusters)</li> <li>Para cada \\(k\\), calcular la inercia (WCSS)</li> <li>Graficar \\(k\\) vs. inercia</li> <li>Buscar el \"codo\": el punto donde la reducci\u00f3n de inercia se desacelera significativamente</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#implementacion-completa","title":"Implementaci\u00f3n Completa","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Generar datos de ejemplo\nX, _ = make_blobs(n_samples=500, centers=4, random_state=42)\n\n# Calcular inercia para diferentes valores de k\ninertias = []\nK_range = range(1, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\n# Graficar el m\u00e9todo del codo\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('N\u00famero de Clusters (k)', fontsize=12)\nplt.ylabel('Inercia (WCSS)', fontsize=12)\nplt.title('M\u00e9todo del Codo para Determinar k \u00d3ptimo', fontsize=14)\nplt.xticks(K_range)\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#interpretacion-visual","title":"Interpretaci\u00f3n Visual","text":"<pre><code>Inercia\n   \u2502\n   \u2502\\\n   \u2502 \\\n   \u2502  \\\n   \u2502   \\____ \u2190 \"Codo\" (k \u00f3ptimo)\n   \u2502        \\____\n   \u2502             \\____\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 k\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#17-visualizacion-de-resultados","title":"1.7. Visualizaci\u00f3n de Resultados","text":"<p>La visualizaci\u00f3n es fundamental en aprendizaje no supervisado para interpretar y comunicar resultados.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#171-visualizacion-de-clusters-en-2d","title":"1.7.1. Visualizaci\u00f3n de Clusters en 2D","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Crear datos\nX, y_true = make_blobs(n_samples=300, centers=4, random_state=42)\n\n# Aplicar K-Means\nkmeans = KMeans(n_clusters=4, random_state=42)\nlabels = kmeans.fit_predict(X)\ncentroids = kmeans.cluster_centers_\n\n# Visualizar\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', \n                      alpha=0.6, edgecolors='w', s=50)\nplt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', \n            s=200, edgecolors='black', linewidths=2, label='Centroides')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Resultados del Clustering K-Means')\nplt.legend()\nplt.colorbar(scatter, label='Cluster')\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#172-grafico-de-silueta","title":"1.7.2. Gr\u00e1fico de Silueta","text":"<pre><code>from sklearn.metrics import silhouette_samples\nimport numpy as np\n\ndef plot_silhouette(X, labels, n_clusters):\n    \"\"\"Grafica el an\u00e1lisis de silueta por cluster.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    silhouette_avg = silhouette_score(X, labels)\n    sample_silhouette_values = silhouette_samples(X, labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Valores de silueta para el cluster i\n        cluster_silhouette_values = sample_silhouette_values[labels == i]\n        cluster_silhouette_values.sort()\n\n        cluster_size = cluster_silhouette_values.shape[0]\n        y_upper = y_lower + cluster_size\n\n        color = plt.cm.viridis(float(i) / n_clusters)\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                         0, cluster_silhouette_values,\n                         facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax.text(-0.05, y_lower + 0.5 * cluster_size, str(i))\n        y_lower = y_upper + 10\n\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", \n               label=f'Silueta media: {silhouette_avg:.3f}')\n    ax.set_xlabel(\"Coeficiente de Silueta\")\n    ax.set_ylabel(\"Cluster\")\n    ax.legend()\n    plt.title(\"An\u00e1lisis de Silueta\")\n    plt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#18-ejemplos-practicos-y-recursos-externos","title":"1.8. Ejemplos Pr\u00e1cticos y Recursos Externos","text":""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#recursos-y-tutoriales-recomendados","title":"Recursos y Tutoriales Recomendados","text":"<ul> <li> <p>Documentaci\u00f3n oficial de scikit-learn - Clustering: https://scikit-learn.org/stable/modules/clustering.html</p> </li> <li> <p>Documentaci\u00f3n oficial de scikit-learn - Reducci\u00f3n de Dimensionalidad: https://scikit-learn.org/stable/modules/decomposition.html</p> </li> <li> <p>Tutorial de K-Means con datos reales (Customer Segmentation): https://www.kaggle.com/code/kushal1996/customer-segmentation-k-means-analysis</p> </li> <li> <p>Ejemplo completo de PCA para visualizaci\u00f3n: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html</p> </li> <li> <p>Market Basket Analysis con Apriori: https://www.kaggle.com/code/datatheque/association-rules-mining-market-basket-analysis</p> </li> <li> <p>Detecci\u00f3n de anomal\u00edas con Isolation Forest: https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html</p> </li> </ul>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#19-comparativa-de-algoritmos-de-clustering","title":"1.9. Comparativa de Algoritmos de Clustering","text":"Algoritmo Forma clusters Escalabilidad Requiere k Maneja ruido Complejidad K-Means Esf\u00e9ricos Muy alta S\u00ed No O(n\u00b7k\u00b7i) DBSCAN Arbitraria Media No S\u00ed O(n\u00b2) o O(n log n) Jer\u00e1rquico Arbitraria Baja No* No O(n\u00b3) Gaussian Mixture El\u00edpticos Alta S\u00ed No O(n\u00b7k\u00b7i) OPTICS Arbitraria Media No S\u00ed O(n\u00b2) <p>*El clustering jer\u00e1rquico no requiere k a priori, pero s\u00ed para \"cortar\" el dendrograma.</p>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#110-buenas-practicas","title":"1.10. Buenas Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#hacer-siempre","title":"\u2705 Hacer siempre:","text":"<ol> <li>Escalar los datos antes de aplicar algoritmos basados en distancias</li> <li>Explorar los datos visualmente antes del clustering (EDA)</li> <li>Probar m\u00faltiples algoritmos y comparar resultados</li> <li>Usar m\u00faltiples m\u00e9tricas para evaluar la calidad</li> <li>Validar los resultados con conocimiento del dominio</li> <li>Documentar las decisiones (por qu\u00e9 se eligi\u00f3 cierto k, algoritmo, etc.)</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#evitar","title":"\u274c Evitar:","text":"<ol> <li>Asumir que existe una estructura de clusters cuando puede no haberla</li> <li>Confiar ciegamente en una sola m\u00e9trica</li> <li>Ignorar outliers sin investigarlos</li> <li>Aplicar algoritmos sin entender sus supuestos</li> <li>Sobrevalorar el n\u00famero de clusters (m\u00e1s no siempre es mejor)</li> </ol>"},{"location":"aprendizaje-no-supervisado/01-aprendizaje-no-supervisado/#111-ejercicio-integrador-pipeline-completo","title":"1.11. Ejercicio Integrador: Pipeline Completo","text":"<pre><code>\"\"\"\nPipeline completo de clustering con evaluaci\u00f3n\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.datasets import load_iris\n\n# 1. CARGAR DATOS\niris = load_iris()\nX = iris.data\nfeature_names = iris.feature_names\n\nprint(\"=\"*50)\nprint(\"PIPELINE DE CLUSTERING NO SUPERVISADO\")\nprint(\"=\"*50)\n\n# 2. PREPROCESAMIENTO\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(f\"\\n[1] Datos escalados: {X_scaled.shape}\")\n\n# 3. M\u00c9TODO DEL CODO\ninertias = []\nsilhouettes = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n\n# Visualizar\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(K_range, inertias, 'bo-', linewidth=2)\naxes[0].set_xlabel('N\u00famero de Clusters (k)')\naxes[0].set_ylabel('Inercia')\naxes[0].set_title('M\u00e9todo del Codo')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(K_range, silhouettes, 'go-', linewidth=2)\naxes[1].set_xlabel('N\u00famero de Clusters (k)')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].set_title('Silueta vs k')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 4. MODELO FINAL (k=3 basado en an\u00e1lisis)\nk_optimo = 3\nkmeans_final = KMeans(n_clusters=k_optimo, random_state=42, n_init=10)\nlabels = kmeans_final.fit_predict(X_scaled)\n\nprint(f\"\\n[2] Clustering con k={k_optimo}\")\nprint(f\"    Distribuci\u00f3n de clusters: {np.bincount(labels)}\")\n\n# 5. EVALUACI\u00d3N\nprint(f\"\\n[3] M\u00c9TRICAS DE EVALUACI\u00d3N:\")\nprint(f\"    - Silhouette Score:      {silhouette_score(X_scaled, labels):.4f}\")\nprint(f\"    - Calinski-Harabasz:     {calinski_harabasz_score(X_scaled, labels):.4f}\")\nprint(f\"    - Davies-Bouldin:        {davies_bouldin_score(X_scaled, labels):.4f}\")\n\n# 6. AN\u00c1LISIS DE RESULTADOS\nprint(f\"\\n[4] AN\u00c1LISIS POR CLUSTER:\")\ndf = pd.DataFrame(X, columns=feature_names)\ndf['cluster'] = labels\nprint(df.groupby('cluster').mean().round(2))\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/","title":"\ud83c\udfaf Unidad 2. Algoritmo K-Means","text":"<p>El algoritmo K-Means es el m\u00e9todo de clustering m\u00e1s popular y ampliamente utilizado en Machine Learning. Su simplicidad, eficiencia y escalabilidad lo convierten en la primera opci\u00f3n para muchas tareas de agrupamiento. K-Means particiona los datos en k grupos (clusters) donde cada observaci\u00f3n pertenece al cluster con el centroide (media) m\u00e1s cercano.</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#21-como-funciona-k-means","title":"2.1. \u00bfC\u00f3mo Funciona K-Means?","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#intuicion-del-algoritmo","title":"Intuici\u00f3n del Algoritmo","text":"<p>K-Means busca dividir \\(n\\) observaciones en \\(k\\) clusters de manera que se minimice la varianza dentro de cada cluster. Es un algoritmo iterativo que alterna entre dos pasos hasta converger:</p> <ol> <li>Asignaci\u00f3n: Cada punto se asigna al centroide m\u00e1s cercano</li> <li>Actualizaci\u00f3n: Cada centroide se recalcula como la media de los puntos asignados</li> </ol>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#algoritmo-paso-a-paso","title":"Algoritmo Paso a Paso","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ALGORITMO K-MEANS                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Entrada: Datos X, n\u00famero de clusters k                      \u2502\n\u2502 Salida: Asignaciones de cluster y centroides                \u2502\n\u2502                                                             \u2502\n\u2502 1. INICIALIZACI\u00d3N                                           \u2502\n\u2502    Seleccionar k puntos aleatorios como centroides iniciales\u2502\n\u2502                                                             \u2502\n\u2502 2. REPETIR hasta convergencia:                              \u2502\n\u2502    a) ASIGNACI\u00d3N: Asignar cada punto al centroide m\u00e1s      \u2502\n\u2502       cercano (seg\u00fan distancia Euclidiana)                  \u2502\n\u2502    b) ACTUALIZACI\u00d3N: Recalcular cada centroide como la     \u2502\n\u2502       media de todos los puntos asignados a \u00e9l              \u2502\n\u2502                                                             \u2502\n\u2502 3. CONVERGENCIA cuando:                                     \u2502\n\u2502    - Los centroides no cambian, O                           \u2502\n\u2502    - Las asignaciones no cambian, O                         \u2502\n\u2502    - Se alcanza el n\u00famero m\u00e1ximo de iteraciones             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#visualizacion-del-proceso","title":"Visualizaci\u00f3n del Proceso","text":"<pre><code>Iteraci\u00f3n 0 (Inicial)     Iteraci\u00f3n 1           Iteraci\u00f3n 2 (Final)\n\n    \u00d7  \u2022  \u2022                 \u00d7 \u2022 \u2022                  \u00d7 \u2022 \u2022\n   \u2022  \u2022    \u2022               \u2022 \u2022   \u2022                \u2022 \u2022   \u2022\n      \u00d7  \u2022                    \u00d7\u2022                     \u00d7\u2022\n  \u2022     \u2022  \u2022              \u2022    \u2022 \u2022               \u2022    \u2022 \u2022\n    \u2022  \u00d7                    \u2022 \u00d7                    \u2022 \u00d7\n\n\u00d7 = Centroide             \u00d7 se mueve              \u00d7 converge\n\u2022 = Punto de datos        \n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#22-explicacion-matematica","title":"2.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#funcion-objetivo","title":"Funci\u00f3n Objetivo","text":"<p>K-Means busca minimizar la inercia (Within-Cluster Sum of Squares - WCSS):</p> \\[J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\] <p>Donde: - \\(k\\) = n\u00famero de clusters - \\(C_i\\) = conjunto de puntos en el cluster \\(i\\) - \\(\\mu_i\\) = centroide del cluster \\(i\\) - \\(||x - \\mu_i||^2\\) = distancia Euclidiana al cuadrado</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#paso-de-asignacion","title":"Paso de Asignaci\u00f3n","text":"<p>Para cada punto \\(x_j\\), se asigna al cluster cuyo centroide est\u00e1 m\u00e1s cerca:</p> \\[c_j = \\arg\\min_{i} ||x_j - \\mu_i||^2\\] <p>Donde \\(c_j\\) es la asignaci\u00f3n de cluster para el punto \\(x_j\\).</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#paso-de-actualizacion","title":"Paso de Actualizaci\u00f3n","text":"<p>Cada centroide se actualiza como la media aritm\u00e9tica de todos los puntos asignados:</p> \\[\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x\\] <p>Donde \\(|C_i|\\) es el n\u00famero de puntos en el cluster \\(i\\).</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#distancia-euclidiana","title":"Distancia Euclidiana","text":"<p>La m\u00e9trica est\u00e1ndar utilizada:</p> \\[d(x, \\mu) = \\sqrt{\\sum_{j=1}^{p} (x_j - \\mu_j)^2}\\] <p>Donde \\(p\\) es el n\u00famero de dimensiones (features).</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#23-pros-y-contras","title":"2.3. Pros y Contras","text":"Ventajas Desventajas Simplicidad: F\u00e1cil de entender e implementar Requiere especificar k: El n\u00famero de clusters debe definirse a priori Eficiencia: Complejidad \\(O(n \\cdot k \\cdot i \\cdot d)\\) donde \\(i\\) = iteraciones, \\(d\\) = dimensiones Sensible a inicializaci\u00f3n: Puede converger a m\u00ednimos locales Escalabilidad: Funciona bien con grandes datasets Asume clusters esf\u00e9ricos: No funciona bien con clusters de formas irregulares Interpretabilidad: Los centroides tienen significado real Sensible a outliers: Los valores at\u00edpicos distorsionan los centroides Garant\u00eda de convergencia: Siempre converge (aunque quiz\u00e1 a \u00f3ptimo local) Clusters de tama\u00f1o similar: Tiende a crear clusters de tama\u00f1o comparable"},{"location":"aprendizaje-no-supervisado/02-kmeans/#24-ejemplo-basico-en-python","title":"2.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso m\u00e1s simple de K-Means con datos sint\u00e9ticos.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: K-Means con datos sint\u00e9ticos\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.cluster import KMeans          # Algoritmo K-Means\nfrom sklearn.datasets import make_blobs     # Generar datos sint\u00e9ticos\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS DE EJEMPLO\n# -------------------------------------------------------------\n# make_blobs crea clusters esf\u00e9ricos bien definidos\n# n_samples: n\u00famero total de puntos\n# centers: n\u00famero de clusters a generar\n# cluster_std: desviaci\u00f3n est\u00e1ndar de cada cluster (dispersi\u00f3n)\n# random_state: semilla para reproducibilidad\nX, y_true = make_blobs(\n    n_samples=300,      # 300 puntos en total\n    centers=4,          # 4 clusters reales\n    cluster_std=0.60,   # Clusters bastante compactos\n    random_state=42     # Reproducible\n)\n\nprint(f\"Forma de los datos: {X.shape}\")\nprint(f\"Primeras 5 filas:\\n{X[:5]}\")\n\n# -------------------------------------------------------------\n# 2. CREAR Y ENTRENAR EL MODELO K-MEANS\n# -------------------------------------------------------------\n# Instanciar el modelo con k=4 clusters\nkmeans = KMeans(\n    n_clusters=4,       # N\u00famero de clusters a formar\n    random_state=42     # Reproducibilidad\n)\n\n# fit_predict: entrena el modelo Y devuelve las etiquetas de cluster\n# Equivalente a: kmeans.fit(X) seguido de kmeans.predict(X)\nlabels = kmeans.fit_predict(X)\n\nprint(f\"\\nEtiquetas asignadas (primeras 10): {labels[:10]}\")\nprint(f\"Distribuci\u00f3n de clusters: {np.bincount(labels)}\")\n\n# -------------------------------------------------------------\n# 3. OBTENER INFORMACI\u00d3N DEL MODELO ENTRENADO\n# -------------------------------------------------------------\n# cluster_centers_: coordenadas de los centroides\ncentroids = kmeans.cluster_centers_\nprint(f\"\\nCentroides:\\n{centroids}\")\n\n# inertia_: suma de distancias al cuadrado (WCSS)\nprint(f\"\\nInercia (WCSS): {kmeans.inertia_:.2f}\")\n\n# n_iter_: n\u00famero de iteraciones hasta convergencia\nprint(f\"Iteraciones hasta convergencia: {kmeans.n_iter_}\")\n\n# -------------------------------------------------------------\n# 4. VISUALIZACI\u00d3N DE RESULTADOS\n# -------------------------------------------------------------\nplt.figure(figsize=(10, 8))\n\n# Graficar puntos coloreados por cluster asignado\n# c=labels asigna color seg\u00fan el cluster\n# cmap='viridis' es la paleta de colores\nscatter = plt.scatter(\n    X[:, 0], X[:, 1],           # Coordenadas x, y\n    c=labels,                    # Color seg\u00fan cluster\n    cmap='viridis',             # Paleta de colores\n    alpha=0.6,                   # Transparencia\n    edgecolors='w',             # Borde blanco\n    s=50                         # Tama\u00f1o de puntos\n)\n\n# Graficar centroides como estrellas rojas\nplt.scatter(\n    centroids[:, 0], centroids[:, 1],  # Coordenadas centroides\n    c='red',                            # Color rojo\n    marker='*',                         # Forma de estrella\n    s=300,                              # Tama\u00f1o grande\n    edgecolors='black',                 # Borde negro\n    linewidths=2,\n    label='Centroides'\n)\n\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('Feature 2', fontsize=12)\nplt.title('Clustering K-Means (k=4)', fontsize=14)\nplt.legend()\nplt.colorbar(scatter, label='Cluster')\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Salida esperada: <pre><code>Forma de los datos: (300, 2)\nEtiquetas asignadas (primeras 10): [1 0 2 2 0 3 1 0 3 2]\nDistribuci\u00f3n de clusters: [75 75 75 75]\nInercia (WCSS): 211.93\nIteraciones hasta convergencia: 3\n</code></pre></p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#25-ejemplo-avanzado-pipeline-completo-con-hiperparametros","title":"2.5. Ejemplo Avanzado: Pipeline Completo con Hiperpar\u00e1metros","text":"<p>Este ejemplo implementa un an\u00e1lisis completo con selecci\u00f3n autom\u00e1tica de \\(k\\), evaluaci\u00f3n de m\u00e9tricas y optimizaci\u00f3n de hiperpar\u00e1metros.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: K-Means con Pipeline Completo\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.datasets import load_iris\n\n# -------------------------------------------------------------\n# 1. CARGAR Y PREPARAR DATOS REALES (IRIS)\n# -------------------------------------------------------------\n# Cargar el dataset Iris (sin usar las etiquetas)\niris = load_iris()\nX = iris.data                    # 150 muestras \u00d7 4 caracter\u00edsticas\nfeature_names = iris.feature_names\ny_true = iris.target             # Solo para validaci\u00f3n final\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS DE CLUSTERING K-MEANS - DATASET IRIS\")\nprint(\"=\"*60)\nprint(f\"\\nDimensiones del dataset: {X.shape}\")\nprint(f\"Caracter\u00edsticas: {feature_names}\")\n\n# -------------------------------------------------------------\n# 2. PREPROCESAMIENTO: ESTANDARIZACI\u00d3N\n# -------------------------------------------------------------\n# K-Means es sensible a la escala, por lo que debemos estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"\\nEstad\u00edsticas despu\u00e9s de estandarizar:\")\nprint(f\"  Media por feature: {X_scaled.mean(axis=0).round(2)}\")\nprint(f\"  Std por feature:   {X_scaled.std(axis=0).round(2)}\")\n\n# -------------------------------------------------------------\n# 3. M\u00c9TODO DEL CODO + AN\u00c1LISIS DE SILUETA\n# -------------------------------------------------------------\n# Probar diferentes valores de k y calcular m\u00e9tricas\nK_range = range(2, 11)  # k de 2 a 10\nmetrics = {\n    'k': [],\n    'inertia': [],\n    'silhouette': [],\n    'calinski': [],\n    'davies_bouldin': []\n}\n\nprint(\"\\nCalculando m\u00e9tricas para diferentes valores de k...\")\n\nfor k in K_range:\n    # Crear modelo con hiperpar\u00e1metros espec\u00edficos\n    kmeans = KMeans(\n        n_clusters=k,           # N\u00famero de clusters\n        init='k-means++',       # Inicializaci\u00f3n inteligente\n        n_init=10,              # N\u00famero de inicializaciones\n        max_iter=300,           # M\u00e1ximo de iteraciones\n        tol=1e-4,               # Tolerancia para convergencia\n        random_state=42         # Reproducibilidad\n    )\n\n    # Entrenar y obtener etiquetas\n    labels = kmeans.fit_predict(X_scaled)\n\n    # Almacenar m\u00e9tricas\n    metrics['k'].append(k)\n    metrics['inertia'].append(kmeans.inertia_)\n    metrics['silhouette'].append(silhouette_score(X_scaled, labels))\n    metrics['calinski'].append(calinski_harabasz_score(X_scaled, labels))\n    metrics['davies_bouldin'].append(davies_bouldin_score(X_scaled, labels))\n\n# Convertir a DataFrame para mejor visualizaci\u00f3n\ndf_metrics = pd.DataFrame(metrics)\nprint(\"\\nM\u00e9tricas por n\u00famero de clusters:\")\nprint(df_metrics.round(3).to_string(index=False))\n\n# -------------------------------------------------------------\n# 4. VISUALIZACI\u00d3N: M\u00c9TODO DEL CODO Y SILUETA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 4.1 M\u00e9todo del Codo (Inercia)\naxes[0, 0].plot(df_metrics['k'], df_metrics['inertia'], 'bo-', linewidth=2, markersize=8)\naxes[0, 0].set_xlabel('N\u00famero de Clusters (k)')\naxes[0, 0].set_ylabel('Inercia (WCSS)')\naxes[0, 0].set_title('M\u00e9todo del Codo')\naxes[0, 0].axvline(x=3, color='r', linestyle='--', alpha=0.7, label='k=3 (codo)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 4.2 Silhouette Score\naxes[0, 1].plot(df_metrics['k'], df_metrics['silhouette'], 'go-', linewidth=2, markersize=8)\naxes[0, 1].set_xlabel('N\u00famero de Clusters (k)')\naxes[0, 1].set_ylabel('Silhouette Score')\naxes[0, 1].set_title('An\u00e1lisis de Silueta')\naxes[0, 1].axvline(x=3, color='r', linestyle='--', alpha=0.7, label='k=3')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 4.3 Calinski-Harabasz Score\naxes[1, 0].plot(df_metrics['k'], df_metrics['calinski'], 'mo-', linewidth=2, markersize=8)\naxes[1, 0].set_xlabel('N\u00famero de Clusters (k)')\naxes[1, 0].set_ylabel('Calinski-Harabasz Score')\naxes[1, 0].set_title('\u00cdndice Calinski-Harabasz (mayor=mejor)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4.4 Davies-Bouldin Score\naxes[1, 1].plot(df_metrics['k'], df_metrics['davies_bouldin'], 'co-', linewidth=2, markersize=8)\naxes[1, 1].set_xlabel('N\u00famero de Clusters (k)')\naxes[1, 1].set_ylabel('Davies-Bouldin Score')\naxes[1, 1].set_title('\u00cdndice Davies-Bouldin (menor=mejor)')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 5. MODELO FINAL CON k \u00d3PTIMO\n# -------------------------------------------------------------\nk_optimo = 3  # Basado en an\u00e1lisis del codo y silueta\n\nprint(f\"\\n{'='*60}\")\nprint(f\"MODELO FINAL CON k={k_optimo}\")\nprint(f\"{'='*60}\")\n\n# Modelo final con todos los hiperpar\u00e1metros optimizados\nkmeans_final = KMeans(\n    n_clusters=k_optimo,    # N\u00famero \u00f3ptimo de clusters\n    init='k-means++',       # Inicializaci\u00f3n inteligente (ver secci\u00f3n 2.6)\n    n_init=20,              # M\u00e1s inicializaciones para robustez\n    max_iter=500,           # M\u00e1s iteraciones permitidas\n    tol=1e-6,               # Mayor precisi\u00f3n en convergencia\n    algorithm='lloyd',      # Algoritmo cl\u00e1sico de Lloyd\n    random_state=42\n)\n\n# Entrenar modelo final\nlabels_final = kmeans_final.fit_predict(X_scaled)\n\n# M\u00e9tricas finales\nprint(f\"\\nResultados del clustering:\")\nprint(f\"  - Inercia: {kmeans_final.inertia_:.2f}\")\nprint(f\"  - Silueta: {silhouette_score(X_scaled, labels_final):.4f}\")\nprint(f\"  - Iteraciones: {kmeans_final.n_iter_}\")\n\n# Distribuci\u00f3n de clusters\nprint(f\"\\nDistribuci\u00f3n de puntos por cluster:\")\nfor i in range(k_optimo):\n    count = np.sum(labels_final == i)\n    print(f\"  Cluster {i}: {count} puntos ({count/len(labels_final)*100:.1f}%)\")\n\n# -------------------------------------------------------------\n# 6. AN\u00c1LISIS DE LOS CENTROIDES\n# -------------------------------------------------------------\nprint(f\"\\nCentroides (valores estandarizados):\")\ncentroids_df = pd.DataFrame(\n    kmeans_final.cluster_centers_,\n    columns=feature_names,\n    index=[f'Cluster {i}' for i in range(k_optimo)]\n)\nprint(centroids_df.round(3))\n\n# Centroides en escala original\ncentroids_original = scaler.inverse_transform(kmeans_final.cluster_centers_)\nprint(f\"\\nCentroides (valores originales):\")\ncentroids_original_df = pd.DataFrame(\n    centroids_original,\n    columns=feature_names,\n    index=[f'Cluster {i}' for i in range(k_optimo)]\n)\nprint(centroids_original_df.round(2))\n\n# -------------------------------------------------------------\n# 7. PERFIL DE CADA CLUSTER\n# -------------------------------------------------------------\nprint(f\"\\n{'='*60}\")\nprint(\"PERFIL DE CADA CLUSTER\")\nprint(f\"{'='*60}\")\n\n# Crear DataFrame con datos y etiquetas\ndf_result = pd.DataFrame(X, columns=feature_names)\ndf_result['cluster'] = labels_final\n\n# Estad\u00edsticas por cluster\nprint(\"\\nMedia por cluster:\")\nprint(df_result.groupby('cluster').mean().round(2))\n\nprint(\"\\nDesviaci\u00f3n est\u00e1ndar por cluster:\")\nprint(df_result.groupby('cluster').std().round(2))\n\n# -------------------------------------------------------------\n# 8. VALIDACI\u00d3N CON ETIQUETAS REALES (si disponibles)\n# -------------------------------------------------------------\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nprint(f\"\\n{'='*60}\")\nprint(\"VALIDACI\u00d3N CON ETIQUETAS REALES\")\nprint(f\"{'='*60}\")\n\nari = adjusted_rand_score(y_true, labels_final)\nnmi = normalized_mutual_info_score(y_true, labels_final)\n\nprint(f\"\\n  Adjusted Rand Index (ARI): {ari:.4f}\")\nprint(f\"  Normalized Mutual Info (NMI): {nmi:.4f}\")\n\n# Matriz de contingencia\nprint(\"\\nMatriz de contingencia (filas=clusters, cols=especies reales):\")\ncontingency = pd.crosstab(\n    pd.Series(labels_final, name='Cluster'),\n    pd.Series(y_true, name='Especie Real')\n)\ncontingency.columns = iris.target_names\nprint(contingency)\n\n# -------------------------------------------------------------\n# 9. VISUALIZACI\u00d3N 2D CON PCA\n# -------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n\n# Reducir a 2D para visualizaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ncentroids_pca = pca.transform(kmeans_final.cluster_centers_)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Clusters encontrados\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_final, \n                           cmap='viridis', alpha=0.6, s=50, edgecolors='w')\naxes[0].scatter(centroids_pca[:, 0], centroids_pca[:, 1], c='red', \n                marker='*', s=300, edgecolors='black', linewidths=2, label='Centroides')\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\naxes[0].set_title('Clusters K-Means')\naxes[0].legend()\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# Clases reales para comparaci\u00f3n\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, \n                           cmap='viridis', alpha=0.6, s=50, edgecolors='w')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\naxes[1].set_title('Clases Reales (Ground Truth)')\nplt.colorbar(scatter2, ax=axes[1], label='Especie')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#26-hiperparametros-de-k-means-en-scikit-learn","title":"2.6. Hiperpar\u00e1metros de K-Means en scikit-learn","text":"Hiperpar\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_clusters</code> N\u00famero de clusters a formar int &gt; 0 Usar m\u00e9todo del codo o silueta <code>init</code> M\u00e9todo de inicializaci\u00f3n de centroides <code>'k-means++'</code>, <code>'random'</code>, array <code>'k-means++'</code> (por defecto, m\u00e1s robusto) <code>n_init</code> N\u00famero de inicializaciones diferentes int &gt; 0 10 (default) o m\u00e1s para resultados robustos <code>max_iter</code> M\u00e1ximo de iteraciones por inicializaci\u00f3n int &gt; 0 300 (default), aumentar si no converge <code>tol</code> Tolerancia para declarar convergencia float &gt; 0 1e-4 (default) <code>algorithm</code> Algoritmo de K-Means <code>'lloyd'</code>, <code>'elkan'</code> <code>'lloyd'</code> para datasets densos <code>random_state</code> Semilla para reproducibilidad int o None Fijar para reproducibilidad"},{"location":"aprendizaje-no-supervisado/02-kmeans/#inicializacion-k-means","title":"Inicializaci\u00f3n K-Means++","text":"<p>La inicializaci\u00f3n <code>k-means++</code> es crucial para obtener buenos resultados. Funciona as\u00ed:</p> <ol> <li>Seleccionar el primer centroide aleatoriamente</li> <li>Para cada centroide siguiente:</li> <li>Calcular la distancia de cada punto al centroide m\u00e1s cercano ya seleccionado</li> <li>Seleccionar el siguiente centroide con probabilidad proporcional a \\(D(x)^2\\)</li> </ol> <p>Esto asegura que los centroides iniciales est\u00e9n bien distribuidos en el espacio de datos.</p>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#27-limitaciones-y-cuando-no-usar-k-means","title":"2.7. Limitaciones y Cu\u00e1ndo NO Usar K-Means","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#formas-de-clusters-no-esfericas","title":"Formas de Clusters No Esf\u00e9ricas","text":"<p>K-Means asume clusters convexos y esf\u00e9ricos. Falla con:</p> <pre><code>from sklearn.datasets import make_moons\n\n# Datos con forma de media luna\nX_moons, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n\n# K-Means NO funcionar\u00e1 bien aqu\u00ed\nkmeans = KMeans(n_clusters=2, random_state=42)\nlabels_moons = kmeans.fit_predict(X_moons)\n\n# Visualizar el problema\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap='viridis')\nplt.title('K-Means (INCORRECTO)')\n\n# Alternativa: DBSCAN maneja esto mejor\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=0.2, min_samples=5)\nlabels_dbscan = dbscan.fit_predict(X_moons)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_dbscan, cmap='viridis')\nplt.title('DBSCAN (CORRECTO)')\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#clusters-de-tamanos-muy-diferentes","title":"Clusters de Tama\u00f1os Muy Diferentes","text":"<p>K-Means tiende a crear clusters de tama\u00f1o similar, lo cual puede ser problem\u00e1tico:</p> <pre><code># Clusters de tama\u00f1os muy desiguales\nX1 = np.random.randn(500, 2) * 0.5 + [0, 0]    # Cluster grande\nX2 = np.random.randn(50, 2) * 0.5 + [5, 5]     # Cluster peque\u00f1o\nX_unequal = np.vstack([X1, X2])\n\n# K-Means puede dividir el cluster grande en lugar de encontrar el peque\u00f1o\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#28-aplicaciones-reales-de-k-means","title":"2.8. Aplicaciones Reales de K-Means","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#1-segmentacion-de-clientes-marketing","title":"1. Segmentaci\u00f3n de Clientes (Marketing)","text":"<pre><code># Ejemplo: RFM Analysis (Recency, Frequency, Monetary)\n# Datos t\u00edpicos: \u00faltima compra, frecuencia de compras, gasto total\n# K-Means agrupa clientes en segmentos como:\n# - VIP (alta frecuencia, alto gasto)\n# - En riesgo (baja recencia)\n# - Nuevos (alta recencia, baja frecuencia)\n</code></pre> <ul> <li>Tutorial completo: Customer Segmentation with K-Means</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#2-compresion-de-imagenes","title":"2. Compresi\u00f3n de Im\u00e1genes","text":"<pre><code># K-Means puede reducir los colores de una imagen\n# Los centroides representan la paleta de colores reducida\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\n\ndef compress_image(image_path, n_colors=16):\n    \"\"\"Comprime una imagen reduciendo el n\u00famero de colores.\"\"\"\n    img = np.array(Image.open(image_path))\n    pixels = img.reshape(-1, 3)  # Aplanar a lista de p\u00edxeles RGB\n\n    kmeans = KMeans(n_clusters=n_colors, random_state=42)\n    labels = kmeans.fit_predict(pixels)\n\n    # Reemplazar cada p\u00edxel por el color de su centroide\n    compressed = kmeans.cluster_centers_[labels].reshape(img.shape)\n    return compressed.astype(np.uint8)\n</code></pre> <ul> <li>Tutorial completo: Image Compression with K-Means</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#3-deteccion-de-anomalias-simple","title":"3. Detecci\u00f3n de Anomal\u00edas Simple","text":"<pre><code># Puntos muy lejos de todos los centroides pueden ser anomal\u00edas\ndistances = kmeans.transform(X)  # Distancia a cada centroide\nmin_distances = distances.min(axis=1)  # Distancia al centroide m\u00e1s cercano\nthreshold = np.percentile(min_distances, 95)  # Top 5% como anomal\u00edas\nanomalies = min_distances &gt; threshold\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#4-preprocesamiento-para-otros-algoritmos","title":"4. Preprocesamiento para Otros Algoritmos","text":"<ul> <li>Feature Engineering: Las distancias a centroides como nuevas features</li> <li>Inicializaci\u00f3n: Clusters de K-Means para inicializar GMM u otros modelos</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#29-variantes-de-k-means","title":"2.9. Variantes de K-Means","text":"Variante Descripci\u00f3n Uso Mini-Batch K-Means Usa subconjuntos aleatorios en cada iteraci\u00f3n Datasets muy grandes K-Medoids (PAM) Usa puntos reales como centroides (medoides) Robustez a outliers Fuzzy C-Means Asignaci\u00f3n probabil\u00edstica a m\u00faltiples clusters Cuando la pertenencia no es binaria K-Means++ Mejor inicializaci\u00f3n de centroides Est\u00e1ndar en sklearn Bisecting K-Means Divisi\u00f3n jer\u00e1rquica de clusters Clusters jer\u00e1rquicos"},{"location":"aprendizaje-no-supervisado/02-kmeans/#mini-batch-k-means-para-big-data","title":"Mini-Batch K-Means para Big Data","text":"<pre><code>from sklearn.cluster import MiniBatchKMeans\n\n# Para datasets de millones de puntos\nmbkmeans = MiniBatchKMeans(\n    n_clusters=5,\n    batch_size=1000,       # Tama\u00f1o del mini-batch\n    max_iter=100,\n    random_state=42\n)\nlabels = mbkmeans.fit_predict(X_large)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#210-resumen-y-mejores-practicas","title":"2.10. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/02-kmeans/#checklist-para-usar-k-means","title":"Checklist para usar K-Means","text":"<ul> <li>[ ] Escalar los datos (StandardScaler o MinMaxScaler)</li> <li>[ ] Determinar k usando m\u00e9todo del codo y/o silueta</li> <li>[ ] Usar init='k-means++' para mejor inicializaci\u00f3n</li> <li>[ ] Aumentar n_init (10-20) para robustez</li> <li>[ ] Verificar convergencia (revisar <code>n_iter_</code>)</li> <li>[ ] Evaluar con m\u00faltiples m\u00e9tricas (silueta, Calinski-Harabasz)</li> <li>[ ] Visualizar resultados para interpretaci\u00f3n</li> <li>[ ] Validar con conocimiento del dominio</li> </ul>"},{"location":"aprendizaje-no-supervisado/02-kmeans/#cuando-elegir-k-means","title":"\u00bfCu\u00e1ndo elegir K-Means?","text":"<p>\u2705 Usar K-Means cuando: - Los clusters son aproximadamente esf\u00e9ricos - Se conoce (o puede estimarse) el n\u00famero de clusters - El dataset es grande (escalabilidad) - Se necesita interpretabilidad (centroides)</p> <p>\u274c Considerar alternativas cuando: - Clusters de formas arbitrarias \u2192 DBSCAN - Clusters jer\u00e1rquicos \u2192 Agglomerative Clustering - Outliers significativos \u2192 K-Medoids - Incertidumbre en el n\u00famero de clusters \u2192 DBSCAN, Mean Shift</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/","title":"\ud83d\udd2c Unidad 3. Algoritmo DBSCAN","text":"<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad. A diferencia de K-Means, DBSCAN puede descubrir clusters de formas arbitrarias y es capaz de identificar autom\u00e1ticamente puntos de ruido (outliers). No requiere especificar el n\u00famero de clusters a priori, lo cual lo hace extremadamente \u00fatil cuando no conocemos la estructura de los datos.</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#31-como-funciona-dbscan","title":"3.1. \u00bfC\u00f3mo Funciona DBSCAN?","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#intuicion-del-algoritmo","title":"Intuici\u00f3n del Algoritmo","text":"<p>DBSCAN agrupa puntos que est\u00e1n densamente empaquetados juntos, marcando como outliers los puntos que est\u00e1n aislados en regiones de baja densidad. La idea central es:</p> <p>\"Un cluster es una regi\u00f3n de alta densidad separada de otras regiones por \u00e1reas de baja densidad.\"</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#conceptos-fundamentales","title":"Conceptos Fundamentales","text":"<p>DBSCAN define tres tipos de puntos bas\u00e1ndose en dos par\u00e1metros: \u03b5 (epsilon) y MinPts:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 CONCEPTOS CLAVE DE DBSCAN                                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u03b5 (epsilon): Radio de vecindad                             \u2502\n\u2502  MinPts: N\u00famero m\u00ednimo de puntos para formar un n\u00facleo      \u2502\n\u2502                                                             \u2502\n\u2502  TIPOS DE PUNTOS:                                           \u2502\n\u2502                                                             \u2502\n\u2502  1. PUNTO N\u00daCLEO (Core Point):                              \u2502\n\u2502     Tiene al menos MinPts puntos dentro de su \u03b5-vecindad    \u2502\n\u2502     (incluy\u00e9ndose a s\u00ed mismo)                               \u2502\n\u2502                                                             \u2502\n\u2502  2. PUNTO FRONTERA (Border Point):                          \u2502\n\u2502     No es un punto n\u00facleo, pero est\u00e1 dentro de la           \u2502\n\u2502     \u03b5-vecindad de un punto n\u00facleo                           \u2502\n\u2502                                                             \u2502\n\u2502  3. PUNTO DE RUIDO (Noise Point):                           \u2502\n\u2502     No es ni punto n\u00facleo ni punto frontera                 \u2502\n\u2502     \u2192 Se considera OUTLIER                                  \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#visualizacion-de-conceptos","title":"Visualizaci\u00f3n de Conceptos","text":"<pre><code>        \u03b5-vecindad\n           ___\n         /     \\\n        |   \u25cf   |  \u2190 Punto N\u00facleo (\u2265 MinPts vecinos)\n        | \u25cf \u25cf \u25cf |\n         \\_\u25cf_\u25cf_/\n\n     \u25cb               \u2190 Punto Frontera (en vecindad de n\u00facleo)\n\n                 \u00d7   \u2190 Punto de Ruido (aislado)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#algoritmo-paso-a-paso","title":"Algoritmo Paso a Paso","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ALGORITMO DBSCAN                                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Entrada: Datos X, epsilon \u03b5, MinPts                         \u2502\n\u2502 Salida: Asignaciones de cluster (o -1 para ruido)           \u2502\n\u2502                                                             \u2502\n\u2502 1. Para cada punto P no visitado:                           \u2502\n\u2502    a) Marcar P como visitado                                \u2502\n\u2502    b) Encontrar todos los vecinos de P en radio \u03b5           \u2502\n\u2502    c) SI |vecinos| &lt; MinPts:                                \u2502\n\u2502          Marcar P como RUIDO (temporalmente)                \u2502\n\u2502       SINO:                                                 \u2502\n\u2502          Crear nuevo cluster C                              \u2502\n\u2502          Agregar P a C                                      \u2502\n\u2502          Para cada punto Q en vecinos:                      \u2502\n\u2502            - Si Q no visitado: visitar y expandir           \u2502\n\u2502            - Si Q no asignado: agregar Q a C                \u2502\n\u2502                                                             \u2502\n\u2502 2. Los puntos de ruido que est\u00e1n en la \u03b5-vecindad de un    \u2502\n\u2502    punto n\u00facleo se reasignan como puntos frontera           \u2502\n\u2502                                                             \u2502\n\u2502 NOTA: No hay fase de \"entrenamiento\" tradicional            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#32-explicacion-matematica","title":"3.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#-vecindad-epsilon-neighborhood","title":"\u03b5-Vecindad (Epsilon-Neighborhood)","text":"<p>La \u03b5-vecindad de un punto \\(p\\) es el conjunto de todos los puntos que est\u00e1n a una distancia menor o igual a \u03b5:</p> \\[N_\\varepsilon(p) = \\{q \\in D \\mid dist(p, q) \\leq \\varepsilon\\}\\] <p>Donde: - \\(D\\) = conjunto de datos - \\(dist(p, q)\\) = funci\u00f3n de distancia (t\u00edpicamente Euclidiana)</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#definiciones-formales","title":"Definiciones Formales","text":"<p>Punto N\u00facleo (Core Point): Un punto \\(p\\) es un punto n\u00facleo si: \\(\\(|N_\\varepsilon(p)| \\geq MinPts\\)\\)</p> <p>Directamente Alcanzable por Densidad: Un punto \\(q\\) es directamente alcanzable por densidad desde \\(p\\) si: 1. \\(p\\) es un punto n\u00facleo 2. \\(q \\in N_\\varepsilon(p)\\)</p> <p>Alcanzable por Densidad: Un punto \\(q\\) es alcanzable por densidad desde \\(p\\) si existe una cadena de puntos \\(p_1, p_2, ..., p_n\\) donde: - \\(p_1 = p\\) - \\(p_n = q\\) - \\(p_{i+1}\\) es directamente alcanzable por densidad desde \\(p_i\\)</p> <p>Conectado por Densidad: Dos puntos \\(p\\) y \\(q\\) est\u00e1n conectados por densidad si existe un punto \\(o\\) tal que tanto \\(p\\) como \\(q\\) son alcanzables por densidad desde \\(o\\).</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#definicion-de-cluster","title":"Definici\u00f3n de Cluster","text":"<p>Un cluster \\(C\\) en DBSCAN satisface: 1. Maximalidad: Si \\(p \\in C\\) y \\(q\\) es alcanzable por densidad desde \\(p\\), entonces \\(q \\in C\\) 2. Conectividad: Todos los puntos en \\(C\\) est\u00e1n conectados por densidad</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#metricas-de-distancia","title":"M\u00e9tricas de Distancia","text":"<p>DBSCAN puede usar diferentes m\u00e9tricas de distancia:</p> <p>Distancia Euclidiana (por defecto): \\(\\(d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\\)\\)</p> <p>Distancia Manhattan: \\(\\(d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\\)\\)</p> <p>Distancia de Minkowski (generalizaci\u00f3n): \\(\\(d(p, q) = \\left(\\sum_{i=1}^{n} |p_i - q_i|^p\\right)^{1/p}\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#33-pros-y-contras","title":"3.3. Pros y Contras","text":"Ventajas Desventajas No requiere k: No necesita especificar el n\u00famero de clusters a priori Sensible a par\u00e1metros: \u03b5 y MinPts son cr\u00edticos y dif\u00edciles de elegir Formas arbitrarias: Puede encontrar clusters de cualquier forma Densidad variable: No maneja bien clusters con densidades muy diferentes Detecci\u00f3n de outliers: Identifica autom\u00e1ticamente puntos de ruido Alta dimensionalidad: Sufre de la \"maldici\u00f3n de la dimensionalidad\" Robusto a outliers: Los outliers no afectan los centroides (no los hay) Computacionalmente costoso: \\(O(n^2)\\) sin \u00edndice espacial Determin\u00edstico: Mismo resultado (casi) con mismos par\u00e1metros Puntos frontera ambiguos: Pueden asignarse arbitrariamente si est\u00e1n entre clusters"},{"location":"aprendizaje-no-supervisado/03-dbscan/#34-ejemplo-basico-en-python","title":"3.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico de DBSCAN con datos que tienen formas no esf\u00e9ricas.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: DBSCAN con datos de forma arbitraria\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.cluster import DBSCAN          # Algoritmo DBSCAN\nfrom sklearn.datasets import make_moons     # Datos en forma de luna\nfrom sklearn.preprocessing import StandardScaler  # Escalado\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS DE EJEMPLO (FORMA DE MEDIA LUNA)\n# -------------------------------------------------------------\n# make_moons genera dos semic\u00edrculos entrelazados\n# Estos datos NO pueden ser correctamente agrupados por K-Means\nX, y_true = make_moons(\n    n_samples=300,      # 300 puntos en total (150 por luna)\n    noise=0.05,         # Peque\u00f1o ruido a\u00f1adido\n    random_state=42     # Reproducibilidad\n)\n\n# A\u00f1adir algunos outliers manualmente para demostraci\u00f3n\noutliers = np.array([[-0.5, 0.8], [1.5, -0.3], [2.0, 0.8]])\nX = np.vstack([X, outliers])\n\nprint(f\"Forma de los datos: {X.shape}\")\nprint(f\"Muestra de datos:\\n{X[:5]}\")\n\n# -------------------------------------------------------------\n# 2. PREPROCESAMIENTO: ESTANDARIZACI\u00d3N\n# -------------------------------------------------------------\n# Aunque DBSCAN no requiere estrictamente escalado,\n# es buena pr\u00e1ctica para que epsilon sea m\u00e1s interpretable\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. CREAR Y APLICAR DBSCAN\n# -------------------------------------------------------------\n# Instanciar DBSCAN con par\u00e1metros b\u00e1sicos\ndbscan = DBSCAN(\n    eps=0.3,            # Radio de vecindad (epsilon)\n    min_samples=5       # M\u00ednimo de puntos para ser n\u00facleo (MinPts)\n)\n\n# fit_predict: aplica DBSCAN y devuelve las etiquetas\n# Etiquetas: 0, 1, 2, ... para clusters\n#            -1 para puntos de RUIDO (outliers)\nlabels = dbscan.fit_predict(X_scaled)\n\nprint(f\"\\nEtiquetas \u00fanicas: {np.unique(labels)}\")\nprint(f\"N\u00famero de clusters encontrados: {len(set(labels)) - (1 if -1 in labels else 0)}\")\nprint(f\"N\u00famero de outliers: {np.sum(labels == -1)}\")\n\n# Distribuci\u00f3n de puntos por cluster\nprint(\"\\nDistribuci\u00f3n de puntos:\")\nfor label in np.unique(labels):\n    count = np.sum(labels == label)\n    if label == -1:\n        print(f\"  Ruido (outliers): {count} puntos\")\n    else:\n        print(f\"  Cluster {label}: {count} puntos\")\n\n# -------------------------------------------------------------\n# 4. OBTENER INFORMACI\u00d3N ADICIONAL\n# -------------------------------------------------------------\n# core_sample_indices_: \u00edndices de los puntos n\u00facleo\ncore_samples = dbscan.core_sample_indices_\nprint(f\"\\nN\u00famero de puntos n\u00facleo: {len(core_samples)}\")\n\n# components_: los puntos n\u00facleo como array\nprint(f\"Forma de components_: {dbscan.components_.shape}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZACI\u00d3N DE RESULTADOS\n# -------------------------------------------------------------\nplt.figure(figsize=(12, 5))\n\n# 5.1 Clusters encontrados por DBSCAN\nplt.subplot(1, 2, 1)\n\n# Crear m\u00e1scara para puntos n\u00facleo\ncore_mask = np.zeros_like(labels, dtype=bool)\ncore_mask[core_samples] = True\n\n# Colores: usar colormap para clusters, negro para ruido\nunique_labels = set(labels)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        # Puntos de ruido en negro\n        color = 'black'\n        marker = 'x'\n        size = 50\n        label_name = 'Ruido'\n    else:\n        marker = 'o'\n        size = 50\n        label_name = f'Cluster {label}'\n\n    mask = labels == label\n    plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1],\n                c=[color], marker=marker, s=size,\n                label=label_name, alpha=0.7, edgecolors='w')\n\n# Resaltar puntos n\u00facleo con borde\nplt.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n            facecolors='none', edgecolors='red', s=100,\n            linewidths=1.5, label='Puntos n\u00facleo')\n\nplt.xlabel('Feature 1 (escalada)')\nplt.ylabel('Feature 2 (escalada)')\nplt.title(f'DBSCAN (eps={dbscan.eps}, min_samples={dbscan.min_samples})')\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\n\n# 5.2 Comparaci\u00f3n con las clases reales\nplt.subplot(1, 2, 2)\ny_true_extended = np.concatenate([y_true, [-1, -1, -1]])  # Outliers\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_true_extended,\n            cmap='viridis', alpha=0.7, edgecolors='w', s=50)\nplt.xlabel('Feature 1 (escalada)')\nplt.ylabel('Feature 2 (escalada)')\nplt.title('Datos originales (Ground Truth)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Salida esperada: <pre><code>Forma de los datos: (303, 2)\nEtiquetas \u00fanicas: [-1  0  1]\nN\u00famero de clusters encontrados: 2\nN\u00famero de outliers: 3\nDistribuci\u00f3n de puntos:\n  Ruido (outliers): 3 puntos\n  Cluster 0: 150 puntos\n  Cluster 1: 150 puntos\n</code></pre></p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#35-ejemplo-avanzado-seleccion-de-parametros-y-analisis-completo","title":"3.5. Ejemplo Avanzado: Selecci\u00f3n de Par\u00e1metros y An\u00e1lisis Completo","text":"<p>Este ejemplo muestra c\u00f3mo elegir los par\u00e1metros \u03b5 y MinPts de forma sistem\u00e1tica.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: DBSCAN con selecci\u00f3n de par\u00e1metros\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.datasets import make_blobs\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS CON RUIDO\n# -------------------------------------------------------------\n# Crear clusters con diferentes densidades y a\u00f1adir ruido\nnp.random.seed(42)\n\n# Cluster 1: denso\nX1 = np.random.randn(200, 2) * 0.5 + [0, 0]\n\n# Cluster 2: m\u00e1s disperso\nX2 = np.random.randn(150, 2) * 0.8 + [5, 5]\n\n# Cluster 3: alargado\nX3 = np.random.randn(100, 2) * np.array([0.3, 1.5]) + [2, -3]\n\n# Ruido uniforme\nnoise = np.random.uniform(-3, 8, size=(30, 2))\n\n# Combinar todos los datos\nX = np.vstack([X1, X2, X3, noise])\ny_true = np.array([0]*200 + [1]*150 + [2]*100 + [-1]*30)\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETO DE DBSCAN\")\nprint(\"=\"*60)\nprint(f\"\\nDatos: {X.shape[0]} puntos, {X.shape[1]} dimensiones\")\nprint(f\"Clusters reales: 3 + ruido\")\n\n# -------------------------------------------------------------\n# 2. PREPROCESAMIENTO\n# -------------------------------------------------------------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. M\u00c9TODO K-DISTANCE PARA ENCONTRAR EPSILON \u00d3PTIMO\n# -------------------------------------------------------------\n# El m\u00e9todo k-distance graph ayuda a encontrar un buen valor de \u03b5\n# Idea: Graficar la distancia al k-\u00e9simo vecino m\u00e1s cercano (ordenado)\n# Buscar el \"codo\" donde la distancia aumenta significativamente\n\ndef plot_k_distance(X, k=5):\n    \"\"\"\n    Grafica el k-distance para ayudar a elegir epsilon.\n\n    El m\u00e9todo funciona as\u00ed:\n    1. Para cada punto, calcula la distancia a su k-\u00e9simo vecino m\u00e1s cercano\n    2. Ordena estas distancias de menor a mayor\n    3. El \"codo\" de la curva sugiere un buen valor de epsilon\n    \"\"\"\n    # Calcular distancias a los k vecinos m\u00e1s cercanos\n    neighbors = NearestNeighbors(n_neighbors=k)\n    neighbors.fit(X)\n    distances, _ = neighbors.kneighbors(X)\n\n    # Tomar la distancia al k-\u00e9simo vecino (\u00faltima columna)\n    k_distances = distances[:, -1]\n\n    # Ordenar de menor a mayor\n    k_distances_sorted = np.sort(k_distances)\n\n    return k_distances_sorted\n\n# Probar diferentes valores de k\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor k in [4, 5, 10]:\n    k_dist = plot_k_distance(X_scaled, k=k)\n    axes[0].plot(range(len(k_dist)), k_dist, label=f'k={k}')\n\naxes[0].set_xlabel('Puntos (ordenados)')\naxes[0].set_ylabel(f'Distancia al k-\u00e9simo vecino')\naxes[0].set_title('M\u00e9todo K-Distance para elegir \u03b5')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Marcar el \"codo\" sugerido\nk_dist_5 = plot_k_distance(X_scaled, k=5)\nknee_idx = 450  # Aproximadamente donde est\u00e1 el codo\naxes[0].axhline(y=k_dist_5[knee_idx], color='r', linestyle='--', \n                label=f'\u03b5 sugerido \u2248 {k_dist_5[knee_idx]:.2f}')\naxes[0].axvline(x=knee_idx, color='r', linestyle='--', alpha=0.5)\naxes[0].legend()\n\n# Zoom en la regi\u00f3n del codo\naxes[1].plot(range(len(k_dist_5)), k_dist_5)\naxes[1].set_xlim([350, 500])\naxes[1].set_ylim([0, 1.5])\naxes[1].set_xlabel('Puntos (ordenados)')\naxes[1].set_ylabel('Distancia al 5-\u00e9simo vecino')\naxes[1].set_title('Zoom en la regi\u00f3n del codo')\naxes[1].axhline(y=k_dist_5[knee_idx], color='r', linestyle='--')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nEpsilon sugerido por m\u00e9todo k-distance: {k_dist_5[knee_idx]:.3f}\")\n\n# -------------------------------------------------------------\n# 4. B\u00daSQUEDA DE PAR\u00c1METROS \u00d3PTIMOS\n# -------------------------------------------------------------\n# Probar diferentes combinaciones de eps y min_samples\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"B\u00daSQUEDA DE PAR\u00c1METROS\")\nprint(\"=\"*60)\n\neps_values = [0.2, 0.3, 0.4, 0.5, 0.6]\nmin_samples_values = [3, 5, 7, 10]\n\nresults = []\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(X_scaled)\n\n        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n        n_noise = np.sum(labels == -1)\n\n        # Silhouette score (solo si hay m\u00e1s de 1 cluster y no todos son ruido)\n        if n_clusters &gt; 1 and n_noise &lt; len(labels) - 1:\n            # Excluir puntos de ruido para el c\u00e1lculo de silueta\n            mask = labels != -1\n            if np.sum(mask) &gt; n_clusters:\n                sil_score = silhouette_score(X_scaled[mask], labels[mask])\n            else:\n                sil_score = -1\n        else:\n            sil_score = -1\n\n        results.append({\n            'eps': eps,\n            'min_samples': min_samples,\n            'n_clusters': n_clusters,\n            'n_noise': n_noise,\n            'noise_pct': n_noise / len(labels) * 100,\n            'silhouette': sil_score\n        })\n\n# Mostrar resultados como DataFrame\ndf_results = pd.DataFrame(results)\nprint(\"\\nResultados de la b\u00fasqueda de par\u00e1metros:\")\nprint(df_results.round(3).to_string(index=False))\n\n# Encontrar mejor configuraci\u00f3n\nbest_idx = df_results[df_results['silhouette'] &gt; 0]['silhouette'].idxmax()\nbest_params = df_results.loc[best_idx]\nprint(f\"\\nMejor configuraci\u00f3n (por Silhouette):\")\nprint(f\"  eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\nprint(f\"  Clusters: {best_params['n_clusters']}, Silueta: {best_params['silhouette']:.3f}\")\n\n# -------------------------------------------------------------\n# 5. MODELO FINAL CON PAR\u00c1METROS \u00d3PTIMOS\n# -------------------------------------------------------------\neps_optimo = best_params['eps']\nmin_samples_optimo = int(best_params['min_samples'])\n\nprint(f\"\\n{'='*60}\")\nprint(f\"MODELO FINAL: eps={eps_optimo}, min_samples={min_samples_optimo}\")\nprint(f\"{'='*60}\")\n\n# Crear modelo final\ndbscan_final = DBSCAN(\n    eps=eps_optimo,\n    min_samples=min_samples_optimo,\n    metric='euclidean',     # M\u00e9trica de distancia\n    algorithm='auto',       # Algoritmo para calcular vecinos\n    leaf_size=30,           # Tama\u00f1o de hoja para ball_tree o kd_tree\n    n_jobs=-1               # Usar todos los n\u00facleos disponibles\n)\n\nlabels_final = dbscan_final.fit_predict(X_scaled)\n\n# Estad\u00edsticas del modelo final\nn_clusters_final = len(set(labels_final)) - (1 if -1 in labels_final else 0)\nn_noise_final = np.sum(labels_final == -1)\nn_core = len(dbscan_final.core_sample_indices_)\n\nprint(f\"\\nResultados del modelo final:\")\nprint(f\"  Clusters encontrados: {n_clusters_final}\")\nprint(f\"  Puntos de ruido: {n_noise_final} ({n_noise_final/len(X)*100:.1f}%)\")\nprint(f\"  Puntos n\u00facleo: {n_core} ({n_core/len(X)*100:.1f}%)\")\n\n# Distribuci\u00f3n por cluster\nprint(\"\\nDistribuci\u00f3n de puntos por cluster:\")\nfor label in sorted(np.unique(labels_final)):\n    count = np.sum(labels_final == label)\n    if label == -1:\n        print(f\"  Ruido: {count} puntos ({count/len(X)*100:.1f}%)\")\n    else:\n        print(f\"  Cluster {label}: {count} puntos ({count/len(X)*100:.1f}%)\")\n\n# -------------------------------------------------------------\n# 6. AN\u00c1LISIS DE CALIDAD DEL CLUSTERING\n# -------------------------------------------------------------\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nprint(f\"\\n{'='*60}\")\nprint(\"M\u00c9TRICAS DE EVALUACI\u00d3N\")\nprint(f\"{'='*60}\")\n\n# Silueta (excluyendo ruido)\nmask_no_noise = labels_final != -1\nif np.sum(mask_no_noise) &gt; 0 and len(np.unique(labels_final[mask_no_noise])) &gt; 1:\n    sil_final = silhouette_score(X_scaled[mask_no_noise], labels_final[mask_no_noise])\n    print(f\"\\nSilhouette Score (sin ruido): {sil_final:.4f}\")\nelse:\n    print(\"\\nNo se puede calcular Silueta (muy pocos clusters o muchos outliers)\")\n\n# Comparaci\u00f3n con ground truth\nari = adjusted_rand_score(y_true, labels_final)\nnmi = normalized_mutual_info_score(y_true, labels_final)\nprint(f\"Adjusted Rand Index: {ari:.4f}\")\nprint(f\"Normalized Mutual Info: {nmi:.4f}\")\n\n# Matriz de contingencia\nprint(\"\\nMatriz de Contingencia:\")\ncontingency = pd.crosstab(\n    pd.Series(labels_final, name='DBSCAN'),\n    pd.Series(y_true, name='Real')\n)\nprint(contingency)\n\n# -------------------------------------------------------------\n# 7. VISUALIZACI\u00d3N COMPLETA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 7.1 Clusters DBSCAN\nax1 = axes[0, 0]\nunique_labels = set(labels_final)\ncolors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n\nfor label, color in zip(unique_labels, colors):\n    if label == -1:\n        color = 'black'\n        marker = 'x'\n        alpha = 0.5\n    else:\n        marker = 'o'\n        alpha = 0.7\n\n    mask = labels_final == label\n    ax1.scatter(X_scaled[mask, 0], X_scaled[mask, 1],\n                c=[color], marker=marker, alpha=alpha, s=30)\n\n# Resaltar puntos n\u00facleo\ncore_mask = np.zeros_like(labels_final, dtype=bool)\ncore_mask[dbscan_final.core_sample_indices_] = True\nax1.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n            facecolors='none', edgecolors='red', s=80, linewidths=1)\nax1.set_title(f'DBSCAN (eps={eps_optimo}, min_samples={min_samples_optimo})')\nax1.set_xlabel('Feature 1')\nax1.set_ylabel('Feature 2')\nax1.grid(True, alpha=0.3)\n\n# 7.2 Ground Truth\nax2 = axes[0, 1]\nscatter = ax2.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_true,\n                      cmap='viridis', alpha=0.7, s=30)\nax2.set_title('Ground Truth')\nax2.set_xlabel('Feature 1')\nax2.set_ylabel('Feature 2')\nax2.grid(True, alpha=0.3)\n\n# 7.3 Puntos clasificados por tipo\nax3 = axes[1, 0]\n# Puntos n\u00facleo\nax3.scatter(X_scaled[core_mask, 0], X_scaled[core_mask, 1],\n            c='green', label=f'N\u00facleo ({np.sum(core_mask)})', alpha=0.6, s=30)\n# Puntos frontera (no n\u00facleo pero no ruido)\nborder_mask = ~core_mask &amp; (labels_final != -1)\nax3.scatter(X_scaled[border_mask, 0], X_scaled[border_mask, 1],\n            c='blue', label=f'Frontera ({np.sum(border_mask)})', alpha=0.6, s=30)\n# Ruido\nnoise_mask = labels_final == -1\nax3.scatter(X_scaled[noise_mask, 0], X_scaled[noise_mask, 1],\n            c='red', marker='x', label=f'Ruido ({np.sum(noise_mask)})', alpha=0.8, s=50)\nax3.set_title('Clasificaci\u00f3n de Puntos')\nax3.set_xlabel('Feature 1')\nax3.set_ylabel('Feature 2')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 7.4 Comparaci\u00f3n con K-Means\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels_kmeans = kmeans.fit_predict(X_scaled)\n\nax4 = axes[1, 1]\nax4.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_kmeans,\n            cmap='viridis', alpha=0.7, s=30)\nax4.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='*', s=200, edgecolors='black')\nax4.set_title('K-Means (k=3) para comparaci\u00f3n')\nax4.set_xlabel('Feature 1')\nax4.set_ylabel('Feature 2')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 8. HEUR\u00cdSTICA PARA ELEGIR MIN_SAMPLES\n# -------------------------------------------------------------\nprint(f\"\\n{'='*60}\")\nprint(\"HEUR\u00cdSTICAS PARA ELEGIR PAR\u00c1METROS\")\nprint(f\"{'='*60}\")\nprint(\"\"\"\nReglas generales para elegir par\u00e1metros:\n\n1. min_samples (MinPts):\n   - Regla b\u00e1sica: min_samples &gt;= dimensiones + 1\n   - En 2D: min_samples &gt;= 3 (m\u00ednimo te\u00f3rico)\n   - Pr\u00e1ctica com\u00fan: min_samples = 2 * dimensiones\n   - Para nuestros datos (2D): min_samples \u2248 4-5\n\n2. epsilon (eps):\n   - Usar el m\u00e9todo k-distance graph\n   - Buscar el \"codo\" donde la pendiente cambia\n   - k = min_samples para consistencia\n\n3. Iteraci\u00f3n:\n   - Empezar con min_samples = 2*dim\n   - Usar k-distance para eps inicial\n   - Ajustar bas\u00e1ndose en resultados\n\"\"\")\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#36-hiperparametros-de-dbscan-en-scikit-learn","title":"3.6. Hiperpar\u00e1metros de DBSCAN en scikit-learn","text":"Hiperpar\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>eps</code> Radio de vecindad \u03b5 float &gt; 0 Usar m\u00e9todo k-distance <code>min_samples</code> M\u00ednimo de puntos para ser n\u00facleo int &gt; 0 \\(\\geq dim + 1\\), t\u00edpico: \\(2 \\times dim\\) <code>metric</code> M\u00e9trica de distancia 'euclidean', 'manhattan', 'cosine', etc. 'euclidean' por defecto <code>algorithm</code> Algoritmo para calcular vecinos 'auto', 'ball_tree', 'kd_tree', 'brute' 'auto' (elige autom\u00e1ticamente) <code>leaf_size</code> Tama\u00f1o de hoja para ball_tree/kd_tree int &gt; 0 30 (default), ajustar para memoria <code>p</code> Potencia para Minkowski float &gt; 0 2 para Euclidiana, 1 para Manhattan <code>n_jobs</code> N\u00facleos para paralelizaci\u00f3n int o -1 -1 para usar todos"},{"location":"aprendizaje-no-supervisado/03-dbscan/#complejidad-computacional","title":"Complejidad Computacional","text":"Algoritmo Sin \u00edndice Con \u00edndice (kd_tree/ball_tree) Tiempo \\(O(n^2)\\) \\(O(n \\log n)\\) Espacio \\(O(n^2)\\) \\(O(n)\\)"},{"location":"aprendizaje-no-supervisado/03-dbscan/#37-aplicaciones-reales-de-dbscan","title":"3.7. Aplicaciones Reales de DBSCAN","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#1-deteccion-de-anomalias-en-sensores","title":"1. Detecci\u00f3n de Anomal\u00edas en Sensores","text":"<pre><code># Los puntos etiquetados como -1 son outliers autom\u00e1ticos\n# Ideal para detectar lecturas an\u00f3malas de sensores\nanomalies = X[labels == -1]\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#2-analisis-geoespacial","title":"2. An\u00e1lisis Geoespacial","text":"<p>DBSCAN es muy popular para agrupar datos geogr\u00e1ficos: - Clustering de ubicaciones GPS - Identificaci\u00f3n de zonas de alta densidad de eventos - Detecci\u00f3n de hotspots de crimen</p> <ul> <li>Tutorial: Geospatial Clustering with DBSCAN</li> </ul>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#3-segmentacion-de-imagenes","title":"3. Segmentaci\u00f3n de Im\u00e1genes","text":"<pre><code># Usar coordenadas de p\u00edxeles + valores RGB como features\n# DBSCAN agrupa p\u00edxeles similares y cercanos espacialmente\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#4-deteccion-de-comunidades-en-redes-sociales","title":"4. Detecci\u00f3n de Comunidades en Redes Sociales","text":"<ul> <li>Documentaci\u00f3n sklearn: DBSCAN Examples</li> </ul>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#38-dbscan-vs-k-means-cuando-usar-cada-uno","title":"3.8. DBSCAN vs K-Means: \u00bfCu\u00e1ndo Usar Cada Uno?","text":"Criterio K-Means DBSCAN Forma de clusters Esf\u00e9ricos Cualquier forma N\u00famero de clusters Debe especificarse Autom\u00e1tico Outliers Los asigna a un cluster Los identifica (-1) Escalabilidad Muy buena Moderada Densidad variable No maneja bien No maneja bien* Interpretabilidad Alta (centroides) Moderada <p>Para densidad variable, considerar OPTICS o HDBSCAN*.</p>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#ejemplo-visual-de-comparacion","title":"Ejemplo Visual de Comparaci\u00f3n","text":"<pre><code>from sklearn.datasets import make_circles\n\n# Datos conc\u00e9ntricos (K-Means fallar\u00e1)\nX_circles, _ = make_circles(n_samples=500, factor=0.5, noise=0.05)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Original\naxes[0].scatter(X_circles[:, 0], X_circles[:, 1], alpha=0.7)\naxes[0].set_title('Datos originales')\n\n# K-Means (incorrecto)\nlabels_km = KMeans(n_clusters=2).fit_predict(X_circles)\naxes[1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_km, cmap='viridis')\naxes[1].set_title('K-Means (INCORRECTO)')\n\n# DBSCAN (correcto)\nlabels_db = DBSCAN(eps=0.2, min_samples=5).fit_predict(X_circles)\naxes[2].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_db, cmap='viridis')\naxes[2].set_title('DBSCAN (CORRECTO)')\n\nplt.show()\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#39-variantes-y-alternativas","title":"3.9. Variantes y Alternativas","text":"Algoritmo Descripci\u00f3n Cu\u00e1ndo usar OPTICS Ordenamiento de puntos para identificar estructura Clusters con densidades variables HDBSCAN DBSCAN jer\u00e1rquico Clusters con densidades variables, m\u00e1s robusto Mean Shift Basado en gradientes de densidad No requiere n\u00famero de clusters ni eps"},{"location":"aprendizaje-no-supervisado/03-dbscan/#hdbscan-hierarchical-dbscan","title":"HDBSCAN (Hierarchical DBSCAN)","text":"<pre><code># pip install hdbscan\nimport hdbscan\n\n# HDBSCAN maneja mejor la variaci\u00f3n de densidad\nclusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\nlabels = clusterer.fit_predict(X_scaled)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#310-resumen-y-mejores-practicas","title":"3.10. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/03-dbscan/#checklist-para-usar-dbscan","title":"Checklist para usar DBSCAN","text":"<ul> <li>[ ] Escalar los datos (especialmente si tienen diferentes unidades)</li> <li>[ ] Usar m\u00e9todo k-distance para estimar epsilon</li> <li>[ ] Empezar con min_samples = 2*dimensiones</li> <li>[ ] Probar varios valores de eps y min_samples</li> <li>[ ] Evaluar con silueta (excluyendo outliers)</li> <li>[ ] Analizar los outliers (pueden ser informaci\u00f3n valiosa)</li> <li>[ ] Visualizar resultados para validaci\u00f3n</li> </ul>"},{"location":"aprendizaje-no-supervisado/03-dbscan/#cuando-elegir-dbscan","title":"\u00bfCu\u00e1ndo elegir DBSCAN?","text":"<p>\u2705 Usar DBSCAN cuando: - No conoces el n\u00famero de clusters - Los clusters tienen formas no esf\u00e9ricas - Hay outliers en los datos - Los clusters tienen densidades similares</p> <p>\u274c Considerar alternativas cuando: - Clusters con densidades muy diferentes \u2192 HDBSCAN, OPTICS - Datasets muy grandes \u2192 Mini-Batch K-Means - Necesitas centroides interpretables \u2192 K-Means - Datos de muy alta dimensionalidad \u2192 Reducir dimensiones primero</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/","title":"\ud83c\udf32 Unidad 4. Clustering Jer\u00e1rquico","text":"<p>El Clustering Jer\u00e1rquico es una familia de algoritmos que construyen una jerarqu\u00eda de clusters en lugar de una partici\u00f3n plana. Su caracter\u00edstica distintiva es que produce un dendrograma: una estructura de \u00e1rbol que muestra c\u00f3mo se forman o dividen los clusters a diferentes niveles de similitud. Este enfoque permite explorar la estructura de los datos a m\u00faltiples escalas sin necesidad de especificar el n\u00famero de clusters a priori.</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#41-como-funciona-el-clustering-jerarquico","title":"4.1. \u00bfC\u00f3mo Funciona el Clustering Jer\u00e1rquico?","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#dos-enfoques-principales","title":"Dos Enfoques Principales","text":"<p>Existen dos estrategias opuestas para construir la jerarqu\u00eda:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TIPOS DE CLUSTERING JER\u00c1RQUICO                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502 1. AGLOMERATIVO (Bottom-Up) - El m\u00e1s com\u00fan                  \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                    \u2502\n\u2502    \u2022 Empieza: cada punto es su propio cluster               \u2502\n\u2502    \u2022 Proceso: fusiona los dos clusters m\u00e1s cercanos         \u2502\n\u2502    \u2022 Termina: todos los puntos en un \u00fanico cluster          \u2502\n\u2502                                                             \u2502\n\u2502         \u25cb \u25cb \u25cb \u25cb \u25cb   \u2192   \u25cb\u25cb \u25cb \u25cb\u25cb   \u2192   \u25cb\u25cb\u25cb \u25cb\u25cb   \u2192   \u25cb\u25cb\u25cb\u25cb\u25cb    \u2502\n\u2502         5 clusters     4 clusters    2 clusters   1 cluster \u2502\n\u2502                                                             \u2502\n\u2502 2. DIVISIVO (Top-Down) - Menos com\u00fan                        \u2502\n\u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                           \u2502\n\u2502    \u2022 Empieza: todos los puntos en un \u00fanico cluster          \u2502\n\u2502    \u2022 Proceso: divide el cluster menos coherente             \u2502\n\u2502    \u2022 Termina: cada punto es su propio cluster               \u2502\n\u2502                                                             \u2502\n\u2502         \u25cb\u25cb\u25cb\u25cb\u25cb   \u2192   \u25cb\u25cb\u25cb \u25cb\u25cb   \u2192   \u25cb\u25cb \u25cb \u25cb\u25cb   \u2192   \u25cb \u25cb \u25cb \u25cb \u25cb    \u2502\n\u2502         1 cluster   2 clusters   4 clusters   5 clusters    \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#algoritmo-aglomerativo-paso-a-paso","title":"Algoritmo Aglomerativo Paso a Paso","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ALGORITMO AGLOMERATIVO (AGNES)                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Entrada: Datos X con n puntos, criterio de enlace           \u2502\n\u2502 Salida: Dendrograma (\u00e1rbol de fusiones)                     \u2502\n\u2502                                                             \u2502\n\u2502 1. INICIALIZACI\u00d3N:                                          \u2502\n\u2502    - Crear n clusters (uno por cada punto)                  \u2502\n\u2502    - Calcular matriz de distancias entre todos los pares    \u2502\n\u2502                                                             \u2502\n\u2502 2. REPETIR n-1 veces:                                       \u2502\n\u2502    a) Encontrar los dos clusters m\u00e1s cercanos               \u2502\n\u2502    b) Fusionarlos en un nuevo cluster                       \u2502\n\u2502    c) Actualizar la matriz de distancias                    \u2502\n\u2502    d) Registrar la fusi\u00f3n y su altura en el dendrograma     \u2502\n\u2502                                                             \u2502\n\u2502 3. RESULTADO:                                               \u2502\n\u2502    - Dendrograma completo                                   \u2502\n\u2502    - Para obtener k clusters: \"cortar\" el dendrograma       \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#el-dendrograma","title":"El Dendrograma","text":"<p>El dendrograma es la visualizaci\u00f3n clave del clustering jer\u00e1rquico:</p> <pre><code>Altura\n(distancia)\n   \u2502\n   \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502     \u250c\u2500\u2500\u2500\u2524         \u2502\n   \u2502     \u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n   \u2502  \u250c\u2500\u2500\u2524        \u2502\n   \u2502  \u2502  \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n   \u2502\u2500\u2500\u2524      \u2502\n   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n   \u2502         \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            A  B  C  D  E  (puntos)\n</code></pre> <ul> <li>Eje vertical: altura de fusi\u00f3n (distancia entre clusters fusionados)</li> <li>Eje horizontal: puntos o clusters individuales</li> <li>L\u00edneas horizontales: fusiones entre clusters</li> <li>Cortar horizontalmente: obtener un n\u00famero espec\u00edfico de clusters</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#42-explicacion-matematica","title":"4.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#matriz-de-distancias","title":"Matriz de Distancias","text":"<p>El clustering jer\u00e1rquico comienza calculando una matriz de distancias \\(D\\) de tama\u00f1o \\(n \\times n\\):</p> \\[D_{ij} = d(x_i, x_j)\\] <p>Donde \\(d\\) es una funci\u00f3n de distancia (t\u00edpicamente Euclidiana).</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#criterios-de-enlace-linkage","title":"Criterios de Enlace (Linkage)","text":"<p>La clave del clustering jer\u00e1rquico es c\u00f3mo se calcula la distancia entre clusters. Existen varios criterios de enlace:</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#1-enlace-simple-single-linkage-vecino-mas-cercano","title":"1. Enlace Simple (Single Linkage) - \"Vecino m\u00e1s cercano\"","text":"<p>Distancia entre los dos puntos m\u00e1s cercanos de cada cluster:</p> \\[d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x, y)\\] <ul> <li>\u2705 Puede detectar clusters de formas arbitrarias</li> <li>\u274c Sensible al \"efecto cadena\" (clusters elongados)</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#2-enlace-completo-complete-linkage-vecino-mas-lejano","title":"2. Enlace Completo (Complete Linkage) - \"Vecino m\u00e1s lejano\"","text":"<p>Distancia entre los dos puntos m\u00e1s lejanos de cada cluster:</p> \\[d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} d(x, y)\\] <ul> <li>\u2705 Produce clusters compactos de tama\u00f1o similar</li> <li>\u274c Sensible a outliers</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#3-enlace-promedio-average-linkage-upgma","title":"3. Enlace Promedio (Average Linkage - UPGMA)","text":"<p>Promedio de todas las distancias entre pares de puntos:</p> \\[d(C_i, C_j) = \\frac{1}{|C_i| \\cdot |C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} d(x, y)\\] <ul> <li>\u2705 Balance entre single y complete</li> <li>\u2705 Menos sensible a outliers</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#4-enlace-de-ward-wards-method","title":"4. Enlace de Ward (Ward's Method)","text":"<p>Minimiza el incremento en la varianza total al fusionar clusters:</p> \\[d(C_i, C_j) = \\sqrt{\\frac{2|C_i||C_j|}{|C_i|+|C_j|}} ||\\mu_i - \\mu_j||\\] <p>Donde \\(\\mu_i\\) y \\(\\mu_j\\) son los centroides de los clusters.</p> <p>Equivalentemente, Ward minimiza:</p> \\[\\Delta = \\sum_{x \\in C_i \\cup C_j} ||x - \\mu_{ij}||^2 - \\sum_{x \\in C_i} ||x - \\mu_i||^2 - \\sum_{x \\in C_j} ||x - \\mu_j||^2\\] <ul> <li>\u2705 Tiende a producir clusters esf\u00e9ricos y de tama\u00f1o similar</li> <li>\u2705 Similar a K-Means pero jer\u00e1rquico</li> <li>\u274c Asume clusters esf\u00e9ricos</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#visualizacion-de-criterios-de-enlace","title":"Visualizaci\u00f3n de Criterios de Enlace","text":"<pre><code>          Cluster A              Cluster B\n\n           \u25cf  \u25cf                    \u25b2  \u25b2\n         \u25cf      \u25cf                \u25b2      \u25b2\n           \u25cf  \u25cf                    \u25b2  \u25b2\n\nSingle:   d = distancia m\u00ednima (m\u00e1s corta)\nComplete: d = distancia m\u00e1xima (m\u00e1s larga)\nAverage:  d = promedio de todas las distancias\nWard:     d = incremento m\u00ednimo en varianza\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#43-pros-y-contras","title":"4.3. Pros y Contras","text":"Ventajas Desventajas No requiere k: El n\u00famero de clusters se elige despu\u00e9s Complejidad alta: \\(O(n^3)\\) tiempo, \\(O(n^2)\\) espacio Dendrograma informativo: Permite explorar estructura a m\u00faltiples niveles No escalable: Impracticable para datasets grandes (&gt;10K puntos) Flexibilidad: Diferentes criterios de enlace para diferentes necesidades Irreversible: Una fusi\u00f3n mala no puede deshacerse Determin\u00edstico: Mismo resultado cada ejecuci\u00f3n Sensible a outliers: Especialmente con complete linkage Interpretable: El dendrograma es f\u00e1cil de entender Sensible a la elecci\u00f3n de linkage: Resultados muy diferentes"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#44-ejemplo-basico-en-python","title":"4.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico del clustering jer\u00e1rquico con visualizaci\u00f3n del dendrograma.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: Clustering Jer\u00e1rquico con Dendrograma\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom scipy.cluster.hierarchy import (       # Funciones de scipy\n    linkage,        # Calcular el enlace jer\u00e1rquico\n    dendrogram,     # Crear el dendrograma\n    fcluster        # Obtener clusters del dendrograma\n)\nfrom sklearn.datasets import make_blobs     # Datos sint\u00e9ticos\nfrom sklearn.preprocessing import StandardScaler  # Escalado\n\n# -------------------------------------------------------------\n# 1. GENERAR DATOS DE EJEMPLO\n# -------------------------------------------------------------\n# Crear 4 clusters bien definidos\nX, y_true = make_blobs(\n    n_samples=50,       # 50 puntos (peque\u00f1o para visualizaci\u00f3n clara)\n    centers=4,          # 4 clusters\n    cluster_std=0.60,   # Dispersi\u00f3n moderada\n    random_state=42\n)\n\nprint(f\"Forma de los datos: {X.shape}\")\n\n# -------------------------------------------------------------\n# 2. CALCULAR LA MATRIZ DE ENLACE\n# -------------------------------------------------------------\n# linkage() calcula el clustering jer\u00e1rquico\n# Devuelve una matriz Z de (n-1) x 4:\n# - Columnas 0 y 1: \u00edndices de clusters fusionados\n# - Columna 2: distancia entre ellos (altura del enlace)\n# - Columna 3: n\u00famero de puntos en el nuevo cluster\n\nZ = linkage(\n    X,                  # Datos\n    method='ward',      # Criterio de enlace\n    metric='euclidean'  # M\u00e9trica de distancia\n)\n\nprint(f\"\\nMatriz de enlace Z (primeras 5 filas):\")\nprint(f\"[cluster_1, cluster_2, distancia, n_puntos]\")\nprint(Z[:5])\n\n# -------------------------------------------------------------\n# 3. VISUALIZAR EL DENDROGRAMA\n# -------------------------------------------------------------\nplt.figure(figsize=(14, 8))\n\n# dendrogram() crea la visualizaci\u00f3n del \u00e1rbol jer\u00e1rquico\n# truncate_mode='level' limita el n\u00famero de niveles mostrados\ndn = dendrogram(\n    Z,\n    truncate_mode='lastp',  # Mostrar los \u00faltimos p clusters fusionados\n    p=20,                    # N\u00famero de clusters a mostrar\n    leaf_rotation=90,        # Rotar etiquetas de hojas\n    leaf_font_size=10,       # Tama\u00f1o de fuente\n    show_contracted=True     # Mostrar clusters contra\u00eddos\n)\n\nplt.xlabel('Punto o Cluster', fontsize=12)\nplt.ylabel('Distancia (Altura)', fontsize=12)\nplt.title('Dendrograma - Clustering Jer\u00e1rquico (Ward Linkage)', fontsize=14)\n\n# A\u00f1adir l\u00ednea horizontal para \"cortar\" el dendrograma\n# Esta l\u00ednea indica d\u00f3nde cortar\u00edamos para obtener cierto n\u00famero de clusters\ncorte = 7  # Altura de corte\nplt.axhline(y=corte, color='r', linestyle='--', \n            label=f'Corte en altura={corte}')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 4. OBTENER ASIGNACIONES DE CLUSTER\n# -------------------------------------------------------------\n# fcluster() \"corta\" el dendrograma para obtener clusters\n# t: umbral de corte\n# criterion: c\u00f3mo interpretar t\n\n# Opci\u00f3n 1: Cortar por distancia (altura)\nlabels_dist = fcluster(Z, t=corte, criterion='distance')\nprint(f\"\\nClusters (corte por distancia={corte}): {np.unique(labels_dist)}\")\nprint(f\"Distribuci\u00f3n: {np.bincount(labels_dist)[1:]}\")  # [1:] porque fcluster empieza en 1\n\n# Opci\u00f3n 2: Especificar n\u00famero de clusters directamente\nlabels_k = fcluster(Z, t=4, criterion='maxclust')\nprint(f\"\\nClusters (k=4): {np.unique(labels_k)}\")\nprint(f\"Distribuci\u00f3n: {np.bincount(labels_k)[1:]}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZAR CLUSTERS RESULTANTES\n# -------------------------------------------------------------\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Clusters por corte de distancia\nscatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=labels_dist,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[0].set_title(f'Clusters (corte altura={corte})')\naxes[0].set_xlabel('Feature 1')\naxes[0].set_ylabel('Feature 2')\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# Clusters especificando k=4\nscatter2 = axes[1].scatter(X[:, 0], X[:, 1], c=labels_k,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[1].set_title('Clusters (k=4 especificado)')\naxes[1].set_xlabel('Feature 1')\naxes[1].set_ylabel('Feature 2')\nplt.colorbar(scatter2, ax=axes[1], label='Cluster')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2705 El dendrograma permite explorar la estructura a diferentes niveles\")\nprint(\"\u2705 Cortando a diferentes alturas obtenemos diferentes n\u00fameros de clusters\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#45-ejemplo-avanzado-comparacion-de-criterios-de-enlace","title":"4.5. Ejemplo Avanzado: Comparaci\u00f3n de Criterios de Enlace","text":"<p>Este ejemplo compara diferentes criterios de enlace y muestra c\u00f3mo elegir el n\u00famero \u00f3ptimo de clusters.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: An\u00e1lisis Completo de Clustering Jer\u00e1rquico\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\nfrom scipy.spatial.distance import pdist\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.datasets import load_iris\n\n# -------------------------------------------------------------\n# 1. CARGAR Y PREPARAR DATOS\n# -------------------------------------------------------------\niris = load_iris()\nX = iris.data\ny_true = iris.target\nfeature_names = iris.feature_names\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS DE CLUSTERING JER\u00c1RQUICO - DATASET IRIS\")\nprint(\"=\"*60)\nprint(f\"\\nDimensiones: {X.shape}\")\nprint(f\"Features: {feature_names}\")\n\n# Estandarizar datos\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 2. COMPARAR DIFERENTES CRITERIOS DE ENLACE\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARACI\u00d3N DE CRITERIOS DE ENLACE\")\nprint(\"=\"*60)\n\n# M\u00e9todos de enlace a comparar\nmethods = ['single', 'complete', 'average', 'ward']\nmethod_names = {\n    'single': 'Single (Vecino m\u00e1s cercano)',\n    'complete': 'Complete (Vecino m\u00e1s lejano)',\n    'average': 'Average (Promedio)',\n    'ward': 'Ward (Minimiza varianza)'\n}\n\n# Crear figura para dendrogramas\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\n# Almacenar resultados\nlinkage_results = {}\n\nfor idx, method in enumerate(methods):\n    # Calcular enlace\n    Z = linkage(X_scaled, method=method)\n    linkage_results[method] = Z\n\n    # Calcular correlaci\u00f3n cophenetica\n    # Mide qu\u00e9 tan bien el dendrograma preserva las distancias originales\n    c, _ = cophenet(Z, pdist(X_scaled))\n\n    # Dibujar dendrograma\n    ax = axes[idx]\n    dendrogram(Z, ax=ax, truncate_mode='lastp', p=30,\n               leaf_rotation=90, leaf_font_size=8,\n               show_contracted=True)\n    ax.set_title(f'{method_names[method]}\\nCorrelaci\u00f3n Cophenetica: {c:.3f}')\n    ax.set_xlabel('Muestra')\n    ax.set_ylabel('Distancia')\n\nplt.tight_layout()\nplt.show()\n\n# Mostrar correlaciones copheneticas\nprint(\"\\nCorrelaci\u00f3n Cophenetica por m\u00e9todo:\")\nprint(\"(Mayor = mejor preservaci\u00f3n de distancias originales)\")\nfor method in methods:\n    c, _ = cophenet(linkage_results[method], pdist(X_scaled))\n    print(f\"  {method_names[method]}: {c:.4f}\")\n\n# -------------------------------------------------------------\n# 3. AN\u00c1LISIS DEL DENDROGRAMA (Ward)\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS DETALLADO - M\u00c9TODO WARD\")\nprint(\"=\"*60)\n\nZ_ward = linkage_results['ward']\n\n# Analizar las \u00faltimas fusiones (m\u00e1s informativas)\nprint(\"\\n\u00daltimas 10 fusiones:\")\nprint(\"Fusi\u00f3n | Cluster1 | Cluster2 | Distancia | Tama\u00f1o\")\nprint(\"-\"*55)\nfor i in range(-10, 0):\n    row = Z_ward[i]\n    print(f\"{len(Z_ward)+i+1:6} | {int(row[0]):8} | {int(row[1]):8} | {row[2]:9.3f} | {int(row[3]):6}\")\n\n# -------------------------------------------------------------\n# 4. DETERMINAR N\u00daMERO \u00d3PTIMO DE CLUSTERS\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"DETERMINACI\u00d3N DEL N\u00daMERO \u00d3PTIMO DE CLUSTERS\")\nprint(\"=\"*60)\n\n# M\u00e9todo 1: An\u00e1lisis de las distancias de fusi\u00f3n\n# Buscar \"saltos\" grandes en las distancias\nheights = Z_ward[:, 2]\nheight_diffs = np.diff(heights)\n\n# Las \u00faltimas fusiones (las m\u00e1s significativas)\nprint(\"\\nSaltos de distancia en \u00faltimas fusiones:\")\nfor i in range(-5, 0):\n    n_clusters = len(Z_ward) - len(Z_ward) - i\n    print(f\"  {n_clusters} \u2192 {n_clusters-1} clusters: salto = {height_diffs[i]:.3f}\")\n\n# M\u00e9todo 2: Usar m\u00e9tricas de evaluaci\u00f3n\nk_range = range(2, 11)\nmetrics = {'k': [], 'silhouette': [], 'calinski': []}\n\nfor k in k_range:\n    labels = fcluster(Z_ward, t=k, criterion='maxclust')\n\n    metrics['k'].append(k)\n    metrics['silhouette'].append(silhouette_score(X_scaled, labels))\n    metrics['calinski'].append(calinski_harabasz_score(X_scaled, labels))\n\ndf_metrics = pd.DataFrame(metrics)\nprint(\"\\nM\u00e9tricas por n\u00famero de clusters:\")\nprint(df_metrics.round(4).to_string(index=False))\n\n# Visualizar m\u00e9tricas\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Dendrograma con l\u00edneas de corte\nax1 = axes[0]\ndendrogram(Z_ward, ax=ax1, truncate_mode='lastp', p=20)\nax1.axhline(y=8, color='r', linestyle='--', label='k=3')\nax1.axhline(y=5, color='g', linestyle='--', label='k=5')\nax1.set_title('Dendrograma Ward')\nax1.legend()\n\n# Silhouette\nax2 = axes[1]\nax2.plot(df_metrics['k'], df_metrics['silhouette'], 'bo-')\nax2.set_xlabel('N\u00famero de clusters')\nax2.set_ylabel('Silhouette Score')\nax2.set_title('Silueta vs k')\nax2.grid(True, alpha=0.3)\n\n# Calinski-Harabasz\nax3 = axes[2]\nax3.plot(df_metrics['k'], df_metrics['calinski'], 'go-')\nax3.set_xlabel('N\u00famero de clusters')\nax3.set_ylabel('Calinski-Harabasz')\nax3.set_title('Calinski-Harabasz vs k')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 5. MODELO FINAL CON SKLEARN\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODELO FINAL CON AgglomerativeClustering\")\nprint(\"=\"*60)\n\nk_optimo = 3  # Basado en an\u00e1lisis\n\n# Usando sklearn.cluster.AgglomerativeClustering\n# M\u00e1s flexible y permite especificar k directamente\nmodel = AgglomerativeClustering(\n    n_clusters=k_optimo,        # N\u00famero de clusters\n    metric='euclidean',         # M\u00e9trica de distancia\n    linkage='ward',             # Criterio de enlace\n    # Par\u00e1metros adicionales:\n    # compute_full_tree: bool, calcular \u00e1rbol completo aunque n_clusters est\u00e9 especificado\n    # distance_threshold: None o float, distancia para el corte (si se usa, n_clusters debe ser None)\n)\n\n# Entrenar y obtener etiquetas\nlabels = model.fit_predict(X_scaled)\n\nprint(f\"\\nResultados:\")\nprint(f\"  N\u00famero de clusters: {model.n_clusters_}\")\nprint(f\"  N\u00famero de hojas: {model.n_leaves_}\")\nprint(f\"  N\u00famero de componentes conectados: {model.n_connected_components_}\")\n\n# Distribuci\u00f3n\nprint(f\"\\nDistribuci\u00f3n de puntos:\")\nfor i in range(k_optimo):\n    count = np.sum(labels == i)\n    print(f\"  Cluster {i}: {count} puntos ({count/len(labels)*100:.1f}%)\")\n\n# M\u00e9tricas\nprint(f\"\\nM\u00e9tricas de evaluaci\u00f3n:\")\nprint(f\"  Silhouette Score: {silhouette_score(X_scaled, labels):.4f}\")\nprint(f\"  Calinski-Harabasz: {calinski_harabasz_score(X_scaled, labels):.4f}\")\n\n# -------------------------------------------------------------\n# 6. COMPARACI\u00d3N CON GROUND TRUTH\n# -------------------------------------------------------------\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"COMPARACI\u00d3N CON ETIQUETAS REALES\")\nprint(\"=\"*60)\n\nari = adjusted_rand_score(y_true, labels)\nnmi = normalized_mutual_info_score(y_true, labels)\n\nprint(f\"\\n  Adjusted Rand Index: {ari:.4f}\")\nprint(f\"  Normalized Mutual Info: {nmi:.4f}\")\n\n# Matriz de contingencia\nprint(\"\\nMatriz de Contingencia:\")\ncontingency = pd.crosstab(\n    pd.Series(labels, name='Cluster'),\n    pd.Series(y_true, name='Especie'),\n    margins=True\n)\ncontingency.columns = list(iris.target_names) + ['Total']\nprint(contingency)\n\n# -------------------------------------------------------------\n# 7. PERFIL DE CLUSTERS\n# -------------------------------------------------------------\nprint(f\"\\n\" + \"=\"*60)\nprint(\"PERFIL DE CADA CLUSTER\")\nprint(\"=\"*60)\n\ndf_result = pd.DataFrame(X, columns=feature_names)\ndf_result['cluster'] = labels\n\nprint(\"\\nMedia por cluster (valores originales):\")\nprint(df_result.groupby('cluster').mean().round(2))\n\n# -------------------------------------------------------------\n# 8. VISUALIZACI\u00d3N FINAL\n# -------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n\n# Reducir a 2D para visualizaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Clusters jer\u00e1rquicos\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[0].set_title('Clustering Jer\u00e1rquico (Ward)')\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\nplt.colorbar(scatter1, ax=axes[0])\n\n# Ground truth\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_true,\n                           cmap='viridis', edgecolors='w', s=50)\naxes[1].set_title('Especies Reales')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\nplt.colorbar(scatter2, ax=axes[1])\n\n# Comparaci\u00f3n de m\u00e9todos de enlace (para k=3)\nlabels_by_method = {}\nfor method in methods:\n    labels_by_method[method] = fcluster(linkage_results[method], \n                                         t=3, criterion='maxclust')\n\n# Mostrar silueta por m\u00e9todo\nsilhouettes = [silhouette_score(X_scaled, labels_by_method[m]) for m in methods]\nbars = axes[2].bar(methods, silhouettes, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\naxes[2].set_ylabel('Silhouette Score')\naxes[2].set_title('Silueta por Criterio de Enlace (k=3)')\naxes[2].set_ylim([0, max(silhouettes) * 1.2])\n\n# A\u00f1adir valores sobre las barras\nfor bar, val in zip(bars, silhouettes):\n    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                 f'{val:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 9. EJEMPLO CON DISTANCE_THRESHOLD\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALTERNATIVA: Clustering por Umbral de Distancia\")\nprint(\"=\"*60)\n\n# En lugar de especificar k, especificar distancia de corte\nmodel_dist = AgglomerativeClustering(\n    n_clusters=None,            # No especificar k\n    distance_threshold=10,      # Umbral de distancia\n    metric='euclidean',\n    linkage='ward'\n)\n\nlabels_dist = model_dist.fit_predict(X_scaled)\nn_clusters_found = len(np.unique(labels_dist))\n\nprint(f\"\\nCon distance_threshold=10:\")\nprint(f\"  Clusters encontrados: {n_clusters_found}\")\nprint(f\"  Distribuci\u00f3n: {np.bincount(labels_dist)}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#46-hiperparametros-en-scikit-learn","title":"4.6. Hiperpar\u00e1metros en scikit-learn","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#scipyclusterhierarchylinkage","title":"scipy.cluster.hierarchy.linkage","text":"Par\u00e1metro Descripci\u00f3n Valores <code>y</code> Datos o matriz de distancias array (n, d) o condensada <code>method</code> Criterio de enlace 'single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward' <code>metric</code> M\u00e9trica de distancia 'euclidean', 'cityblock', 'cosine', etc. <code>optimal_ordering</code> Reordenar hojas para minimizar distancias True/False"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#sklearnclusteragglomerativeclustering","title":"sklearn.cluster.AgglomerativeClustering","text":"Par\u00e1metro Descripci\u00f3n Valores <code>n_clusters</code> N\u00famero de clusters (None si se usa distance_threshold) int o None <code>distance_threshold</code> Umbral de distancia para corte float o None <code>metric</code> M\u00e9trica de distancia 'euclidean', 'manhattan', 'cosine', etc. <code>linkage</code> Criterio de enlace 'ward', 'complete', 'average', 'single' <code>compute_full_tree</code> Calcular \u00e1rbol completo 'auto', True, False <code>compute_distances</code> Calcular distancias entre clusters True/False"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#47-criterio-de-enlace-cual-elegir","title":"4.7. Criterio de Enlace: \u00bfCu\u00e1l Elegir?","text":"Criterio Forma de Clusters Sensibilidad a Outliers Cu\u00e1ndo Usar Single Arbitraria, elongada Baja Detectar clusters de forma irregular Complete Compacta, esf\u00e9rica Alta Clusters de tama\u00f1o similar Average Moderada Media Balance general Ward Compacta, esf\u00e9rica Media Similar a K-Means, varianza m\u00ednima"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#visualizacion-del-efecto-del-enlace","title":"Visualizaci\u00f3n del Efecto del Enlace","text":"<pre><code># Datos con forma irregular (dos lunas)\nfrom sklearn.datasets import make_moons\nX_moons, _ = make_moons(n_samples=200, noise=0.05)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nmethods = ['single', 'complete', 'average', 'ward']\n\nfor ax, method in zip(axes.flatten(), methods):\n    model = AgglomerativeClustering(n_clusters=2, linkage=method)\n    labels = model.fit_predict(X_moons)\n    ax.scatter(X_moons[:, 0], X_moons[:, 1], c=labels, cmap='viridis')\n    ax.set_title(f'{method.capitalize()} Linkage')\n\nplt.tight_layout()\nplt.show()\n# Single linkage funcionar\u00e1 mejor con estos datos irregulares\n</code></pre>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#48-aplicaciones-reales","title":"4.8. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#1-analisis-filogenetico-biologia","title":"1. An\u00e1lisis Filogen\u00e9tico (Biolog\u00eda)","text":"<p>Construir \u00e1rboles evolutivos basados en similitud gen\u00e9tica. * Tutorial: Phylogenetic Trees with Hierarchical Clustering</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#2-segmentacion-de-documentos","title":"2. Segmentaci\u00f3n de Documentos","text":"<p>Agrupar documentos similares para organizaci\u00f3n autom\u00e1tica.</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#3-analisis-de-expresion-genica","title":"3. An\u00e1lisis de Expresi\u00f3n G\u00e9nica","text":"<p>Identificar grupos de genes con patrones de expresi\u00f3n similares. * Ejemplo: Gene Expression Clustering</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#4-segmentacion-de-mercado","title":"4. Segmentaci\u00f3n de Mercado","text":"<p>El dendrograma permite identificar segmentos a diferentes niveles de granularidad.</p>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#49-comparacion-con-otros-metodos","title":"4.9. Comparaci\u00f3n con Otros M\u00e9todos","text":"Aspecto Jer\u00e1rquico K-Means DBSCAN Requiere k No (a priori) S\u00ed No Escalabilidad Baja (\\(O(n^3)\\)) Alta (\\(O(nk)\\)) Media (\\(O(n^2)\\)) Formas de cluster Depende del enlace Esf\u00e9ricas Arbitrarias Outliers No los detecta No los detecta S\u00ed los detecta Interpretabilidad Alta (dendrograma) Alta (centroides) Moderada Exploraci\u00f3n multinivel S\u00ed No No"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#410-resumen-y-mejores-practicas","title":"4.10. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#checklist-para-clustering-jerarquico","title":"Checklist para Clustering Jer\u00e1rquico","text":"<ul> <li>[ ] Escalar los datos (especialmente para Ward)</li> <li>[ ] Elegir criterio de enlace apropiado para la forma esperada de clusters</li> <li>[ ] Calcular correlaci\u00f3n cophenetica para validar el dendrograma</li> <li>[ ] Analizar el dendrograma visualmente antes de cortar</li> <li>[ ] Probar varios puntos de corte y evaluar con m\u00e9tricas</li> <li>[ ] Comparar con otros m\u00e9todos (K-Means, DBSCAN)</li> </ul>"},{"location":"aprendizaje-no-supervisado/04-clustering-jerarquico/#cuando-elegir-clustering-jerarquico","title":"\u00bfCu\u00e1ndo Elegir Clustering Jer\u00e1rquico?","text":"<p>\u2705 Usar Jer\u00e1rquico cuando: - Dataset peque\u00f1o-mediano (&lt; 10K puntos) - Quieres explorar la estructura a m\u00faltiples niveles - El dendrograma es informativo para el dominio - No sabes cu\u00e1ntos clusters hay</p> <p>\u274c Considerar alternativas cuando: - Dataset grande \u2192 K-Means o Mini-Batch K-Means - Necesitas identificar outliers \u2192 DBSCAN - Clusters de formas muy irregulares \u2192 DBSCAN + Single linkage - Eficiencia computacional es cr\u00edtica</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/05-pca/","title":"\ud83d\udcc9 Unidad 5. PCA - An\u00e1lisis de Componentes Principales","text":"<p>El An\u00e1lisis de Componentes Principales (PCA, del ingl\u00e9s Principal Component Analysis) es la t\u00e9cnica de reducci\u00f3n de dimensionalidad m\u00e1s utilizada en Machine Learning y estad\u00edstica. PCA transforma los datos a un nuevo sistema de coordenadas donde las primeras dimensiones (componentes principales) capturan la mayor parte de la varianza de los datos. Es una t\u00e9cnica fundamental para visualizaci\u00f3n, compresi\u00f3n de datos y preprocesamiento.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#51-que-es-la-reduccion-de-dimensionalidad","title":"5.1. \u00bfQu\u00e9 es la Reducci\u00f3n de Dimensionalidad?","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#el-problema-de-la-alta-dimensionalidad","title":"El Problema de la Alta Dimensionalidad","text":"<p>En muchos datasets reales, el n\u00famero de caracter\u00edsticas (dimensiones) puede ser muy alto: - Im\u00e1genes: miles de p\u00edxeles - Texto: miles de palabras - Gen\u00f3mica: miles de genes - Finanzas: cientos de indicadores</p> <p>Esto causa varios problemas:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PROBLEMAS DE LA ALTA DIMENSIONALIDAD                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502 1. \"MALDICI\u00d3N DE LA DIMENSIONALIDAD\"                        \u2502\n\u2502    - Los datos se vuelven muy dispersos                     \u2502\n\u2502    - Las distancias pierden significado                     \u2502\n\u2502    - Aumenta el overfitting                                 \u2502\n\u2502                                                             \u2502\n\u2502 2. VISUALIZACI\u00d3N IMPOSIBLE                                  \u2502\n\u2502    - No podemos visualizar m\u00e1s de 3 dimensiones             \u2502\n\u2502                                                             \u2502\n\u2502 3. COSTO COMPUTACIONAL                                      \u2502\n\u2502    - M\u00e1s dimensiones = m\u00e1s tiempo y memoria                 \u2502\n\u2502                                                             \u2502\n\u2502 4. MULTICOLINEALIDAD                                        \u2502\n\u2502    - Variables correlacionadas = redundancia                \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#que-hace-pca","title":"\u00bfQu\u00e9 Hace PCA?","text":"<p>PCA encuentra las direcciones de m\u00e1xima varianza en los datos y proyecta los datos en estas direcciones:</p> <pre><code>Datos originales (2D)         Despu\u00e9s de PCA\n        \u2502                          \u2502\n    y   \u2502    \u2571\u2571\u2571\u2571                   \u2502    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        \u2502   \u2571\u2571\u2571\u2571                    \u2502    PC1 (m\u00e1xima varianza)\n        \u2502  \u2571\u2571\u2571\u2571                     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 x                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n   Los datos tienen              PC1 captura la direcci\u00f3n\n   varianza en diagonal          de m\u00e1xima dispersi\u00f3n\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#52-explicacion-matematica","title":"5.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#intuicion-geometrica","title":"Intuici\u00f3n Geom\u00e9trica","text":"<p>PCA busca los ejes de m\u00e1xima varianza: 1. Encontrar la direcci\u00f3n donde los datos est\u00e1n m\u00e1s dispersos \u2192 PC1 2. Encontrar la siguiente direcci\u00f3n ortogonal con m\u00e1xima varianza \u2192 PC2 3. Repetir hasta tener d componentes (d = dimensiones originales)</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#pasos-matematicos","title":"Pasos Matem\u00e1ticos","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#paso-1-centrar-los-datos","title":"Paso 1: Centrar los Datos","text":"<p>Restar la media de cada variable para que los datos est\u00e9n centrados en el origen:</p> \\[X_{centered} = X - \\bar{X}\\] <p>Donde \\(\\bar{X}\\) es el vector de medias de cada columna.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-2-calcular-la-matriz-de-covarianza","title":"Paso 2: Calcular la Matriz de Covarianza","text":"<p>La matriz de covarianza \\(\\Sigma\\) captura las relaciones entre variables:</p> \\[\\Sigma = \\frac{1}{n-1} X_{centered}^T X_{centered}\\] <p>Para dos variables \\(x\\) y \\(y\\): \\(\\(Cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\)\\)</p> <p>La matriz de covarianza es de tama\u00f1o \\(d \\times d\\) (donde \\(d\\) = n\u00famero de variables):</p> \\[\\Sigma = \\begin{bmatrix} Var(x_1) &amp; Cov(x_1, x_2) &amp; \\cdots \\\\ Cov(x_2, x_1) &amp; Var(x_2) &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\]"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-3-calcular-autovalores-y-autovectores","title":"Paso 3: Calcular Autovalores y Autovectores","text":"<p>Los autovectores de \\(\\Sigma\\) son las direcciones de los componentes principales. Los autovalores correspondientes indican cu\u00e1nta varianza captura cada componente.</p> \\[\\Sigma v = \\lambda v\\] <p>Donde: - \\(v\\) = autovector (direcci\u00f3n del componente principal) - \\(\\lambda\\) = autovalor (varianza explicada por ese componente)</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-4-ordenar-y-seleccionar-componentes","title":"Paso 4: Ordenar y Seleccionar Componentes","text":"<p>Los componentes se ordenan por autovalor descendente: - \\(\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_d\\) - PC1 tiene el mayor autovalor, PC2 el segundo, etc.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#paso-5-proyectar-los-datos","title":"Paso 5: Proyectar los Datos","text":"<p>Para reducir de \\(d\\) dimensiones a \\(k\\) dimensiones:</p> \\[X_{reducido} = X_{centered} \\cdot W_k\\] <p>Donde \\(W_k\\) es la matriz de los \\(k\\) primeros autovectores (columnas).</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#varianza-explicada","title":"Varianza Explicada","text":"<p>La varianza explicada por cada componente es:</p> \\[\\text{Varianza explicada}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\\] <p>La varianza acumulada indica cu\u00e1nta informaci\u00f3n total preservamos:</p> \\[\\text{Varianza acumulada}_k = \\sum_{i=1}^{k} \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\\]"},{"location":"aprendizaje-no-supervisado/05-pca/#53-pros-y-contras","title":"5.3. Pros y Contras","text":"Ventajas Desventajas Reducci\u00f3n efectiva: Elimina redundancia y ruido Transformaci\u00f3n lineal: No captura relaciones no lineales Visualizaci\u00f3n: Permite visualizar datos de alta dimensi\u00f3n P\u00e9rdida de informaci\u00f3n: Los componentes descartados contienen algo de informaci\u00f3n Decorrelaci\u00f3n: Los componentes son ortogonales (no correlacionados) Dif\u00edcil interpretaci\u00f3n: Los componentes son combinaciones de todas las variables Sin hiperpar\u00e1metros: Solo decidir cu\u00e1ntos componentes mantener Sensible a escala: Requiere estandarizaci\u00f3n previa Eficiente: Complejidad \\(O(nd^2 + d^3)\\) Sensible a outliers: Los outliers afectan la direcci\u00f3n de m\u00e1xima varianza"},{"location":"aprendizaje-no-supervisado/05-pca/#54-ejemplo-basico-en-python","title":"5.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra c\u00f3mo aplicar PCA para visualizar datos de alta dimensi\u00f3n.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: PCA para visualizaci\u00f3n del dataset Iris\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.decomposition import PCA       # Algoritmo PCA\nfrom sklearn.preprocessing import StandardScaler  # Estandarizaci\u00f3n\nfrom sklearn.datasets import load_iris      # Dataset de ejemplo\n\n# -------------------------------------------------------------\n# 1. CARGAR DATOS\n# -------------------------------------------------------------\niris = load_iris()\nX = iris.data           # 150 muestras \u00d7 4 caracter\u00edsticas\ny = iris.target         # Etiquetas de especie\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\nprint(\"=\"*50)\nprint(\"PCA - EJEMPLO B\u00c1SICO CON DATASET IRIS\")\nprint(\"=\"*50)\nprint(f\"\\nDimensiones originales: {X.shape}\")\nprint(f\"Features: {feature_names}\")\n\n# -------------------------------------------------------------\n# 2. ESTANDARIZAR LOS DATOS (CRUCIAL)\n# -------------------------------------------------------------\n# PCA es sensible a la escala, por lo que SIEMPRE debemos estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"\\nDatos estandarizados:\")\nprint(f\"  Media por feature: {X_scaled.mean(axis=0).round(4)}\")\nprint(f\"  Std por feature: {X_scaled.std(axis=0).round(4)}\")\n\n# -------------------------------------------------------------\n# 3. APLICAR PCA\n# -------------------------------------------------------------\n# Reducir de 4 dimensiones a 2 para poder visualizar\npca = PCA(n_components=2)   # Mantener solo 2 componentes\n\n# fit_transform: calcula los componentes y transforma los datos\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"\\nDimensiones despu\u00e9s de PCA: {X_pca.shape}\")\n\n# -------------------------------------------------------------\n# 4. ANALIZAR LOS COMPONENTES PRINCIPALES\n# -------------------------------------------------------------\n# explained_variance_ratio_: proporci\u00f3n de varianza explicada por cada componente\nprint(f\"\\nVarianza explicada por componente:\")\nfor i, var in enumerate(pca.explained_variance_ratio_):\n    print(f\"  PC{i+1}: {var*100:.2f}%\")\nprint(f\"  Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n\n# explained_variance_: autovalores (varianza absoluta)\nprint(f\"\\nAutovalores (varianza absoluta):\")\nfor i, var in enumerate(pca.explained_variance_):\n    print(f\"  PC{i+1}: {var:.4f}\")\n\n# components_: autovectores (direcciones de los componentes)\n# Cada fila es un componente, cada columna es el peso de esa variable original\nprint(f\"\\nAutovectores (componentes_):\")\nprint(f\"Formato: [sepal_length, sepal_width, petal_length, petal_width]\")\nfor i, comp in enumerate(pca.components_):\n    print(f\"  PC{i+1}: {comp.round(4)}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZAR LOS DATOS REDUCIDOS\n# -------------------------------------------------------------\nplt.figure(figsize=(10, 8))\n\n# Crear scatter plot coloreado por especie\ncolors = ['navy', 'turquoise', 'darkorange']\nfor i, (color, name) in enumerate(zip(colors, target_names)):\n    mask = y == i\n    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],\n                c=color, label=name, alpha=0.7, edgecolors='w', s=60)\n\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% varianza)', fontsize=12)\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% varianza)', fontsize=12)\nplt.title('PCA del Dataset Iris (4D \u2192 2D)', fontsize=14)\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\n\n# A\u00f1adir vectores de carga (loadings) para interpretar los componentes\n# Los loadings muestran c\u00f3mo contribuye cada variable a cada componente\nfor i, (var, name) in enumerate(zip(pca.components_.T, feature_names)):\n    plt.annotate('', xy=(var[0]*3, var[1]*3), xytext=(0, 0),\n                 arrowprops=dict(arrowstyle='-&gt;', color='red', lw=2))\n    plt.text(var[0]*3.2, var[1]*3.2, name.replace(' (cm)', ''),\n             fontsize=10, color='red', ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 6. INTERPRETACI\u00d3N DE LOS COMPONENTES\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"INTERPRETACI\u00d3N DE LOS COMPONENTES\")\nprint(\"=\"*50)\n\n# Crear DataFrame de loadings para mejor visualizaci\u00f3n\nimport pandas as pd\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=feature_names\n)\nprint(\"\\nLoadings (contribuci\u00f3n de cada variable):\")\nprint(loadings.round(4))\n\nprint(\"\"\"\nInterpretaci\u00f3n:\n- PC1: Combinaci\u00f3n principalmente de petal_length y petal_width (tama\u00f1o del p\u00e9talo)\n       Variables con signos similares y magnitudes altas.\n- PC2: Contrasta sepal_width contra las dem\u00e1s (forma del s\u00e9palo vs resto)\n       sepal_width tiene signo opuesto a las otras variables.\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#55-ejemplo-avanzado-seleccion-de-componentes-y-reconstruccion","title":"5.5. Ejemplo Avanzado: Selecci\u00f3n de Componentes y Reconstrucci\u00f3n","text":"<p>Este ejemplo muestra c\u00f3mo elegir el n\u00famero \u00f3ptimo de componentes y c\u00f3mo reconstruir los datos.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: PCA completo con an\u00e1lisis de varianza\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import fetch_openml\n\n# -------------------------------------------------------------\n# 1. CARGAR DATOS DE ALTA DIMENSIONALIDAD (MNIST)\n# -------------------------------------------------------------\nprint(\"=\"*60)\nprint(\"PCA AVANZADO - DATASET MNIST (D\u00cdGITOS)\")\nprint(\"=\"*60)\nprint(\"\\nCargando datos... (puede tardar un momento)\")\n\n# Cargar un subconjunto de MNIST para eficiencia\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data     # 1797 muestras \u00d7 64 p\u00edxeles (8\u00d78)\ny = digits.target\n\nprint(f\"\\nDimensiones originales: {X.shape}\")\nprint(f\"Cada imagen es de {int(np.sqrt(X.shape[1]))}\u00d7{int(np.sqrt(X.shape[1]))} p\u00edxeles\")\n\n# -------------------------------------------------------------\n# 2. ESTANDARIZAR\n# -------------------------------------------------------------\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. PCA COMPLETO PARA AN\u00c1LISIS DE VARIANZA\n# -------------------------------------------------------------\n# Primero, ajustar PCA con todos los componentes para analizar varianza\npca_full = PCA()  # n_components=None \u2192 todos los componentes\npca_full.fit(X_scaled)\n\n# Varianza explicada por cada componente\nvar_ratio = pca_full.explained_variance_ratio_\nvar_cumsum = np.cumsum(var_ratio)\n\nprint(f\"\\nVarianza explicada por los primeros 10 componentes:\")\nfor i in range(10):\n    print(f\"  PC{i+1}: {var_ratio[i]*100:.2f}% (acumulada: {var_cumsum[i]*100:.2f}%)\")\n\n# -------------------------------------------------------------\n# 4. VISUALIZAR VARIANZA EXPLICADA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Gr\u00e1fico de varianza individual\nax1 = axes[0]\nax1.bar(range(1, len(var_ratio)+1), var_ratio, alpha=0.6, label='Individual')\nax1.set_xlabel('Componente Principal')\nax1.set_ylabel('Proporci\u00f3n de Varianza Explicada')\nax1.set_title('Varianza Explicada por Componente')\nax1.set_xlim([0, 30])\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Gr\u00e1fico de varianza acumulada\nax2 = axes[1]\nax2.plot(range(1, len(var_cumsum)+1), var_cumsum, 'bo-', markersize=4)\nax2.axhline(y=0.90, color='r', linestyle='--', label='90% varianza')\nax2.axhline(y=0.95, color='g', linestyle='--', label='95% varianza')\nax2.axhline(y=0.99, color='orange', linestyle='--', label='99% varianza')\n\n# Encontrar n\u00famero de componentes para diferentes umbrales\nn_90 = np.argmax(var_cumsum &gt;= 0.90) + 1\nn_95 = np.argmax(var_cumsum &gt;= 0.95) + 1\nn_99 = np.argmax(var_cumsum &gt;= 0.99) + 1\n\nax2.axvline(x=n_90, color='r', linestyle=':', alpha=0.5)\nax2.axvline(x=n_95, color='g', linestyle=':', alpha=0.5)\nax2.axvline(x=n_99, color='orange', linestyle=':', alpha=0.5)\n\nax2.set_xlabel('N\u00famero de Componentes')\nax2.set_ylabel('Varianza Acumulada')\nax2.set_title('Varianza Acumulada')\nax2.legend(loc='lower right')\nax2.grid(True, alpha=0.3)\nax2.set_xlim([0, 64])\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nComponentes necesarios para:\")\nprint(f\"  90% varianza: {n_90} componentes\")\nprint(f\"  95% varianza: {n_95} componentes\")\nprint(f\"  99% varianza: {n_99} componentes\")\n\n# -------------------------------------------------------------\n# 5. REDUCCI\u00d3N A 2D PARA VISUALIZACI\u00d3N\n# -------------------------------------------------------------\npca_2d = PCA(n_components=2)\nX_2d = pca_2d.fit_transform(X_scaled)\n\nplt.figure(figsize=(12, 10))\nscatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10',\n                      alpha=0.6, edgecolors='w', s=20)\nplt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\nplt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\nplt.title('D\u00edgitos MNIST Proyectados a 2D con PCA')\nplt.colorbar(scatter, label='D\u00edgito')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# -------------------------------------------------------------\n# 6. RECONSTRUCCI\u00d3N DE IM\u00c1GENES\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECONSTRUCCI\u00d3N DE IM\u00c1GENES\")\nprint(\"=\"*60)\n\n# Funci\u00f3n para reconstruir y visualizar\ndef reconstruct_and_show(X, pca, scaler, n_components_list, sample_idx=0):\n    \"\"\"\n    Reconstruye una imagen con diferente n\u00famero de componentes.\n    \"\"\"\n    original = X[sample_idx].reshape(8, 8)\n\n    fig, axes = plt.subplots(1, len(n_components_list)+1, figsize=(15, 3))\n\n    # Imagen original\n    axes[0].imshow(original, cmap='gray')\n    axes[0].set_title(f'Original\\n(64 dims)')\n    axes[0].axis('off')\n\n    X_sample = X[sample_idx:sample_idx+1]\n    X_scaled = scaler.transform(X_sample)\n\n    for i, n_comp in enumerate(n_components_list):\n        # Ajustar PCA con n componentes\n        pca_temp = PCA(n_components=n_comp)\n        pca_temp.fit(scaler.transform(X))\n\n        # Transformar y reconstruir\n        X_reduced = pca_temp.transform(X_scaled)\n        X_reconstructed = pca_temp.inverse_transform(X_reduced)\n        X_reconstructed = scaler.inverse_transform(X_reconstructed)\n\n        # Calcular error de reconstrucci\u00f3n\n        mse = np.mean((X_sample - X_reconstructed)**2)\n\n        # Mostrar\n        axes[i+1].imshow(X_reconstructed.reshape(8, 8), cmap='gray')\n        axes[i+1].set_title(f'{n_comp} componentes\\nMSE: {mse:.2f}')\n        axes[i+1].axis('off')\n\n    plt.suptitle('Reconstrucci\u00f3n con Diferente N\u00famero de Componentes', fontsize=14)\n    plt.tight_layout()\n    plt.show()\n\n# Reconstruir el primer d\u00edgito con diferentes componentes\nn_components_list = [2, 5, 10, 20, 40]\nreconstruct_and_show(X, pca_full, scaler, n_components_list, sample_idx=0)\n\nprint(\"\\nObservaci\u00f3n: Con ~20 componentes ya se obtiene una buena reconstrucci\u00f3n\")\nprint(\"del d\u00edgito original, a pesar de reducir de 64 a 20 dimensiones (~70% compresi\u00f3n)\")\n\n# -------------------------------------------------------------\n# 7. VISUALIZAR LOS COMPONENTES PRINCIPALES (EIGENFACES)\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPONENTES PRINCIPALES COMO IM\u00c1GENES\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor i in range(10):\n    # Cada componente se puede visualizar como una imagen\n    component = pca_full.components_[i].reshape(8, 8)\n    axes[i].imshow(component, cmap='RdBu')\n    axes[i].set_title(f'PC{i+1}\\n({var_ratio[i]*100:.1f}%)')\n    axes[i].axis('off')\n\nplt.suptitle('Primeros 10 Componentes Principales (como im\u00e1genes 8\u00d78)', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"Los primeros componentes capturan patrones globales (bordes, formas)\")\nprint(\"Los componentes posteriores capturan detalles m\u00e1s finos\")\n\n# -------------------------------------------------------------\n# 8. PCA COMO PREPROCESAMIENTO PARA CLASIFICACI\u00d3N\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"PCA COMO PREPROCESAMIENTO PARA ML\")\nprint(\"=\"*60)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.3, random_state=42\n)\n\n# Comparar rendimiento con diferentes niveles de reducci\u00f3n\nresults = []\n\nfor n_comp in [None, 40, 20, 10, 5, 2]:\n    if n_comp is None:\n        # Sin reducci\u00f3n\n        X_train_pca = X_train\n        X_test_pca = X_test\n        n_dims = X_train.shape[1]\n    else:\n        # Con PCA\n        pca = PCA(n_components=n_comp)\n        X_train_pca = pca.fit_transform(X_train)\n        X_test_pca = pca.transform(X_test)\n        n_dims = n_comp\n\n    # Entrenar clasificador\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    start_time = time.time()\n    knn.fit(X_train_pca, y_train)\n    y_pred = knn.predict(X_test_pca)\n    elapsed = time.time() - start_time\n\n    acc = accuracy_score(y_test, y_pred)\n\n    results.append({\n        'Dimensiones': n_dims,\n        'Accuracy': acc,\n        'Tiempo (s)': elapsed\n    })\n\ndf_results = pd.DataFrame(results)\nprint(\"\\nRendimiento de KNN con diferentes niveles de reducci\u00f3n:\")\nprint(df_results.to_string(index=False))\n\n# Visualizar\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(df_results['Dimensiones'], df_results['Accuracy'], 'bo-', markersize=10)\nax.set_xlabel('N\u00famero de Dimensiones (componentes)')\nax.set_ylabel('Accuracy')\nax.set_title('Accuracy vs Dimensionalidad (KNN con PCA)')\nax.invert_xaxis()  # Invertir eje x para mejor lectura\nax.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\"\"\nConclusi\u00f3n:\n- PCA puede reducir significativamente las dimensiones manteniendo buen rendimiento\n- Menos dimensiones = entrenamiento/predicci\u00f3n m\u00e1s r\u00e1pido\n- Hay un punto \u00f3ptimo donde reducimos complejidad sin perder mucha precisi\u00f3n\n\"\"\")\n\n# -------------------------------------------------------------\n# 9. PCA INCREMENTAL PARA DATASETS GRANDES\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"VARIANTES DE PCA\")\nprint(\"=\"*60)\n\nprint(\"\"\"\nscikit-learn ofrece varias implementaciones de PCA:\n\n1. PCA (est\u00e1ndar):\n   - Usa SVD completo\n   - Para datasets que caben en memoria\n   - from sklearn.decomposition import PCA\n\n2. IncrementalPCA:\n   - Procesa datos en batches\n   - Para datasets muy grandes que no caben en memoria\n   - from sklearn.decomposition import IncrementalPCA\n\n3. SparsePCA:\n   - Produce componentes con muchos ceros (sparse)\n   - Mejor interpretabilidad\n   - from sklearn.decomposition import SparsePCA\n\n4. KernelPCA:\n   - PCA no lineal usando kernels\n   - Para relaciones no lineales en los datos\n   - from sklearn.decomposition import KernelPCA\n\"\"\")\n\n# Ejemplo de KernelPCA\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.datasets import make_moons\n\n# Datos no linealmente separables\nX_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=42)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Original\naxes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis')\naxes[0].set_title('Datos Originales')\n\n# PCA lineal\npca_linear = PCA(n_components=2)\nX_pca_linear = pca_linear.fit_transform(X_moons)\naxes[1].scatter(X_pca_linear[:, 0], X_pca_linear[:, 1], c=y_moons, cmap='viridis')\naxes[1].set_title('PCA Lineal (igual que original)')\n\n# Kernel PCA (RBF)\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\nX_kpca = kpca.fit_transform(X_moons)\naxes[2].scatter(X_kpca[:, 0], X_kpca[:, 1], c=y_moons, cmap='viridis')\naxes[2].set_title('Kernel PCA (RBF)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"KernelPCA puede capturar relaciones no lineales que PCA est\u00e1ndar no puede\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#56-hiperparametros-de-pca-en-scikit-learn","title":"5.6. Hiperpar\u00e1metros de PCA en scikit-learn","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_components</code> N\u00famero de componentes a mantener int, float, 'mle', None Ver nota abajo <code>svd_solver</code> Algoritmo para calcular SVD 'auto', 'full', 'arpack', 'randomized' 'auto' para la mayor\u00eda de casos <code>whiten</code> Si blanquear los datos (varianza unitaria) True/False True si se usa para clasificaci\u00f3n <code>random_state</code> Semilla para svd_solver='randomized' int o None Fijar para reproducibilidad"},{"location":"aprendizaje-no-supervisado/05-pca/#especificar-n_components","title":"Especificar <code>n_components</code>","text":"<pre><code># Formas de especificar n_components:\n\n# 1. N\u00famero exacto de componentes\npca = PCA(n_components=10)  # Exactamente 10 componentes\n\n# 2. Proporci\u00f3n de varianza a preservar (0-1)\npca = PCA(n_components=0.95)  # Componentes para retener 95% de varianza\n\n# 3. Estimaci\u00f3n autom\u00e1tica con MLE\npca = PCA(n_components='mle')  # Estimaci\u00f3n de m\u00e1xima verosimilitud\n\n# 4. Todos los componentes\npca = PCA(n_components=None)  # min(n_samples, n_features) componentes\n</code></pre>"},{"location":"aprendizaje-no-supervisado/05-pca/#57-aplicaciones-reales-de-pca","title":"5.7. Aplicaciones Reales de PCA","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#1-compresion-de-imagenes","title":"1. Compresi\u00f3n de Im\u00e1genes","text":"<ul> <li>Eigenfaces: Reconocimiento facial con dimensionalidad reducida</li> <li>Tutorial: Eigenfaces</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#2-visualizacion-de-datos","title":"2. Visualizaci\u00f3n de Datos","text":"<p>Proyectar datos de alta dimensi\u00f3n a 2D/3D para exploraci\u00f3n.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#3-preprocesamiento-para-ml","title":"3. Preprocesamiento para ML","text":"<ul> <li>Reducir overfitting al eliminar features redundantes</li> <li>Acelerar el entrenamiento de modelos</li> <li>Eliminar multicolinealidad</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#4-analisis-de-expresion-genica","title":"4. An\u00e1lisis de Expresi\u00f3n G\u00e9nica","text":"<p>Identificar genes importantes y visualizar grupos de muestras.</p>"},{"location":"aprendizaje-no-supervisado/05-pca/#5-finanzas","title":"5. Finanzas","text":"<ul> <li>An\u00e1lisis de portafolio (factores de riesgo)</li> <li>Detecci\u00f3n de patrones en series financieras</li> <li>Tutorial: PCA in Finance</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#58-pca-vs-otras-tecnicas","title":"5.8. PCA vs Otras T\u00e9cnicas","text":"T\u00e9cnica Tipo Preserva Mejor para PCA Lineal Varianza global Datos lineales, preprocesamiento t-SNE No lineal Estructura local Visualizaci\u00f3n de clusters UMAP No lineal Estructura local + global Visualizaci\u00f3n + ML LDA Supervisado Separabilidad de clases Clasificaci\u00f3n Autoencoders No lineal Representaci\u00f3n aprendida Relaciones complejas"},{"location":"aprendizaje-no-supervisado/05-pca/#59-resumen-y-mejores-practicas","title":"5.9. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/05-pca/#checklist-para-usar-pca","title":"Checklist para usar PCA","text":"<ul> <li>[ ] Estandarizar los datos (StandardScaler) - \u00a1Obligatorio!</li> <li>[ ] Analizar la varianza explicada para elegir n_components</li> <li>[ ] Usar 95% de varianza como regla general</li> <li>[ ] Visualizar loadings para interpretar componentes</li> <li>[ ] Verificar que la reducci\u00f3n no afecta negativamente el modelo downstream</li> <li>[ ] Considerar alternativas no lineales si PCA no funciona bien</li> </ul>"},{"location":"aprendizaje-no-supervisado/05-pca/#cuando-usar-pca","title":"\u00bfCu\u00e1ndo usar PCA?","text":"<p>\u2705 Usar PCA cuando: - Los datos tienen alta dimensionalidad - Hay multicolinealidad entre variables - Necesitas visualizar datos multidimensionales - Quieres preprocesar para acelerar ML - Las relaciones son aproximadamente lineales</p> <p>\u274c Considerar alternativas cuando: - Las relaciones son altamente no lineales \u2192 Kernel PCA, Autoencoders - Necesitas visualizaci\u00f3n con clusters claros \u2192 t-SNE, UMAP - Tienes un problema de clasificaci\u00f3n \u2192 LDA</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/","title":"\ud83d\udd2e Unidad 6. t-SNE - Visualizaci\u00f3n de Alta Dimensi\u00f3n","text":"<p>t-SNE (t-distributed Stochastic Neighbor Embedding) es una t\u00e9cnica de reducci\u00f3n de dimensionalidad no lineal dise\u00f1ada espec\u00edficamente para visualizaci\u00f3n de datos de alta dimensi\u00f3n. A diferencia de PCA que preserva la varianza global, t-SNE se enfoca en preservar la estructura local: los puntos que son similares en el espacio original permanecen cercanos en el espacio reducido. Esto lo hace excepcional para revelar clusters y patrones ocultos.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#61-por-que-t-sne","title":"6.1. \u00bfPor Qu\u00e9 t-SNE?","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#limitaciones-de-pca","title":"Limitaciones de PCA","text":"<p>PCA es una t\u00e9cnica lineal que preserva la varianza global. Sin embargo: - No captura relaciones no lineales - No preserva bien la estructura de clusters - Los datos proyectados pueden solaparse incluso si los clusters originales est\u00e1n bien separados</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#la-idea-de-t-sne","title":"La Idea de t-SNE","text":"<p>t-SNE se pregunta: \"\u00bfC\u00f3mo puedo proyectar los datos de manera que los vecinos cercanos en alta dimensi\u00f3n sigan siendo vecinos cercanos en baja dimensi\u00f3n?\"</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INTUICI\u00d3N DE t-SNE                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502 Espacio Original (Alta Dimensi\u00f3n)                           \u2502\n\u2502                                                             \u2502\n\u2502     A est\u00e1 cerca de B y C                                   \u2502\n\u2502     A est\u00e1 lejos de X e Y                                   \u2502\n\u2502                                                             \u2502\n\u2502              A \u25cf \u25cf B                                        \u2502\n\u2502                \u25cf C                                          \u2502\n\u2502                                                             \u2502\n\u2502                           X \u25cf \u25cf Y                           \u2502\n\u2502                                                             \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502                                                             \u2502\n\u2502 t-SNE preserva estas relaciones de vecindad:                \u2502\n\u2502                                                             \u2502\n\u2502 Espacio Reducido (2D)                                       \u2502\n\u2502                                                             \u2502\n\u2502         \u25cfA \u25cfB               \u25cfX                              \u2502\n\u2502          \u25cfC                  \u25cfY                             \u2502\n\u2502                                                             \u2502\n\u2502 \u2713 A sigue cerca de B y C                                   \u2502\n\u2502 \u2713 A sigue lejos de X e Y                                   \u2502\n\u2502 \u2713 Clusters visualmente separados                            \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/06-tsne/#62-explicacion-matematica","title":"6.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#paso-1-calcular-similitudes-en-alta-dimension","title":"Paso 1: Calcular Similitudes en Alta Dimensi\u00f3n","text":"<p>Para cada par de puntos \\((x_i, x_j)\\) en el espacio original, t-SNE calcula una probabilidad condicional \\(p_{j|i}\\) que representa qu\u00e9 tan probable es que \\(x_i\\) elija a \\(x_j\\) como su vecino si los vecinos se eligieran proporcionalmente a una distribuci\u00f3n Gaussiana centrada en \\(x_i\\):</p> \\[p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}\\] <p>Donde \\(\\sigma_i\\) es la varianza de la Gaussiana centrada en \\(x_i\\) (se ajusta autom\u00e1ticamente seg\u00fan el par\u00e1metro perplexity).</p> <p>Las probabilidades se simetrizan: \\(\\(p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#paso-2-calcular-similitudes-en-baja-dimension","title":"Paso 2: Calcular Similitudes en Baja Dimensi\u00f3n","text":"<p>En el espacio reducido, t-SNE usa una distribuci\u00f3n t de Student (con 1 grado de libertad, es decir, distribuci\u00f3n de Cauchy) en lugar de Gaussiana:</p> \\[q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1 + ||y_k - y_l||^2)^{-1}}\\] <p>Donde \\(y_i\\) y \\(y_j\\) son las representaciones en baja dimensi\u00f3n.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#por-que-la-distribucion-t","title":"\u00bfPor Qu\u00e9 la Distribuci\u00f3n t?","text":"<p>La distribuci\u00f3n t tiene colas m\u00e1s pesadas que la Gaussiana:</p> <pre><code>          Gaussiana          Distribuci\u00f3n t\n             ___                  ___\n            /   \\               /     \\\n           /     \\             /       \\\n          /       \\           /         \\\n      ___/         \\___     _/           \\_\n           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500              \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n         Colas ligeras       Colas pesadas\n</code></pre> <p>Esto resuelve el problema del amontonamiento (crowding problem): - En alta dimensi\u00f3n hay mucho \"espacio\" para que los puntos se dispersen - En baja dimensi\u00f3n hay menos espacio - Las colas pesadas permiten que puntos moderadamente lejanos se separen m\u00e1s, dejando espacio para los clusters</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#paso-3-minimizar-la-divergencia-kl","title":"Paso 3: Minimizar la Divergencia KL","text":"<p>t-SNE minimiza la divergencia de Kullback-Leibler entre las distribuciones P (alta dim) y Q (baja dim):</p> \\[KL(P||Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\\] <p>Esta funci\u00f3n de costo penaliza fuertemente cuando: - Puntos cercanos en alta dimensi\u00f3n (\\(p_{ij}\\) alto) quedan lejos en baja dimensi\u00f3n (\\(q_{ij}\\) bajo)</p> <p>La minimizaci\u00f3n se hace mediante descenso de gradiente.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#el-parametro-perplexity","title":"El Par\u00e1metro Perplexity","text":"<p>La perplexity es el hiperpar\u00e1metro m\u00e1s importante de t-SNE. Intuitivamente, es una medida del n\u00famero efectivo de vecinos cercanos:</p> \\[Perplexity = 2^{H(P_i)}\\] <p>Donde \\(H(P_i)\\) es la entrop\u00eda de Shannon de la distribuci\u00f3n de probabilidad centrada en \\(x_i\\).</p> <ul> <li>Perplexity baja (5-10): Solo considera vecinos muy cercanos \u2192 estructura muy local</li> <li>Perplexity alta (30-50): Considera m\u00e1s vecinos \u2192 estructura m\u00e1s global</li> <li>Regla: Debe ser menor que el n\u00famero de puntos</li> </ul>"},{"location":"aprendizaje-no-supervisado/06-tsne/#63-pros-y-contras","title":"6.3. Pros y Contras","text":"Ventajas Desventajas Excelente para visualizaci\u00f3n: Revela clusters claramente Solo para visualizaci\u00f3n: No usar para preprocesamiento de ML Preserva estructura local: Vecinos cercanos permanecen juntos Lento: Complejidad \\(O(n^2)\\), aunque hay aproximaciones No lineal: Captura relaciones complejas No determin\u00edstico: Diferentes ejecuciones dan diferentes resultados Funciona bien con clusters: Separa grupos visualmente Distancias no interpretables: Las distancias entre clusters no tienen significado Hiperpar\u00e1metros simples: Principalmente perplexity Sensible a hiperpar\u00e1metros: Perplexity afecta mucho el resultado"},{"location":"aprendizaje-no-supervisado/06-tsne/#64-ejemplo-basico-en-python","title":"6.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico de t-SNE para visualizar el dataset de d\u00edgitos.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: t-SNE para visualizaci\u00f3n de d\u00edgitos\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.manifold import TSNE           # Algoritmo t-SNE\nfrom sklearn.preprocessing import StandardScaler  # Estandarizaci\u00f3n\nfrom sklearn.datasets import load_digits    # Dataset de d\u00edgitos\n\n# -------------------------------------------------------------\n# 1. CARGAR DATOS\n# -------------------------------------------------------------\ndigits = load_digits()\nX = digits.data     # 1797 muestras \u00d7 64 caracter\u00edsticas (8\u00d78 p\u00edxeles)\ny = digits.target   # Etiquetas de d\u00edgitos (0-9)\n\nprint(\"=\"*50)\nprint(\"t-SNE - EJEMPLO B\u00c1SICO CON D\u00cdGITOS\")\nprint(\"=\"*50)\nprint(f\"\\nDimensiones originales: {X.shape}\")\nprint(f\"Clases: {np.unique(y)}\")\n\n# -------------------------------------------------------------\n# 2. ESTANDARIZAR LOS DATOS\n# -------------------------------------------------------------\n# Aunque t-SNE es robusto a la escala, es buena pr\u00e1ctica estandarizar\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------------------------------------\n# 3. APLICAR t-SNE\n# -------------------------------------------------------------\n# Reducir de 64 dimensiones a 2 para visualizaci\u00f3n\nprint(\"\\nAplicando t-SNE (puede tardar un momento)...\")\n\ntsne = TSNE(\n    n_components=2,         # Reducir a 2 dimensiones\n    perplexity=30,          # N\u00famero efectivo de vecinos (t\u00edpico: 5-50)\n    random_state=42,        # Reproducibilidad\n    n_iter=1000,            # N\u00famero de iteraciones de optimizaci\u00f3n\n    learning_rate='auto'    # Tasa de aprendizaje autom\u00e1tica\n)\n\n# fit_transform: ajusta el modelo y transforma los datos\nX_tsne = tsne.fit_transform(X_scaled)\n\nprint(f\"Dimensiones despu\u00e9s de t-SNE: {X_tsne.shape}\")\nprint(f\"Divergencia KL final: {tsne.kl_divergence_:.4f}\")\n\n# -------------------------------------------------------------\n# 4. VISUALIZAR RESULTADOS\n# -------------------------------------------------------------\nplt.figure(figsize=(12, 10))\n\n# Crear scatter plot con colores por d\u00edgito\nscatter = plt.scatter(\n    X_tsne[:, 0], X_tsne[:, 1],\n    c=y,                    # Color seg\u00fan el d\u00edgito\n    cmap='tab10',           # Paleta de 10 colores\n    alpha=0.7,              # Transparencia\n    edgecolors='w',         # Borde blanco\n    s=30                    # Tama\u00f1o de puntos\n)\n\n# A\u00f1adir etiquetas en los centroides de cada cluster\nfor digit in range(10):\n    mask = y == digit\n    centroid = X_tsne[mask].mean(axis=0)\n    plt.annotate(\n        str(digit),\n        centroid,\n        fontsize=20,\n        fontweight='bold',\n        ha='center',\n        va='center',\n        color='black',\n        bbox=dict(boxstyle='circle', facecolor='white', alpha=0.8)\n    )\n\nplt.xlabel('t-SNE Dimensi\u00f3n 1', fontsize=12)\nplt.ylabel('t-SNE Dimensi\u00f3n 2', fontsize=12)\nplt.title('Visualizaci\u00f3n de D\u00edgitos con t-SNE (64D \u2192 2D)', fontsize=14)\nplt.colorbar(scatter, label='D\u00edgito')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# -------------------------------------------------------------\n# 5. COMPARAR CON PCA\n# -------------------------------------------------------------\nfrom sklearn.decomposition import PCA\n\n# Aplicar PCA para comparaci\u00f3n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Comparar visualizaciones\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# PCA\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10',\n                           alpha=0.7, edgecolors='w', s=20)\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\naxes[0].set_title('PCA (lineal) - D\u00edgitos')\nplt.colorbar(scatter1, ax=axes[0])\n\n# t-SNE\nscatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                           alpha=0.7, edgecolors='w', s=20)\naxes[1].set_xlabel('t-SNE 1')\naxes[1].set_ylabel('t-SNE 2')\naxes[1].set_title('t-SNE (no lineal) - D\u00edgitos')\nplt.colorbar(scatter2, ax=axes[1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nObservaciones:\n- t-SNE separa claramente los clusters de d\u00edgitos\n- PCA muestra m\u00e1s solapamiento entre clases\n- t-SNE es superior para visualizar estructura de clusters\n- Las distancias en t-SNE no son interpretables (solo la estructura)\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/06-tsne/#65-ejemplo-avanzado-efecto-de-hiperparametros-y-buenas-practicas","title":"6.5. Ejemplo Avanzado: Efecto de Hiperpar\u00e1metros y Buenas Pr\u00e1cticas","text":"<p>Este ejemplo explora el efecto de la perplexity y otros par\u00e1metros.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: An\u00e1lisis de hiperpar\u00e1metros de t-SNE\n# ============================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_digits\nimport time\n\n# -------------------------------------------------------------\n# 1. CARGAR Y PREPARAR DATOS\n# -------------------------------------------------------------\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"=\"*60)\nprint(\"AN\u00c1LISIS DE HIPERPAR\u00c1METROS DE t-SNE\")\nprint(\"=\"*60)\n\n# -------------------------------------------------------------\n# 2. EFECTO DE LA PERPLEXITY\n# -------------------------------------------------------------\nprint(\"\\n[1] EFECTO DE LA PERPLEXITY\")\nprint(\"-\"*40)\n\nperplexities = [5, 15, 30, 50, 100]\n\nfig, axes = plt.subplots(1, len(perplexities), figsize=(20, 4))\n\nfor i, perp in enumerate(perplexities):\n    print(f\"  Calculando perplexity={perp}...\", end=\" \")\n    start = time.time()\n\n    tsne = TSNE(n_components=2, perplexity=perp, random_state=42,\n                n_iter=1000, learning_rate='auto')\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    elapsed = time.time() - start\n    print(f\"({elapsed:.1f}s)\")\n\n    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                    alpha=0.6, s=10, edgecolors='none')\n    axes[i].set_title(f'Perplexity = {perp}')\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n\nplt.suptitle('Efecto de la Perplexity en t-SNE', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nInterpretaci\u00f3n de Perplexity:\n- Perplexity baja (5-10): Estructura muy local, clusters peque\u00f1os\n- Perplexity media (30): Balance entre local y global (recomendado)\n- Perplexity alta (50-100): Estructura m\u00e1s global, clusters m\u00e1s grandes\n- Perplexity &gt; n_samples/3 puede causar problemas\n\"\"\")\n\n# -------------------------------------------------------------\n# 3. EFECTO DEL N\u00daMERO DE ITERACIONES\n# -------------------------------------------------------------\nprint(\"\\n[2] EFECTO DEL N\u00daMERO DE ITERACIONES\")\nprint(\"-\"*40)\n\nn_iters = [250, 500, 1000, 2000]\n\nfig, axes = plt.subplots(1, len(n_iters), figsize=(16, 4))\n\nfor i, n_iter in enumerate(n_iters):\n    print(f\"  Calculando n_iter={n_iter}...\", end=\" \")\n    start = time.time()\n\n    tsne = TSNE(n_components=2, perplexity=30, random_state=42,\n                n_iter=n_iter, learning_rate='auto')\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    elapsed = time.time() - start\n    print(f\"KL={tsne.kl_divergence_:.4f} ({elapsed:.1f}s)\")\n\n    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                    alpha=0.6, s=10, edgecolors='none')\n    axes[i].set_title(f'n_iter={n_iter}\\nKL={tsne.kl_divergence_:.3f}')\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n\nplt.suptitle('Efecto del N\u00famero de Iteraciones', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nInterpretaci\u00f3n:\n- Muy pocas iteraciones: t-SNE no converge (estructura incompleta)\n- 1000 iteraciones suele ser suficiente para la mayor\u00eda de casos\n- M\u00e1s iteraciones mejoran hasta un punto, luego estabilizan\n\"\"\")\n\n# -------------------------------------------------------------\n# 4. ESTABILIDAD: M\u00daLTIPLES EJECUCIONES\n# -------------------------------------------------------------\nprint(\"\\n[3] ESTABILIDAD DE t-SNE\")\nprint(\"-\"*40)\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\n\nfor i in range(4):\n    # Diferentes random_state\n    tsne = TSNE(n_components=2, perplexity=30, random_state=i*10,\n                n_iter=1000, learning_rate='auto')\n    X_tsne = tsne.fit_transform(X_scaled)\n\n    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10',\n                    alpha=0.6, s=10, edgecolors='none')\n    axes[i].set_title(f'random_state={i*10}')\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n\nplt.suptitle('Diferentes Inicializaciones de t-SNE', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nObservaciones sobre estabilidad:\n- t-SNE NO es determin\u00edstico (diferente resultado cada vez)\n- Los CLUSTERS se preservan, pero su POSICI\u00d3N y ORIENTACI\u00d3N cambian\n- No comparar posiciones entre diferentes ejecuciones\n- Usar random_state fijo para reproducibilidad\n\"\"\")\n\n# -------------------------------------------------------------\n# 5. INICIALIZACI\u00d3N CON PCA\n# -------------------------------------------------------------\nprint(\"\\n[4] INICIALIZACI\u00d3N CON PCA (Recomendado)\")\nprint(\"-\"*40)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Sin inicializaci\u00f3n PCA\ntsne_random = TSNE(n_components=2, perplexity=30, random_state=42,\n                   n_iter=1000, init='random', learning_rate='auto')\nX_tsne_random = tsne_random.fit_transform(X_scaled)\n\n# Con inicializaci\u00f3n PCA\ntsne_pca = TSNE(n_components=2, perplexity=30, random_state=42,\n                n_iter=1000, init='pca', learning_rate='auto')\nX_tsne_pca = tsne_pca.fit_transform(X_scaled)\n\naxes[0].scatter(X_tsne_random[:, 0], X_tsne_random[:, 1], c=y, \n                cmap='tab10', alpha=0.6, s=15)\naxes[0].set_title(f\"init='random'\\nKL={tsne_random.kl_divergence_:.4f}\")\naxes[0].set_xticks([])\naxes[0].set_yticks([])\n\naxes[1].scatter(X_tsne_pca[:, 0], X_tsne_pca[:, 1], c=y,\n                cmap='tab10', alpha=0.6, s=15)\naxes[1].set_title(f\"init='pca' (recomendado)\\nKL={tsne_pca.kl_divergence_:.4f}\")\naxes[1].set_xticks([])\naxes[1].set_yticks([])\n\nplt.suptitle('Efecto de la Inicializaci\u00f3n', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\ninit='pca' es recomendado porque:\n- M\u00e1s reproducible\n- Convergencia m\u00e1s r\u00e1pida\n- Mejor preservaci\u00f3n de la estructura global\n\"\"\")\n\n# -------------------------------------------------------------\n# 6. t-SNE CON DATOS GRANDES (Barnes-Hut)\n# -------------------------------------------------------------\nprint(\"\\n[5] ESCALABILIDAD: Barnes-Hut vs Exact\")\nprint(\"-\"*40)\n\nprint(\"\"\"\nPara datasets grandes, usar method='barnes_hut':\n- Complejidad: O(n\u00b2) \u2192 O(n log n)\n- Aproximaci\u00f3n del algoritmo exacto\n- Por defecto cuando n_samples &gt; 10000\n\"\"\")\n\n# Ejemplo con datos m\u00e1s grandes\nfrom sklearn.datasets import make_blobs\nX_large, y_large = make_blobs(n_samples=5000, n_features=50, centers=10, random_state=42)\nX_large_scaled = StandardScaler().fit_transform(X_large)\n\n# Barnes-Hut (aproximado)\nprint(\"  Barnes-Hut (aproximado)...\", end=\" \")\nstart = time.time()\ntsne_bh = TSNE(n_components=2, perplexity=30, method='barnes_hut', \n               random_state=42, n_iter=1000)\nX_bh = tsne_bh.fit_transform(X_large_scaled)\nprint(f\"{time.time()-start:.1f}s\")\n\n# Exact (para comparaci\u00f3n - ser\u00e1 m\u00e1s lento)\nprint(\"  Exact...\", end=\" \")\nstart = time.time()\ntsne_exact = TSNE(n_components=2, perplexity=30, method='exact',\n                  random_state=42, n_iter=1000)\nX_exact = tsne_exact.fit_transform(X_large_scaled)\nprint(f\"{time.time()-start:.1f}s\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(X_bh[:, 0], X_bh[:, 1], c=y_large, cmap='tab10', alpha=0.5, s=5)\naxes[0].set_title(\"method='barnes_hut' (R\u00e1pido)\")\n\naxes[1].scatter(X_exact[:, 0], X_exact[:, 1], c=y_large, cmap='tab10', alpha=0.5, s=5)\naxes[1].set_title(\"method='exact' (Preciso)\")\n\nplt.suptitle('Comparaci\u00f3n de M\u00e9todos para n=5000', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 7. INTERPRETACI\u00d3N CORRECTA DE t-SNE\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"C\u00d3MO INTERPRETAR (Y NO INTERPRETAR) t-SNE\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n\u2705 LO QUE S\u00cd PUEDES INTERPRETAR:\n   - La existencia de clusters separados\n   - Puntos cercanos en t-SNE \u2192 similares en alta dimensi\u00f3n\n   - Estructura general de los datos\n\n\u274c LO QUE NO PUEDES INTERPRETAR:\n   - Tama\u00f1o de los clusters (distorsionado)\n   - Distancia entre clusters (no tiene significado)\n   - Densidad de los clusters\n   - Posici\u00f3n absoluta (rotaci\u00f3n/reflejo arbitrarios)\n\n\u26a0\ufe0f ERRORES COMUNES:\n   1. \"El cluster A es m\u00e1s grande que B\" \u2192 FALSO\n   2. \"Los clusters A y B est\u00e1n m\u00e1s cerca que A y C\" \u2192 PUEDE SER FALSO\n   3. \"Hay m\u00e1s densidad en esta regi\u00f3n\" \u2192 NO NECESARIAMENTE\n   4. Usar t-SNE como preprocesamiento para ML \u2192 NO RECOMENDADO\n\"\"\")\n\n# Demostraci\u00f3n del problema de distancias entre clusters\nprint(\"\\n[Demostraci\u00f3n: Distancias entre clusters NO son confiables]\")\n\n# Crear datos con distancias conocidas\nfrom sklearn.datasets import make_blobs\ncenters = [[0, 0], [10, 0], [100, 0]]  # Distancias 10 y 90\nX_demo, y_demo = make_blobs(n_samples=300, centers=centers, \n                            cluster_std=1, random_state=42)\n\n# Aplicar t-SNE\ntsne_demo = TSNE(n_components=2, perplexity=30, random_state=42)\nX_demo_tsne = tsne_demo.fit_transform(X_demo)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Espacio original\naxes[0].scatter(X_demo[:, 0], X_demo[:, 1], c=y_demo, cmap='tab10', alpha=0.7)\naxes[0].set_title('Espacio Original\\nDistancias: A-B=10, B-C=90')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\n\n# Espacio t-SNE\naxes[1].scatter(X_demo_tsne[:, 0], X_demo_tsne[:, 1], c=y_demo, cmap='tab10', alpha=0.7)\naxes[1].set_title('Espacio t-SNE\\n\u00bfSe preservan las distancias relativas?')\naxes[1].set_xlabel('t-SNE 1')\naxes[1].set_ylabel('t-SNE 2')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nConclusi\u00f3n: Las distancias relativas entre clusters NO se preservan en t-SNE\nEl cluster C que estaba 9x m\u00e1s lejos puede aparecer a distancia similar en t-SNE\n\"\"\")\n\n# -------------------------------------------------------------\n# 8. RESUMEN DE MEJORES PR\u00c1CTICAS\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEJORES PR\u00c1CTICAS PARA t-SNE\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n1. PREPROCESAMIENTO:\n   - Siempre estandarizar (StandardScaler)\n   - Considerar reducir con PCA primero si dim &gt; 50\n\n2. HIPERPAR\u00c1METROS:\n   - perplexity: 5-50, t\u00edpicamente 30\n   - n_iter: al menos 1000, verificar convergencia (KL divergence)\n   - learning_rate: 'auto' o n_samples/12\n   - init: 'pca' para mayor reproducibilidad\n\n3. VISUALIZACI\u00d3N:\n   - No confiar en tama\u00f1os de clusters\n   - No confiar en distancias entre clusters\n   - Ejecutar varias veces para verificar estabilidad\n\n4. NO USAR PARA:\n   - Preprocesamiento de ML\n   - Clustering (usar los datos originales)\n   - Comparar posiciones entre diferentes ejecuciones\n\"\"\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/06-tsne/#66-hiperparametros-de-t-sne-en-scikit-learn","title":"6.6. Hiperpar\u00e1metros de t-SNE en scikit-learn","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_components</code> Dimensiones de salida 2 o 3 2 para visualizaci\u00f3n <code>perplexity</code> N\u00famero efectivo de vecinos 5-50 30 es un buen inicio <code>learning_rate</code> Tasa de aprendizaje 'auto', 10-1000 'auto' (n_samples/12) <code>n_iter</code> N\u00famero de iteraciones int &gt; 0 1000 m\u00ednimo <code>init</code> Inicializaci\u00f3n 'random', 'pca' 'pca' para reproducibilidad <code>method</code> Algoritmo 'barnes_hut', 'exact' 'barnes_hut' si n &gt; 10000 <code>metric</code> M\u00e9trica de distancia 'euclidean', 'cosine', etc. 'euclidean' <code>random_state</code> Semilla int o None Fijar para reproducibilidad"},{"location":"aprendizaje-no-supervisado/06-tsne/#67-aplicaciones-reales","title":"6.7. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#1-visualizacion-de-word-embeddings","title":"1. Visualizaci\u00f3n de Word Embeddings","text":"<p>Visualizar relaciones sem\u00e1nticas entre palabras (Word2Vec, GloVe). * Tutorial: Visualizing Word Embeddings</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#2-analisis-de-imagenes","title":"2. An\u00e1lisis de Im\u00e1genes","text":"<p>Explorar similitud entre im\u00e1genes en datasets como MNIST, CIFAR. * Ejemplo: t-SNE on MNIST</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#3-bioinformatica","title":"3. Bioinform\u00e1tica","text":"<p>Visualizar expresi\u00f3n g\u00e9nica, scRNA-seq (single-cell RNA sequencing).</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#4-deteccion-de-fraude","title":"4. Detecci\u00f3n de Fraude","text":"<p>Visualizar transacciones para identificar patrones an\u00f3malos.</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#68-t-sne-vs-otras-tecnicas-de-visualizacion","title":"6.8. t-SNE vs Otras T\u00e9cnicas de Visualizaci\u00f3n","text":"T\u00e9cnica Tipo Velocidad Estructura Mejor para PCA Lineal Muy r\u00e1pida Global Preprocesamiento, interpretaci\u00f3n t-SNE No lineal Lenta Local Visualizaci\u00f3n de clusters UMAP No lineal R\u00e1pida Local + Global Visualizaci\u00f3n + ML MDS No lineal Media Global Preservar distancias"},{"location":"aprendizaje-no-supervisado/06-tsne/#umap-la-alternativa-moderna","title":"UMAP: La Alternativa Moderna","text":"<p>UMAP (Uniform Manifold Approximation and Projection) es una alternativa m\u00e1s reciente a t-SNE:</p> <pre><code># pip install umap-learn\nimport umap\n\nreducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\nX_umap = reducer.fit_transform(X_scaled)\n</code></pre> <p>Ventajas de UMAP sobre t-SNE: - M\u00e1s r\u00e1pido - Mejor preservaci\u00f3n de estructura global - Se puede usar para ML (transformar nuevos datos)</p>"},{"location":"aprendizaje-no-supervisado/06-tsne/#69-resumen-y-mejores-practicas","title":"6.9. Resumen y Mejores Pr\u00e1cticas","text":""},{"location":"aprendizaje-no-supervisado/06-tsne/#checklist-para-usar-t-sne","title":"Checklist para usar t-SNE","text":"<ul> <li>[ ] Estandarizar los datos</li> <li>[ ] Reducir dimensionalidad primero con PCA si dim &gt; 50</li> <li>[ ] Empezar con perplexity=30 y ajustar</li> <li>[ ] Usar n_iter &gt;= 1000 y verificar convergencia</li> <li>[ ] Usar init='pca' para reproducibilidad</li> <li>[ ] Ejecutar m\u00faltiples veces para verificar estabilidad</li> <li>[ ] NO interpretar tama\u00f1os ni distancias entre clusters</li> </ul>"},{"location":"aprendizaje-no-supervisado/06-tsne/#cuando-usar-t-sne","title":"\u00bfCu\u00e1ndo usar t-SNE?","text":"<p>\u2705 Usar t-SNE cuando: - Quieres visualizar datos de alta dimensi\u00f3n - Buscas identificar clusters visualmente - El dataset es de tama\u00f1o moderado (&lt; 50K puntos) - Solo necesitas visualizaci\u00f3n (no ML downstream)</p> <p>\u274c Considerar alternativas cuando: - Necesitas velocidad con datos grandes \u2192 UMAP - Quieres preservar distancias globales \u2192 PCA, MDS - Necesitas transformar nuevos datos \u2192 UMAP, PCA - Quieres interpretabilidad \u2192 PCA</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/","title":"\ud83d\udd0d Unidad 7. Isolation Forest - Detecci\u00f3n de Anomal\u00edas","text":"<p>Isolation Forest es un algoritmo de detecci\u00f3n de anomal\u00edas no supervisado basado en \u00e1rboles de decisi\u00f3n. A diferencia de otros m\u00e9todos que intentan modelar los datos normales, Isolation Forest se enfoca en aislar las anomal\u00edas. La idea clave es que las anomal\u00edas son m\u00e1s f\u00e1ciles de aislar porque son raras y tienen valores at\u00edpicos, lo que significa que requieren menos divisiones para separarlas del resto.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#71-como-funciona-isolation-forest","title":"7.1. \u00bfC\u00f3mo Funciona Isolation Forest?","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#la-intuicion","title":"La Intuici\u00f3n","text":"<p>Imagina que tienes un bosque de puntos de datos. Si eliges un punto al azar y empiezas a hacer divisiones aleatorias:</p> <ul> <li>Puntos normales: Est\u00e1n en regiones densas, rodeados de muchos puntos \u2192 necesitan muchas divisiones para ser aislados</li> <li>Anomal\u00edas: Est\u00e1n solos, lejos del resto \u2192 se a\u00edslan con pocas divisiones</li> </ul> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INTUICI\u00d3N DE ISOLATION FOREST                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502     Datos originales:                                       \u2502\n\u2502                                                             \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf             \u2190 Cluster denso                     \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf             (puntos normales)                   \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf                                                 \u2502\n\u2502                                                             \u2502\n\u2502                                         \u2297 \u2190 Anomal\u00eda       \u2502\n\u2502                                                             \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502                                                             \u2502\n\u2502     Una divisi\u00f3n aleatoria puede aislar la anomal\u00eda:       \u2502\n\u2502                                                             \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf \u2502                                               \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf \u2502                                               \u2502\n\u2502     \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf \u2502                   \u2297 \u2190 \u00a1Aislada con 1 corte!   \u2502\n\u2502            \u2502                                               \u2502\n\u2502                                                             \u2502\n\u2502     Pero un punto normal necesita m\u00e1s cortes:              \u2502\n\u2502                                                             \u2502\n\u2502     \u25cf\u25cf\u2500\u252c\u2500\u25cf\u25cf \u2502                                              \u2502\n\u2502     \u25cf\u25cf\u2500\u253c\u2500\u25cf\u25cf \u2502    Necesita 3+ cortes para                   \u2502\n\u2502     \u25cf\u25cf\u2500\u2534\u2500\u25cf\u25cf \u2502    aislar un punto del cluster               \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#el-algoritmo","title":"El Algoritmo","text":"<ol> <li>Construir m\u00faltiples \u00e1rboles de aislamiento (iForest)</li> <li>Para cada \u00e1rbol:</li> <li>Seleccionar aleatoriamente un subconjunto de datos</li> <li>Seleccionar aleatoriamente una caracter\u00edstica</li> <li>Seleccionar aleatoriamente un valor de corte entre min y max</li> <li>Dividir los datos recursivamente hasta que cada punto quede aislado o se alcance un l\u00edmite</li> <li>Calcular la \"path length\" (longitud del camino) promedio para cada punto</li> <li>Puntos con path length corta = Anomal\u00edas</li> </ol>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#72-explicacion-matematica","title":"7.2. Explicaci\u00f3n Matem\u00e1tica","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#path-length-longitud-del-camino","title":"Path Length (Longitud del Camino)","text":"<p>La path length \\(h(x)\\) de un punto \\(x\\) es el n\u00famero de divisiones necesarias para aislar ese punto desde la ra\u00edz hasta el nodo terminal.</p> <p>Para un punto \\(x\\), calculamos el path length promedio sobre todos los \u00e1rboles: \\(\\(E[h(x)] = \\frac{1}{t} \\sum_{i=1}^{t} h_i(x)\\)\\)</p> <p>Donde \\(t\\) es el n\u00famero de \u00e1rboles y \\(h_i(x)\\) es la path length en el \u00e1rbol \\(i\\).</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#path-length-esperada","title":"Path Length Esperada","text":"<p>Para un \u00e1rbol binario construido con \\(n\\) puntos, la path length promedio esperada para un punto normal es aproximadamente:</p> \\[c(n) = 2H(n-1) - \\frac{2(n-1)}{n}\\] <p>Donde \\(H(i)\\) es el n\u00famero harm\u00f3nico: \\(H(i) = \\ln(i) + \\gamma\\) (\u03b3 \u2248 0.5772 es la constante de Euler-Mascheroni).</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#anomaly-score","title":"Anomaly Score","text":"<p>El anomaly score normaliza la path length:</p> \\[s(x, n) = 2^{-\\frac{E[h(x)]}{c(n)}}\\] <p>Interpretaci\u00f3n: - \\(s(x, n) \\approx 1\\): Punto es una anomal\u00eda (path length muy corta) - \\(s(x, n) \\approx 0.5\\): Punto normal (path length promedio) - \\(s(x, n) &lt; 0.5\\): Punto muy normal (path length larga)</p> <pre><code>      Anomaly Score\n\n      1.0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2297 Anomal\u00edas\n\n      0.5 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u25cf Puntos normales\n\n      0.0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#73-pros-y-contras","title":"7.3. Pros y Contras","text":"Ventajas Desventajas No necesita etiquetas: Completamente no supervisado Sensible a contaminaci\u00f3n: El par\u00e1metro contamination afecta mucho Muy eficiente: Complejidad \\(O(n \\log n)\\) No ideal para datos de alta dimensi\u00f3n: Pierde efectividad Escalable: Funciona bien con datasets grandes Asume anomal\u00edas son aislables: No funciona si las anomal\u00edas forman clusters Robusto: No asume distribuci\u00f3n de los datos No probabil\u00edstico: Solo da scores, no probabilidades Pocos hiperpar\u00e1metros: F\u00e1cil de configurar Puede perder anomal\u00edas sutiles: Si est\u00e1n cerca de datos normales"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#74-ejemplo-basico-en-python","title":"7.4. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo muestra el uso b\u00e1sico de Isolation Forest para detectar anomal\u00edas.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: Isolation Forest para detecci\u00f3n de anomal\u00edas\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport numpy as np                          # Operaciones num\u00e9ricas\nimport matplotlib.pyplot as plt             # Visualizaci\u00f3n\nfrom sklearn.ensemble import IsolationForest  # Algoritmo principal\nfrom sklearn.datasets import make_blobs     # Generar datos sint\u00e9ticos\n\n# -------------------------------------------------------------\n# 1. CREAR DATOS CON ANOMAL\u00cdAS\n# -------------------------------------------------------------\n# Generar datos normales (cluster)\nnp.random.seed(42)\nX_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n\n# A\u00f1adir anomal\u00edas (puntos lejanos del cluster)\nn_anomalies = 20\nX_anomalies = np.random.uniform(low=-6, high=6, size=(n_anomalies, 2))\n\n# Combinar datos\nX = np.vstack([X_normal, X_anomalies])\n\n# Crear etiquetas verdaderas para evaluaci\u00f3n\n# 1 = normal, -1 = anomal\u00eda\ny_true = np.array([1] * len(X_normal) + [-1] * n_anomalies)\n\nprint(\"=\"*50)\nprint(\"ISOLATION FOREST - DETECCI\u00d3N DE ANOMAL\u00cdAS\")\nprint(\"=\"*50)\nprint(f\"\\nTotal de puntos: {len(X)}\")\nprint(f\"Puntos normales: {len(X_normal)}\")\nprint(f\"Anomal\u00edas verdaderas: {n_anomalies}\")\nprint(f\"Tasa de contaminaci\u00f3n real: {n_anomalies/len(X):.2%}\")\n\n# -------------------------------------------------------------\n# 2. VISUALIZAR DATOS ORIGINALES\n# -------------------------------------------------------------\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_normal[:, 0], X_normal[:, 1], c='blue', alpha=0.6, label='Normal', s=30)\nplt.scatter(X_anomalies[:, 0], X_anomalies[:, 1], c='red', marker='x', s=100, \n            label='Anomal\u00edas', linewidths=2)\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Datos Originales (con etiquetas verdaderas)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# -------------------------------------------------------------\n# 3. APLICAR ISOLATION FOREST\n# -------------------------------------------------------------\n# Crear y entrenar el modelo\n# contamination: proporci\u00f3n esperada de anomal\u00edas\niso_forest = IsolationForest(\n    n_estimators=100,           # N\u00famero de \u00e1rboles\n    contamination=0.1,          # Proporci\u00f3n esperada de anomal\u00edas (10%)\n    random_state=42,            # Reproducibilidad\n    max_samples='auto'          # Muestras por \u00e1rbol\n)\n\n# Entrenar y predecir\n# fit_predict devuelve: 1 (normal), -1 (anomal\u00eda)\ny_pred = iso_forest.fit_predict(X)\n\n# -------------------------------------------------------------\n# 4. ANALIZAR RESULTADOS\n# -------------------------------------------------------------\n# Contar predicciones\nn_pred_normal = np.sum(y_pred == 1)\nn_pred_anomaly = np.sum(y_pred == -1)\n\nprint(f\"\\n--- Resultados de Isolation Forest ---\")\nprint(f\"Predichos como normal: {n_pred_normal}\")\nprint(f\"Predichos como anomal\u00eda: {n_pred_anomaly}\")\n\n# Calcular m\u00e9tricas de rendimiento\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Convertir etiquetas para m\u00e9tricas\nprint(f\"\\n--- Reporte de Clasificaci\u00f3n ---\")\nprint(classification_report(y_true, y_pred, target_names=['Anomal\u00eda (-1)', 'Normal (1)']))\n\n# Matriz de confusi\u00f3n\ncm = confusion_matrix(y_true, y_pred)\nprint(f\"Matriz de Confusi\u00f3n:\")\nprint(f\"                 Predicho Anomal\u00eda | Predicho Normal\")\nprint(f\"Real Anomal\u00eda:          {cm[0,0]:4d}      |      {cm[0,1]:4d}\")\nprint(f\"Real Normal:            {cm[1,0]:4d}      |      {cm[1,1]:4d}\")\n\n# -------------------------------------------------------------\n# 5. VISUALIZAR PREDICCIONES\n# -------------------------------------------------------------\nplt.subplot(1, 2, 2)\n\n# Separar por predicci\u00f3n\nmask_pred_normal = y_pred == 1\nmask_pred_anomaly = y_pred == -1\n\nplt.scatter(X[mask_pred_normal, 0], X[mask_pred_normal, 1], \n            c='green', alpha=0.6, label='Predicho Normal', s=30)\nplt.scatter(X[mask_pred_anomaly, 0], X[mask_pred_anomaly, 1], \n            c='red', marker='x', s=100, label='Predicho Anomal\u00eda', linewidths=2)\n\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Predicciones de Isolation Forest')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 6. OBTENER ANOMALY SCORES\n# -------------------------------------------------------------\n# decision_function devuelve el score de anomal\u00eda\n# Valores m\u00e1s negativos = m\u00e1s an\u00f3malo\nscores = iso_forest.decision_function(X)\n\n# score_samples devuelve valores similares (opuesto de la depth)\n# Valores m\u00e1s negativos = m\u00e1s an\u00f3malo\n\nprint(f\"\\n--- Anomaly Scores ---\")\nprint(f\"Score medio (normales): {scores[y_true == 1].mean():.3f}\")\nprint(f\"Score medio (anomal\u00edas): {scores[y_true == -1].mean():.3f}\")\nprint(f\"Rango de scores: [{scores.min():.3f}, {scores.max():.3f}]\")\n\n# Visualizar distribuci\u00f3n de scores\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.hist(scores[y_true == 1], bins=30, alpha=0.7, label='Normal', color='blue')\nplt.hist(scores[y_true == -1], bins=10, alpha=0.7, label='Anomal\u00eda', color='red')\nplt.xlabel('Anomaly Score (decision_function)')\nplt.ylabel('Frecuencia')\nplt.title('Distribuci\u00f3n de Anomaly Scores')\nplt.legend()\nplt.axvline(x=0, color='black', linestyle='--', label='Umbral (0)')\n\nplt.subplot(1, 2, 2)\n# Colorear puntos por score\nscatter = plt.scatter(X[:, 0], X[:, 1], c=scores, cmap='RdYlGn', \n                      alpha=0.7, s=30, edgecolors='k', linewidths=0.5)\nplt.colorbar(scatter, label='Anomaly Score')\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Mapa de Anomaly Scores\\n(Verde=Normal, Rojo=Anomal\u00eda)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nInterpretaci\u00f3n de decision_function:\n- Valores positivos \u2192 punto probablemente normal\n- Valores negativos \u2192 punto probablemente an\u00f3malo\n- El umbral por defecto es 0 (controlado por contamination)\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#75-ejemplo-avanzado-analisis-de-hiperparametros-y-casos-de-uso","title":"7.5. Ejemplo Avanzado: An\u00e1lisis de Hiperpar\u00e1metros y Casos de Uso","text":"<p>Este ejemplo explora la configuraci\u00f3n \u00f3ptima y casos de uso reales.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: Isolation Forest - An\u00e1lisis profundo\n# ============================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# -------------------------------------------------------------\n# 1. CREAR DATASET COMPLEJO\n# -------------------------------------------------------------\nnp.random.seed(42)\n\n# Datos normales: dos clusters\nfrom sklearn.datasets import make_blobs\nX_normal1, _ = make_blobs(n_samples=400, centers=[[2, 2]], cluster_std=0.8)\nX_normal2, _ = make_blobs(n_samples=400, centers=[[-2, -2]], cluster_std=0.8)\nX_normal = np.vstack([X_normal1, X_normal2])\n\n# Anomal\u00edas de diferentes tipos\nanomalies_global = np.random.uniform(-6, 6, (15, 2))  # Aleatorias\nanomalies_local = np.array([[0, 0], [0.5, 0.5], [-0.5, -0.5]])  # Entre clusters\nanomalies_edge = np.array([[4, 2], [2, 4], [-4, -2]])  # En bordes\n\nX_anomalies = np.vstack([anomalies_global, anomalies_local, anomalies_edge])\n\nX = np.vstack([X_normal, X_anomalies])\ny_true = np.array([1] * len(X_normal) + [-1] * len(X_anomalies))\n\nprint(\"=\"*60)\nprint(\"ISOLATION FOREST - AN\u00c1LISIS AVANZADO\")\nprint(\"=\"*60)\nprint(f\"\\nDataset: {len(X)} puntos ({len(X_normal)} normales, {len(X_anomalies)} anomal\u00edas)\")\n\n# -------------------------------------------------------------\n# 2. EFECTO DEL N\u00daMERO DE ESTIMADORES\n# -------------------------------------------------------------\nprint(\"\\n[1] EFECTO DEL N\u00daMERO DE ESTIMADORES (n_estimators)\")\nprint(\"-\"*50)\n\nn_estimators_list = [10, 50, 100, 200, 500]\nresults_estimators = []\n\nfor n_est in n_estimators_list:\n    iso = IsolationForest(n_estimators=n_est, contamination=0.05, random_state=42)\n    y_pred = iso.fit_predict(X)\n    f1 = f1_score(y_true, y_pred, pos_label=-1)\n    results_estimators.append(f1)\n    print(f\"  n_estimators={n_est:3d}: F1={f1:.3f}\")\n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(n_estimators_list, results_estimators, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('N\u00famero de Estimadores')\nplt.ylabel('F1-Score (Anomal\u00edas)')\nplt.title('Efecto del N\u00famero de \u00c1rboles')\nplt.grid(True, alpha=0.3)\n\n# -------------------------------------------------------------\n# 3. EFECTO DE LA CONTAMINACI\u00d3N\n# -------------------------------------------------------------\nprint(\"\\n[2] EFECTO DE LA CONTAMINACI\u00d3N (contamination)\")\nprint(\"-\"*50)\n\ncontamination_list = [0.01, 0.03, 0.05, 0.1, 0.15, 0.2]\nresults_contamination = {'precision': [], 'recall': [], 'f1': []}\n\nfor cont in contamination_list:\n    iso = IsolationForest(n_estimators=100, contamination=cont, random_state=42)\n    y_pred = iso.fit_predict(X)\n\n    prec = precision_score(y_true, y_pred, pos_label=-1)\n    rec = recall_score(y_true, y_pred, pos_label=-1)\n    f1 = f1_score(y_true, y_pred, pos_label=-1)\n\n    results_contamination['precision'].append(prec)\n    results_contamination['recall'].append(rec)\n    results_contamination['f1'].append(f1)\n\n    print(f\"  contamination={cont:.2f}: Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}\")\n\nplt.subplot(1, 2, 2)\nplt.plot(contamination_list, results_contamination['precision'], 'g^-', label='Precision', linewidth=2)\nplt.plot(contamination_list, results_contamination['recall'], 'rs-', label='Recall', linewidth=2)\nplt.plot(contamination_list, results_contamination['f1'], 'bo-', label='F1-Score', linewidth=2)\nplt.axvline(x=len(X_anomalies)/len(X), color='k', linestyle='--', alpha=0.5, label='Contam. real')\nplt.xlabel('Contamination')\nplt.ylabel('Score')\nplt.title('Efecto del Par\u00e1metro Contamination')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nContaminaci\u00f3n real en los datos: {len(X_anomalies)/len(X):.3f}\")\n\n# -------------------------------------------------------------\n# 4. EFECTO DE max_samples\n# -------------------------------------------------------------\nprint(\"\\n[3] EFECTO DE max_samples\")\nprint(\"-\"*50)\n\nmax_samples_list = [32, 64, 128, 256, 'auto']\nresults_samples = []\n\nfor max_s in max_samples_list:\n    iso = IsolationForest(n_estimators=100, contamination=0.05, \n                          max_samples=max_s, random_state=42)\n    y_pred = iso.fit_predict(X)\n    f1 = f1_score(y_true, y_pred, pos_label=-1)\n    results_samples.append(f1)\n    print(f\"  max_samples={str(max_s):5s}: F1={f1:.3f}\")\n\n# -------------------------------------------------------------\n# 5. CURVA ROC CON DIFERENTES UMBRALES\n# -------------------------------------------------------------\nprint(\"\\n[4] AN\u00c1LISIS DE UMBRALES Y CURVA ROC\")\nprint(\"-\"*50)\n\n# Entrenar modelo\niso = IsolationForest(n_estimators=100, random_state=42)\niso.fit(X)\n\n# Obtener scores (invertir signo para ROC)\nscores = -iso.decision_function(X)  # M\u00e1s alto = m\u00e1s an\u00f3malo\n\n# Calcular ROC\nfpr, tpr, thresholds = roc_curve(y_true == -1, scores)\nroc_auc = auc(fpr, tpr)\n\nprint(f\"  AUC-ROC: {roc_auc:.3f}\")\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {roc_auc:.3f}')\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Curva ROC')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Visualizar diferentes umbrales\nplt.subplot(1, 3, 2)\n\n# Umbral autom\u00e1tico (contamination=0.05)\niso_auto = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\ny_pred_auto = iso_auto.fit_predict(X)\n\nmask_normal = y_pred_auto == 1\nmask_anomaly = y_pred_auto == -1\n\nplt.scatter(X[mask_normal, 0], X[mask_normal, 1], c='blue', alpha=0.5, s=20, label='Normal')\nplt.scatter(X[mask_anomaly, 0], X[mask_anomaly, 1], c='red', marker='x', s=80, \n            label='Anomal\u00eda', linewidths=2)\nplt.title('Contamination = 5%')\nplt.legend()\n\n# Umbral m\u00e1s estricto\nplt.subplot(1, 3, 3)\n\niso_strict = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\ny_pred_strict = iso_strict.fit_predict(X)\n\nmask_normal = y_pred_strict == 1\nmask_anomaly = y_pred_strict == -1\n\nplt.scatter(X[mask_normal, 0], X[mask_normal, 1], c='blue', alpha=0.5, s=20, label='Normal')\nplt.scatter(X[mask_anomaly, 0], X[mask_anomaly, 1], c='red', marker='x', s=80, \n            label='Anomal\u00eda', linewidths=2)\nplt.title('Contamination = 2%')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 6. VISUALIZAR REGIONES DE DECISI\u00d3N\n# -------------------------------------------------------------\nprint(\"\\n[5] VISUALIZACI\u00d3N DE REGIONES DE DECISI\u00d3N\")\nprint(\"-\"*50)\n\n# Crear grid para visualizaci\u00f3n\nxx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Entrenar modelo\niso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\niso.fit(X)\n\n# Obtener scores para el grid\nZ = iso.decision_function(grid)\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\n# Contour de scores\ncontour = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlGn', alpha=0.8)\nplt.colorbar(contour, label='Anomaly Score')\n\n# Puntos reales\nplt.scatter(X[y_true == 1, 0], X[y_true == 1, 1], c='blue', edgecolors='k', \n            s=20, alpha=0.6, label='Normal')\nplt.scatter(X[y_true == -1, 0], X[y_true == -1, 1], c='red', marker='x',\n            s=100, linewidths=2, label='Anomal\u00eda real')\n\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Mapa de Anomaly Scores\\n(Verde=Normal, Rojo=An\u00f3malo)')\nplt.legend()\n\nplt.subplot(1, 2, 2)\n# Frontera de decisi\u00f3n\nplt.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\nplt.contourf(xx, yy, Z, levels=[Z.min(), 0], colors=['red'], alpha=0.3)\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=['green'], alpha=0.3)\n\nplt.scatter(X[y_true == 1, 0], X[y_true == 1, 1], c='blue', edgecolors='k',\n            s=20, alpha=0.6, label='Normal')\nplt.scatter(X[y_true == -1, 0], X[y_true == -1, 1], c='red', marker='x',\n            s=100, linewidths=2, label='Anomal\u00eda real')\n\nplt.xlabel('Caracter\u00edstica 1')\nplt.ylabel('Caracter\u00edstica 2')\nplt.title('Frontera de Decisi\u00f3n\\n(Rojo=Regi\u00f3n an\u00f3mala)')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 7. CASO REAL: DETECCI\u00d3N DE FRAUDE (SIMULADO)\n# -------------------------------------------------------------\nprint(\"\\n[6] CASO DE USO: DETECCI\u00d3N DE FRAUDE\")\nprint(\"-\"*50)\n\n# Simular datos de transacciones\nnp.random.seed(42)\nn_normal = 10000\nn_fraud = 100  # 1% de fraude\n\n# Transacciones normales: monto bajo-medio, hora comercial\nnormal_amount = np.abs(np.random.normal(50, 30, n_normal))\nnormal_hour = np.random.normal(14, 4, n_normal)  # Centrado en 2pm\nnormal_hour = np.clip(normal_hour, 0, 24)\n\n# Transacciones fraudulentas: montos altos, horas inusuales\nfraud_amount = np.abs(np.random.normal(500, 200, n_fraud))\nfraud_hour = np.random.uniform(0, 6, n_fraud)  # Madrugada\n\n# Combinar\nX_fraud = np.column_stack([\n    np.concatenate([normal_amount, fraud_amount]),\n    np.concatenate([normal_hour, fraud_hour])\n])\ny_fraud = np.array([1] * n_normal + [-1] * n_fraud)\n\n# Estandarizar\nscaler = StandardScaler()\nX_fraud_scaled = scaler.fit_transform(X_fraud)\n\n# Entrenar Isolation Forest\niso_fraud = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\ny_pred_fraud = iso_fraud.fit_predict(X_fraud_scaled)\n\n# Resultados\nprint(f\"  Transacciones totales: {len(X_fraud):,}\")\nprint(f\"  Fraudes reales: {n_fraud}\")\nprint(f\"  Fraudes detectados: {np.sum(y_pred_fraud == -1)}\")\n\n# M\u00e9tricas\nprec = precision_score(y_fraud, y_pred_fraud, pos_label=-1)\nrec = recall_score(y_fraud, y_pred_fraud, pos_label=-1)\nf1 = f1_score(y_fraud, y_pred_fraud, pos_label=-1)\n\nprint(f\"\\n  Precision: {prec:.3f} (De los detectados, qu\u00e9 % son fraude)\")\nprint(f\"  Recall: {rec:.3f} (De los fraudes reales, qu\u00e9 % detectamos)\")\nprint(f\"  F1-Score: {f1:.3f}\")\n\n# Visualizar\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_fraud[y_fraud == 1, 0], X_fraud[y_fraud == 1, 1], \n            c='green', alpha=0.3, s=5, label='Normal')\nplt.scatter(X_fraud[y_fraud == -1, 0], X_fraud[y_fraud == -1, 1],\n            c='red', marker='x', s=50, label='Fraude real')\nplt.xlabel('Monto ($)')\nplt.ylabel('Hora del d\u00eda')\nplt.title('Datos Reales')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_fraud[y_pred_fraud == 1, 0], X_fraud[y_pred_fraud == 1, 1],\n            c='green', alpha=0.3, s=5, label='Predicho Normal')\nplt.scatter(X_fraud[y_pred_fraud == -1, 0], X_fraud[y_pred_fraud == -1, 1],\n            c='red', marker='x', s=50, label='Predicho Fraude')\nplt.xlabel('Monto ($)')\nplt.ylabel('Hora del d\u00eda')\nplt.title('Predicciones de Isolation Forest')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 8. MEJORES PR\u00c1CTICAS\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEJORES PR\u00c1CTICAS PARA ISOLATION FOREST\")\nprint(\"=\"*60)\n\nprint(\"\"\"\n1. PREPROCESAMIENTO:\n   - Estandarizar/normalizar las caracter\u00edsticas\n   - Considerar encoding adecuado para categor\u00edas\n   - Manejar valores faltantes antes\n\n2. HIPERPAR\u00c1METROS:\n   - n_estimators: 100 suele ser suficiente\n   - contamination: Usar conocimiento del dominio si es posible\n   - max_samples: 'auto' o sqrt(n_samples)\n\n3. EVALUACI\u00d3N:\n   - Si hay etiquetas: Precision, Recall, F1, AUC-ROC\n   - Sin etiquetas: Inspecci\u00f3n manual de anomal\u00edas detectadas\n   - Analizar distribuci\u00f3n de scores\n\n4. CONSIDERACIONES:\n   - No asume distribuci\u00f3n de los datos\n   - Funciona mejor con anomal\u00edas globales/aisladas\n   - Puede fallar con anomal\u00edas en clusters\n   - Revisar puntos cerca del umbral manualmente\n\"\"\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AN\u00c1LISIS COMPLETADO\")\nprint(\"=\"*60)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#76-hiperparametros-de-isolation-forest-en-scikit-learn","title":"7.6. Hiperpar\u00e1metros de Isolation Forest en scikit-learn","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>n_estimators</code> N\u00famero de \u00e1rboles int &gt; 0 100 (m\u00e1s = m\u00e1s estable) <code>contamination</code> Proporci\u00f3n esperada de anomal\u00edas float (0, 0.5) o 'auto' Basado en conocimiento del dominio <code>max_samples</code> Muestras por \u00e1rbol int, float, 'auto' 'auto' (min(256, n_samples)) <code>max_features</code> Caracter\u00edsticas por \u00e1rbol int o float 1.0 (todas) <code>bootstrap</code> Muestreo con reemplazo bool False <code>random_state</code> Semilla int o None Fijar para reproducibilidad <code>n_jobs</code> Procesadores paralelos int -1 para usar todos <code>warm_start</code> A\u00f1adir \u00e1rboles incrementalmente bool False"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#77-aplicaciones-reales","title":"7.7. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#1-deteccion-de-fraude-financiero","title":"1. Detecci\u00f3n de Fraude Financiero","text":"<p>Identificar transacciones sospechosas en tarjetas de cr\u00e9dito. * Ejemplo: Credit Card Fraud Detection</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#2-deteccion-de-intrusiones-en-redes","title":"2. Detecci\u00f3n de Intrusiones en Redes","text":"<p>Identificar patrones de tr\u00e1fico an\u00f3malos que puedan indicar ataques.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#3-mantenimiento-predictivo","title":"3. Mantenimiento Predictivo","text":"<p>Detectar comportamientos an\u00f3malos en sensores de maquinaria industrial.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#4-control-de-calidad","title":"4. Control de Calidad","text":"<p>Identificar productos defectuosos en l\u00edneas de producci\u00f3n.</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#5-salud","title":"5. Salud","text":"<p>Detectar patrones an\u00f3malos en datos m\u00e9dicos (ECG, diagn\u00f3sticos).</p>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#78-comparacion-con-otros-metodos-de-deteccion-de-anomalias","title":"7.8. Comparaci\u00f3n con Otros M\u00e9todos de Detecci\u00f3n de Anomal\u00edas","text":"M\u00e9todo Tipo Velocidad Escalabilidad Mejor para Isolation Forest Basado en \u00e1rboles R\u00e1pido Excelente Anomal\u00edas globales One-Class SVM Basado en kernel Lento Pobre Datasets peque\u00f1os LOF Basado en densidad Medio Media Anomal\u00edas locales DBSCAN Clustering Medio Buena Clusters + anomal\u00edas Autoencoder Deep Learning Lento (train) Buena Alta dimensionalidad"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#79-resumen-y-checklist","title":"7.9. Resumen y Checklist","text":""},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#checklist-para-usar-isolation-forest","title":"Checklist para usar Isolation Forest","text":"<ul> <li>[ ] Preprocesar los datos (estandarizar, manejar NaN)</li> <li>[ ] Estimar contamination si es posible</li> <li>[ ] Empezar con n_estimators=100</li> <li>[ ] Visualizar anomaly scores para entender la distribuci\u00f3n</li> <li>[ ] Ajustar contamination seg\u00fan resultados</li> <li>[ ] Validar con m\u00e9tricas si hay etiquetas disponibles</li> <li>[ ] Inspeccionar manualmente las anomal\u00edas detectadas</li> </ul>"},{"location":"aprendizaje-no-supervisado/07-isolation-forest/#cuando-usar-isolation-forest","title":"\u00bfCu\u00e1ndo usar Isolation Forest?","text":"<p>\u2705 Usar Isolation Forest cuando: - Tienes un dataset grande - Las anomal\u00edas son raras y diferentes al resto - No tienes etiquetas de anomal\u00edas - Necesitas un m\u00e9todo r\u00e1pido y escalable</p> <p>\u274c Considerar alternativas cuando: - Las anomal\u00edas forman clusters \u2192 LOF o DBSCAN - Dataset muy peque\u00f1o \u2192 One-Class SVM - Datos de alta dimensi\u00f3n complejos \u2192 Autoencoders - Necesitas probabilidades \u2192 Gaussian Mixture</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/","title":"\ud83d\uded2 Unidad 8. Apriori - Reglas de Asociaci\u00f3n","text":"<p>Apriori es un algoritmo cl\u00e1sico de miner\u00eda de reglas de asociaci\u00f3n utilizado para descubrir patrones frecuentes y relaciones entre \u00edtems en grandes datasets transaccionales. Es famoso por su aplicaci\u00f3n en el an\u00e1lisis de la cesta de la compra (Market Basket Analysis), donde se identifican qu\u00e9 productos suelen comprarse juntos. A diferencia de otros algoritmos de aprendizaje no supervisado, Apriori trabaja con datos transaccionales discretos.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#81-que-son-las-reglas-de-asociacion","title":"8.1. \u00bfQu\u00e9 son las Reglas de Asociaci\u00f3n?","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#el-problema-de-la-cesta-de-la-compra","title":"El Problema de la Cesta de la Compra","text":"<p>Imagina que tienes datos de transacciones de un supermercado:</p> Transacci\u00f3n \u00cdtems Comprados T1 Pan, Leche, Mantequilla T2 Pan, Cerveza T3 Leche, Pa\u00f1ales, Cerveza T4 Pan, Leche, Pa\u00f1ales, Cerveza T5 Pan, Leche, Pa\u00f1ales <p>Pregunta: \u00bfQu\u00e9 productos tienden a comprarse juntos?</p> <p>Una regla de asociaci\u00f3n tiene la forma:</p> \\[\\text{Si } \\{A, B\\} \\rightarrow \\text{ entonces } \\{C\\}\\] <p>Por ejemplo: \"Si un cliente compra Pan y Leche, es probable que tambi\u00e9n compre Mantequilla\"</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 REGLA DE ASOCIACI\u00d3N                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   {Pan, Leche} \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 {Mantequilla}               \u2502\n\u2502                                                             \u2502\n\u2502   \u25b2                               \u25b2                        \u2502\n\u2502   \u2502                               \u2502                        \u2502\n\u2502   Antecedente (LHS)              Consecuente (RHS)         \u2502\n\u2502   \"Si compra esto...\"            \"...probablemente         \u2502\n\u2502                                   tambi\u00e9n compra esto\"      \u2502\n\u2502                                                             \u2502\n\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n\u2502                                                             \u2502\n\u2502   M\u00e9tricas de la regla:                                     \u2502\n\u2502                                                             \u2502\n\u2502   Support (Soporte): \u00bfQu\u00e9 tan frecuente es {Pan,Leche}?    \u2502\n\u2502   Confidence (Confianza): \u00bfCon qu\u00e9 frecuencia se cumple?   \u2502\n\u2502   Lift: \u00bfEs la asociaci\u00f3n significativa?                   \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#82-conceptos-fundamentales-y-metricas","title":"8.2. Conceptos Fundamentales y M\u00e9tricas","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#itemset","title":"Itemset","text":"<p>Un itemset es un conjunto de \u00edtems. Por ejemplo: {Pan, Leche, Mantequilla}</p> <p>Un itemset frecuente es uno que aparece en al menos un n\u00famero m\u00ednimo de transacciones.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#support-soporte","title":"Support (Soporte)","text":"<p>El soporte mide qu\u00e9 tan frecuente es un itemset en el dataset:</p> \\[Support(A) = \\frac{\\text{N\u00famero de transacciones que contienen } A}{\\text{N\u00famero total de transacciones}}\\] <p>Para una regla \\(A \\rightarrow B\\):</p> \\[Support(A \\rightarrow B) = \\frac{|\\{t : A \\cup B \\subseteq t\\}|}{|T|}\\] <p>Ejemplo: Si {Pan, Leche} aparece en 3 de 5 transacciones: \\(\\(Support(\\{Pan, Leche\\}) = \\frac{3}{5} = 0.6 = 60\\%\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#confidence-confianza","title":"Confidence (Confianza)","text":"<p>La confianza mide qu\u00e9 tan a menudo se cumple la regla cuando el antecedente est\u00e1 presente:</p> \\[Confidence(A \\rightarrow B) = \\frac{Support(A \\cup B)}{Support(A)} = P(B|A)\\] <p>Ejemplo: Si {Pan} aparece en 4 transacciones y {Pan, Leche} en 3: \\(\\(Confidence(\\{Pan\\} \\rightarrow \\{Leche\\}) = \\frac{3}{4} = 0.75 = 75\\%\\)\\)</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#lift","title":"Lift","text":"<p>El lift mide si la asociaci\u00f3n es significativa o simplemente debida al azar:</p> \\[Lift(A \\rightarrow B) = \\frac{Confidence(A \\rightarrow B)}{Support(B)} = \\frac{P(A \\cap B)}{P(A) \\cdot P(B)}\\] <p>Interpretaci\u00f3n: - Lift &gt; 1: A y B aparecen juntos m\u00e1s de lo esperado por azar \u2192 Asociaci\u00f3n positiva - Lift = 1: A y B son independientes - Lift &lt; 1: A y B aparecen juntos menos de lo esperado \u2192 Asociaci\u00f3n negativa</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#conviction","title":"Conviction","text":"<p>La conviction mide qu\u00e9 tan diferente es la regla de una asociaci\u00f3n aleatoria:</p> \\[Conviction(A \\rightarrow B) = \\frac{1 - Support(B)}{1 - Confidence(A \\rightarrow B)}\\] <p>Interpretaci\u00f3n: - Conviction alto: La regla es muy \u00fatil - Conviction = 1: A y B son independientes - Conviction = \u221e: La regla siempre se cumple</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#83-el-algoritmo-apriori","title":"8.3. El Algoritmo Apriori","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#la-propiedad-apriori-antimonotonia","title":"La Propiedad Apriori (Antimonoton\u00eda)","text":"<p>El algoritmo se basa en un principio clave:</p> <p>\"Si un itemset es infrecuente, todos sus superconjuntos tambi\u00e9n ser\u00e1n infrecuentes\"</p> <p>Esto permite podar el espacio de b\u00fasqueda eficientemente.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PRINCIPIO APRIORI                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   Si {A, B} es INFRECUENTE                                 \u2502\n\u2502                                                             \u2502\n\u2502   Entonces NO necesitamos verificar:                       \u2502\n\u2502   - {A, B, C}                                               \u2502\n\u2502   - {A, B, D}                                               \u2502\n\u2502   - {A, B, C, D}                                            \u2502\n\u2502   - ... (todos los superconjuntos)                         \u2502\n\u2502                                                             \u2502\n\u2502   \u00a1Esto ahorra much\u00edsimo c\u00f3mputo!                          \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#pasos-del-algoritmo","title":"Pasos del Algoritmo","text":"<ol> <li>Paso 1: Encontrar todos los itemsets de tama\u00f1o 1 que cumplan min_support</li> <li>Paso 2: Generar candidatos de tama\u00f1o k+1 combinando itemsets frecuentes de tama\u00f1o k</li> <li>Paso 3: Podar candidatos que contengan subconjuntos infrecuentes</li> <li>Paso 4: Calcular soporte de candidatos restantes</li> <li>Paso 5: Repetir hasta que no haya m\u00e1s itemsets frecuentes</li> <li>Paso 6: Generar reglas de los itemsets frecuentes que cumplan min_confidence</li> </ol>"},{"location":"aprendizaje-no-supervisado/08-apriori/#84-pros-y-contras","title":"8.4. Pros y Contras","text":"Ventajas Desventajas F\u00e1cil de entender e implementar: Algoritmo intuitivo Puede ser lento: M\u00faltiples escaneos del dataset Interpretable: Las reglas son f\u00e1ciles de explicar Genera muchos candidatos: Especialmente con min_support bajo Escalable con poda: La propiedad apriori reduce b\u00fasqueda Requiere datos discretos: No funciona con datos continuos directamente Ampliamente usado: Implementaciones optimizadas disponibles Sensible a par\u00e1metros: min_support y min_confidence afectan mucho Resultados accionables: \u00datil para decisiones de negocio No captura contexto: Solo frecuencia, no causa-efecto"},{"location":"aprendizaje-no-supervisado/08-apriori/#85-ejemplo-basico-en-python","title":"8.5. Ejemplo B\u00e1sico en Python","text":"<p>Este ejemplo usa la biblioteca <code>mlxtend</code> para implementar Apriori y reglas de asociaci\u00f3n.</p> <pre><code># ============================================================\n# EJEMPLO B\u00c1SICO: Apriori para an\u00e1lisis de cesta de compra\n# ============================================================\n\n# Importar bibliotecas necesarias\nimport pandas as pd                          # Manipulaci\u00f3n de datos\nimport numpy as np                           # Operaciones num\u00e9ricas\nfrom mlxtend.preprocessing import TransactionEncoder  # Codificar transacciones\nfrom mlxtend.frequent_patterns import apriori, association_rules  # Algoritmo Apriori\n\n# -------------------------------------------------------------\n# 1. CREAR DATASET DE TRANSACCIONES\n# -------------------------------------------------------------\n# Lista de transacciones (cada transacci\u00f3n es una lista de \u00edtems)\ntransactions = [\n    ['Pan', 'Leche', 'Mantequilla'],\n    ['Pan', 'Cerveza'],\n    ['Leche', 'Pa\u00f1ales', 'Cerveza'],\n    ['Pan', 'Leche', 'Pa\u00f1ales', 'Cerveza'],\n    ['Pan', 'Leche', 'Pa\u00f1ales'],\n    ['Leche', 'Pa\u00f1ales', 'Cerveza'],\n    ['Pan', 'Leche'],\n    ['Pan', 'Cerveza', 'Pa\u00f1ales'],\n    ['Pan', 'Leche', 'Cerveza', 'Pa\u00f1ales'],\n    ['Leche', 'Mantequilla']\n]\n\nprint(\"=\"*60)\nprint(\"APRIORI - AN\u00c1LISIS DE CESTA DE LA COMPRA\")\nprint(\"=\"*60)\n\nprint(f\"\\n--- Transacciones Originales ---\")\nfor i, t in enumerate(transactions, 1):\n    print(f\"T{i}: {t}\")\n\nprint(f\"\\nTotal de transacciones: {len(transactions)}\")\n\n# -------------------------------------------------------------\n# 2. PREPARAR DATOS PARA APRIORI\n# -------------------------------------------------------------\n# TransactionEncoder convierte listas a matriz binaria\nte = TransactionEncoder()\nte_array = te.fit_transform(transactions)\n\n# Crear DataFrame con nombres de columnas (\u00edtems)\ndf = pd.DataFrame(te_array, columns=te.columns_)\n\nprint(f\"\\n--- Matriz de Transacciones (One-Hot Encoded) ---\")\nprint(df)\nprint(f\"\\n\u00cdtems \u00fanicos: {list(te.columns_)}\")\n\n# -------------------------------------------------------------\n# 3. ENCONTRAR ITEMSETS FRECUENTES\n# -------------------------------------------------------------\nprint(f\"\\n--- Paso 1: Encontrar Itemsets Frecuentes ---\")\n\n# Aplicar algoritmo Apriori\n# min_support: soporte m\u00ednimo (% de transacciones)\nfrequent_itemsets = apriori(\n    df, \n    min_support=0.3,  # Itemset debe aparecer en al menos 30% de transacciones\n    use_colnames=True  # Usar nombres de \u00edtems en lugar de \u00edndices\n)\n\n# Ordenar por soporte\nfrequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)\n\nprint(f\"\\nItemsets frecuentes (min_support=30%):\")\nprint(frequent_itemsets.to_string(index=False))\n\nprint(f\"\\nTotal de itemsets frecuentes: {len(frequent_itemsets)}\")\n\n# -------------------------------------------------------------\n# 4. GENERAR REGLAS DE ASOCIACI\u00d3N\n# -------------------------------------------------------------\nprint(f\"\\n--- Paso 2: Generar Reglas de Asociaci\u00f3n ---\")\n\n# Generar reglas con confianza m\u00ednima\nrules = association_rules(\n    frequent_itemsets,\n    metric='confidence',  # M\u00e9trica para filtrar\n    min_threshold=0.5     # Confianza m\u00ednima del 50%\n)\n\n# Seleccionar columnas relevantes y ordenar\nrules_display = rules[['antecedents', 'consequents', 'support', \n                        'confidence', 'lift', 'conviction']]\nrules_display = rules_display.sort_values('lift', ascending=False)\n\nprint(f\"\\nReglas de asociaci\u00f3n (min_confidence=50%):\")\nprint(rules_display.to_string(index=False))\n\nprint(f\"\\nTotal de reglas: {len(rules)}\")\n\n# -------------------------------------------------------------\n# 5. INTERPRETAR REGLAS\n# -------------------------------------------------------------\nprint(f\"\\n--- Interpretaci\u00f3n de las Mejores Reglas ---\")\n\n# Top 3 reglas por Lift\ntop_rules = rules.nlargest(3, 'lift')\n\nfor idx, (_, rule) in enumerate(top_rules.iterrows(), 1):\n    ant = list(rule['antecedents'])\n    cons = list(rule['consequents'])\n    supp = rule['support']\n    conf = rule['confidence']\n    lift = rule['lift']\n\n    print(f\"\\n{idx}. {ant} \u2192 {cons}\")\n    print(f\"   Soporte: {supp:.1%} (aparece en {supp*10:.0f}/10 transacciones)\")\n    print(f\"   Confianza: {conf:.1%} (cuando se compra {ant}, {conf:.0%} compra {cons})\")\n    print(f\"   Lift: {lift:.2f} ({lift:.2f}x m\u00e1s probable que por azar)\")\n\n# -------------------------------------------------------------\n# 6. VISUALIZAR REGLAS\n# -------------------------------------------------------------\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Scatter: Support vs Confidence, coloreado por Lift\nscatter = axes[0].scatter(rules['support'], rules['confidence'], \n                          c=rules['lift'], cmap='RdYlGn', s=100, alpha=0.7)\naxes[0].set_xlabel('Support')\naxes[0].set_ylabel('Confidence')\naxes[0].set_title('Reglas: Support vs Confidence\\n(color=Lift)')\nplt.colorbar(scatter, ax=axes[0], label='Lift')\n\n# Histograma de Lift\naxes[1].hist(rules['lift'], bins=10, color='steelblue', edgecolor='black')\naxes[1].axvline(x=1, color='red', linestyle='--', label='Lift=1 (independencia)')\naxes[1].set_xlabel('Lift')\naxes[1].set_ylabel('Frecuencia')\naxes[1].set_title('Distribuci\u00f3n del Lift')\naxes[1].legend()\n\n# Top 5 reglas por Lift\ntop_5 = rules.nlargest(5, 'lift')\nlabels = [f\"{list(r['antecedents'])} \u2192 {list(r['consequents'])}\" \n          for _, r in top_5.iterrows()]\nlabels = [l[:30] + '...' if len(l) &gt; 30 else l for l in labels]  # Truncar\n\naxes[2].barh(labels, top_5['lift'], color='steelblue')\naxes[2].axvline(x=1, color='red', linestyle='--', alpha=0.5)\naxes[2].set_xlabel('Lift')\naxes[2].set_title('Top 5 Reglas por Lift')\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 7. FILTRAR REGLAS \u00daTILES\n# -------------------------------------------------------------\nprint(f\"\\n--- Reglas Recomendadas para el Negocio ---\")\n\n# Reglas con alto lift Y alta confianza\ngood_rules = rules[(rules['lift'] &gt; 1.0) &amp; (rules['confidence'] &gt; 0.5)]\ngood_rules = good_rules.sort_values('lift', ascending=False)\n\nprint(f\"\\nReglas con Lift &gt; 1 y Confidence &gt; 50%:\")\nfor _, rule in good_rules.head(5).iterrows():\n    ant = ', '.join(list(rule['antecedents']))\n    cons = ', '.join(list(rule['consequents']))\n    print(f\"  \u2022 Si compra [{ant}], es {rule['confidence']:.0%} probable que compre [{cons}]\")\n    print(f\"    (Lift: {rule['lift']:.2f})\")\n\nprint(\"\"\"\nRecomendaciones basadas en las reglas:\n1. Colocar productos asociados cerca en el supermercado\n2. Crear promociones de productos que se compran juntos\n3. Recomendar productos complementarios en el checkout\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#86-ejemplo-avanzado-dataset-real-y-optimizacion","title":"8.6. Ejemplo Avanzado: Dataset Real y Optimizaci\u00f3n","text":"<p>Este ejemplo trabaja con un dataset m\u00e1s grande y explora diferentes par\u00e1metros.</p> <pre><code># ============================================================\n# EJEMPLO AVANZADO: Apriori con dataset de retail\n# ============================================================\n\nimport pandas as pd\nimport numpy as np\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# -------------------------------------------------------------\n# 1. CREAR DATASET SINT\u00c9TICO REALISTA\n# -------------------------------------------------------------\nnp.random.seed(42)\n\n# Simular 1000 transacciones de supermercado\nproducts = {\n    'Desayuno': ['Pan', 'Leche', 'Cereales', 'Huevos', 'Yogurt', 'Caf\u00e9', 'Zumo'],\n    'Snacks': ['Galletas', 'Chocolate', 'Patatas', 'Frutos_secos'],\n    'Bebidas': ['Agua', 'Cerveza', 'Vino', 'Refresco'],\n    'Limpieza': ['Detergente', 'Jab\u00f3n', 'Papel_higienico'],\n    'Beb\u00e9s': ['Pa\u00f1ales', 'Toallitas', 'Papilla']\n}\n\nall_products = [p for sublist in products.values() for p in sublist]\n\ndef generate_transaction():\n    \"\"\"Genera una transacci\u00f3n con productos correlacionados\"\"\"\n    items = []\n\n    # Desayuno: alta probabilidad de combinar\n    if np.random.random() &lt; 0.6:\n        items.extend(np.random.choice(['Pan', 'Leche', 'Cereales'], \n                                      size=np.random.randint(2, 4), replace=False))\n        if np.random.random() &lt; 0.3:\n            items.append('Mantequilla')\n\n    # Cerveza y pa\u00f1ales (ejemplo cl\u00e1sico)\n    if np.random.random() &lt; 0.15:\n        items.extend(['Cerveza', 'Pa\u00f1ales'])\n\n    # Productos aleatorios adicionales\n    n_random = np.random.randint(1, 5)\n    items.extend(np.random.choice(all_products, size=n_random, replace=False))\n\n    return list(set(items))\n\n# Generar transacciones\nn_transactions = 1000\ntransactions = [generate_transaction() for _ in range(n_transactions)]\n\nprint(\"=\"*60)\nprint(\"APRIORI - AN\u00c1LISIS AVANZADO\")\nprint(\"=\"*60)\n\nprint(f\"\\nTransacciones generadas: {n_transactions}\")\nprint(f\"Productos \u00fanicos: {len(all_products)}\")\n\n# Estad\u00edsticas b\u00e1sicas\nsizes = [len(t) for t in transactions]\nprint(f\"Tama\u00f1o promedio de transacci\u00f3n: {np.mean(sizes):.1f} \u00edtems\")\nprint(f\"Rango: {min(sizes)} - {max(sizes)} \u00edtems\")\n\n# -------------------------------------------------------------\n# 2. PREPARAR DATOS\n# -------------------------------------------------------------\nte = TransactionEncoder()\nte_array = te.fit_transform(transactions)\ndf = pd.DataFrame(te_array, columns=te.columns_)\n\nprint(f\"\\nMatriz de transacciones: {df.shape}\")\n\n# Frecuencia de productos individuales\nproduct_freq = df.sum().sort_values(ascending=False)\nprint(f\"\\n--- Top 10 Productos m\u00e1s Frecuentes ---\")\nprint(product_freq.head(10))\n\n# -------------------------------------------------------------\n# 3. COMPARAR PAR\u00c1METROS DE SOPORTE M\u00cdNIMO\n# -------------------------------------------------------------\nprint(f\"\\n--- Efecto del Soporte M\u00ednimo ---\")\n\nmin_supports = [0.01, 0.05, 0.1, 0.2, 0.3]\nresults = []\n\nfor min_sup in min_supports:\n    freq_items = apriori(df, min_support=min_sup, use_colnames=True)\n    n_items = len(freq_items)\n    max_size = freq_items['itemsets'].apply(len).max() if n_items &gt; 0 else 0\n    results.append({\n        'min_support': min_sup,\n        'n_itemsets': n_items,\n        'max_size': max_size\n    })\n    print(f\"  min_support={min_sup:.0%}: {n_items:4d} itemsets, max tama\u00f1o={max_size}\")\n\n# -------------------------------------------------------------\n# 4. ENCONTRAR ITEMSETS FRECUENTES (APRIORI vs FP-GROWTH)\n# -------------------------------------------------------------\nprint(f\"\\n--- Comparaci\u00f3n: Apriori vs FP-Growth ---\")\n\nimport time\n\n# Apriori\nstart = time.time()\nfreq_apriori = apriori(df, min_support=0.05, use_colnames=True)\ntime_apriori = time.time() - start\n\n# FP-Growth (m\u00e1s eficiente)\nstart = time.time()\nfreq_fpgrowth = fpgrowth(df, min_support=0.05, use_colnames=True)\ntime_fpgrowth = time.time() - start\n\nprint(f\"  Apriori:   {len(freq_apriori):4d} itemsets en {time_apriori:.3f}s\")\nprint(f\"  FP-Growth: {len(freq_fpgrowth):4d} itemsets en {time_fpgrowth:.3f}s\")\nprint(f\"  Speedup FP-Growth: {time_apriori/time_fpgrowth:.2f}x\")\n\n# Usar FP-Growth para el resto del an\u00e1lisis\nfrequent_itemsets = freq_fpgrowth\n\n# -------------------------------------------------------------\n# 5. GENERAR Y ANALIZAR REGLAS\n# -------------------------------------------------------------\nprint(f\"\\n--- Generaci\u00f3n de Reglas de Asociaci\u00f3n ---\")\n\n# Generar reglas con diferentes m\u00e9tricas\nmetrics = ['confidence', 'lift', 'conviction']\n\nfor metric in metrics:\n    rules = association_rules(frequent_itemsets, metric=metric, \n                              min_threshold=0.5 if metric=='confidence' else 1.0)\n    print(f\"  M\u00e9trica '{metric}': {len(rules)} reglas\")\n\n# Usar confianza como m\u00e9trica principal\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\nrules = rules[rules['lift'] &gt; 1]  # Solo asociaciones positivas\n\nprint(f\"\\nReglas finales (confidence&gt;50%, lift&gt;1): {len(rules)}\")\n\n# -------------------------------------------------------------\n# 6. TOP REGLAS POR DIFERENTES CRITERIOS\n# -------------------------------------------------------------\nprint(f\"\\n--- Top Reglas por Diferentes Criterios ---\")\n\ndef format_rule(row):\n    ant = ', '.join(list(row['antecedents']))\n    cons = ', '.join(list(row['consequents']))\n    return f\"{ant} \u2192 {cons}\"\n\n# Por Lift (asociaci\u00f3n m\u00e1s fuerte)\nprint(f\"\\n[Top 5 por LIFT - Asociaci\u00f3n m\u00e1s fuerte]\")\ntop_lift = rules.nlargest(5, 'lift')\nfor _, r in top_lift.iterrows():\n    print(f\"  {format_rule(r)}\")\n    print(f\"    Lift: {r['lift']:.2f}, Conf: {r['confidence']:.1%}, Supp: {r['support']:.1%}\")\n\n# Por Confidence (m\u00e1s confiables)\nprint(f\"\\n[Top 5 por CONFIDENCE - M\u00e1s confiables]\")\ntop_conf = rules.nlargest(5, 'confidence')\nfor _, r in top_conf.iterrows():\n    print(f\"  {format_rule(r)}\")\n    print(f\"    Conf: {r['confidence']:.1%}, Lift: {r['lift']:.2f}, Supp: {r['support']:.1%}\")\n\n# Por Support (m\u00e1s frecuentes)\nprint(f\"\\n[Top 5 por SUPPORT - M\u00e1s frecuentes]\")\ntop_supp = rules.nlargest(5, 'support')\nfor _, r in top_supp.iterrows():\n    print(f\"  {format_rule(r)}\")\n    print(f\"    Supp: {r['support']:.1%}, Conf: {r['confidence']:.1%}, Lift: {r['lift']:.2f}\")\n\n# -------------------------------------------------------------\n# 7. VISUALIZACI\u00d3N AVANZADA\n# -------------------------------------------------------------\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 1. Heatmap de m\u00e9tricas\nmetrics_df = rules[['support', 'confidence', 'lift', 'leverage', 'conviction']].copy()\nmetrics_df.index = [format_rule(rules.iloc[i])[:20] + '...' for i in range(len(rules))]\n# Tomar solo top 15 para visualizaci\u00f3n\nmetrics_sample = metrics_df.head(15)\n\nax1 = axes[0, 0]\nfrom matplotlib.colors import Normalize\n# Normalizar para heatmap\nnormalized = (metrics_sample - metrics_sample.min()) / (metrics_sample.max() - metrics_sample.min())\nim = ax1.imshow(normalized.values, cmap='YlOrRd', aspect='auto')\nax1.set_xticks(range(len(metrics_sample.columns)))\nax1.set_xticklabels(metrics_sample.columns, rotation=45)\nax1.set_yticks(range(len(metrics_sample.index)))\nax1.set_yticklabels(metrics_sample.index, fontsize=8)\nax1.set_title('M\u00e9tricas de Reglas (normalizado)')\nplt.colorbar(im, ax=ax1)\n\n# 2. Scatter 3D: Support, Confidence, Lift\nfrom mpl_toolkits.mplot3d import Axes3D\nax2 = fig.add_subplot(2, 2, 2, projection='3d')\nax2.scatter(rules['support'], rules['confidence'], rules['lift'],\n            c=rules['lift'], cmap='viridis', s=50)\nax2.set_xlabel('Support')\nax2.set_ylabel('Confidence')\nax2.set_zlabel('Lift')\nax2.set_title('Reglas en 3D')\n\n# 3. An\u00e1lisis de productos en reglas\nax3 = axes[1, 0]\n\n# Contar apariciones de cada producto en reglas\nproduct_counts = {}\nfor _, row in rules.iterrows():\n    for item in row['antecedents']:\n        product_counts[item] = product_counts.get(item, 0) + 1\n    for item in row['consequents']:\n        product_counts[item] = product_counts.get(item, 0) + 1\n\nproduct_counts_sorted = sorted(product_counts.items(), key=lambda x: x[1], reverse=True)[:10]\nproducts_names, counts = zip(*product_counts_sorted)\n\nax3.barh(products_names, counts, color='steelblue')\nax3.set_xlabel('Apariciones en reglas')\nax3.set_title('Productos m\u00e1s frecuentes en reglas')\n\n# 4. Distribuci\u00f3n de tama\u00f1os de itemsets\nax4 = axes[1, 1]\n\n# Tama\u00f1o de antecedentes y consecuentes\nant_sizes = rules['antecedents'].apply(len)\ncons_sizes = rules['consequents'].apply(len)\n\nax4.hist(ant_sizes, bins=range(1, max(ant_sizes)+2), alpha=0.7, \n         label='Antecedentes', color='blue')\nax4.hist(cons_sizes, bins=range(1, max(cons_sizes)+2), alpha=0.7,\n         label='Consecuentes', color='green')\nax4.set_xlabel('N\u00famero de \u00edtems')\nax4.set_ylabel('Frecuencia')\nax4.set_title('Tama\u00f1o de Antecedentes y Consecuentes')\nax4.legend()\n\nplt.tight_layout()\nplt.show()\n\n# -------------------------------------------------------------\n# 8. FILTRAR REGLAS ACCIONABLES\n# -------------------------------------------------------------\nprint(f\"\\n--- Reglas Accionables para el Negocio ---\")\n\n# Definir umbrales para reglas \u00fatiles\nactionable_rules = rules[\n    (rules['support'] &gt; 0.05) &amp;      # Al menos 5% de transacciones\n    (rules['confidence'] &gt; 0.6) &amp;     # Al menos 60% de confianza\n    (rules['lift'] &gt; 1.2)             # Al menos 20% mejor que azar\n].copy()\n\nactionable_rules = actionable_rules.sort_values('lift', ascending=False)\n\nprint(f\"\\nReglas accionables encontradas: {len(actionable_rules)}\")\nprint(\"\\n[Recomendaciones estrat\u00e9gicas]\")\n\nfor i, (_, rule) in enumerate(actionable_rules.head(5).iterrows(), 1):\n    ant = ', '.join(list(rule['antecedents']))\n    cons = ', '.join(list(rule['consequents']))\n\n    print(f\"\\n{i}. Cuando el cliente compra: {ant}\")\n    print(f\"   \u2192 Recomendar: {cons}\")\n    print(f\"   Efectividad: {rule['confidence']:.0%} de probabilidad\")\n    print(f\"   Impacto: {rule['lift']:.2f}x m\u00e1s probable que aleatorio\")\n\n# -------------------------------------------------------------\n# 9. AN\u00c1LISIS DE PATRONES ESPEC\u00cdFICOS\n# -------------------------------------------------------------\nprint(f\"\\n--- An\u00e1lisis de Patrones Espec\u00edficos ---\")\n\n# Buscar reglas que contengan un producto espec\u00edfico\ntarget_product = 'Cerveza'\n\nrules_with_target = rules[\n    rules['antecedents'].apply(lambda x: target_product in x) |\n    rules['consequents'].apply(lambda x: target_product in x)\n]\n\nprint(f\"\\nReglas relacionadas con '{target_product}': {len(rules_with_target)}\")\nfor _, r in rules_with_target.head(3).iterrows():\n    print(f\"  {format_rule(r)} (Lift: {r['lift']:.2f})\")\n\n# -------------------------------------------------------------\n# 10. RESUMEN\n# -------------------------------------------------------------\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESUMEN Y MEJORES PR\u00c1CTICAS\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nAn\u00e1lisis completado:\n- Transacciones analizadas: {n_transactions:,}\n- Itemsets frecuentes encontrados: {len(frequent_itemsets)}\n- Reglas generadas: {len(rules)}\n- Reglas accionables: {len(actionable_rules)}\n\nMejores pr\u00e1cticas para Apriori:\n1. min_support: Empezar alto (0.1) y bajar si hay pocos resultados\n2. min_confidence: 0.5-0.7 para reglas confiables\n3. Lift: Siempre filtrar por lift &gt; 1 para asociaciones reales\n4. FP-Growth: Usar en lugar de Apriori para datasets grandes\n5. Validaci\u00f3n: Revisar reglas con expertos del dominio\n\nUso empresarial:\n- Cross-selling: Recomendar productos complementarios\n- Layout de tienda: Colocar productos asociados juntos\n- Promociones: Crear bundles basados en asociaciones\n- Inventario: Mantener stock de productos asociados\n\"\"\")\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#87-hiperparametros-y-parametros","title":"8.7. Hiperpar\u00e1metros y Par\u00e1metros","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#parametros-de-apriori-mlxtend","title":"Par\u00e1metros de Apriori (mlxtend)","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>min_support</code> Soporte m\u00ednimo para itemsets 0.0-1.0 0.01-0.1 (depende del dataset) <code>use_colnames</code> Usar nombres de columnas bool True <code>max_len</code> Tama\u00f1o m\u00e1ximo de itemsets int None (sin l\u00edmite) <code>low_memory</code> Modo bajo memoria bool False"},{"location":"aprendizaje-no-supervisado/08-apriori/#parametros-de-association-rules-mlxtend","title":"Par\u00e1metros de Association Rules (mlxtend)","text":"Par\u00e1metro Descripci\u00f3n Valores Recomendaci\u00f3n <code>metric</code> M\u00e9trica para filtrar 'support', 'confidence', 'lift', etc. 'confidence' o 'lift' <code>min_threshold</code> Umbral m\u00ednimo de la m\u00e9trica float Depende de la m\u00e9trica"},{"location":"aprendizaje-no-supervisado/08-apriori/#88-alternativas-a-apriori","title":"8.8. Alternativas a Apriori","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#fp-growth","title":"FP-Growth","text":"<p>FP-Growth (Frequent Pattern Growth) es m\u00e1s eficiente que Apriori: - No genera candidatos expl\u00edcitamente - Usa una estructura de \u00e1rbol (FP-tree) - Solo requiere 2 escaneos del dataset</p> <pre><code>from mlxtend.frequent_patterns import fpgrowth\n\nfrequent_itemsets = fpgrowth(df, min_support=0.05, use_colnames=True)\n</code></pre>"},{"location":"aprendizaje-no-supervisado/08-apriori/#eclat","title":"Eclat","text":"<p>Eclat usa intersecci\u00f3n de conjuntos verticales: - Puede ser m\u00e1s r\u00e1pido para datasets densos</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#spmf","title":"SPMF","text":"<p>Para an\u00e1lisis m\u00e1s avanzado, la biblioteca SPMF ofrece muchos algoritmos de patrones secuenciales.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#89-aplicaciones-reales","title":"8.9. Aplicaciones Reales","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#1-retail-y-e-commerce","title":"1. Retail y E-commerce","text":"<p>An\u00e1lisis de cesta de compra, recomendaciones de productos. * Amazon: \"Customers who bought this also bought...\"</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#2-banca-y-finanzas","title":"2. Banca y Finanzas","text":"<p>Detectar patrones de fraude, servicios complementarios.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#3-medicina","title":"3. Medicina","text":"<p>Asociaciones entre s\u00edntomas y diagn\u00f3sticos, efectos secundarios de medicamentos.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#4-web-mining","title":"4. Web Mining","text":"<p>Analizar patrones de navegaci\u00f3n, p\u00e1ginas visitadas juntas.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#5-telecomunicaciones","title":"5. Telecomunicaciones","text":"<p>An\u00e1lisis de servicios contratados juntos, patrones de uso.</p>"},{"location":"aprendizaje-no-supervisado/08-apriori/#810-resumen-y-checklist","title":"8.10. Resumen y Checklist","text":""},{"location":"aprendizaje-no-supervisado/08-apriori/#checklist-para-usar-apriori","title":"Checklist para usar Apriori","text":"<ul> <li>[ ] Datos en formato transaccional (lista de \u00edtems por transacci\u00f3n)</li> <li>[ ] Codificar con TransactionEncoder para matriz binaria</li> <li>[ ] Empezar con min_support alto y ajustar</li> <li>[ ] Generar reglas con confidence razonable (&gt;50%)</li> <li>[ ] Filtrar por lift &gt; 1 para asociaciones reales</li> <li>[ ] Validar reglas con conocimiento del dominio</li> <li>[ ] Usar FP-Growth para datasets grandes</li> </ul>"},{"location":"aprendizaje-no-supervisado/08-apriori/#cuando-usar-apriori","title":"\u00bfCu\u00e1ndo usar Apriori?","text":"<p>\u2705 Usar Apriori cuando: - Tienes datos transaccionales discretos - Buscas patrones de co-ocurrencia - Necesitas reglas interpretables - Dataset de tama\u00f1o moderado (&lt;100K transacciones)</p> <p>\u274c Considerar alternativas cuando: - Datos continuos \u2192 Primero discretizar o usar clustering - Dataset muy grande \u2192 FP-Growth - Patrones secuenciales \u2192 GSP, PrefixSpan - Necesitas predicci\u00f3n \u2192 Modelos supervisados</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/","title":"Aprendizaje Supervisado en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 En este documento encontrar\u00e1s una introducci\u00f3n a los conceptos b\u00e1sicos del aprendizaje supervisado, una de las ramas fundamentales de la Inteligencia Artificial (IA) y del Machine Learning (ML).</p>"},{"location":"aprendizaje-supervisado/#que-es-el-aprendizaje-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje Supervisado?","text":"<p>El aprendizaje supervisado es un tipo de aprendizaje autom\u00e1tico en el que un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada ejemplo del conjunto de entrenamiento incluye tanto la entrada (X) como la salida deseada (Y).</p> <p>El objetivo del modelo es aprender la relaci\u00f3n entre las entradas y las salidas para poder predecir correctamente la salida de nuevos datos nunca vistos.</p>"},{"location":"aprendizaje-supervisado/#tipos-de-problemas-supervisados","title":"\ud83e\udde0 Tipos de Problemas Supervisados","text":"<ol> <li> <p>Clasificaci\u00f3n:    El modelo aprende a asignar una etiqueta o categor\u00eda a cada ejemplo.    \ud83d\udccd Ejemplo: Clasificar correos como \"spam\" o \"no spam\".</p> </li> <li> <p>Regresi\u00f3n:    El modelo aprende a predecir un valor num\u00e9rico continuo.    \ud83d\udccd Ejemplo: Predecir el precio de una vivienda seg\u00fan sus caracter\u00edsticas.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/#flujo-de-trabajo-del-aprendizaje-supervisado","title":"\u2699\ufe0f Flujo de Trabajo del Aprendizaje Supervisado","text":"<ol> <li>Recolecci\u00f3n de datos: Se obtiene un conjunto de datos con ejemplos representativos del problema.</li> <li>Preprocesamiento: Limpieza, normalizaci\u00f3n y divisi\u00f3n del conjunto de datos (entrenamiento y prueba).</li> <li>Selecci\u00f3n del modelo: Elegir el algoritmo m\u00e1s adecuado (por ejemplo, SVM, \u00c1rboles de decisi\u00f3n, Redes neuronales, etc.).</li> <li>Entrenamiento: El modelo aprende a partir de los datos etiquetados.</li> <li>Evaluaci\u00f3n: Se mide el rendimiento utilizando m\u00e9tricas como precisi\u00f3n, recall o error cuadr\u00e1tico medio.</li> <li>Predicci\u00f3n: Se aplican los conocimientos adquiridos a nuevos datos.</li> </ol>"},{"location":"aprendizaje-supervisado/#ejemplos-de-algoritmos-comunes","title":"\ud83d\udd0d Ejemplos de Algoritmos Comunes","text":"<ul> <li>Regresi\u00f3n lineal</li> <li>K-Nearest Neighbors (KNN)</li> <li>\u00c1rboles de decisi\u00f3n</li> <li>Random Forest</li> <li>M\u00e1quinas de Vectores de Soporte (SVM)</li> <li>Redes neuronales artificiales</li> </ul>"},{"location":"aprendizaje-supervisado/#evaluacion-del-modelo","title":"\ud83d\udcca Evaluaci\u00f3n del Modelo","text":"<p>Para medir la calidad del modelo se utilizan m\u00e9tricas que dependen del tipo de problema:</p> Tipo de problema M\u00e9tricas comunes Clasificaci\u00f3n Exactitud, Precisi\u00f3n, Recall, F1-score Regresi\u00f3n Error absoluto medio (MAE), Error cuadr\u00e1tico medio (MSE), R\u00b2"},{"location":"aprendizaje-supervisado/#consejos-practicos","title":"\ud83d\udca1 Consejos Pr\u00e1cticos","text":"<ul> <li>Siempre divide tus datos en entrenamiento y prueba (por ejemplo, 80% / 20%).</li> <li>Evita el sobreajuste (overfitting): si el modelo aprende demasiado bien los datos de entrenamiento, fallar\u00e1 en los nuevos.</li> <li>Usa validaci\u00f3n cruzada para estimar el rendimiento real del modelo.</li> </ul>"},{"location":"aprendizaje-supervisado/#conclusion","title":"\ud83e\udde9 Conclusi\u00f3n","text":"<p>El aprendizaje supervisado es la base de muchas aplicaciones modernas de IA, desde sistemas de recomendaci\u00f3n hasta diagn\u00f3stico m\u00e9dico. Dominar sus fundamentos te permitir\u00e1 avanzar hacia t\u00e9cnicas m\u00e1s complejas y poderosas.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 26/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/","title":"\ud83e\udd16 Unidad 1. Machine Learning Basado en el An\u00e1lisis de Datos","text":"<p>Esta unidad introduce los conceptos fundamentales del Machine Learning (ML), su flujo de trabajo, las herramientas clave de la biblioteca <code>scikit-learn</code> del lenguaje Python, y las metodolog\u00edas esenciales para la preparaci\u00f3n, divisi\u00f3n y preprocesamiento de datos.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#11-que-es-el-machine-learning","title":"1.1. \u00bfQu\u00e9 es el Machine Learning?","text":"<p>El Machine Learning (ML) se define como un campo de estudio que utiliza modelos estad\u00edsticos para aprender de los datos. Un aspecto clave es que modelos relativamente simples pueden realizar predicciones complejas.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#definiciones-clave","title":"Definiciones Clave","text":"<ul> <li>Definici\u00f3n temprana (Samuel, 1959): \"Programar computadoras para que aprendan de la experiencia deber\u00eda eliminar la necesidad de gran parte de este esfuerzo de programaci\u00f3n detallado\".</li> <li>Definici\u00f3n moderna (Mitchell, 1997): \"Se dice que un programa de computadora aprende de la experiencia E con respecto a alguna clase de tareas T y una medida de rendimiento P, si su rendimiento en las tareas T, medido por P, mejora con la experiencia E\".</li> <li>Definici\u00f3n matem\u00e1tica (Ej. Regresi\u00f3n Lineal): Un modelo matem\u00e1tico que intenta encontrar la relaci\u00f3n \u00f3ptima entre variables. Por ejemplo, predecir ventas (Target, \\(y\\)) bas\u00e1ndose en gastos de publicidad (Feature, \\(x\\)). El modelo \\(y = wx + b\\) aprende los par\u00e1metros \\(w\\) (peso) y \\(b\\) iterando desde valores arbitrarios (\\(f_1\\)) hasta un valor \u00f3ptimo (\\(f_3\\)) que minimiza el error.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#ml-y-otros-campos","title":"ML y Otros Campos","text":"<p>El Machine Learning est\u00e1 profundamente interconectado con otros campos:</p> <ul> <li>Es un subcampo de la Inteligencia Artificial.</li> <li>Deep Learning es un subcampo del Machine Learning.</li> <li>Se solapa significativamente con Estad\u00edstica, Miner\u00eda de Datos y Reconocimiento de Patrones.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#tipos-de-machine-learning","title":"Tipos de Machine Learning","text":"<p>Seg\u00fan el m\u00e9todo de supervisi\u00f3n, el ML se divide en:</p> <ol> <li> <p>Supervisado: Se proporciona un patr\u00f3n objetivo (datos etiquetados). El cap\u00edtulo se centra en este tipo, que incluye algoritmos como Regresi\u00f3n Lineal, Regresi\u00f3n Log\u00edstica, \u00c1rboles de Decisi\u00f3n, KNN, SVM y Redes Neuronales.</p> </li> <li> <p>No Supervisado: El patr\u00f3n objetivo debe ser descubierto (datos no etiquetados). Incluye Clustering, PCA y An\u00e1lisis de Asociaci\u00f3n.</p> </li> <li> <p>Refuerzo: Se aprende mediante la optimizaci\u00f3n de pol\u00edticas (recompensas y castigos).</p> </li> </ol>"},{"location":"aprendizaje-supervisado/01-machine-learning/#flujo-de-trabajo-del-machine-learning","title":"Flujo de Trabajo del Machine Learning","text":"<p>El proceso general para construir un modelo de ML es:</p> <ol> <li> <p>Definici\u00f3n del Problema: Comprender el objetivo de negocio.</p> </li> <li> <p>Preparaci\u00f3n de Datos: Recolecci\u00f3n de datos brutos (Raw Data) y preprocesamiento.</p> </li> <li> <p>Machine Learning (Modelado): Se divide la data en conjuntos de Train (Entrenamiento), Validate (Validaci\u00f3n) y Test (Prueba).</p> </li> <li> <p>Entrenamiento y Evaluaci\u00f3n: Esta fase incluye Ingenier\u00eda de caracter\u00edsticas (Feature engineering), Modelado y optimizaci\u00f3n (entrenar el modelo con los datos), y Evaluaci\u00f3n de rendimiento (Performance metrics).</p> </li> <li> <p>Aplicaci\u00f3n: Aplicar el modelo en la vida real.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/01-machine-learning/#parametros-vs-hiperparametros","title":"Par\u00e1metros vs. Hiperpar\u00e1metros","text":"<ul> <li> <p>Par\u00e1metros: Se aprenden desde los datos durante el entrenamiento. Contienen el patr\u00f3n de los datos (ej. \\(w\\) y \\(b\\) en regresi\u00f3n lineal, pesos de una red neuronal).</p> </li> <li> <p>Hiperpar\u00e1metros: Se configuran manualmente por el practicante antes del entrenamiento. Se \"afinan\" (tunan) para optimizar el rendimiento (ej. el valor \\(k\\) en KNN, la tasa de aprendizaje, la profundidad m\u00e1xima de un \u00e1rbol).</p> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#12-biblioteca-python-scikit-learn","title":"1.2. Biblioteca Python scikit-learn","text":"<p><code>scikit-learn</code> es la biblioteca de ML m\u00e1s representativa de Python.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Proporciona una interfaz de biblioteca integrada y unificada.</li> <li>Incluye una amplia variedad de algoritmos de ML, funciones de preprocesamiento y selecci\u00f3n de modelos.</li> <li>Es simple, eficiente y est\u00e1 construida sobre NumPy, SciPy y matplotlib.</li> <li>Es de c\u00f3digo abierto y puede usarse comercialmente.</li> <li>No soporta GPU.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#mecanismo-de-scikit-learn","title":"Mecanismo de <code>scikit-learn</code>","text":"<p>El flujo de trabajo de la API de <code>scikit-learn</code> es intuitivo y sigue tres pasos:</p> <ol> <li> <p>Instance: Crear una instancia del objeto del modelo (Estimator).</p> </li> <li> <p>Fit: Entrenar el modelo con los datos.</p> </li> <li> <p>Predict / transform: Usar el modelo entrenado para hacer predicciones o transformar datos.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/01-machine-learning/#estimator-classifier-y-regressor","title":"Estimator, Classifier y Regressor","text":"<ul> <li> <p><code>Estimator</code>: El objeto base. Aprende de los datos usando el m\u00e9todo <code>.fit()</code> y puede hacer predicciones usando <code>.predict()</code>.</p> </li> <li> <p><code>Classifier</code>: Un estimador para tareas de clasificaci\u00f3n (ej. <code>DecisionTreeClassifier</code>, <code>KNeighborsClassifier</code>).</p> </li> <li> <p><code>Regressor</code>: Un estimador para tareas de regresi\u00f3n (predicci\u00f3n num\u00e9rica) (ej. <code>LinearRegression</code>, <code>KNeighborsRegressor</code>).</p> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#sintaxis-basica-de-scikit-learn","title":"Sintaxis B\u00e1sica de <code>scikit-learn</code>","text":"<p>Importaciones:</p> <pre><code># Importar un estimador\nfrom sklearn.linear_model import LinearRegression\n\n# Importar un preprocesador\nfrom sklearn.preprocessing import StandardScaler\n\n# Importar divisi\u00f3n de datos\nfrom sklearn.model_selection import train_test_split\n\n# Importar m\u00e9tricas\nfrom sklearn import metrics\n</code></pre> <p>Uso b\u00e1sico:</p> <pre><code># Instanciar (con hiperpar\u00e1metros)\nmyModel = KNeighborsClassifier(n_neighbors=10)\n\n# Dividir los datos (Hold-out)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n\n# Entrenar el modelo (Supervisado)\nmyModel.fit(X_train, Y_train)\n\n# Hacer predicciones\nY_pred = myModel.predict(X_test)\n\n# Evaluar el rendimiento\nmetrics.accuracy_score(Y_test, Y_pred)\n\n# Afinar hiperpar\u00e1metros (con Cross-Validation)\nmyGridCV = GridSearchCV(estimator, parameter_grid, cv=5)\n</code></pre>"},{"location":"aprendizaje-supervisado/01-machine-learning/#ejemplo-practico-estandarizacion","title":"Ejemplo Pr\u00e1ctico: Estandarizaci\u00f3n","text":"<p>El preprocesamiento, como la estandarizaci\u00f3n, es crucial para mejorar el rendimiento. La estandarizaci\u00f3n (o z-transformation) convierte los datos para que sigan una distribuci\u00f3n normal est\u00e1ndar, usando la f\u00f3rmula \\(z = \\frac{x - m}{\\sigma}\\) (donde \\(m\\) es la media y \\(\\sigma\\) la desviaci\u00f3n est\u00e1ndar).</p> <p>En <code>scikit-learn</code>, se usa <code>StandardScaler</code>:</p> <pre><code># 1. Importar\nfrom sklearn.preprocessing import StandardScaler\n\n# 2. Instanciar\nscaler = StandardScaler()\n\n# 3. Ajustar (Fit): Se aprende la media y desviaci\u00f3n solo de datos de entrenamiento\nscaler.fit(X_train)\n\n# 4. Transformar: Se aplica a datos de entrenamiento y prueba\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Alternativa: fit_transform combina pasos 3 y 4 (solo para X_train)\nX_train = scaler.fit_transform(X_train)\n</code></pre> <p>Antes de la estandarizaci\u00f3n, las columnas pueden tener rangos de valores muy diferentes. Despu\u00e9s, todos los valores est\u00e1n centrados alrededor de 0, lo que ayuda a muchos algoritmos a converger mejor.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#modulos-principales-de-scikit-learn","title":"M\u00f3dulos Principales de <code>scikit-learn</code>","text":"M\u00f3dulo Funci\u00f3n Principal Ejemplos <code>sklearn.datasets</code> Cargar datasets de ejemplo. <code>load_iris()</code>, <code>load_breast_cancer()</code> <code>sklearn.preprocessing</code> Preprocesamiento de datos (escalado, codificaci\u00f3n). <code>StandardScaler</code>, <code>LabelEncoder</code>, <code>OneHotEncoder</code> <code>sklearn.model_selection</code> Divisi\u00f3n de datos, validaci\u00f3n y afinado de hiperpar\u00e1metros. <code>train_test_split</code>, <code>GridSearchCV</code>, <code>KFold</code> <code>sklearn.metrics</code> Evaluaci\u00f3n de rendimiento del modelo. <code>accuracy_score</code>, <code>precision_score</code>, <code>recall_score</code>, <code>roc_auc_score</code> <code>sklearn.linear_model</code> Algoritmos lineales. <code>LinearRegression</code>, <code>LogisticRegression</code> <code>sklearn.tree</code> Algoritmos de \u00c1rboles de Decisi\u00f3n. <code>DecisionTreeClassifier</code> <code>sklearn.neighbors</code> Algoritmos de vecinos cercanos. <code>KNeighborsClassifier</code> (K-NN) <code>sklearn.svm</code> Support Vector Machine (M\u00e1quinas de Vectores de Soporte). <code>SVC</code> <code>sklearn.ensemble</code> Algoritmos de Ensamblado (Ensemble). <code>RandomForestClassifier</code>, <code>AdaBoostClassifier</code> <code>sklearn.cluster</code> Algoritmos de clustering (No supervisado). <code>KMeans</code>, <code>DBSCAN</code> <code>sklearn.pipeline</code> Herramienta para encadenar pasos de preprocesamiento y modelado. <code>Pipeline</code>"},{"location":"aprendizaje-supervisado/01-machine-learning/#13-preparacion-y-division-del-dataset","title":"1.3. Preparaci\u00f3n y Divisi\u00f3n del Dataset","text":"<p>La divisi\u00f3n de datos es fundamental para evaluar un modelo de ML. El conjunto de datos general se divide en un conjunto de entrenamiento y uno de evaluaci\u00f3n (prueba).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#overfitting-sobreajuste-y-generalizacion","title":"Overfitting (Sobreajuste) y Generalizaci\u00f3n","text":"<ul> <li> <p>Generalizaci\u00f3n: Es la capacidad del modelo para predecir con precisi\u00f3n datos nuevos que no ha visto antes.</p> </li> <li> <p>Overfitting: Ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento, aprendiendo incluso el ruido.</p> </li> <li> <p>Underfitting (Subajuste): Ocurre cuando un modelo es demasiado simple (baja capacidad) y no puede capturar el patr\u00f3n subyacente de los datos.</p> </li> </ul> <p>El Dilema: A medida que aumenta la complejidad (flexibilidad) del modelo: * El error en el conjunto de entrenamiento (Training set) siempre disminuye. * El error en el conjunto de prueba (Test set) disminuye al principio, pero luego comienza a aumentar. El punto donde el error de prueba empieza a subir es donde comienza el overfitting.</p> <p>El conjunto de prueba es esencial para detectar el overfitting y seleccionar un modelo que generalice bien.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#cross-validation-validacion-cruzada","title":"Cross-Validation (Validaci\u00f3n Cruzada)","text":"<p>El conjunto de prueba (Test set) debe usarse \u00a1solo una vez! al final, para la evaluaci\u00f3n final.</p> <p>Para evaluar el modelo durante el entrenamiento (por ejemplo, para afinar hiperpar\u00e1metros), necesitamos una forma de simular un \"conjunto de prueba\" sin tocar el real. Para esto, dividimos el conjunto de entrenamiento (Training Data) en dos partes m\u00e1s peque\u00f1as: un nuevo conjunto de <code>Train</code> y un conjunto de <code>Cross Validate</code> (Validaci\u00f3n).</p> <p>M\u00e9todo: k-Fold Cross-Validation</p> <p>Es el m\u00e9todo m\u00e1s com\u00fan:</p> <ol> <li> <p>Se subdivide el conjunto de entrenamiento (original) en k partes iguales (folds). (Usualmente k=10).</p> </li> <li> <p>Se itera k veces (rondas).</p> </li> <li> <p>En cada ronda, se usa 1 fold como conjunto de validaci\u00f3n y los k-1 folds restantes como conjunto de entrenamiento.</p> </li> <li> <p>Se calcula la m\u00e9trica de rendimiento (ej. accuracy) en cada ronda.</p> </li> <li> <p>El rendimiento final del modelo es el promedio de las m\u00e9tricas de las k rondas.</p> </li> </ol> <p>M\u00e9todo: Leave One Out (LOO) Es un caso extremo de k-Fold donde \\(k\\) es igual al n\u00famero total de muestras. Se entrena con todos los datos menos uno, y se valida con ese \u00fanico dato. Es computacionalmente muy costoso.</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#14-preprocesamiento-de-datos","title":"1.4. Preprocesamiento de Datos","text":"<p>Preparar los datos es vital para un buen modelo. Esto incluye la limpieza (manejo de valores at\u00edpicos y faltantes) y la transformaci\u00f3n (escalado y codificaci\u00f3n).</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#manejo-de-valores-faltantes-missing-values","title":"Manejo de Valores Faltantes (Missing Values)","text":"<p>Los valores faltantes (identificados en Python como <code>np.nan</code> o <code>NaN</code>) deben ser tratados.</p> <p>1. Identificaci\u00f3n: Se pueden contar usando <code>df.isnull().sum()</code>.</p> <p>2. Eliminaci\u00f3n (con Pandas <code>dropna()</code>):</p> <ul> <li> <p><code>df.dropna()</code>: Elimina cualquier fila que contenga al menos un <code>NaN</code> (eje por defecto 0).</p> </li> <li> <p><code>df.dropna(axis=1)</code>: Elimina cualquier columna que contenga un <code>NaN</code>.</p> </li> <li> <p><code>df.dropna(how='all')</code>: Elimina filas/columnas donde todos los valores son <code>NaN</code>.</p> </li> <li> <p><code>df.dropna(thresh=N)</code>: Mantiene las filas que tienen al menos <code>N</code> valores no-<code>NaN</code>.</p> </li> <li> <p><code>df.dropna(subset=['col_name'])</code>: Elimina filas que tienen <code>NaN</code> espec\u00edficamente en la columna 'col_name'.</p> </li> </ul> <p>3. Imputaci\u00f3n (Relleno):</p> <p>Se usa cuando eliminar datos resultar\u00eda en una p\u00e9rdida significativa de informaci\u00f3n.</p> <ul> <li> <p>M\u00e9todos simples: Rellenar con un valor (ej. 'unknown'), la media, la mediana o el valor m\u00e1s frecuente (moda) de la columna.</p> </li> <li> <p>Con <code>scikit-learn</code> (<code>SimpleImputer</code>): Es el m\u00e9todo preferido.</p> </li> </ul> <pre><code>from sklearn.impute import SimpleImputer\n\n# Estrategias: 'mean', 'median', 'most_frequent'\nimpt = SimpleImputer(strategy='mean')\n\n# Aprende la media del set de entrenamiento\nimpt.fit(X_train)\n\n# Aplica la imputaci\u00f3n\nX_train_imputed = impt.transform(X_train)\n\n# Aplica la misma imputaci\u00f3n (con la media de train) al set de prueba\nX_test_imputed = impt.transform(X_test)\n</code></pre>"},{"location":"aprendizaje-supervisado/01-machine-learning/#manejo-de-datos-categoricos","title":"Manejo de Datos Categ\u00f3ricos","text":"<p>Los algoritmos de ML requieren entradas num\u00e9ricas. Los datos categ\u00f3ricos deben ser convertidos.</p> <p>1. Datos Ordinales (con orden):</p> <p>Ej. Tallas: 'M' &lt; 'L' &lt; 'XL'. Se deben mapear a enteros que respeten ese orden.</p> <pre><code>size_mapping = {'M': 1, 'L': 2, 'XL': 3}\ndf['size'] = df['size'].map(size_mapping)\n</code></pre> <p>2. Datos Nominales (sin orden) y Etiquetas de Clase:</p> <p>Ej. Colores: 'red', 'green', 'blue' o Etiquetas: 'setosa', 'versicolor'.</p> <p>Codificaci\u00f3n de Etiquetas (Label Encoding):</p> <p>Convierte cada etiqueta \u00fanica en un entero (ej. 'class1': 0, 'class2': 1). Se usa <code>LabelEncoder</code> de <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nenc = LabelEncoder()\ny_encoded = enc.fit_transform(df['classlabel'])\n</code></pre> <p>One-Hot Encoding (para caracter\u00edsticas nominales):</p> <p>Usar <code>LabelEncoder</code> para caracter\u00edsticas (X) es incorrecto, ya que crea un orden artificial. Se debe usar One-Hot Encoding. Crea nuevas columnas \"dummy\" (0 o 1) para cada categor\u00eda, indicando presencia (1) o ausencia (0).</p> <ul> <li> <p>M\u00e9todo Pandas: <code>pd.get_dummies(df['species'])</code>.</p> </li> <li> <p>M\u00e9todo <code>scikit-learn</code>: <code>OneHotEncoder</code>. Este m\u00e9todo es preferido en pipelines y a menudo devuelve una matriz dispersa (sparse matrix) para ahorrar memoria, ya que la mayor\u00eda de los valores ser\u00e1n 0.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#division-de-datos-estratificada-stratify","title":"Divisi\u00f3n de Datos Estratificada (Stratify)","text":"<p>Al usar <code>train_test_split</code>, si el dataset est\u00e1 desbalanceado (ej. 90% clase A, 10% clase B), una divisi\u00f3n aleatoria simple podr\u00eda resultar en un set de prueba sin muestras de la clase B.</p> <ul> <li> <p>Soluci\u00f3n: Usar el par\u00e1metro <code>stratify=y</code>.</p> </li> <li> <p>Esto asegura que la proporci\u00f3n de las clases (ej. 90/10) se mantenga id\u00e9ntica tanto en el conjunto de entrenamiento como en el de prueba, reflejando el dataset original.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#topicos-avanzados-tradeoff-de-sesgo-varianza-y-regularizacion","title":"T\u00f3picos Avanzados: Tradeoff de Sesgo-Varianza y Regularizaci\u00f3n","text":"<ul> <li>Tradeoff de Sesgo-Varianza:<ul> <li>Sesgo (Bias): Error por suposiciones incorrectas (Underfitting).</li> <li>Varianza (Variance): Error por sensibilidad excesiva a los datos de entrenamiento (Overfitting).</li> <li>Error Total \\(\\approx\\) Sesgo\u00b2 + Varianza. El objetivo es encontrar la complejidad \u00f3ptima que minimice este error total.</li> </ul> </li> <li>Regularizaci\u00f3n: T\u00e9cnica para prevenir el overfitting en modelos lineales penalizando coeficientes (pesos) grandes.<ul> <li>Ridge (L2): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{w_j^2}\\). Encoge los pesos, pero no los hace cero.</li> <li>Lasso (L1): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{|w_j|}\\). Puede forzar que algunos pesos sean exactamente cero, realizando una selecci\u00f3n de caracter\u00edsticas autom\u00e1tica.</li> <li>ElasticNet: Combina penalizaciones L1 y L2.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#15-practica-solucion-de-problemas-con-scikit-learn-ej-iris","title":"1.5. Pr\u00e1ctica: Soluci\u00f3n de Problemas con scikit-learn (Ej. Iris)","text":"<p>Esta secci\u00f3n aplica todos los conceptos anteriores en un caso pr\u00e1ctico completo usando el dataset \"Iris\".</p>"},{"location":"aprendizaje-supervisado/01-machine-learning/#1-entendimiento-del-problema-y-datos-eda","title":"1. Entendimiento del Problema y Datos (EDA)","text":"<ul> <li>Objetivo: Clasificar la especie de una flor Iris (Target).</li> <li>Clases (Target): 3 especies (Setosa, Versicolor, Virginica).</li> <li>Caracter\u00edsticas (Features): <code>sepal_length</code>, <code>sepal_width</code>, <code>petal_length</code>, <code>petal_width</code>.</li> <li>An\u00e1lisis de Datos:<ul> <li>Se cargan los datos y se convierten a un DataFrame de Pandas.</li> <li>Valores Faltantes: Se comprueba con <code>iris.isnull().sum()</code>. No se encontraron.</li> <li>Distribuci\u00f3n de Clases: Se comprueba con <code>iris.groupby('target').size()</code>. Hay 50 muestras de cada clase (33.3% cada una). Es un dataset balanceado.</li> <li>Estad\u00edsticas y Correlaci\u00f3n: <code>iris.describe()</code> y <code>iris.corr()</code>. Se observa que <code>petal_length</code> y <code>petal_width</code> est\u00e1n altamente correlacionados (0.96), sugiriendo un problema de multicolinealidad.</li> <li>Visualizaci\u00f3n: Se usan <code>pairplot</code> y <code>heatmap</code> para confirmar visualmente las relaciones y la alta correlaci\u00f3n.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#2-division-y-preparacion-de-datos","title":"2. Divisi\u00f3n y Preparaci\u00f3n de Datos","text":"<ul> <li>Separaci\u00f3n X/y: Se separan las caracter\u00edsticas (X) del objetivo (y).</li> <li>Divisi\u00f3n Train/Test: Se usa <code>train_test_split</code> (ej. 80% train, 20% test).</li> <li>Validaci\u00f3n Cruzada:<ul> <li>Se muestra c\u00f3mo usar <code>KFold</code> (CV est\u00e1ndar) y <code>StratifiedKFold</code> (CV estratificada).</li> <li><code>StratifiedKFold</code> es preferible porque mantiene la distribuci\u00f3n 33/33/33 de las clases en cada fold, asegurando que la validaci\u00f3n sea representativa.</li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#3-seleccion-y-evaluacion-del-modelo","title":"3. Selecci\u00f3n y Evaluaci\u00f3n del Modelo","text":"<ul> <li>Curva de Aprendizaje (<code>Learning Curve</code>):     Se usa para diagnosticar bias vs. variance. Muestra el rendimiento del modelo a medida que ve m\u00e1s datos de entrenamiento.</li> <li>Afinado de Hiperpar\u00e1metros (<code>GridSearchCV</code>):     Se utiliza para encontrar la mejor combinaci\u00f3n de hiperpar\u00e1metros (ej. <code>criterion</code>, <code>max_depth</code> para un <code>DecisionTreeClassifier</code>) probando todas las combinaciones posibles mediante validaci\u00f3n cruzada.</li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#4-metricas-de-evaluacion-clasificacion","title":"4. M\u00e9tricas de Evaluaci\u00f3n (Clasificaci\u00f3n)","text":"<p>Una vez que el modelo (<code>GridSearchCV</code>) est\u00e1 entrenado y se hacen predicciones sobre el <code>X_test</code>, se eval\u00faa el rendimiento.</p> <ul> <li> <p>Matriz de Confusi\u00f3n (<code>Confusion Matrix</code>):     Es la base para todas las m\u00e9tricas. Compara los valores reales (True label) con los predichos (Predicted label).</p> <ul> <li>TP (True Positive): Real = 1, Predicho = 1.</li> <li>FN (False Negative): Real = 1, Predicho = 0.</li> <li>FP (False Positive): Real = 0, Predicho = 1.</li> <li>TN (True Negative): Real = 0, Predicho = 0.</li> </ul> </li> <li> <p>M\u00e9tricas Clave:</p> <ul> <li>Accuracy (Exactitud): \\(\\frac{TP + TN}{Total}\\). Proporci\u00f3n de predicciones correctas. (Usar con cuidado en datasets desbalanceados).</li> <li>Precision (Precisi\u00f3n): \\(\\frac{TP}{TP + FP}\\). De los que dijimos que eran positivos, \u00bfcu\u00e1ntos acertamos?.</li> <li>Recall (Sensibilidad o TPR): \\(\\frac{TP}{TP + FN}\\). De todos los positivos reales, \u00bfcu\u00e1ntos encontramos?.</li> <li>F1-Score: La media arm\u00f3nica de Precision y Recall. Es una m\u00e9trica excelente para datasets desbalanceados. \\(F_1 = 2 \\frac{Precision \\times Recall}{Precision + Recall}\\).</li> <li>FPR (Tasa de Falsos Positivos): \\(\\frac{FP}{FP + TN}\\). Proporci\u00f3n de negativos reales que clasificamos incorrectamente como positivos.</li> </ul> </li> <li> <p>Curva ROC y AUC:</p> <ul> <li>Curva ROC: Gr\u00e1fica que muestra el rendimiento de un clasificador en todos los umbrales de clasificaci\u00f3n. Muestra TPR (Eje Y) vs. FPR (Eje X).</li> <li>AUC (Area Under the Curve): El \u00e1rea bajo la curva ROC. Es una m\u00e9trica \u00fanica que resume el rendimiento del modelo.<ul> <li>AUC = 1.0: Clasificador perfecto.</li> <li>AUC = 0.5: Clasificador in\u00fatil (aleatorio).</li> <li>Un AUC de 0.85 o m\u00e1s se considera bueno.</li> </ul> </li> </ul> </li> </ul>"},{"location":"aprendizaje-supervisado/01-machine-learning/#5-prediccion-final","title":"5. Predicci\u00f3n Final","text":"<ul> <li>Se carga el modelo final (el mejor <code>estimator_</code> encontrado por <code>GridSearchCV</code>).</li> <li>Se realizan las predicciones finales sobre el conjunto de prueba (<code>X_test</code>).</li> <li>Los resultados se guardan, por ejemplo, en un archivo CSV.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 27/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/","title":"\ud83e\udd16 Unidad 2. Regresi\u00f3n Lineal para la Inteligencia Artificial","text":"<p>La regresi\u00f3n lineal es uno de los modelos m\u00e1s simples y fundamentales en el campo de la inteligencia artificial y el aprendizaje autom\u00e1tico. A pesar de su simplicidad, proporciona una base s\u00f3lida para entender c\u00f3mo los algoritmos de regresi\u00f3n pueden ser usados para hacer predicciones. En este art\u00edculo exploraremos c\u00f3mo funciona la regresi\u00f3n lineal, sus aplicaciones, y la compararemos con otros modelos de regresi\u00f3n como Ridge y Lasso.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#que-es-la-regresion-lineal","title":"\u00bfQu\u00e9 es la Regresi\u00f3n Lineal?","text":"<p>La regresi\u00f3n lineal es un m\u00e9todo estad\u00edstico que intenta modelar la relaci\u00f3n entre una variable dependiente y una o m\u00e1s variables independientes mediante una l\u00ednea recta. La ecuaci\u00f3n que representa una regresi\u00f3n lineal simple tiene la siguiente forma:</p> \\[ y = b_0 + b_1X + \\epsilon \\] <ul> <li>y: Variable dependiente (la que se intenta predecir).</li> <li>X: Variable independiente (el predictor).</li> <li>b_0: Intercepto, valor de y cuando X es cero.</li> <li>b_1: Coeficiente que representa la pendiente de la l\u00ednea.</li> <li>\\(\\epsilon\\): Error o ruido, la diferencia entre la predicci\u00f3n y el valor real.</li> </ul> <p>La regresi\u00f3n lineal se utiliza principalmente para problemas de predicci\u00f3n num\u00e9rica, como el precio de una vivienda, el rendimiento de una acci\u00f3n o cualquier otra situaci\u00f3n en la que exista una relaci\u00f3n lineal entre las variables.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#aplicaciones-de-la-regresion-lineal","title":"Aplicaciones de la Regresi\u00f3n Lineal","text":"<p>La regresi\u00f3n lineal es ampliamente utilizada en una variedad de aplicaciones, como:</p> <ul> <li> <p>Econom\u00eda: Predicci\u00f3n de precios de bienes y servicios. Por ejemplo, podemos usar la regresi\u00f3n lineal para modelar la relaci\u00f3n entre la inflaci\u00f3n y el precio de los alimentos. En este caso, la regresi\u00f3n lineal simple puede ser suficiente si se trata de una relaci\u00f3n clara y directa. Ejemplo en Python</p> </li> <li> <p>Finanzas: Estimaci\u00f3n del rendimiento de acciones o bonos. La regresi\u00f3n lineal puede ayudar a estimar c\u00f3mo factores como las tasas de inter\u00e9s y el crecimiento econ\u00f3mico afectan los precios de las acciones. Si existen muchas variables correlacionadas, Ridge Regression ser\u00eda una mejor opci\u00f3n para estabilizar el modelo y evitar el sobreajuste. Ejemplo en Python</p> </li> <li> <p>Salud: Modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente. En este \u00e1mbito, se podr\u00eda usar la regresi\u00f3n lineal para entender c\u00f3mo var\u00eda la presi\u00f3n sangu\u00ednea en respuesta a diferentes dosis de un medicamento. Si existen m\u00faltiples factores (como edad, peso, y otras condiciones de salud), Lasso Regression podr\u00eda ayudar a identificar cu\u00e1les son las caracter\u00edsticas m\u00e1s relevantes. Ejemplo en Python</p> </li> <li> <p>Marketing: Determinaci\u00f3n de la relaci\u00f3n entre el gasto publicitario y las ventas. La regresi\u00f3n lineal se utiliza para estimar el impacto de diferentes estrategias publicitarias en las ventas. Si existen muchas campa\u00f1as publicitarias y se necesita identificar cu\u00e1les son las m\u00e1s efectivas, Lasso Regression podr\u00eda ayudar a eliminar las menos significativas y reducir la complejidad del modelo. </p> </li> <li> <p>Educaci\u00f3n: Predicci\u00f3n de calificaciones de estudiantes en funci\u00f3n de variables como el tiempo de estudio y la asistencia. Si el objetivo es identificar los factores que tienen mayor influencia en el rendimiento acad\u00e9mico, Lasso Regression ser\u00eda \u00fatil para seleccionar solo las caracter\u00edsticas m\u00e1s relevantes, como participaci\u00f3n en clase, tiempo de estudio, o participaci\u00f3n en actividades extracurriculares. </p> </li> <li> <p>Inmobiliaria: Predicci\u00f3n del valor de una propiedad con base en caracter\u00edsticas como la ubicaci\u00f3n, el tama\u00f1o y el n\u00famero de habitaciones. En este contexto, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que caracter\u00edsticas como la ubicaci\u00f3n y el tama\u00f1o de una propiedad suelen estar correlacionadas. Ridge ayuda a estabilizar los coeficientes y mejorar la capacidad predictiva del modelo. </p> </li> <li> <p>Agricultura: Estimaci\u00f3n del rendimiento de cultivos en funci\u00f3n de factores como el clima, la cantidad de fertilizante y el tipo de suelo. Ridge Regression es adecuada cuando hay m\u00faltiples factores que pueden estar correlacionados, como la temperatura y la precipitaci\u00f3n. Esto ayuda a manejar mejor la multicolinealidad y a mejorar la generalizaci\u00f3n del modelo. Ejemplo en Python</p> </li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#que-tecnica-es-mas-apropiada","title":"\u00bfQu\u00e9 t\u00e9cnica es m\u00e1s apropiada?","text":"<ul> <li>Econom\u00eda y Finanzas: En estos campos, la regresi\u00f3n lineal puede ser \u00fatil cuando se trata de problemas simples, como la predicci\u00f3n de precios basada en una o dos caracter\u00edsticas. Sin embargo, si hay muchas variables que est\u00e1n altamente correlacionadas, Ridge Regression ser\u00eda m\u00e1s apropiada para evitar el sobreajuste. Ejemplo en Python</li> <li>Salud: Para el modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente, Lasso Regression ser\u00eda adecuada si hay muchas caracter\u00edsticas potenciales, ya que podr\u00eda simplificar el modelo eliminando caracter\u00edsticas irrelevantes. Ejemplo en Python</li> <li>Marketing: Si hay muchas variables de marketing, como diferentes tipos de publicidad, Lasso Regression puede ayudar a identificar cu\u00e1les de ellas son las m\u00e1s importantes, eliminando las menos significativas. </li> <li>Inmobiliaria: En el caso de la predicci\u00f3n de precios de propiedades, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que a menudo las caracter\u00edsticas como ubicaci\u00f3n, tama\u00f1o y tipo de propiedad est\u00e1n correlacionadas. </li> <li>Educaci\u00f3n: Si queremos predecir las calificaciones de los estudiantes y hay muchas caracter\u00edsticas (como el historial acad\u00e9mico, asistencia, participaci\u00f3n en clase, etc.), Lasso ser\u00eda \u00fatil para identificar las variables m\u00e1s relevantes y eliminar las menos importantes. </li> <li>Agricultura: Para la estimaci\u00f3n del rendimiento de cultivos, Ridge Regression ser\u00eda adecuada si existen m\u00faltiples factores correlacionados, ya que permite manejar mejor la multicolinealidad. Ejemplo en Python</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#limitaciones-de-la-regresion-lineal","title":"Limitaciones de la Regresi\u00f3n Lineal","text":"<p>Aunque la regresi\u00f3n lineal es f\u00e1cil de entender y usar, presenta algunas limitaciones importantes que deben tenerse en cuenta al aplicar este modelo:</p> <ul> <li> <p>Supone una relaci\u00f3n lineal: La regresi\u00f3n lineal solo puede modelar relaciones lineales entre las variables. Si la relaci\u00f3n es no lineal, el modelo tendr\u00e1 un rendimiento pobre. Esto implica que, si los datos muestran una relaci\u00f3n m\u00e1s compleja (por ejemplo, cuadr\u00e1tica o exponencial), la regresi\u00f3n lineal no podr\u00e1 capturar dicha complejidad, resultando en predicciones inexactas. En estos casos, ser\u00eda mejor utilizar modelos que puedan capturar la no linealidad, como la regresi\u00f3n polin\u00f3mica o t\u00e9cnicas m\u00e1s avanzadas como redes neuronales.</p> </li> <li> <p>Sensibilidad a los outliers: La presencia de valores at\u00edpicos puede afectar significativamente el ajuste de la l\u00ednea, ya que la regresi\u00f3n lineal minimiza la suma de los errores al cuadrado. Los outliers, al tener errores m\u00e1s grandes, influyen desproporcionadamente en la l\u00ednea de ajuste, lo cual puede distorsionar el modelo. Para mitigar este problema, se pueden utilizar t\u00e9cnicas como la detecci\u00f3n y eliminaci\u00f3n de outliers, o emplear m\u00e9todos de regresi\u00f3n robusta que minimicen el impacto de estos valores extremos.</p> </li> <li> <p>Multicolinealidad: Cuando las variables independientes est\u00e1n altamente correlacionadas, el modelo puede producir resultados inestables. La multicolinealidad genera problemas en la estimaci\u00f3n de los coeficientes, haciendo que sean muy sensibles a peque\u00f1as variaciones en los datos y, por lo tanto, menos interpretables. Esto puede llevar a una disminuci\u00f3n en la precisi\u00f3n de las predicciones y a problemas en la generalizaci\u00f3n del modelo. En estos casos, se recomienda usar t\u00e9cnicas de regularizaci\u00f3n, como Ridge Regression, que penaliza los coeficientes grandes y ayuda a reducir los efectos de la multicolinealidad, estabilizando el modelo.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ridge-regression-y-lasso-regression","title":"Ridge Regression y Lasso Regression","text":"<p>Para superar algunas de las limitaciones de la regresi\u00f3n lineal est\u00e1ndar, se han desarrollado t\u00e9cnicas de regularizaci\u00f3n como Ridge y Lasso. Ambas t\u00e9cnicas son versiones modificadas de la regresi\u00f3n lineal que incluyen un t\u00e9rmino de penalizaci\u00f3n para mejorar el rendimiento y evitar el sobreajuste.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ridge-regression","title":"Ridge Regression","text":"<p>Ridge Regression, tambi\u00e9n conocida como regresi\u00f3n de cresta, a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L2 a la funci\u00f3n de p\u00e9rdida. Esto significa que el modelo penaliza los coeficientes grandes, haciendo que los valores de los par\u00e1metros sean m\u00e1s peque\u00f1os y estables. La ecuaci\u00f3n para Ridge es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum     heta_j^2 \\] <ul> <li>\\lambda: Par\u00e1metro de regularizaci\u00f3n que controla la cantidad de penalizaci\u00f3n.</li> <li>**    \\(heta_j\\)**: Coeficientes del modelo.</li> </ul> <p>El t\u00e9rmino de penalizaci\u00f3n ayuda a reducir la complejidad del modelo, lo cual resulta \u00fatil especialmente cuando existen m\u00faltiples variables independientes correlacionadas (multicolinealidad).</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ventajas-de-ridge-regression","title":"Ventajas de Ridge Regression","text":"<ul> <li>Reducci\u00f3n del sobreajuste: Ridge ayuda a reducir el riesgo de sobreajuste al penalizar los coeficientes grandes.</li> <li>Mejora la estabilidad: Especialmente en presencia de multicolinealidad, el modelo Ridge tiende a ser m\u00e1s estable.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#lasso-regression","title":"Lasso Regression","text":"<p>Lasso Regression a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L1 a la funci\u00f3n de p\u00e9rdida. Este t\u00e9rmino tiene la capacidad de hacer que algunos coeficientes sean exactamente cero, eliminando efectivamente ciertas caracter\u00edsticas del modelo. La ecuaci\u00f3n de Lasso es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum |   heta_j| \\] <ul> <li>\\(\\lambda\\): Par\u00e1metro de regularizaci\u00f3n que controla la penalizaci\u00f3n.</li> </ul> <p>Lasso es \u00fatil no solo para reducir el sobreajuste, sino tambi\u00e9n para la selecci\u00f3n de caracter\u00edsticas, ya que elimina autom\u00e1ticamente aquellas que no son \u00fatiles para la predicci\u00f3n.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ventajas-de-lasso-regression","title":"Ventajas de Lasso Regression","text":"<ul> <li>Selecci\u00f3n de caracter\u00edsticas: Lasso simplifica el modelo seleccionando solo las caracter\u00edsticas m\u00e1s relevantes.</li> <li>Reducci\u00f3n del sobreajuste: Similar a Ridge, Lasso ayuda a evitar el sobreajuste del modelo.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#comparacion-entre-regresion-lineal-ridge-y-lasso","title":"Comparaci\u00f3n entre Regresi\u00f3n Lineal, Ridge y Lasso","text":"Caracter\u00edstica Regresi\u00f3n Lineal Ridge Regression Lasso Regression Regularizaci\u00f3n No L2 L1 Penalizaci\u00f3n Ninguna Penaliza coeficientes grandes Algunos coeficientes se hacen cero Sobreajuste Alta posibilidad Baja Baja Multicolinealidad Problemas con multicolinealidad Mejor manejo Mejor manejo Selecci\u00f3n de caracter\u00edsticas No No S\u00ed <ul> <li>Regresi\u00f3n Lineal: Ideal para problemas simples y cuando existe una relaci\u00f3n lineal clara entre las variables. Sin embargo, es propensa al sobreajuste si no se maneja adecuadamente.</li> <li>Ridge Regression: \u00datil cuando existe multicolinealidad, ya que la regularizaci\u00f3n L2 ayuda a estabilizar el modelo. No elimina caracter\u00edsticas, pero hace que los coeficientes sean m\u00e1s peque\u00f1os.</li> <li>Lasso Regression: \u00datil para la selecci\u00f3n de caracter\u00edsticas, ya que fuerza algunos coeficientes a ser exactamente cero. Esto resulta en un modelo m\u00e1s sencillo y f\u00e1cil de interpretar.</li> </ul>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#conclusiones","title":"Conclusiones","text":"<p>La regresi\u00f3n lineal es una excelente herramienta para comenzar a entender los modelos de regresi\u00f3n. Sin embargo, cuando nos enfrentamos a datos m\u00e1s complejos, con m\u00faltiples caracter\u00edsticas y posibles problemas de sobreajuste, Ridge y Lasso se presentan como mejores alternativas. Estos modelos ayudan a mejorar la capacidad de generalizaci\u00f3n del modelo y a reducir la complejidad, haciendo que la predicci\u00f3n sea m\u00e1s precisa y confiable.</p> <p>La elecci\u00f3n entre la regresi\u00f3n lineal, Ridge y Lasso depender\u00e1 de la naturaleza de los datos y los objetivos del an\u00e1lisis. Si se desea simplicidad y no hay riesgo de multicolinealidad, la regresi\u00f3n lineal puede ser suficiente. Si el modelo tiende a sobreajustarse o hay muchas caracter\u00edsticas correlacionadas, Ridge y Lasso son opciones a considerar, siendo Lasso ideal si se desea simplificar el modelo eliminando caracter\u00edsticas irrelevantes.</p>"},{"location":"aprendizaje-supervisado/02-prediccion-numerica/#ejemplos-adicionales-de-uso","title":"Ejemplos Adicionales de Uso","text":"<ul> <li>Predicci\u00f3n de Ventas Minoristas: En un negocio minorista donde existen m\u00faltiples caracter\u00edsticas que afectan las ventas (promociones, temporadas, clima, ubicaci\u00f3n), Ridge Regression ser\u00eda \u00fatil para manejar la posible multicolinealidad entre estas caracter\u00edsticas. Ejemplo en Python</li> <li>Modelado de la Demanda Energ\u00e9tica: En la predicci\u00f3n del consumo de energ\u00eda el\u00e9ctrica, que depende de variables como temperatura, hora del d\u00eda, y tipo de d\u00eda (laboral o festivo), Ridge podr\u00eda ayudar a manejar la complejidad y multicolinealidad. </li> <li>An\u00e1lisis de Sentimientos: Al predecir la polaridad de una opini\u00f3n (positiva o negativa) en base a muchas palabras o frases, Lasso Regression ser\u00eda ideal para seleccionar las palabras m\u00e1s relevantes y reducir la dimensionalidad. </li> <li>Predicci\u00f3n de Costos de Seguros M\u00e9dicos: Para estimar los costos de seguros m\u00e9dicos en funci\u00f3n de caracter\u00edsticas como edad, estado de salud, h\u00e1bitos de vida y ubicaci\u00f3n geogr\u00e1fica, Lasso podr\u00eda ayudar a eliminar caracter\u00edsticas redundantes, haciendo el modelo m\u00e1s interpretable. </li> <li>Optimizaci\u00f3n de Cadenas de Suministro: Para predecir el tiempo de entrega de productos considerando m\u00faltiples variables (tr\u00e1fico, distancia, clima, inventario), Ridge Regression puede ser \u00fatil para manejar la correlaci\u00f3n entre factores como tr\u00e1fico y distancia. </li> <li>Reconocimiento de Actividad Humana: En la clasificaci\u00f3n de actividades humanas usando sensores port\u00e1tiles (como aceler\u00f3metros y giroscopios), Lasso podr\u00eda ayudar a identificar cu\u00e1les de las se\u00f1ales del sensor son m\u00e1s importantes para diferenciar entre actividades como caminar, correr o estar de pie. Ejemplo en Python</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/","title":"\ud83e\udd16 Unidad 3. Modelos de Aprendizaje Supervisado para Predicci\u00f3n Categ\u00f3rica","text":"<p>La clasificaci\u00f3n es una subcategor\u00eda del aprendizaje supervisado donde el objetivo es predecir una etiqueta de clase categ\u00f3rica (discreta) para una instancia de datos dada. A diferencia de la regresi\u00f3n, que predice valores continuos, la clasificaci\u00f3n asigna entradas a una de varias categor\u00edas predefinidas.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#31-entrenamiento-y-testing-en-clasificacion","title":"3.1. Entrenamiento y Testing en Clasificaci\u00f3n","text":"<p>El proceso de construcci\u00f3n de un modelo de clasificaci\u00f3n sigue el flujo est\u00e1ndar de Machine Learning:</p> <ol> <li>Divisi\u00f3n de Datos: Se divide el dataset en un conjunto de entrenamiento (para ajustar el modelo) y un conjunto de prueba (para evaluar su rendimiento en datos no vistos).</li> <li>Entrenamiento: El algoritmo aprende la frontera de decisi\u00f3n que separa las diferentes clases bas\u00e1ndose en las caracter\u00edsticas (features) de los datos de entrenamiento.</li> <li>Testing (Predicci\u00f3n): El modelo asigna etiquetas a los datos de prueba.</li> <li>Evaluaci\u00f3n: Se comparan las etiquetas predichas con las etiquetas reales para calcular m\u00e9tricas de rendimiento.</li> </ol>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#32-ejemplos-frecuentes-de-uso","title":"3.2. Ejemplos Frecuentes de Uso","text":"<p>La clasificaci\u00f3n est\u00e1 omnipresente en aplicaciones modernas:</p> <ul> <li>Detecci\u00f3n de Spam: Clasificar correos como \"Spam\" o \"No Spam\".</li> <li>Diagn\u00f3stico M\u00e9dico: Determinar si un paciente tiene una enfermedad (\"Positivo\") o no (\"Negativo\") bas\u00e1ndose en s\u00edntomas y an\u00e1lisis.</li> <li>Reconocimiento de Im\u00e1genes: Identificar si una imagen contiene un \"Gato\", \"Perro\" o \"Coche\".</li> <li>Aprobaci\u00f3n de Cr\u00e9ditos: Clasificar a un solicitante como de \"Alto Riesgo\" o \"Bajo Riesgo\".</li> <li>An\u00e1lisis de Sentimientos: Clasificar opiniones como \"Positivas\", \"Negativas\" o \"Neutrales\".</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#33-algoritmos-de-clasificacion-en-machine-learning","title":"3.3. Algoritmos de Clasificaci\u00f3n en Machine Learning","text":"<p>Existen diversos algoritmos para abordar problemas de clasificaci\u00f3n:</p> <ul> <li>Regresi\u00f3n Log\u00edstica: Simple, interpretable y base para redes neuronales.</li> <li>K-Nearest Neighbors (KNN): Basado en similitud y distancia.</li> <li>Support Vector Machines (SVM): Busca el hiperplano de separaci\u00f3n \u00f3ptimo.</li> <li>\u00c1rboles de Decisi\u00f3n y Random Forest: Basados en reglas de decisi\u00f3n jer\u00e1rquicas.</li> <li>Naive Bayes: Basado en probabilidad y el teorema de Bayes.</li> <li>Redes Neuronales: Para patrones complejos y datos no estructurados.</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#34-regresion-logistica","title":"3.4. Regresi\u00f3n Log\u00edstica","text":"<p>A pesar de su nombre, la Regresi\u00f3n Log\u00edstica es un algoritmo de clasificaci\u00f3n, no de regresi\u00f3n. Se utiliza para estimar la probabilidad de que una instancia pertenezca a una clase particular (por ejemplo, probabilidad de que un correo sea spam).</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#conceptos-basicos-y-matematicos","title":"Conceptos B\u00e1sicos y Matem\u00e1ticos","text":"<p>La regresi\u00f3n log\u00edstica utiliza la funci\u00f3n sigmoide (o log\u00edstica) para transformar la salida de una ecuaci\u00f3n lineal en un valor de probabilidad entre 0 y 1.</p> <ol> <li>Funci\u00f3n Lineal: \\(z = w \\cdot x + b\\) (donde \\(w\\) son los pesos y \\(x\\) las caracter\u00edsticas).</li> <li>Funci\u00f3n Sigmoide: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</li> </ol> <p>Si la probabilidad estimada \\(\\hat{p} = \\sigma(z)\\) es mayor o igual a 0.5, el modelo predice la clase 1; de lo contrario, predice la clase 0.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#algoritmo-del-gradiente-descendente","title":"Algoritmo del Gradiente Descendente","text":"<p>Para entrenar el modelo, necesitamos encontrar los pesos \\(w\\) y el sesgo \\(b\\) que minimicen el error. La funci\u00f3n de costo utilizada es la Log Loss (P\u00e9rdida Logar\u00edtmica), ya que el error cuadr\u00e1tico medio no es convexo para esta funci\u00f3n.</p> <p>El Gradiente Descendente es un algoritmo de optimizaci\u00f3n iterativo: 1.  Inicializa los pesos aleatoriamente. 2.  Calcula el gradiente de la funci\u00f3n de costo (la direcci\u00f3n en la que el error aumenta m\u00e1s r\u00e1pido). 3.  Actualiza los pesos movi\u00e9ndose en la direcci\u00f3n opuesta al gradiente para reducir el error.     \\(\\(w_{nuevo} = w_{viejo} - \\eta \\cdot \\nabla Costo\\)\\)     (Donde \\(\\eta\\) es la tasa de aprendizaje).</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#ejemplo-en-python","title":"Ejemplo en Python","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\n\n# Cargar datos\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Dividir y Escalar (Importante para Gradiente Descendente)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Entrenar modelo\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n\n# Predecir\ny_pred = log_reg.predict(X_test)\n</code></pre>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#35-metricas-de-rendimiento","title":"3.5. M\u00e9tricas de Rendimiento","text":"<p>Evaluar un clasificador va m\u00e1s all\u00e1 de simplemente contar cu\u00e1ntos aciertos tuvo.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#matriz-de-confusion","title":"Matriz de Confusi\u00f3n","text":"<p>Es una tabla que resume el rendimiento del modelo comparando las clases reales con las predichas.</p> Predicho Negativo (0) Predicho Positivo (1) Real Negativo (0) TN (True Negative) FP (False Positive) Real Positivo (1) FN (False Negative) TP (True Positive) <ul> <li>TP: Enfermos detectados correctamente.</li> <li>TN: Sanos detectados correctamente.</li> <li>FP (Error Tipo I): Sanos detectados err\u00f3neamente como enfermos (\"Falsa Alarma\").</li> <li>FN (Error Tipo II): Enfermos no detectados (\"Peligroso\").</li> </ul>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#metricas-derivadas","title":"M\u00e9tricas Derivadas","text":"<ol> <li> <p>Accuracy (Exactitud): Proporci\u00f3n total de predicciones correctas.     \\(\\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\\)</p> </li> <li> <p>Error Rate (Tasa de Error): Proporci\u00f3n de predicciones incorrectas.     \\(\\(Error Rate = 1 - Accuracy = \\frac{FP + FN}{Total}\\)\\)</p> </li> <li> <p>Sensitivity / Recall / TPR (Tasa de Verdaderos Positivos): Capacidad para detectar la clase positiva.     \\(\\(Sensitivity = \\frac{TP}{TP + FN}\\)\\)</p> </li> <li> <p>Specificity / TNR (Tasa de Verdaderos Negativos): Capacidad para detectar la clase negativa.     \\(\\(Specificity = \\frac{TN}{TN + FP}\\)\\)</p> </li> <li> <p>False Positive Rate (FPR): \\(\\(FPR = 1 - Specificity = \\frac{FP}{TN + FP}\\)\\)</p> </li> <li> <p>Precision (Precisi\u00f3n): De los que predije positivos, \u00bfcu\u00e1ntos lo son realmente?     \\(\\(Precision = \\frac{TP}{TP + FP}\\)\\)</p> </li> <li> <p>F1-Score (F-Measure): Media arm\u00f3nica de Precision y Recall. \u00datil cuando las clases est\u00e1n desbalanceadas.     \\(\\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\\)</p> </li> <li> <p>Kappa Statistic (Cohen's Kappa): Mide la concordancia entre la predicci\u00f3n y la realidad, ajustada por el azar. Un valor de 1 es concordancia perfecta, 0 es igual al azar.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#ejemplo-en-python_1","title":"Ejemplo en Python","text":"<pre><code>from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n\nprint(\"Matriz de Confusi\u00f3n:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nReporte de Clasificaci\u00f3n:\\n\", classification_report(y_test, y_pred))\nprint(f\"Kappa Score: {cohen_kappa_score(y_test, y_pred):.4f}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#36-curva-roc-y-auc","title":"3.6. Curva ROC y AUC","text":"<p>La Curva ROC (Receiver Operating Characteristic) es un gr\u00e1fico que ilustra el rendimiento de un clasificador binario a medida que var\u00eda el umbral de discriminaci\u00f3n. *   Eje X: False Positive Rate (1 - Specificity). *   Eje Y: True Positive Rate (Sensitivity).</p> <p>Un modelo ideal se acerca a la esquina superior izquierda (TPR=1, FPR=0). La l\u00ednea diagonal representa un clasificador aleatorio.</p> <p>AUC (Area Under Curve): Es el \u00e1rea bajo la curva ROC. Resume el rendimiento en un solo n\u00famero. *   AUC = 0.5: Aleatorio. *   AUC = 1.0: Perfecto.</p>"},{"location":"aprendizaje-supervisado/03-prediccion-categorica/#37-sensibilidad-especificidad-y-el-teorema-de-bayes","title":"3.7. Sensibilidad, Especificidad y el Teorema de Bayes","text":"<p>Estos conceptos est\u00e1n \u00edntimamente ligados al Teorema de Bayes cuando queremos calcular la probabilidad real de tener una condici\u00f3n dado un resultado positivo en un test (Probabilidad a Posteriori).</p> <p>Supongamos un test m\u00e9dico para una enfermedad rara: *   \\(P(E)\\): Probabilidad a priori de tener la enfermedad (Prevalencia). *   \\(P(+|E)\\): Sensibilidad del test. *   \\(P(-|No E)\\): Especificidad del test.</p> <p>Si un paciente da positivo, \u00bfcu\u00e1l es la probabilidad de que realmente tenga la enfermedad \\(P(E|+)\\)?</p> \\[P(E|+) = \\frac{P(+|E) \\cdot P(E)}{P(+|E) \\cdot P(E) + P(+|No E) \\cdot P(No E)}\\] <p>Donde \\(P(+|No E)\\) es el False Positive Rate (\\(1 - Especificidad\\)). Este c\u00e1lculo demuestra que si la prevalencia de la enfermedad es muy baja, incluso un test con alta sensibilidad y especificidad puede generar muchos falsos positivos, haciendo que la probabilidad real de estar enfermo sea baja a pesar del resultado positivo.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/","title":"\ud83e\udd16 Unidad 4. \u00c1rbol de Decisi\u00f3n en Inteligencia Artificial: Explicaci\u00f3n Detallada","text":"<p>El algoritmo de \u00c1rbol de Decisi\u00f3n (Decision Tree) es un modelo de aprendizaje supervisado que se utiliza tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n. Su objetivo es dividir el espacio de datos en subconjuntos homog\u00e9neos bas\u00e1ndose en una serie de reglas, de modo que cada subconjunto sea lo m\u00e1s puro posible con respecto a la variable objetivo. Los \u00e1rboles de decisi\u00f3n son f\u00e1ciles de interpretar, muy \u00fatiles para entender las relaciones en los datos, y se aplican ampliamente en una variedad de campos.</p> <p>A continuaci\u00f3n, exploraremos la teor\u00eda detr\u00e1s de los \u00e1rboles de decisi\u00f3n, c\u00f3mo se construyen, y daremos ejemplos que ilustran c\u00f3mo funciona este algoritmo en la pr\u00e1ctica. Tambi\u00e9n incluiremos casos reales en los que este algoritmo ha demostrado ser \u00fatil, as\u00ed como c\u00f3mo encontrar los mejores valores para los metapar\u00e1metros del modelo.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#1-estructura-de-un-arbol-de-decision","title":"1. Estructura de un \u00c1rbol de Decisi\u00f3n","text":"<p>Un \u00e1rbol de decisi\u00f3n est\u00e1 compuesto por varios elementos fundamentales: - Nodos de Decisi\u00f3n: Representan la divisi\u00f3n de los datos seg\u00fan una caracter\u00edstica espec\u00edfica. Aqu\u00ed se toma una decisi\u00f3n sobre qu\u00e9 atributo se usa para dividir el conjunto de datos. - Ramas: Las conexiones entre nodos representan el resultado de una decisi\u00f3n. Cada rama lleva a un nuevo nodo o a un nodo hoja. - Nodos Hoja: Son los puntos finales del \u00e1rbol. Representan la categor\u00eda final o el valor predicho para una determinada observaci\u00f3n.</p> <p>Cada divisi\u00f3n en un \u00e1rbol de decisi\u00f3n intenta dividir los datos de manera que maximice la pureza de los subconjuntos resultantes, es decir, que agrupe datos similares juntos. Este proceso contin\u00faa hasta que se cumplen ciertas condiciones, como alcanzar un n\u00famero m\u00ednimo de muestras en un nodo o una profundidad m\u00e1xima del \u00e1rbol.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#2-criterios-de-division-y-formulas-matematicas","title":"2. Criterios de Divisi\u00f3n y F\u00f3rmulas Matem\u00e1ticas","text":"<p>Los \u00e1rboles de decisi\u00f3n se construyen utilizando una serie de divisiones, cada una de las cuales se elige bas\u00e1ndose en un criterio que mide la calidad de la divisi\u00f3n. Existen varias m\u00e9tricas para seleccionar la caracter\u00edstica que mejor divide los datos:</p> <ul> <li>Entrop\u00eda e \u00cdndice de Ganancia de Informaci\u00f3n</li> <li>\u00cdndice Gini</li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#21-entropia-e-indice-de-ganancia-de-informacion","title":"2.1. Entrop\u00eda e \u00cdndice de Ganancia de Informaci\u00f3n","text":"<p>La entrop\u00eda mide la pureza de un nodo. Se define de la siguiente manera para un nodo que tiene dos clases (positiva y negativa):</p> \\[ H(S) = -p_+ \\cdot \\log_2(p_+) - p_- \\cdot \\log_2(p_-) \\] <p>Donde: - \\( p_+ \\) y \\( p_- \\) son las proporciones de ejemplos positivos y negativos en el nodo.</p> <p>El objetivo es minimizar la entrop\u00eda en cada nodo, lo que equivale a hacer los nodos lo m\u00e1s homog\u00e9neos posible.</p> <p>La ganancia de informaci\u00f3n mide la reducci\u00f3n de la entrop\u00eda despu\u00e9s de dividir un nodo. La f\u00f3rmula para la ganancia de informaci\u00f3n (\\( IG \\)) es:</p> \\[ IG(S, A) = H(S) - \\sum_{v \\in Valores(A)} \\frac{|S_v|}{|S|} H(S_v) \\] <p>Donde: - S es el conjunto de datos original. - A es el atributo por el cual se est\u00e1 dividiendo. - S_v son los subconjuntos de S resultantes de la divisi\u00f3n por el valor v de la caracter\u00edstica A.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#22-indice-gini","title":"2.2. \u00cdndice Gini","text":"<p>El \u00edndice Gini es otra medida utilizada para evaluar la calidad de una divisi\u00f3n. Representa la probabilidad de que una observaci\u00f3n seleccionada aleatoriamente sea clasificada incorrectamente si se realiza una predicci\u00f3n aleatoria basada en la distribuci\u00f3n de clases del nodo. La f\u00f3rmula para el \u00edndice Gini es:</p> \\[ Gini(S) = 1 - \\sum_{i=1}^C p_i^2 \\] <p>Donde \\( p_i \\) es la proporci\u00f3n de elementos pertenecientes a la clase \\( i \\) en el conjunto \\( S \\), y \\( C \\) es el n\u00famero de clases.</p> <p>La idea detr\u00e1s del \u00edndice Gini es minimizar el valor de \\( Gini(S) \\) en cada divisi\u00f3n, buscando nodos lo m\u00e1s puros posible.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#23-calculo-de-ejemplos-con-entropia-e-indice-gini","title":"2.3. C\u00e1lculo de Ejemplos con Entrop\u00eda e \u00cdndice Gini","text":"<p>Veamos un ejemplo detallado de c\u00f3mo calcular la entrop\u00eda y el \u00edndice Gini para una divisi\u00f3n espec\u00edfica de datos.</p> <p>Supongamos que tenemos un conjunto de datos con la siguiente distribuci\u00f3n para la variable objetivo (S\u00ed/No):</p> Caracter\u00edstica Clase: S\u00ed Clase: No A 4 2 B 1 3"},{"location":"aprendizaje-supervisado/04-arboles-decision/#231-calculo-de-la-entropia","title":"2.3.1. C\u00e1lculo de la Entrop\u00eda","text":"<p>Primero calculamos la entrop\u00eda para cada uno de los nodos resultantes de dividir el conjunto de datos seg\u00fan la caracter\u00edstica \"A\":</p> <p>Para A: - Total de ejemplos: \\( 4 + 2 = 6 \\) - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{4}{6})\\) - Proporci\u00f3n de clase No \\(( p_- ): (\\frac{2}{6})\\)</p> <p>La entrop\u00eda para la caracter\u00edstica A es: $$ H(A) = -\\left( \\frac{4}{6} \\right) \\log_2\\left( \\frac{4}{6} \\right) - \\left( \\frac{2}{6} \\right) \\log_2\\left( \\frac{2}{6} \\right) = 0.918 $$</p> <p>Para B: - Total de ejemplos: \\( 1 + 3 = 4 \\) - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{1}{4})\\) - Proporci\u00f3n de clase No \\(( p_+ ): (\\frac{3}{4})\\)</p> <p>La entrop\u00eda para la caracter\u00edstica B es: $$ H(B) = -\\left( \\frac{1}{4} \\right) \\log_2\\left( \\frac{1}{4} \\right) - \\left( \\frac{3}{4} \\right) \\log_2\\left( \\frac{3}{4} \\right) = 0.811 $$</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#232-calculo-del-indice-gini","title":"2.3.2. C\u00e1lculo del \u00cdndice Gini","text":"<p>Ahora calculamos el \u00edndice Gini para la misma divisi\u00f3n:</p> <p>Para A: - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{4}{6})\\) - Proporci\u00f3n de clase No \\(( p_- ): (\\frac{2}{6})\\)</p> <p>El \u00edndice Gini para la caracter\u00edstica A es: $$ Gini(A) = 1 - \\left( \\frac{4}{6} \\right)^2 - \\left( \\frac{2}{6} \\right)^2 = 0.444 $$</p> <p>Para B: - Proporci\u00f3n de clase S\u00ed \\(( p_+ ): (\\frac{1}{4})\\) - Proporci\u00f3n de clase No \\(( p_+ ): (\\frac{3}{4})\\)</p> <p>El \u00edndice Gini para la caracter\u00edstica B es: $$ Gini(B) = 1 - \\left( \\frac{1}{4} \\right)^2 - \\left( \\frac{3}{4} \\right)^2 = 0.375 $$</p> <p>Con estos valores, podemos comparar las caracter\u00edsticas y elegir cu\u00e1l proporciona una mejor divisi\u00f3n de los datos seg\u00fan el criterio seleccionado (en este caso, el que minimice la entrop\u00eda o el \u00edndice Gini).</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#3-construccion-de-un-arbol-de-decision","title":"3. Construcci\u00f3n de un \u00c1rbol de Decisi\u00f3n","text":"<p>La construcci\u00f3n de un \u00e1rbol de decisi\u00f3n se realiza de manera recursiva, siguiendo estos pasos:</p> <ol> <li>Seleccionar el Mejor Atributo: Se elige el atributo que maximiza la ganancia de informaci\u00f3n o minimiza el \u00edndice Gini.</li> <li>Dividir el Conjunto de Datos: Se divide el conjunto de datos en funci\u00f3n del atributo seleccionado.</li> <li>Repetir el Proceso: Se repiten los pasos anteriores para cada subconjunto resultante hasta alcanzar un criterio de parada.</li> </ol> <p>Criterios de Parada pueden ser, por ejemplo, que todos los datos del nodo sean de la misma clase, que el nodo contenga muy pocas instancias (por debajo de un umbral m\u00ednimo), o que se haya alcanzado una profundidad m\u00e1xima predefinida.</p>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#4-ejemplo-de-arbol-de-decision","title":"4. Ejemplo de \u00c1rbol de Decisi\u00f3n","text":"<p>Supongamos que queremos predecir si una persona har\u00e1 ejercicio al aire libre en funci\u00f3n de dos variables: tiempo (soleado, nublado, lluvioso) y temperatura (alta, baja).</p> <ol> <li>Ra\u00edz del \u00c1rbol: Elegimos la primera divisi\u00f3n. Si utilizamos la ganancia de informaci\u00f3n, tal vez encontremos que la variable tiempo tiene la mayor ganancia.</li> <li>Nodo Ra\u00edz: Tiempo</li> <li> <p>Ramas: Soleado, Nublado, Lluvioso</p> </li> <li> <p>Divisiones Subsiguientes: Para cada valor del tiempo, examinamos la temperatura.</p> </li> <li> <p>Para tiempo = Soleado, podemos tener otra divisi\u00f3n por temperatura.</p> </li> <li> <p>Nodos Hoja: Al final de las ramas, llegamos a los nodos hoja, que pueden ser \"S\u00ed\" o \"No\" indicando si la persona har\u00e1 ejercicio o no.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#5-ventajas-y-limitaciones-de-los-arboles-de-decision","title":"5. Ventajas y Limitaciones de los \u00c1rboles de Decisi\u00f3n","text":"<ul> <li>Ventajas:</li> <li>F\u00e1cil Interpretaci\u00f3n: Los \u00e1rboles de decisi\u00f3n son f\u00e1ciles de interpretar, ya que se asemejan a c\u00f3mo los humanos toman decisiones.</li> <li>Pocos Supuestos sobre los Datos: No necesitan normalizaci\u00f3n de datos ni que las caracter\u00edsticas sean escaladas.</li> <li> <p>Manejo de Datos Categ\u00f3ricos y Num\u00e9ricos: Los \u00e1rboles de decisi\u00f3n pueden trabajar con ambos tipos de datos.</p> </li> <li> <p>Limitaciones:</p> </li> <li>Sobreajuste: Los \u00e1rboles de decisi\u00f3n tienden a sobreajustarse si no se limitan adecuadamente (por ejemplo, estableciendo una profundidad m\u00e1xima).</li> <li>Inestabilidad: Los \u00e1rboles de decisi\u00f3n son sensibles a peque\u00f1as variaciones en los datos, lo cual puede generar \u00e1rboles diferentes para conjuntos de datos similares.</li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#6-optimizacion-de-los-metaparametros","title":"6. Optimizaci\u00f3n de los Metapar\u00e1metros","text":"<p>La calidad de un \u00e1rbol de decisi\u00f3n depende en gran medida de los metapar\u00e1metros que se elijan. Algunos de los metapar\u00e1metros clave para un \u00e1rbol de decisi\u00f3n son:</p> <ol> <li> <p>Profundidad M\u00e1xima (<code>max_depth</code>): Limitar la profundidad del \u00e1rbol ayuda a evitar el sobreajuste. La profundidad m\u00e1xima determina cu\u00e1ntos niveles puede tener el \u00e1rbol. Una profundidad muy alta puede llevar al sobreajuste, mientras que una profundidad muy baja puede causar un subajuste.</p> </li> <li> <p>N\u00famero M\u00ednimo de Muestras por Hoja (<code>min_samples_leaf</code>): Controla el n\u00famero m\u00ednimo de muestras que debe haber en un nodo hoja. Un valor m\u00e1s alto reduce el sobreajuste, ya que asegura que las hojas tengan un n\u00famero significativo de ejemplos.</p> </li> <li> <p>N\u00famero M\u00ednimo de Muestras para Dividir (<code>min_samples_split</code>): Especifica el n\u00famero m\u00ednimo de muestras requerido para dividir un nodo. Un valor m\u00e1s alto evita divisiones innecesarias, lo cual ayuda a mantener el \u00e1rbol m\u00e1s simple y reducir el riesgo de sobreajuste.</p> </li> <li> <p>Criterio de Divisi\u00f3n (<code>criterion</code>): Define la funci\u00f3n que se usa para medir la calidad de una divisi\u00f3n. Los criterios comunes son <code>gini</code> e <code>entropy</code>. La elecci\u00f3n del criterio puede influir en la estructura del \u00e1rbol y su capacidad de generalizaci\u00f3n.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#busqueda-de-los-mejores-valores-de-los-metaparametros","title":"B\u00fasqueda de los Mejores Valores de los Metapar\u00e1metros","text":"<p>Para encontrar los valores \u00f3ptimos de estos metapar\u00e1metros, se suelen usar t\u00e9cnicas como la b\u00fasqueda en cuadr\u00edcula (Grid Search) o la b\u00fasqueda aleatoria (Random Search), combinadas con la validaci\u00f3n cruzada.</p> <ul> <li> <p>Grid Search: Busca de manera exhaustiva entre una lista predefinida de valores para cada metapar\u00e1metro. Es eficaz pero puede ser computacionalmente costosa si hay muchos par\u00e1metros y valores posibles.</p> </li> <li> <p>Random Search: Busca valores de metapar\u00e1metros de manera aleatoria dentro de un rango definido. Es m\u00e1s eficiente que Grid Search cuando se trabaja con un gran n\u00famero de combinaciones posibles.</p> </li> <li> <p>Validaci\u00f3n Cruzada: Tanto en Grid Search como en Random Search, se utiliza validaci\u00f3n cruzada para evaluar el rendimiento del modelo para cada combinaci\u00f3n de metapar\u00e1metros y seleccionar aquella que maximice la m\u00e9trica de rendimiento, como la precisi\u00f3n o el F1-score.</p> </li> </ul>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#7-aplicaciones-reales-de-los-arboles-de-decision","title":"7. Aplicaciones Reales de los \u00c1rboles de Decisi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n se aplican en una amplia gama de problemas reales debido a su versatilidad y facilidad de interpretaci\u00f3n. Algunos ejemplos son:</p> <ol> <li> <p>Diagn\u00f3stico M\u00e9dico: En la medicina, los \u00e1rboles de decisi\u00f3n se utilizan para ayudar a los m\u00e9dicos a diagnosticar enfermedades bas\u00e1ndose en s\u00edntomas y pruebas de laboratorio. Por ejemplo, un \u00e1rbol de decisi\u00f3n puede ayudar a predecir si un paciente tiene diabetes en funci\u00f3n de caracter\u00edsticas como nivel de glucosa, presi\u00f3n arterial y edad.</p> </li> <li> <p>Cr\u00e9dito y Riesgo Financiero: En el sector financiero, los \u00e1rboles de decisi\u00f3n se usan para evaluar la probabilidad de que un cliente incumpla un pr\u00e9stamo. Las caracter\u00edsticas utilizadas pueden incluir el historial crediticio, los ingresos mensuales y el monto del pr\u00e9stamo solicitado.</p> </li> <li> <p>M\u00e1rketing y Segmentaci\u00f3n de Clientes: En el marketing, los \u00e1rboles de decisi\u00f3n ayudan a segmentar a los clientes y a predecir si un cliente potencial realizar\u00e1 una compra. Los datos analizados pueden incluir el historial de compras, la interacci\u00f3n con campa\u00f1as publicitarias y la demograf\u00eda del cliente.</p> </li> <li> <p>Control de Calidad en Manufactura: En el sector manufacturero, los \u00e1rboles de decisi\u00f3n pueden ayudar a detectar productos defectuosos durante el proceso de producci\u00f3n, bas\u00e1ndose en caracter\u00edsticas como la temperatura, el tiempo de producci\u00f3n, y otras m\u00e9tricas de calidad.</p> </li> <li> <p>Predicci\u00f3n de Deserci\u00f3n Escolar: En educaci\u00f3n, los \u00e1rboles de decisi\u00f3n se usan para predecir la probabilidad de que un estudiante abandone sus estudios, bas\u00e1ndose en factores como la asistencia, las calificaciones y el apoyo familiar.</p> </li> <li> <p>Clasificaci\u00f3n de Especies: En la biolog\u00eda, se utilizan para clasificar especies de plantas o animales seg\u00fan caracter\u00edsticas observadas. Un ejemplo cl\u00e1sico es el conjunto de datos Iris, donde se clasifica una flor en una de tres especies seg\u00fan el largo y ancho de los p\u00e9talos y s\u00e9palos.</p> </li> </ol>"},{"location":"aprendizaje-supervisado/04-arboles-decision/#8-conclusion","title":"8. Conclusi\u00f3n","text":"<p>Los \u00e1rboles de decisi\u00f3n son una herramienta fundamental en el aprendizaje autom\u00e1tico debido a su capacidad para dividir los datos de manera iterativa y sencilla, maximizando la pureza de los nodos en cada divisi\u00f3n. Aunque presentan ciertas limitaciones, como el riesgo de sobreajuste, son particularmente valiosos cuando se necesita una explicaci\u00f3n clara y comprensible del proceso de decisi\u00f3n. Los \u00e1rboles de decisi\u00f3n se utilizan ampliamente en muchos sectores, y sus aplicaciones van desde el diagn\u00f3stico m\u00e9dico hasta la predicci\u00f3n del comportamiento de los clientes. Son una excelente elecci\u00f3n cuando la interpretabilidad y la facilidad de uso son factores importantes a considerar.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/","title":"\ud83e\udd16 Unidad 5. Algoritmo de Bayes y Naive Bayes en Inteligencia Artificial","text":"<p>El algoritmo de Bayes, tambi\u00e9n conocido como Teorema de Bayes, es un enfoque probabil\u00edstico utilizado para la clasificaci\u00f3n y el an\u00e1lisis en inteligencia artificial y aprendizaje autom\u00e1tico. Este algoritmo se basa en la probabilidad condicional, lo cual permite actualizar la probabilidad de un evento en funci\u00f3n de nueva evidencia.</p> <p>El algoritmo Naive Bayes simplifica el Teorema de Bayes haciendo una suposici\u00f3n fundamental: que todas las caracter\u00edsticas (o atributos) son independientes entre s\u00ed. Esta simplificaci\u00f3n permite construir modelos de clasificaci\u00f3n r\u00e1pidos y eficientes, especialmente \u00fatiles en aplicaciones de clasificaci\u00f3n de texto, como la clasificaci\u00f3n de correos electr\u00f3nicos o el an\u00e1lisis de sentimientos.</p> <p>A continuaci\u00f3n, veremos en detalle c\u00f3mo se deriva el modelo Naive Bayes a partir del Teorema de Bayes y c\u00f3mo funciona, junto con ejemplos y las f\u00f3rmulas matem\u00e1ticas correspondientes.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#1-teorema-de-bayes","title":"1. Teorema de Bayes","text":"<p>El Teorema de Bayes describe la probabilidad de que ocurra un evento \\( A \\) dado que ya ha ocurrido otro evento \\( B \\). La f\u00f3rmula se expresa de la siguiente manera:</p> \\[ P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)} \\] <p>Donde: - \\( P(A|B) \\): Probabilidad de que ocurra el evento \\( A \\) dado que \\( B \\) ha ocurrido (probabilidad posterior). - \\( P(B|A) \\): Probabilidad de que ocurra el evento \\( B \\) dado que \\( A \\) ha ocurrido (verosimilitud). - \\( P(A) \\): Probabilidad a priori del evento \\( A \\). - \\( P(B) \\): Probabilidad del evento \\( B \\).</p> <p>El Teorema de Bayes permite actualizar la probabilidad a priori de un evento a partir de nueva informaci\u00f3n (evidencia).</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#2-naive-bayes","title":"2. Naive Bayes","text":"<p>El clasificador Naive Bayes se deriva del Teorema de Bayes con la suposici\u00f3n de independencia entre caracter\u00edsticas. En lugar de considerar todas las relaciones posibles entre los atributos, se asume que cada caracter\u00edstica es independiente de las dem\u00e1s, dado el resultado. Esto simplifica el c\u00e1lculo de la probabilidad conjunta.</p> <p>La probabilidad de que un ejemplo  $$   x = (x_1, x_2, \\dots, x_n) $$  pertenezca a una clase \\( C_k \\) se puede calcular como:</p> \\[ P(C_k | x) = \\frac{P(C_k) \\prod_{i=1}^n P(x_i | C_k)}{P(x)} \\] <p>Dado que \\( P(x) \\) es constante para todas las clases, podemos simplificar la f\u00f3rmula a:</p> \\[ P(C_k | x) \\propto P(C_k) \\prod_{i=1}^n P(x_i | C_k) \\] <p>Donde: - P(C_k | x): Probabilidad posterior de que el ejemplo pertenezca a la clase C_k . - P(C_k): Probabilidad a priori de la clase C_k. - P(x_i | C_k): Probabilidad condicional de la caracter\u00edstica x_i dada la clase C_k.</p> <p>El clasificador Naive Bayes elige la clase que maximiza esta probabilidad posterior.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#3-tipos-de-clasificadores-naive-bayes","title":"3. Tipos de Clasificadores Naive Bayes","text":"<p>Existen diferentes tipos de clasificadores Naive Bayes, dependiendo del tipo de datos y de c\u00f3mo se calcula la probabilidad condicional:</p> <ul> <li>Naive Bayes Gaussiano: Se utiliza cuando las caracter\u00edsticas tienen una distribuci\u00f3n continua que se puede aproximar a una distribuci\u00f3n normal (gaussiana).</li> <li>Naive Bayes Multinomial: Es adecuado para datos discretos, como el conteo de palabras en un documento. Es ampliamente utilizado en clasificaci\u00f3n de texto.</li> <li>Naive Bayes Bernoulli: Se utiliza para caracter\u00edsticas binarias. Es \u00fatil cuando cada caracter\u00edstica es booleana (por ejemplo, si una palabra aparece o no en un documento).</li> </ul>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#4-ejemplo-completo-de-naive-bayes","title":"4. Ejemplo Completo de Naive Bayes","text":"<p>Vamos a clasificar correos electr\u00f3nicos como \"spam\" o \"no spam\" usando el algoritmo de Naive Bayes. Para este ejemplo, supongamos que tenemos los siguientes datos de entrenamiento, con algunas palabras clave y la clase correspondiente (\"spam\" o \"no spam\"):</p> Correo ID Contenido Clase 1 Oferta barata, gana dinero Spam 2 Proyecto pendiente de trabajo No Spam 3 Oferta especial gratis Spam 4 Reuni\u00f3n de equipo ma\u00f1ana No Spam 5 Gana premios y dinero ahora Spam 6 Informe mensual adjunto No Spam <p>Vamos a suponer que queremos clasificar un nuevo correo con el contenido: \"Oferta gratis y premios\". Para esto, usaremos el clasificador de Naive Bayes, asumiendo que todas las palabras son independientes (el supuesto \"naive\").</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-1-calcular-las-probabilidades-previas","title":"Paso 1: Calcular las Probabilidades Previas","text":"<p>Primero calculamos la probabilidad previa de cada clase.</p> <ul> <li>Probabilidad de Spam (\u03c0(spam)):</li> </ul> <p>$$   P(Spam) = \\frac{N_{Spam}}{N_{Total}} = \\frac{3}{6} = 0.5   $$</p> <ul> <li>Probabilidad de No Spam (\u03c0(no_spam)):</li> </ul> <p>$$   P(No\\,Spam) = \\frac{N_{No\\,Spam}}{N_{Total}} = \\frac{3}{6} = 0.5   $$</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-2-calcular-la-probabilidad-de-cada-palabra","title":"Paso 2: Calcular la Probabilidad de Cada Palabra","text":"<p>Ahora, necesitamos calcular la probabilidad de cada palabra en el contexto de cada clase (es decir, \"spam\" y \"no spam\"). Las palabras \u00fanicas en nuestro conjunto de entrenamiento son:</p> <ul> <li>\"oferta\", \"barata\", \"gana\", \"dinero\", \"proyecto\", \"pendiente\", \"trabajo\", \"especial\", \"gratis\", \"reuni\u00f3n\", \"equipo\", \"ma\u00f1ana\", \"premios\", \"ahora\", \"informe\", \"mensual\", \"adjunto\".</li> </ul> <p>Vamos a usar suavizado de Laplace (adicionando 1 a cada recuento) para evitar probabilidades de cero.</p> <p>Por ejemplo, calculamos la probabilidad de cada palabra para spam:</p> <ul> <li>P(oferta | Spam):</li> </ul> <p>La palabra \"oferta\" aparece en 2 de los 3 correos spam.</p> <p>$$   P(oferta \\mid Spam) = \\frac{2 + 1}{N_{Spam} + V} = \\frac{2 + 1}{3 + 17} = \\frac{3}{20} = 0.15   $$</p> <ul> <li>P(gratis | Spam) y P(premios | Spam) tambi\u00e9n se calculan de manera similar.</li> </ul> <p>Donde: - \\(N_{Spam}\\) : N\u00famero de correos spam. - V : N\u00famero de palabras \u00fanicas en el vocabulario.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-3-clasificar-el-nuevo-correo","title":"Paso 3: Clasificar el Nuevo Correo","text":"<p>El nuevo correo es: \"Oferta gratis y premios\". Queremos calcular la probabilidad de que sea spam o no spam.</p> <p>Para calcular esto, usamos la f\u00f3rmula de Naive Bayes:</p> \\[ P(Clase \\mid X) \\propto P(X \\mid Clase) \\cdot P(Clase) \\] <p>Primero calculamos la probabilidad de que el correo sea spam:</p> \\[ P(Spam \\mid X) \\propto P(oferta \\mid Spam) \\cdot P(gratis \\mid Spam) \\cdot P(premios \\mid Spam) \\cdot P(Spam) \\] <p>Sustituimos los valores y multiplicamos:</p> \\[ P(Spam \\mid X) \\propto 0.15 \\times 0.15 \\times 0.1 \\times 0.5 \\] <p>Hacemos el mismo c\u00e1lculo para No Spam y comparamos ambas probabilidades.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#paso-4-decision","title":"Paso 4: Decisi\u00f3n","text":"<p>Finalmente, elegimos la clase que tiene la probabilidad mayor. Si $ P(Spam \\mid X) $ es mayor que $ P(No\\,Spam \\mid X) $, clasificamos el correo como spam; de lo contrario, como no spam.</p>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#5-aplicaciones-reales-de-naive-bayes","title":"5. Aplicaciones Reales de Naive Bayes","text":"<p>Debido a su eficiencia y capacidad para manejar grandes vol\u00famenes de datos, Naive Bayes se utiliza en una amplia variedad de aplicaciones del mundo real:</p> <ul> <li>Filtrado de Spam: Es el uso m\u00e1s cl\u00e1sico. Servicios como Gmail o Outlook utilizan variantes de Naive Bayes para clasificar correos entrantes como deseados o no deseados bas\u00e1ndose en la frecuencia de ciertas palabras.<ul> <li>Ejemplo de implementaci\u00f3n de filtro Spam</li> </ul> </li> <li>An\u00e1lisis de Sentimientos: Determinar si una opini\u00f3n en redes sociales (Twitter, rese\u00f1as de productos) es positiva, negativa o neutral. Es muy usado en marketing para monitorear la reputaci\u00f3n de marca.<ul> <li>An\u00e1lisis de sentimientos en Twitter</li> </ul> </li> <li>Clasificaci\u00f3n de Documentos: Organizar noticias en categor\u00edas (Deportes, Pol\u00edtica, Tecnolog\u00eda) o clasificar documentos legales y m\u00e9dicos.</li> <li>Sistemas de Recomendaci\u00f3n: Filtrado colaborativo para predecir si a un usuario le gustar\u00e1 un recurso dado.</li> </ul>"},{"location":"aprendizaje-supervisado/05-naive-bayes/#6-ventajas-y-limitaciones-de-naive-bayes","title":"6. Ventajas y Limitaciones de Naive Bayes","text":"<ul> <li>Ventajas:</li> <li>Simplicidad y rapidez: Naive Bayes es simple de implementar y muy r\u00e1pido para entrenar y hacer predicciones, incluso con grandes vol\u00famenes de datos.</li> <li>Escalabilidad: Funciona bien con datos de alta dimensionalidad, como texto.</li> <li> <p>Robustez frente al ruido: A pesar de la suposici\u00f3n de independencia, suele funcionar sorprendentemente bien en muchos problemas reales.</p> </li> <li> <p>Limitaciones:</p> </li> <li>Suposici\u00f3n de independencia: La suposici\u00f3n de independencia rara vez se cumple en problemas reales. Esto puede llevar a predicciones menos precisas cuando las caracter\u00edsticas est\u00e1n altamente correlacionadas.</li> <li>Problemas con datos cero: Si una caracter\u00edstica no se presenta en los datos de entrenamiento, la probabilidad condicional se convierte en cero, lo que hace que la probabilidad posterior tambi\u00e9n sea cero. Esto se suele solucionar con la suavizaci\u00f3n de Laplace.</li> </ul> <p>El clasificador Naive Bayes sigue siendo una herramienta poderosa para muchas aplicaciones de inteligencia artificial y aprendizaje autom\u00e1tico, especialmente en la clasificaci\u00f3n de texto y otros problemas donde la independencia de las caracter\u00edsticas no afecta significativamente el rendimiento del modelo.</p>"},{"location":"aprendizaje-supervisado/06-knn/","title":"\ud83e\udd16 Unidad 6. Algoritmo K-Nearest Neighbors (KNN)","text":"<p>El algoritmo K-Nearest Neighbors (K-Vecinos M\u00e1s Cercanos) es uno de los m\u00e9todos m\u00e1s simples y efectivos en Machine Learning supervisado. Se utiliza tanto para problemas de clasificaci\u00f3n como de regresi\u00f3n. Es un algoritmo no param\u00e9trico (no hace suposiciones sobre la distribuci\u00f3n de los datos subyacentes) y de aprendizaje perezoso (lazy learning), lo que significa que no \"aprende\" un modelo discriminativo durante la fase de entrenamiento, sino que memoriza los datos de entrenamiento para realizar predicciones en el momento necesario.</p>"},{"location":"aprendizaje-supervisado/06-knn/#61-como-funciona-el-algoritmo","title":"6.1. \u00bfC\u00f3mo funciona el algoritmo?","text":"<p>La intuici\u00f3n detr\u00e1s de KNN es sencilla y se basa en la proximidad: \"Dime con qui\u00e9n andas y te dir\u00e9 qui\u00e9n eres\". Para clasificar un nuevo punto de datos, el algoritmo busca en todo el conjunto de datos de entrenamiento los 'k' puntos m\u00e1s cercanos (vecinos) a ese nuevo punto.</p> <ol> <li>Calcular distancias: Se calcula la distancia matem\u00e1tica entre el punto nuevo que queremos predecir y todos los puntos existentes en el dataset.</li> <li>Buscar vecinos: Se seleccionan los \\(k\\) puntos con las distancias m\u00e1s cortas.</li> <li>Votaci\u00f3n (Clasificaci\u00f3n): La clase del nuevo punto se determina por mayor\u00eda de votos de sus vecinos. La clase m\u00e1s frecuente entre los \\(k\\) vecinos se asigna al nuevo punto.</li> <li>Promedio (Regresi\u00f3n): El valor del nuevo punto es el promedio (o media ponderada) de los valores num\u00e9ricos de sus vecinos.</li> </ol>"},{"location":"aprendizaje-supervisado/06-knn/#62-explicacion-matematica","title":"6.2. Explicaci\u00f3n Matem\u00e1tica","text":"<p>El n\u00facleo de KNN es la medici\u00f3n de la distancia para determinar la similitud. La m\u00e9trica m\u00e1s com\u00fan es la Distancia Euclidiana, aunque existen otras dependiendo del tipo de datos y el problema.</p> <p>Dados dos puntos \\(P\\) y \\(Q\\) en un espacio n-dimensional (donde \\(n\\) es el n\u00famero de caracter\u00edsticas): \\(P = (p_1, p_2, ..., p_n)\\) \\(Q = (q_1, q_2, ..., q_n)\\)</p> <ul> <li> <p>Distancia Euclidiana (L2): Es la distancia en l\u00ednea recta \"a vuelo de p\u00e1jaro\". Es la m\u00e1s utilizada por defecto.     \\(\\(d(P, Q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}\\)\\)</p> </li> <li> <p>Distancia Manhattan (L1): Suma de las diferencias absolutas. Es \u00fatil en sistemas tipo cuadr\u00edcula (como manzanas de una ciudad).     \\(\\(d(P, Q) = \\sum_{i=1}^{n} |q_i - p_i|\\)\\)</p> </li> <li> <p>Distancia Minkowski: Una generalizaci\u00f3n matem\u00e1tica de las anteriores.     \\(\\(d(P, Q) = (\\sum_{i=1}^{n} |q_i - p_i|^p)^{1/p}\\)\\)     (Si \\(p=1\\) es Manhattan, si \\(p=2\\) es Euclidiana).</p> </li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#63-pros-y-contras","title":"6.3. Pros y Contras","text":"Ventajas Desventajas Simplicidad: Es extremadamente f\u00e1cil de entender, explicar e implementar. Costo Computacional: Es lento en la fase de predicci\u00f3n con grandes datasets, ya que debe calcular distancias con todos los puntos cada vez. Sin Entrenamiento: La fase de entrenamiento es casi instant\u00e1nea (solo almacena datos), ideal si los datos cambian constantemente. Sensible a Outliers: Los valores at\u00edpicos o ruido pueden afectar significativamente la predicci\u00f3n si \\(k\\) es peque\u00f1o. Versatilidad: Sirve tanto para tareas de clasificaci\u00f3n como de regresi\u00f3n. Sensible a la Escala: Requiere estrictamente que los datos est\u00e9n normalizados o estandarizados. No Lineal: Se adapta bien a fronteras de decisi\u00f3n irregulares y complejas. Maldici\u00f3n de la Dimensionalidad: Su rendimiento decae dr\u00e1sticamente cuando hay muchas dimensiones (features) irrelevantes."},{"location":"aprendizaje-supervisado/06-knn/#64-ejemplo-en-python-con-scikit-learn","title":"6.4. Ejemplo en Python con <code>scikit-learn</code>","text":"<p>A continuaci\u00f3n, un ejemplo b\u00e1sico de clasificaci\u00f3n usando el dataset Iris y la clase <code>KNeighborsClassifier</code>.</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# 1. Cargar datos\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 2. Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 3. Escalar datos (CRUCIAL para KNN)\n# Como KNN usa distancias, las variables con rangos grandes dominar\u00e1n a las peque\u00f1as.\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 4. Instanciar el modelo (k=3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# 5. Entrenar (En KNN esto es solo almacenar los datos)\nknn.fit(X_train, y_train)\n\n# 6. Predecir y Evaluar\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"aprendizaje-supervisado/06-knn/#65-ejemplos-comunes-de-uso","title":"6.5. Ejemplos Comunes de Uso","text":"<ul> <li>Sistemas de Recomendaci\u00f3n: Sugerir productos, pel\u00edculas o m\u00fasica bas\u00e1ndose en las preferencias de usuarios \"vecinos\" con gustos similares (Filtrado Colaborativo).</li> <li>Reconocimiento de Patrones: Reconocimiento de caracteres escritos a mano (OCR) o clasificaci\u00f3n de im\u00e1genes simples bas\u00e1ndose en la similitud de p\u00edxeles.</li> <li>Detecci\u00f3n de Anomal\u00edas: Identificar fraudes bancarios o intrusiones en redes detectando eventos que est\u00e1n \"lejos\" de los grupos de vecinos normales.</li> <li>Imputaci\u00f3n de Datos Faltantes: Rellenar valores nulos en un dataset bas\u00e1ndose en los valores de los vecinos m\u00e1s cercanos (<code>KNNImputer</code>).</li> <li>Medicina: Clasificaci\u00f3n de pacientes con perfiles similares para predecir riesgos de enfermedades.</li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#66-aplicaciones-reales-de-knn","title":"6.6. Aplicaciones Reales de KNN","text":"<p>Aunque es un algoritmo simple, KNN se utiliza en sistemas donde la interpretabilidad y la simplicidad son clave:</p> <ul> <li>Sistemas de Recomendaci\u00f3n (Retail): Empresas como Amazon o Netflix utilizan variantes de algoritmos basados en vecindad para recomendar productos (\"Los usuarios que compraron X tambi\u00e9n compraron Y\").<ul> <li>Sistemas de recomendaci\u00f3n con KNN</li> </ul> </li> <li>Reconocimiento de Escritura a Mano: El servicio postal de EE.UU. (USPS) utiliz\u00f3 m\u00e9todos basados en vecindad para reconocer d\u00edgitos escritos a mano en c\u00f3digos postales.<ul> <li>Dataset MNIST y KNN</li> </ul> </li> <li>Detecci\u00f3n de Intrusiones (Ciberseguridad): Clasificar actividades de red como normales o sospechosas bas\u00e1ndose en su similitud con patrones de ataques conocidos.</li> <li>Bioinform\u00e1tica: Clasificaci\u00f3n de muestras de genes o prote\u00ednas bas\u00e1ndose en su similitud con perfiles conocidos para el diagn\u00f3stico de enfermedades.</li> </ul>"},{"location":"aprendizaje-supervisado/06-knn/#67-consideraciones-finales","title":"6.7. Consideraciones Finales","text":"<ol> <li> <p>Elecci\u00f3n de 'k' (Hiperpar\u00e1metro clave):</p> <ul> <li>Un \\(k\\) muy peque\u00f1o (ej. \\(k=1\\)) hace que el modelo sea muy sensible al ruido (Overfitting).</li> <li>Un \\(k\\) muy grande suaviza demasiado la frontera de decisi\u00f3n y puede incluir vecinos de otras clases lejanas (Underfitting).</li> <li>Se suele elegir un \\(k\\) impar para evitar empates en clasificaci\u00f3n binaria.</li> <li>El valor \u00f3ptimo se encuentra usualmente mediante validaci\u00f3n cruzada (t\u00e9cnica del codo o Elbow Method).</li> </ul> </li> <li> <p>Escalado de Caracter\u00edsticas:</p> <ul> <li>Dado que KNN se basa puramente en distancias, es obligatorio escalar las variables (usando <code>StandardScaler</code> o <code>MinMaxScaler</code>). Si una variable tiene una magnitud mucho mayor que otra (ej. Salario [1000-5000] vs Edad [20-60]), la variable de mayor magnitud dominar\u00e1 completamente el c\u00e1lculo de la distancia, haciendo que la otra sea irrelevante.</li> </ul> </li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/07-svm/","title":"\ud83e\udd16 Unidad 7. M\u00e1quinas de Vectores de Soporte (SVM)","text":"<p>Las M\u00e1quinas de Vectores de Soporte (Support Vector Machines o SVM) son un conjunto de algoritmos de aprendizaje supervisado potentes y vers\u00e1tiles, utilizados tanto para clasificaci\u00f3n (SVC) como para regresi\u00f3n (SVR). Su objetivo principal es encontrar el hiperplano \u00f3ptimo que mejor separe las clases en el espacio de caracter\u00edsticas.</p>"},{"location":"aprendizaje-supervisado/07-svm/#71-como-funciona-el-algoritmo","title":"7.1. \u00bfC\u00f3mo funciona el algoritmo?","text":"<p>La idea central de SVM es encontrar una l\u00ednea (en 2D), un plano (en 3D) o un hiperplano (en m\u00e1s dimensiones) que divida los datos en clases distintas. Pero no cualquier separaci\u00f3n sirve; SVM busca la separaci\u00f3n que tenga el mayor margen posible.</p> <ol> <li>Hiperplano: Es la frontera de decisi\u00f3n que separa las clases.</li> <li>Vectores de Soporte: Son los puntos de datos m\u00e1s cercanos al hiperplano. Estos puntos son los m\u00e1s \"dif\u00edciles\" de clasificar y son los \u00fanicos que importan para definir la posici\u00f3n del hiperplano.</li> <li>Margen: Es la distancia entre el hiperplano y los vectores de soporte m\u00e1s cercanos de cada clase. SVM intenta maximizar este margen para mejorar la generalizaci\u00f3n del modelo.</li> </ol>"},{"location":"aprendizaje-supervisado/07-svm/#72-explicacion-matematica-y-el-kernel-trick","title":"7.2. Explicaci\u00f3n Matem\u00e1tica y el \"Kernel Trick\"","text":"<p>Matem\u00e1ticamente, para un problema linealmente separable, buscamos los par\u00e1metros \\(w\\) (vector de pesos) y \\(b\\) (sesgo) tal que el hiperplano se defina como: \\(\\(w \\cdot x + b = 0\\)\\)</p> <p>El objetivo es minimizar \\(||w||\\) (lo que equivale a maximizar el margen) sujeto a que todas las muestras est\u00e9n correctamente clasificadas fuera del margen.</p>"},{"location":"aprendizaje-supervisado/07-svm/#el-truco-del-kernel-kernel-trick","title":"El Truco del Kernel (Kernel Trick)","text":"<p>Cuando los datos no son separables linealmente (ej. un c\u00edrculo dentro de otro), SVM utiliza una t\u00e9cnica llamada Kernel Trick. Esta t\u00e9cnica proyecta los datos originales a un espacio de mayor dimensi\u00f3n donde s\u00ed son linealmente separables, sin necesidad de calcular expl\u00edcitamente las coordenadas en ese espacio complejo (lo cual ser\u00eda computacionalmente costoso).</p> <p>Kernels comunes: *   Lineal: Para datos linealmente separables. *   Polin\u00f3mico: Mapea a espacios de dimensiones polin\u00f3micas. *   RBF (Radial Basis Function): El m\u00e1s popular. Mapea a un espacio de dimensi\u00f3n infinita. Es muy efectivo para fronteras de decisi\u00f3n complejas y curvas.</p>"},{"location":"aprendizaje-supervisado/07-svm/#73-pros-y-contras","title":"7.3. Pros y Contras","text":"Ventajas Desventajas Alta Dimensionalidad: Es muy efectivo en espacios con muchas dimensiones (incluso si hay m\u00e1s dimensiones que muestras). Grandes Datasets: No escala bien con datasets muy grandes (el tiempo de entrenamiento crece c\u00fabicamente). Eficiencia de Memoria: Solo usa un subconjunto de puntos de entrenamiento (los vectores de soporte) para definir el modelo. Ruido: Es sensible al ruido y a clases que se solapan mucho (si no se ajustan bien los par\u00e1metros). Versatilidad: Gracias a los Kernels, puede modelar relaciones lineales y no lineales complejas. Probabilidades: No proporciona estimaciones de probabilidad directas (se calculan mediante validaci\u00f3n cruzada costosa). Robustez: Maximizar el margen ayuda a reducir el riesgo de overfitting. Ajuste de Par\u00e1metros: Requiere un ajuste cuidadoso de hiperpar\u00e1metros clave (\\(C\\), \\(\\gamma\\), Kernel)."},{"location":"aprendizaje-supervisado/07-svm/#74-ejemplo-en-python-con-scikit-learn","title":"7.4. Ejemplo en Python con <code>scikit-learn</code>","text":"<p>Ejemplo de clasificaci\u00f3n usando <code>SVC</code> (Support Vector Classification) con un kernel RBF.</p> <pre><code>from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# 1. Cargar datos\niris = load_iris()\nX, y = iris.data, iris.target\n\n# 2. Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 3. Escalar datos (IMPORTANTE para SVM)\n# SVM es sensible a la escala porque intenta maximizar la distancia (margen).\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# 4. Instanciar el modelo\n# kernel='rbf' es el valor por defecto. C es el par\u00e1metro de regularizaci\u00f3n.\nclf = svm.SVC(kernel='rbf', C=1.0, gamma='scale')\n\n# 5. Entrenar\nclf.fit(X_train, y_train)\n\n# 6. Predecir y Evaluar\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"aprendizaje-supervisado/07-svm/#75-ejemplos-comunes-de-uso","title":"7.5. Ejemplos Comunes de Uso","text":"<ul> <li>Clasificaci\u00f3n de Texto: Categorizaci\u00f3n de noticias, detecci\u00f3n de spam y an\u00e1lisis de sentimientos. SVM maneja muy bien la alta dimensionalidad de los vectores de texto (Bag of Words).</li> <li>Reconocimiento de Im\u00e1genes: Clasificaci\u00f3n de im\u00e1genes, reconocimiento facial y reconocimiento de escritura a mano (OCR).</li> <li>Bioinform\u00e1tica: Clasificaci\u00f3n de prote\u00ednas y genes, donde los datos suelen tener muchas caracter\u00edsticas y pocas muestras.</li> <li>Detecci\u00f3n de Intrusos: Identificar actividad maliciosa en redes bas\u00e1ndose en patrones de tr\u00e1fico.</li> </ul>"},{"location":"aprendizaje-supervisado/07-svm/#76-aplicaciones-reales-de-svm","title":"7.6. Aplicaciones Reales de SVM","text":"<p>SVM ha sido uno de los algoritmos m\u00e1s exitosos antes del auge del Deep Learning y sigue siendo muy relevante:</p> <ul> <li>Clasificaci\u00f3n de Im\u00e1genes (Hist\u00f3rico): Antes de las redes neuronales convolucionales (CNN), SVM era el est\u00e1ndar para clasificaci\u00f3n de im\u00e1genes y detecci\u00f3n de objetos (ej. detecci\u00f3n de peatones).</li> <li>Bioinform\u00e1tica (Clasificaci\u00f3n de Prote\u00ednas): Se utiliza para clasificar prote\u00ednas en familias funcionales y predecir la estructura secundaria de las prote\u00ednas, dado que maneja muy bien la alta dimensionalidad de los datos gen\u00f3micos.<ul> <li>SVM en Bioinform\u00e1tica</li> </ul> </li> <li>Reconocimiento de Escritura: SVM ha demostrado ser muy eficaz en el reconocimiento de caracteres manuscritos (OCR), compitiendo con redes neuronales en datasets como MNIST.</li> <li>Geolog\u00eda y Miner\u00eda: Clasificaci\u00f3n de tipos de suelo y rocas a partir de datos s\u00edsmicos o im\u00e1genes satelitales.</li> </ul>"},{"location":"aprendizaje-supervisado/07-svm/#77-consideraciones-finales","title":"7.7. Consideraciones Finales","text":"<ol> <li> <p>Par\u00e1metro C (Regularizaci\u00f3n):</p> <ul> <li>Controla el equilibrio entre tener un margen amplio y clasificar correctamente los puntos de entrenamiento.</li> <li>C alto: Intenta clasificar todo correctamente (riesgo de Overfitting, margen estrecho).</li> <li>C bajo: Permite algunos errores para obtener un margen m\u00e1s amplio (mejor generalizaci\u00f3n, margen suave).</li> </ul> </li> <li> <p>Par\u00e1metro Gamma (\\(\\gamma\\)) (Solo para kernels RBF/Poly):</p> <ul> <li>Define qu\u00e9 tan lejos llega la influencia de un solo ejemplo de entrenamiento.</li> <li>Gamma alto: Solo los puntos muy cercanos influyen. Puede llevar a fronteras de decisi\u00f3n muy ajustadas e irregulares (Overfitting).</li> <li>Gamma bajo: La influencia llega lejos. La frontera de decisi\u00f3n es m\u00e1s suave (Underfitting si es muy bajo).</li> </ul> </li> <li> <p>Escalado: Al igual que KNN, SVM se basa en distancias. Es cr\u00edtico estandarizar los datos antes de entrenar.</p> </li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/","title":"\ud83e\udd16 Unidad 8. Algoritmos de Ensamblado (Ensemble Learning)","text":"<p>Los Algoritmos de Ensamblado (Ensemble Methods) son una t\u00e9cnica de Machine Learning que combina las predicciones de m\u00faltiples modelos base (conocidos como weak learners o aprendices d\u00e9biles) para construir un modelo final m\u00e1s robusto y preciso (strong learner).</p> <p>La intuici\u00f3n detr\u00e1s de esto es la \"Sabidur\u00eda de las Masas\": as\u00ed como la opini\u00f3n colectiva de un grupo de expertos suele ser mejor que la de un solo experto, un grupo de modelos predictivos suele superar el rendimiento de un modelo individual.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#81-conceptos-clave-y-categorias","title":"8.1. Conceptos Clave y Categor\u00edas","text":"<p>El objetivo principal es reducir el sesgo (bias) o la varianza (variance), o ambos. Los m\u00e9todos de ensamblado se dividen principalmente en tres categor\u00edas seg\u00fan c\u00f3mo combinan los modelos:</p> <ol> <li>Voting (Votaci\u00f3n): Se entrenan varios modelos diferentes (ej. KNN, SVM, \u00c1rbol) y se \"vota\" para decidir la clase final.</li> <li>Bagging (Bootstrap Aggregating): Se entrena el mismo algoritmo muchas veces en paralelo, pero con diferentes subconjuntos aleatorios de los datos de entrenamiento. Su objetivo es reducir la varianza (evitar overfitting). El ejemplo cl\u00e1sico es Random Forest.</li> <li>Boosting: Se entrena el mismo algoritmo de forma secuencial. Cada nuevo modelo intenta corregir los errores cometidos por el modelo anterior. Su objetivo es reducir el sesgo (evitar underfitting). Ejemplos: AdaBoost, XGBoost.</li> </ol>"},{"location":"aprendizaje-supervisado/08-ensamblado/#82-voting-classifiers-votacion","title":"8.2. Voting Classifiers (Votaci\u00f3n)","text":"<p>Es la forma m\u00e1s simple de ensamblado. Consiste en agregar las predicciones de clasificadores totalmente diferentes.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#tipos-de-votacion","title":"Tipos de Votaci\u00f3n","text":"<ul> <li>Hard Voting (Votaci\u00f3n Dura): Cada clasificador vota por una clase. La clase con la mayor\u00eda de votos gana (moda).</li> <li>Soft Voting (Votaci\u00f3n Suave): Si los clasificadores pueden estimar probabilidades (tienen m\u00e9todo <code>predict_proba</code>), se promedian las probabilidades de cada clase. La clase con el promedio de probabilidad m\u00e1s alto gana. El Soft Voting suele funcionar mejor porque da m\u00e1s peso a los votos con \"alta confianza\".</li> </ul>"},{"location":"aprendizaje-supervisado/08-ensamblado/#ejemplo-en-python-voting","title":"Ejemplo en Python (Voting)","text":"<pre><code>from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Datos de ejemplo\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Modelos individuales\nlog_clf = LogisticRegression(random_state=42)\nrnd_clf = DecisionTreeClassifier(random_state=42)\nsvm_clf = SVC(probability=True, random_state=42) # probability=True necesario para Soft Voting\n\n# Ensamblado por Votaci\u00f3n (Soft)\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='soft'\n)\n\n# Entrenamiento y Comparaci\u00f3n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(f\"{clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/08-ensamblado/#83-bagging-y-random-forest","title":"8.3. Bagging y Random Forest","text":"<p>Bagging (Bootstrap Aggregating) implica entrenar el mismo algoritmo en diferentes subconjuntos aleatorios del dataset de entrenamiento. *   Bootstrap: El muestreo se hace con reemplazo (una misma muestra puede aparecer varias veces en el mismo subconjunto). *   Pasting: El muestreo se hace sin reemplazo.</p> <p>Una vez entrenados, los modelos agregan sus predicciones (moda para clasificaci\u00f3n, promedio para regresi\u00f3n).</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#random-forest-bosques-aleatorios","title":"Random Forest (Bosques Aleatorios)","text":"<p>Es una implementaci\u00f3n espec\u00edfica y optimizada de Bagging usando \u00c1rboles de Decisi\u00f3n. Introduce aleatoriedad extra: al dividir un nodo en el \u00e1rbol, no busca la mejor caracter\u00edstica de todas las disponibles, sino la mejor caracter\u00edstica dentro de un subconjunto aleatorio de caracter\u00edsticas. Esto hace que los \u00e1rboles sean m\u00e1s diversos (descorrelacionados), lo que reduce dr\u00e1sticamente la varianza.</p> <p>Hiperpar\u00e1metros Clave: *   <code>n_estimators</code>: N\u00famero de \u00e1rboles (m\u00e1s es mejor, pero m\u00e1s lento). *   <code>max_features</code>: N\u00famero m\u00e1ximo de caracter\u00edsticas a considerar en cada divisi\u00f3n. *   <code>bootstrap</code>: Si usar muestreo con reemplazo (True por defecto). *   <code>n_jobs</code>: N\u00famero de n\u00facleos de CPU a usar (-1 para todos).</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#ejemplo-en-python-random-forest","title":"Ejemplo en Python (Random Forest)","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Instanciar Random Forest\n# 500 \u00e1rboles, usando todos los n\u00facleos de CPU\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)\nprint(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n\n# Importancia de Caracter\u00edsticas\n# Random Forest permite ver qu\u00e9 variables son m\u00e1s \u00fatiles\nfor name, score in zip([\"Feature 1\", \"Feature 2\"], rnd_clf.feature_importances_):\n    print(f\"{name}: {score}\")\n</code></pre>"},{"location":"aprendizaje-supervisado/08-ensamblado/#84-boosting-impulso","title":"8.4. Boosting (Impulso)","text":"<p>El Boosting entrena predictores secuencialmente, cada uno intentando corregir a su predecesor.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#841-adaboost-adaptive-boosting","title":"8.4.1. AdaBoost (Adaptive Boosting)","text":"<p>El algoritmo presta m\u00e1s atenci\u00f3n a las instancias de entrenamiento que el predecesor clasific\u00f3 incorrectamente. 1.  Entrena un clasificador base. 2.  Aumenta el peso relativo de las instancias mal clasificadas. 3.  Entrena un segundo clasificador con los pesos actualizados. 4.  Repite el proceso.</p> <p>Hiperpar\u00e1metros Clave: *   <code>n_estimators</code>: N\u00famero de iteraciones. *   <code>learning_rate</code>: Cu\u00e1nto contribuye cada modelo. Un valor bajo requiere m\u00e1s estimadores.</p> <p>Ejemplo Python (AdaBoost): <pre><code>from sklearn.ensemble import AdaBoostClassifier\n\n# AdaBoost usando \u00c1rboles de Decisi\u00f3n muy simples (stumps)\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42\n)\nada_clf.fit(X_train, y_train)\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#842-gradient-boosting-machine-gbm","title":"8.4.2. Gradient Boosting Machine (GBM)","text":"<p>En lugar de ajustar los pesos de las instancias, GBM intenta ajustar el nuevo predictor a los errores residuales (la diferencia entre el valor real y el predicho) del predictor anterior.</p> <p>Ejemplo Python (GradientBoosting de sklearn): <pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\ngbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\ngbrt.fit(X_train, y_train)\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#843-xgboost-extreme-gradient-boosting","title":"8.4.3. XGBoost (Extreme Gradient Boosting)","text":"<p>Es una versi\u00f3n optimizada de Gradient Boosting dise\u00f1ada para ser altamente eficiente, flexible y port\u00e1til. Es el algoritmo dominante en competiciones de Machine Learning (Kaggle). *   Regularizaci\u00f3n: Incluye regularizaci\u00f3n L1 y L2 para evitar overfitting. *   Paralelizaci\u00f3n: Construcci\u00f3n de \u00e1rboles en paralelo. *   Manejo de nulos: Aprende autom\u00e1ticamente la mejor direcci\u00f3n para valores faltantes.</p> <p>Hiperpar\u00e1metros Clave: *   <code>eta</code> (learning_rate): Paso de reducci\u00f3n de pesos para prevenir overfitting. *   <code>max_depth</code>: Profundidad m\u00e1xima del \u00e1rbol. *   <code>subsample</code>: Ratio de muestras de entrenamiento usadas. *   <code>colsample_bytree</code>: Ratio de columnas usadas por \u00e1rbol.</p> <p>Ejemplo Python (XGBoost): <pre><code>import xgboost as xgb\n\n# XGBoost tiene su propia estructura de datos optimizada (DMatrix), pero es compatible con sklearn\nxgb_clf = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=3,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    n_jobs=-1,\n    random_state=42\n)\n\nxgb_clf.fit(X_train, y_train)\nprint(f\"XGBoost Accuracy: {accuracy_score(y_test, xgb_clf.predict(X_test)):.4f}\")\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#844-lightgbm-light-gradient-boosting-machine","title":"8.4.4. LightGBM (Light Gradient Boosting Machine)","text":"<p>Desarrollado por Microsoft. A diferencia de otros que crecen el \u00e1rbol por niveles (level-wise), LightGBM crece por hojas (leaf-wise). Elige la hoja con mayor p\u00e9rdida para crecer. *   Ventajas: Mucho m\u00e1s r\u00e1pido que XGBoost en grandes datasets y consume menos memoria. *   Desventajas: Puede hacer overfitting f\u00e1cilmente en datasets peque\u00f1os (&lt; 10,000 filas).</p> <p>Hiperpar\u00e1metros Clave: *   <code>num_leaves</code>: Par\u00e1metro principal para controlar la complejidad (en lugar de max_depth). *   <code>min_data_in_leaf</code>: Importante para evitar overfitting.</p> <p>Ejemplo Python (LightGBM): <pre><code>import lightgbm as lgb\n\nlgb_clf = lgb.LGBMClassifier(\n    num_leaves=31,\n    learning_rate=0.05,\n    n_estimators=100,\n    random_state=42\n)\n\nlgb_clf.fit(X_train, y_train)\nprint(f\"LightGBM Accuracy: {accuracy_score(y_test, lgb_clf.predict(X_test)):.4f}\")\n</code></pre></p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#85-resumen-comparativo","title":"8.5. Resumen Comparativo","text":"T\u00e9cnica Algoritmo Principal Estrategia Objetivo Paralelizable Voting VotingClassifier Promedio de modelos distintos Robustez general S\u00ed Bagging Random Forest Modelos iguales, datos aleatorios (independientes) Reducir Varianza S\u00ed (Muy r\u00e1pido) Boosting AdaBoost, XGBoost Modelos iguales, secuenciales (dependientes) Reducir Sesgo No (Secuencial)* <p>* Nota: XGBoost y LightGBM paralelizan la construcci\u00f3n dentro del \u00e1rbol, pero los \u00e1rboles se crean secuencialmente.</p>"},{"location":"aprendizaje-supervisado/08-ensamblado/#86-aplicaciones-reales-de-algoritmos-de-ensamblado","title":"8.6. Aplicaciones Reales de Algoritmos de Ensamblado","text":"<p>Los m\u00e9todos de ensamblado dominan actualmente las competiciones de ciencia de datos y las aplicaciones industriales en datos estructurados:</p> <ul> <li>Detecci\u00f3n de Fraude (Banca): Algoritmos como XGBoost y Random Forest son el est\u00e1ndar en la industria financiera para detectar transacciones fraudulentas en tiempo real debido a su alta precisi\u00f3n y velocidad.<ul> <li>Detecci\u00f3n de fraude con XGBoost</li> </ul> </li> <li>Diagn\u00f3stico M\u00e9dico: Random Forest se utiliza para diagnosticar enfermedades (como la retinopat\u00eda diab\u00e9tica) analizando m\u00faltiples variables de pacientes, ya que proporciona una medida de qu\u00e9 s\u00edntomas son m\u00e1s relevantes.</li> <li>Ranking de B\u00fasqueda (Search Engines): Motores de b\u00fasqueda utilizan Gradient Boosting para ordenar los resultados de b\u00fasqueda (Learning to Rank), optimizando la relevancia para el usuario.<ul> <li>Learning to Rank con LightGBM</li> </ul> </li> <li>Predicci\u00f3n de Demanda (Retail): Cadenas de suministro usan estos modelos para predecir la demanda futura de productos, optimizando el inventario y reduciendo desperdicios.</li> </ul>"},{"location":"aprendizaje-supervisado/08-ensamblado/#87-consideraciones-finales","title":"8.7. Consideraciones Finales","text":"<ol> <li>Random Forest es una excelente \"primera opci\u00f3n\". Es robusto, requiere poco ajuste de hiperpar\u00e1metros y nos da la importancia de las caracter\u00edsticas.</li> <li>XGBoost / LightGBM suelen ofrecer el mejor rendimiento (Accuracy) en datos tabulares estructurados, pero requieren m\u00e1s ajuste de hiperpar\u00e1metros y cuidado con el overfitting.</li> <li>Escalado: Los algoritmos basados en \u00e1rboles (Random Forest, Boosting) NO requieren escalado de caracter\u00edsticas (StandardScaler), lo cual es una gran ventaja pr\u00e1ctica.</li> <li>Interpretabilidad: Los modelos de ensamblado son \"Cajas Negras\". Perdemos la interpretabilidad simple de un solo \u00c1rbol de Decisi\u00f3n o una Regresi\u00f3n Lineal, aunque podemos usar la \"Importancia de Caracter\u00edsticas\" para entender qu\u00e9 variables pesan m\u00e1s.</li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 19/11/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/","title":"\ud83d\udcda Conocimientos B\u00e1sicos","text":"<p>Bienvenido a la secci\u00f3n de Conocimientos B\u00e1sicos. Esta secci\u00f3n est\u00e1 dise\u00f1ada para proporcionarte todas las herramientas y conceptos fundamentales que necesitas dominar antes de adentrarte en el mundo de la Inteligencia Artificial y el Machine Learning.</p>"},{"location":"conocimientos-basicos/#objetivo-de-esta-seccion","title":"\ud83c\udfaf Objetivo de esta Secci\u00f3n","text":"<p>Si nunca has programado antes o tienes conocimientos b\u00e1sicos de programaci\u00f3n, esta secci\u00f3n te guiar\u00e1 paso a paso desde los fundamentos de Python hasta el dominio de las librer\u00edas esenciales utilizadas en ciencia de datos e IA.</p>"},{"location":"conocimientos-basicos/#contenido","title":"\ud83d\udcd6 Contenido","text":""},{"location":"conocimientos-basicos/#curso-de-python","title":"Curso de Python","text":"<p>Un curso completo de Python desde cero, dise\u00f1ado para principiantes absolutos:</p> Unidad Tema Descripci\u00f3n 1 Introducci\u00f3n a Python Qu\u00e9 es Python, instalaci\u00f3n, tu primer programa 2 Variables y Tipos de Datos Variables, n\u00fameros, strings, booleanos 3 Operadores Aritm\u00e9ticos, comparaci\u00f3n, l\u00f3gicos, asignaci\u00f3n 4 Estructuras de Control Condicionales if/else, bucles for/while 5 Estructuras de Datos Listas, tuplas, diccionarios, conjuntos 6 Funciones Definici\u00f3n, par\u00e1metros, return, lambda 7 M\u00f3dulos y Paquetes Import, m\u00f3dulos est\u00e1ndar, pip 8 Manejo de Archivos Lectura/escritura, CSV, JSON 9 Excepciones Manejo de errores, try/except 10 Programaci\u00f3n Orientada a Objetos Clases, herencia, polimorfismo 11 Trabajando con JSON Lectura, escritura y APIs REST 12 Librer\u00eda Faker Generaci\u00f3n de datos ficticios"},{"location":"conocimientos-basicos/#librerias-esenciales-para-ia","title":"Librer\u00edas Esenciales para IA","text":"<p>Documentaci\u00f3n detallada de las librer\u00edas fundamentales:</p> Librer\u00eda Descripci\u00f3n NumPy Computaci\u00f3n num\u00e9rica y arrays multidimensionales Pandas Manipulaci\u00f3n y an\u00e1lisis de datos Matplotlib Visualizaci\u00f3n de datos y gr\u00e1ficos Seaborn Visualizaci\u00f3n estad\u00edstica avanzada Scikit-learn Introducci\u00f3n al Machine Learning"},{"location":"conocimientos-basicos/#requisitos","title":"\ud83d\udee0\ufe0f Requisitos","text":"<ul> <li>Un ordenador con Windows, macOS o Linux.</li> <li>Conexi\u00f3n a Internet para descargar Python y librer\u00edas.</li> <li>Ganas de aprender.</li> </ul>"},{"location":"conocimientos-basicos/#tiempo-estimado","title":"\u23f1\ufe0f Tiempo Estimado","text":"<ul> <li>Curso de Python: 20-30 horas.</li> <li>Librer\u00edas para IA: 15-20 horas.</li> </ul>"},{"location":"conocimientos-basicos/#recomendaciones","title":"\ud83d\udca1 Recomendaciones","text":"<ol> <li>Practica cada ejemplo: No te limites a leer, escribe y ejecuta cada c\u00f3digo.</li> <li>Experimenta: Modifica los ejemplos para ver qu\u00e9 sucede.</li> <li>No te saltes temas: Cada concepto se construye sobre el anterior.</li> <li>Usa un cuaderno: Toma notas de los conceptos importantes.</li> </ol> <p>\ud83d\udcc5 \u00daltima actualizaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/01-introduccion-python/","title":"\ud83d\udc0d Unidad 1. Introducci\u00f3n a Python","text":""},{"location":"conocimientos-basicos/01-introduccion-python/#11-que-es-python","title":"1.1. \u00bfQu\u00e9 es Python?","text":"<p>Python es un lenguaje de programaci\u00f3n de alto nivel, interpretado y de prop\u00f3sito general. Fue creado por Guido van Rossum y lanzado por primera vez en 1991.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#caracteristicas-principales","title":"Caracter\u00edsticas Principales","text":"<ul> <li>F\u00e1cil de aprender: Su sintaxis es clara y legible, similar al ingl\u00e9s.</li> <li>Vers\u00e1til: Se usa en web, ciencia de datos, IA, automatizaci\u00f3n, juegos...</li> <li>Interpretado: No necesita compilaci\u00f3n, se ejecuta l\u00ednea por l\u00ednea.</li> <li>Gran comunidad: Miles de librer\u00edas y recursos disponibles.</li> <li>Gratuito y Open Source: Puedes usarlo libremente.</li> </ul>"},{"location":"conocimientos-basicos/01-introduccion-python/#por-que-python-para-ia","title":"\u00bfPor qu\u00e9 Python para IA?","text":"<p>Python es el lenguaje dominante en Inteligencia Artificial y Machine Learning por:</p> <ol> <li>Librer\u00edas especializadas (NumPy, Pandas, TensorFlow, PyTorch).</li> <li>Sintaxis sencilla que permite enfocarse en los algoritmos.</li> <li>Gran comunidad cient\u00edfica y abundante documentaci\u00f3n.</li> <li>Integraci\u00f3n con otras herramientas y lenguajes.</li> </ol>"},{"location":"conocimientos-basicos/01-introduccion-python/#12-instalacion-de-python","title":"1.2. Instalaci\u00f3n de Python","text":""},{"location":"conocimientos-basicos/01-introduccion-python/#windows","title":"Windows","text":"<ol> <li>Ve a python.org/downloads.</li> <li>Descarga la \u00faltima versi\u00f3n de Python 3.x.</li> <li>Ejecuta el instalador.</li> <li>\u00a1IMPORTANTE! Marca la casilla \"Add Python to PATH\".</li> <li>Haz clic en \"Install Now\".</li> </ol>"},{"location":"conocimientos-basicos/01-introduccion-python/#macos","title":"macOS","text":"<p>Python suele venir preinstalado, pero es recomendable instalar la \u00faltima versi\u00f3n:</p> <pre><code># Usando Homebrew\nbrew install python3\n</code></pre> <p>O descarga el instalador desde python.org.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code>sudo apt update\nsudo apt install python3 python3-pip\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#verificar-la-instalacion","title":"Verificar la Instalaci\u00f3n","text":"<p>Abre una terminal (o CMD en Windows) y escribe:</p> <pre><code>python --version\n</code></pre> <p>O en algunos sistemas:</p> <pre><code>python3 --version\n</code></pre> <p>Deber\u00edas ver algo como:</p> <pre><code>Python 3.12.0\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#13-entornos-de-desarrollo-ides","title":"1.3. Entornos de Desarrollo (IDEs)","text":"<p>Un IDE (Integrated Development Environment) es un programa que facilita escribir c\u00f3digo. Aqu\u00ed tienes las opciones m\u00e1s populares:</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#visual-studio-code-recomendado","title":"Visual Studio Code (Recomendado)","text":"<ul> <li>Gratuito y muy popular.</li> <li>Extensiones para Python excelentes.</li> <li>Descarga: code.visualstudio.com</li> </ul> <p>Despu\u00e9s de instalarlo, instala la extensi\u00f3n \"Python\" de Microsoft.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#pycharm","title":"PyCharm","text":"<ul> <li>IDE espec\u00edfico para Python.</li> <li>Versi\u00f3n Community gratuita.</li> <li>Descarga: jetbrains.com/pycharm</li> </ul>"},{"location":"conocimientos-basicos/01-introduccion-python/#jupyter-notebook","title":"Jupyter Notebook","text":"<ul> <li>Ideal para ciencia de datos y aprendizaje.</li> <li>Permite combinar c\u00f3digo con explicaciones.</li> <li>Se instala con: <code>pip install jupyter</code></li> </ul>"},{"location":"conocimientos-basicos/01-introduccion-python/#idle","title":"IDLE","text":"<ul> <li>Viene incluido con Python.</li> <li>B\u00e1sico pero funcional para empezar.</li> </ul>"},{"location":"conocimientos-basicos/01-introduccion-python/#14-tu-primer-programa","title":"1.4. Tu Primer Programa","text":""},{"location":"conocimientos-basicos/01-introduccion-python/#el-clasico-hola-mundo","title":"El Cl\u00e1sico \"Hola Mundo\"","text":"<p>Crea un archivo llamado <code>hola.py</code> y escribe:</p> <pre><code>print(\"\u00a1Hola, Mundo!\")\n</code></pre> <p>Para ejecutarlo, abre una terminal en la carpeta donde guardaste el archivo y escribe:</p> <pre><code>python hola.py\n</code></pre> <p>Salida:</p> <pre><code>\u00a1Hola, Mundo!\n</code></pre> <p>\u00a1Felicidades! Acabas de escribir y ejecutar tu primer programa en Python.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#explicacion","title":"Explicaci\u00f3n","text":"<ul> <li><code>print()</code> es una funci\u00f3n incorporada de Python.</li> <li>Muestra en pantalla lo que pongas entre los par\u00e9ntesis.</li> <li>El texto entre comillas se llama cadena de texto o string.</li> </ul>"},{"location":"conocimientos-basicos/01-introduccion-python/#15-la-funcion-print","title":"1.5. La Funci\u00f3n print()","text":"<p>La funci\u00f3n <code>print()</code> es fundamental. Veamos m\u00e1s ejemplos:</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#imprimir-texto","title":"Imprimir Texto","text":"<pre><code>print(\"Bienvenido al curso de Python\")\nprint('Tambi\u00e9n puedes usar comillas simples')\n</code></pre> <p>Salida:</p> <pre><code>Bienvenido al curso de Python\nTambi\u00e9n puedes usar comillas simples\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#imprimir-numeros","title":"Imprimir N\u00fameros","text":"<pre><code>print(42)\nprint(3.14159)\nprint(-100)\n</code></pre> <p>Salida:</p> <pre><code>42\n3.14159\n-100\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#imprimir-varios-elementos","title":"Imprimir Varios Elementos","text":"<pre><code>print(\"Mi edad es\", 25, \"a\u00f1os\")\nprint(\"Python\", \"es\", \"genial\")\n</code></pre> <p>Salida:</p> <pre><code>Mi edad es 25 a\u00f1os\nPython es genial\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#cambiar-el-separador","title":"Cambiar el Separador","text":"<pre><code>print(\"uno\", \"dos\", \"tres\", sep=\"-\")\nprint(\"a\", \"b\", \"c\", sep=\" | \")\n</code></pre> <p>Salida:</p> <pre><code>uno-dos-tres\na | b | c\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#cambiar-el-final-de-linea","title":"Cambiar el Final de L\u00ednea","text":"<p>Por defecto, <code>print()</code> a\u00f1ade un salto de l\u00ednea al final. Puedes cambiarlo:</p> <pre><code>print(\"Hola\", end=\" \")\nprint(\"Mundo\")\n</code></pre> <p>Salida:</p> <pre><code>Hola Mundo\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#imprimir-lineas-vacias","title":"Imprimir L\u00edneas Vac\u00edas","text":"<pre><code>print(\"Primera l\u00ednea\")\nprint()  # L\u00ednea vac\u00eda\nprint(\"Segunda l\u00ednea\")\n</code></pre> <p>Salida:</p> <pre><code>Primera l\u00ednea\n\nSegunda l\u00ednea\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#16-comentarios","title":"1.6. Comentarios","text":"<p>Los comentarios son texto que Python ignora. Sirven para explicar el c\u00f3digo.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#comentarios-de-una-linea","title":"Comentarios de Una L\u00ednea","text":"<p>Usa el s\u00edmbolo <code>#</code>:</p> <pre><code># Esto es un comentario\nprint(\"Hola\")  # Esto tambi\u00e9n es un comentario\n\n# Los comentarios no se ejecutan\n# print(\"Esto no se mostrar\u00e1\")\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#comentarios-de-varias-lineas","title":"Comentarios de Varias L\u00edneas","text":"<p>Usa triple comilla:</p> <pre><code>\"\"\"\nEste es un comentario\nde varias l\u00edneas.\nMuy \u00fatil para explicaciones largas.\n\"\"\"\n\n'''\nTambi\u00e9n puedes usar\ncomillas simples triples.\n'''\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#buenas-practicas-con-comentarios","title":"Buenas Pr\u00e1cticas con Comentarios","text":"<pre><code># MAL: Comentario obvio\nx = 5  # Asigna 5 a x\n\n# BIEN: Comentario \u00fatil\n# Velocidad m\u00e1xima permitida en zona urbana (km/h)\nvelocidad_maxima = 50\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#17-la-funcion-input","title":"1.7. La Funci\u00f3n input()","text":"<p>La funci\u00f3n <code>input()</code> permite al usuario introducir datos:</p> <pre><code>nombre = input(\"\u00bfC\u00f3mo te llamas? \")\nprint(\"\u00a1Hola,\", nombre + \"!\")\n</code></pre> <p>Ejecuci\u00f3n:</p> <pre><code>\u00bfC\u00f3mo te llamas? Ana\n\u00a1Hola, Ana!\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#ejemplo-programa-interactivo","title":"Ejemplo: Programa Interactivo","text":"<pre><code># Programa de saludo personalizado\nprint(\"=== Programa de Bienvenida ===\")\nprint()\n\nnombre = input(\"Introduce tu nombre: \")\nciudad = input(\"\u00bfDe qu\u00e9 ciudad eres? \")\n\nprint()\nprint(\"\u00a1Bienvenido/a,\", nombre + \"!\")\nprint(\"\u00a1Qu\u00e9 bonita es\", ciudad + \"!\")\n</code></pre> <p>Ejecuci\u00f3n:</p> <pre><code>=== Programa de Bienvenida ===\n\nIntroduce tu nombre: Carlos\n\u00bfDe qu\u00e9 ciudad eres? Madrid\n\n\u00a1Bienvenido/a, Carlos!\n\u00a1Qu\u00e9 bonita es Madrid!\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#nota-importante-sobre-input","title":"Nota Importante sobre input()","text":"<p><code>input()</code> siempre devuelve texto (string), incluso si el usuario escribe n\u00fameros:</p> <pre><code>edad = input(\"\u00bfCu\u00e1ntos a\u00f1os tienes? \")\nprint(type(edad))  # &lt;class 'str'&gt;\n</code></pre> <p>M\u00e1s adelante veremos c\u00f3mo convertir ese texto a n\u00fameros.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#18-el-modo-interactivo","title":"1.8. El Modo Interactivo","text":"<p>Puedes usar Python de forma interactiva sin crear archivos. Es \u00fatil para pruebas r\u00e1pidas.</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#iniciar-el-modo-interactivo","title":"Iniciar el Modo Interactivo","text":"<p>En la terminal, escribe simplemente:</p> <pre><code>python\n</code></pre> <p>Ver\u00e1s algo como:</p> <pre><code>Python 3.12.0 (main, Oct  2 2024, 00:00:00)\n&gt;&gt;&gt; \n</code></pre> <p>El <code>&gt;&gt;&gt;</code> es el prompt donde puedes escribir c\u00f3digo:</p> <pre><code>&gt;&gt;&gt; print(\"Hola\")\nHola\n&gt;&gt;&gt; 2 + 2\n4\n&gt;&gt;&gt; nombre = \"Python\"\n&gt;&gt;&gt; print(nombre)\nPython\n&gt;&gt;&gt; exit()  # Para salir\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#ventajas-del-modo-interactivo","title":"Ventajas del Modo Interactivo","text":"<ul> <li>Pruebas r\u00e1pidas de c\u00f3digo.</li> <li>Explorar funciones y librer\u00edas.</li> <li>Calculadora avanzada.</li> </ul>"},{"location":"conocimientos-basicos/01-introduccion-python/#19-estructura-de-un-programa-python","title":"1.9. Estructura de un Programa Python","text":"<p>Un programa Python t\u00edpico tiene esta estructura:</p> <pre><code># 1. Comentario inicial (descripci\u00f3n del programa)\n\"\"\"\nPrograma: Calculadora de edad\nAutor: Tu Nombre\nFecha: Enero 2026\n\"\"\"\n\n# 2. Importaciones (librer\u00edas que necesitamos)\n# import math  # Ejemplo, no lo usamos aqu\u00ed\n\n# 3. Definiciones (funciones, clases) - Lo veremos m\u00e1s adelante\n\n# 4. C\u00f3digo principal\nprint(\"=== Calculadora de Edad ===\")\nprint()\n\nnombre = input(\"Tu nombre: \")\nanio_nacimiento = input(\"A\u00f1o de nacimiento: \")\n\n# Convertimos el a\u00f1o a n\u00famero (lo veremos en detalle)\nanio_actual = 2026\nedad = anio_actual - int(anio_nacimiento)\n\nprint()\nprint(f\"Hola {nombre}, tienes aproximadamente {edad} a\u00f1os.\")\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#110-errores-comunes-de-principiante","title":"1.10. Errores Comunes de Principiante","text":""},{"location":"conocimientos-basicos/01-introduccion-python/#error-de-sintaxis-syntaxerror","title":"Error de Sintaxis (SyntaxError)","text":"<pre><code># MAL: Falta cerrar el par\u00e9ntesis\nprint(\"Hola\"\n\n# MAL: Falta cerrar las comillas\nprint(\"Hola)\n\n# BIEN:\nprint(\"Hola\")\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#error-de-nombre-nameerror","title":"Error de Nombre (NameError)","text":"<pre><code># MAL: La variable no existe\nprint(mensaje)\n\n# BIEN: Primero definimos la variable\nmensaje = \"Hola\"\nprint(mensaje)\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#error-de-indentacion-indentationerror","title":"Error de Indentaci\u00f3n (IndentationError)","text":"<p>Python usa la indentaci\u00f3n (espacios al inicio) para estructurar el c\u00f3digo:</p> <pre><code># MAL: Indentaci\u00f3n incorrecta\nprint(\"Hola\")\n   print(\"Mundo\")  # Error: espacio innecesario\n\n# BIEN:\nprint(\"Hola\")\nprint(\"Mundo\")\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#consejo-lee-los-mensajes-de-error","title":"Consejo: Lee los Mensajes de Error","text":"<p>Python te dice qu\u00e9 est\u00e1 mal y en qu\u00e9 l\u00ednea:</p> <pre><code>  File \"programa.py\", line 3\n    print(\"Hola\"\n               ^\nSyntaxError: '(' was never closed\n</code></pre> <p>Esto te indica: - El archivo: <code>programa.py</code> - La l\u00ednea: 3 - El problema: Un par\u00e9ntesis sin cerrar</p>"},{"location":"conocimientos-basicos/01-introduccion-python/#111-ejercicios-practicos","title":"1.11. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/01-introduccion-python/#ejercicio-1-presentacion","title":"Ejercicio 1: Presentaci\u00f3n","text":"<p>Crea un programa que muestre tu presentaci\u00f3n:</p> <pre><code>print(\"========================\")\nprint(\"    MI PRESENTACI\u00d3N\")\nprint(\"========================\")\nprint()\nprint(\"Nombre: [Tu nombre]\")\nprint(\"Edad: [Tu edad]\")\nprint(\"Ciudad: [Tu ciudad]\")\nprint(\"Hobby: [Tu hobby]\")\nprint()\nprint(\"========================\")\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#ejercicio-2-datos-del-usuario","title":"Ejercicio 2: Datos del Usuario","text":"<p>Crea un programa que pida datos al usuario y los muestre:</p> <pre><code>print(\"=== Formulario de Registro ===\")\nprint()\n\nnombre = input(\"Nombre: \")\napellido = input(\"Apellido: \")\nemail = input(\"Email: \")\ntelefono = input(\"Tel\u00e9fono: \")\n\nprint()\nprint(\"=== Datos Registrados ===\")\nprint(\"Nombre completo:\", nombre, apellido)\nprint(\"Email:\", email)\nprint(\"Tel\u00e9fono:\", telefono)\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#ejercicio-3-calculadora-simple-texto","title":"Ejercicio 3: Calculadora Simple (Texto)","text":"<pre><code>print(\"=== Mini Calculadora ===\")\nprint()\n\nnumero1 = input(\"Primer n\u00famero: \")\nnumero2 = input(\"Segundo n\u00famero: \")\n\n# Por ahora solo mostramos los n\u00fameros\n# En la siguiente unidad aprenderemos a hacer operaciones\nprint()\nprint(\"Has introducido:\", numero1, \"y\", numero2)\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#ejercicio-4-generador-de-frases","title":"Ejercicio 4: Generador de Frases","text":"<pre><code>print(\"=== Generador de Frases ===\")\nprint()\n\nsustantivo = input(\"Escribe un sustantivo: \")\nadjetivo = input(\"Escribe un adjetivo: \")\nverbo = input(\"Escribe un verbo: \")\nlugar = input(\"Escribe un lugar: \")\n\nprint()\nprint(\"Tu frase es:\")\nprint(f\"El {sustantivo} {adjetivo} {verbo} en {lugar}.\")\n</code></pre>"},{"location":"conocimientos-basicos/01-introduccion-python/#112-resumen","title":"1.12. Resumen","text":"<p>En esta unidad has aprendido:</p> Concepto Descripci\u00f3n Python Lenguaje de programaci\u00f3n vers\u00e1til y f\u00e1cil de aprender Instalaci\u00f3n Descargar de python.org y a\u00f1adir al PATH IDE Programa para escribir c\u00f3digo (VS Code recomendado) <code>print()</code> Funci\u00f3n para mostrar informaci\u00f3n en pantalla <code>input()</code> Funci\u00f3n para pedir datos al usuario Comentarios Texto explicativo que Python ignora (<code>#</code>) Modo interactivo Usar Python directamente en la terminal"},{"location":"conocimientos-basicos/01-introduccion-python/#113-proximo-paso","title":"1.13. Pr\u00f3ximo Paso","text":"<p>En la siguiente unidad aprenderemos sobre Variables y Tipos de Datos, donde descubrir\u00e1s c\u00f3mo almacenar informaci\u00f3n y trabajar con n\u00fameros, texto y otros tipos de datos.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/","title":"\ud83d\udce6 Unidad 2. Variables y Tipos de Datos","text":""},{"location":"conocimientos-basicos/02-variables-tipos-datos/#21-que-es-una-variable","title":"2.1. \u00bfQu\u00e9 es una Variable?","text":"<p>Una variable es un espacio en la memoria del ordenador donde guardamos informaci\u00f3n. Piensa en ella como una \"caja\" con una etiqueta (nombre) donde almacenamos un valor.</p> <pre><code># Creamos una variable llamada \"edad\" y le asignamos el valor 25\nedad = 25\n\n# Ahora podemos usar esa variable\nprint(edad)  # Muestra: 25\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#asignacion-de-variables","title":"Asignaci\u00f3n de Variables","text":"<p>El s\u00edmbolo <code>=</code> se usa para asignar un valor a una variable:</p> <pre><code>nombre = \"Ana\"        # Texto\nedad = 30             # N\u00famero entero\naltura = 1.75         # N\u00famero decimal\nes_estudiante = True  # Valor booleano (verdadero/falso)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#reglas-para-nombrar-variables","title":"Reglas para Nombrar Variables","text":"Regla Ejemplo V\u00e1lido Ejemplo Inv\u00e1lido Debe empezar con letra o <code>_</code> <code>nombre</code>, <code>_edad</code> <code>1nombre</code> Solo letras, n\u00fameros y <code>_</code> <code>mi_variable</code>, <code>edad2</code> <code>mi-variable</code>, <code>edad@</code> No puede ser palabra reservada <code>mi_class</code> <code>class</code>, <code>if</code>, <code>for</code> Distingue may\u00fasculas/min\u00fasculas <code>Nombre</code> \u2260 <code>nombre</code> -"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#convenciones-de-nombres-pep-8","title":"Convenciones de Nombres (PEP 8)","text":"<pre><code># BIEN: snake_case (recomendado en Python)\nnombre_completo = \"Juan Garc\u00eda\"\nnumero_de_telefono = \"123456789\"\nes_mayor_de_edad = True\n\n# MAL: Otros estilos (v\u00e1lidos pero no recomendados en Python)\nNombreCompleto = \"Juan Garc\u00eda\"     # PascalCase (se usa para clases)\nnumeroTelefono = \"123456789\"       # camelCase (se usa en otros lenguajes)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejemplos-de-asignacion","title":"Ejemplos de Asignaci\u00f3n","text":"<pre><code># Asignaci\u00f3n simple\nx = 10\nmensaje = \"Hola Mundo\"\n\n# Asignaci\u00f3n m\u00faltiple\na, b, c = 1, 2, 3\nprint(a)  # 1\nprint(b)  # 2\nprint(c)  # 3\n\n# Mismo valor a varias variables\nx = y = z = 0\nprint(x, y, z)  # 0 0 0\n\n# Intercambiar valores (muy \u00fatil en Python)\na = 5\nb = 10\na, b = b, a  # Intercambio\nprint(a)  # 10\nprint(b)  # 5\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#22-tipos-de-datos-basicos","title":"2.2. Tipos de Datos B\u00e1sicos","text":"<p>Python tiene varios tipos de datos fundamentales:</p> Tipo Nombre en Python Ejemplo Entero <code>int</code> <code>42</code>, <code>-7</code>, <code>0</code> Decimal <code>float</code> <code>3.14</code>, <code>-0.5</code>, <code>2.0</code> Texto <code>str</code> <code>\"Hola\"</code>, <code>'Python'</code> Booleano <code>bool</code> <code>True</code>, <code>False</code> Nulo <code>NoneType</code> <code>None</code>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#funcion-type","title":"Funci\u00f3n type()","text":"<p>Puedes saber el tipo de una variable con <code>type()</code>:</p> <pre><code>edad = 25\nprint(type(edad))  # &lt;class 'int'&gt;\n\nprecio = 19.99\nprint(type(precio))  # &lt;class 'float'&gt;\n\nnombre = \"Python\"\nprint(type(nombre))  # &lt;class 'str'&gt;\n\nactivo = True\nprint(type(activo))  # &lt;class 'bool'&gt;\n\nnada = None\nprint(type(nada))  # &lt;class 'NoneType'&gt;\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#23-numeros-enteros-int","title":"2.3. N\u00fameros Enteros (int)","text":"<p>Los enteros son n\u00fameros sin decimales, positivos o negativos.</p> <pre><code># Enteros positivos\nedad = 25\npoblacion = 47000000\na\u00f1o = 2026\n\n# Enteros negativos\ntemperatura = -5\ndeuda = -1000\n\n# Cero\ncontador = 0\n\n# N\u00fameros grandes (puedes usar _ para legibilidad)\nmillones = 1_000_000\nprint(millones)  # 1000000\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#operaciones-con-enteros","title":"Operaciones con Enteros","text":"<pre><code>a = 10\nb = 3\n\n# Operaciones b\u00e1sicas\nprint(a + b)   # Suma: 13\nprint(a - b)   # Resta: 7\nprint(a * b)   # Multiplicaci\u00f3n: 30\nprint(a / b)   # Divisi\u00f3n: 3.3333... (devuelve float)\nprint(a // b)  # Divisi\u00f3n entera: 3\nprint(a % b)   # M\u00f3dulo (resto): 1\nprint(a ** b)  # Potencia: 1000\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejemplos-practicos","title":"Ejemplos Pr\u00e1cticos","text":"<pre><code># Ejemplo 1: C\u00e1lculo de edad\na\u00f1o_actual = 2026\na\u00f1o_nacimiento = 1990\nedad = a\u00f1o_actual - a\u00f1o_nacimiento\nprint(f\"Tienes {edad} a\u00f1os\")  # Tienes 36 a\u00f1os\n\n# Ejemplo 2: Conversi\u00f3n de minutos a horas\nminutos_totales = 150\nhoras = minutos_totales // 60\nminutos_restantes = minutos_totales % 60\nprint(f\"{minutos_totales} minutos = {horas} horas y {minutos_restantes} minutos\")\n# 150 minutos = 2 horas y 30 minutos\n\n# Ejemplo 3: C\u00e1lculo de precio total\nprecio_unitario = 15\ncantidad = 7\ntotal = precio_unitario * cantidad\nprint(f\"Total: {total}\u20ac\")  # Total: 105\u20ac\n\n# Ejemplo 4: Potencias\nbase = 2\nexponente = 10\nresultado = base ** exponente\nprint(f\"{base}^{exponente} = {resultado}\")  # 2^10 = 1024\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#24-numeros-decimales-float","title":"2.4. N\u00fameros Decimales (float)","text":"<p>Los float son n\u00fameros con parte decimal.</p> <pre><code># Declaraci\u00f3n de floats\nprecio = 19.99\npi = 3.14159\ntemperatura = -2.5\nporcentaje = 0.75\n\n# Tambi\u00e9n puedes escribirlos as\u00ed\nx = 1.0      # Es float aunque sea un n\u00famero \"entero\"\ny = .5       # Equivale a 0.5\nz = 2.       # Equivale a 2.0\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#notacion-cientifica","title":"Notaci\u00f3n Cient\u00edfica","text":"<pre><code># Para n\u00fameros muy grandes o muy peque\u00f1os\ndistancia_sol = 1.496e11  # 1.496 \u00d7 10\u00b9\u00b9 metros\nmasa_electron = 9.109e-31  # 9.109 \u00d7 10\u207b\u00b3\u00b9 kg\n\nprint(distancia_sol)   # 149600000000.0\nprint(masa_electron)   # 9.109e-31\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#operaciones-con-decimales","title":"Operaciones con Decimales","text":"<pre><code>a = 10.5\nb = 3.2\n\nprint(a + b)   # 13.7\nprint(a - b)   # 7.3\nprint(a * b)   # 33.6\nprint(a / b)   # 3.28125\nprint(a // b)  # 3.0 (divisi\u00f3n entera, pero sigue siendo float)\nprint(a % b)   # 0.8999999... (resto)\nprint(a ** b)  # 1613.16...\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#precision-de-los-float","title":"Precisi\u00f3n de los Float","text":"<p>\u00a1Cuidado! Los float tienen limitaciones de precisi\u00f3n:</p> <pre><code># Esto puede dar resultados inesperados\nprint(0.1 + 0.2)  # 0.30000000000000004 (no exactamente 0.3)\n\n# Para comparaciones, usa redondeo o tolerancia\nresultado = 0.1 + 0.2\nprint(round(resultado, 1) == 0.3)  # True\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejemplos-practicos_1","title":"Ejemplos Pr\u00e1cticos","text":"<pre><code># Ejemplo 1: C\u00e1lculo de IVA\nprecio_base = 100.00\niva = 0.21\nprecio_con_iva = precio_base * (1 + iva)\nprint(f\"Precio con IVA: {precio_con_iva}\u20ac\")  # 121.0\u20ac\n\n# Ejemplo 2: Conversi\u00f3n de temperatura\ncelsius = 25.0\nfahrenheit = (celsius * 9/5) + 32\nprint(f\"{celsius}\u00b0C = {fahrenheit}\u00b0F\")  # 25.0\u00b0C = 77.0\u00b0F\n\n# Ejemplo 3: Calcular promedio\nnota1 = 7.5\nnota2 = 8.0\nnota3 = 6.5\npromedio = (nota1 + nota2 + nota3) / 3\nprint(f\"Promedio: {promedio:.2f}\")  # Promedio: 7.33\n\n# Ejemplo 4: \u00c1rea de un c\u00edrculo\nradio = 5.0\npi = 3.14159\narea = pi * (radio ** 2)\nprint(f\"\u00c1rea del c\u00edrculo: {area:.2f} unidades\u00b2\")  # 78.54 unidades\u00b2\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#25-cadenas-de-texto-str","title":"2.5. Cadenas de Texto (str)","text":"<p>Las strings o cadenas son secuencias de caracteres (texto).</p>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#creacion-de-strings","title":"Creaci\u00f3n de Strings","text":"<pre><code># Con comillas dobles\nsaludo = \"Hola, mundo\"\n\n# Con comillas simples\nnombre = 'Python'\n\n# Ambas son equivalentes, elige una y s\u00e9 consistente\nmensaje1 = \"Esto es un string\"\nmensaje2 = 'Esto tambi\u00e9n es un string'\n\n# Strings vac\u00edos\nvacio = \"\"\ntambien_vacio = ''\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#strings-multilinea","title":"Strings Multil\u00ednea","text":"<pre><code># Con triple comilla\npoema = \"\"\"Caminante, no hay camino,\nse hace camino al andar.\n- Antonio Machado\"\"\"\n\nprint(poema)\n\n# Tambi\u00e9n con comillas simples\ntexto = '''Primera l\u00ednea\nSegunda l\u00ednea\nTercera l\u00ednea'''\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#comillas-dentro-de-strings","title":"Comillas Dentro de Strings","text":"<pre><code># Usar comillas diferentes\nfrase1 = \"\u00c9l dijo: 'Hola'\"\nfrase2 = 'Ella respondi\u00f3: \"Buenos d\u00edas\"'\n\n# O usar escape con \\\nfrase3 = \"\u00c9l dijo: \\\"Hola\\\"\"\nfrase4 = 'No puedo ir a Mar\u00eda\\'s house'\n\nprint(frase1)  # \u00c9l dijo: 'Hola'\nprint(frase3)  # \u00c9l dijo: \"Hola\"\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#caracteres-especiales-escape","title":"Caracteres Especiales (Escape)","text":"Secuencia Significado <code>\\n</code> Nueva l\u00ednea <code>\\t</code> Tabulaci\u00f3n <code>\\\\</code> Barra invertida <code>\\'</code> Comilla simple <code>\\\"</code> Comilla doble <pre><code># Ejemplos de escape\nprint(\"Primera l\u00ednea\\nSegunda l\u00ednea\")\n# Primera l\u00ednea\n# Segunda l\u00ednea\n\nprint(\"Columna1\\tColumna2\\tColumna3\")\n# Columna1    Columna2    Columna3\n\nprint(\"Ruta: C:\\\\Users\\\\Documentos\")\n# Ruta: C:\\Users\\Documentos\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#raw-strings","title":"Raw Strings","text":"<p>Para evitar que Python interprete los escapes:</p> <pre><code># String normal\nruta1 = \"C:\\nuevo\\texto\"  # \\n y \\t se interpretan como escape\n\n# Raw string (r al inicio)\nruta2 = r\"C:\\nuevo\\texto\"  # Se mantiene literal\nprint(ruta2)  # C:\\nuevo\\texto\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#26-operaciones-con-strings","title":"2.6. Operaciones con Strings","text":""},{"location":"conocimientos-basicos/02-variables-tipos-datos/#concatenacion-unir-strings","title":"Concatenaci\u00f3n (Unir Strings)","text":"<pre><code>nombre = \"Juan\"\napellido = \"Garc\u00eda\"\n\n# Con el operador +\nnombre_completo = nombre + \" \" + apellido\nprint(nombre_completo)  # Juan Garc\u00eda\n\n# Con coma en print (a\u00f1ade espacio autom\u00e1tico)\nprint(nombre, apellido)  # Juan Garc\u00eda\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#repeticion","title":"Repetici\u00f3n","text":"<pre><code>linea = \"-\" * 30\nprint(linea)  # ------------------------------\n\neco = \"Hola \" * 3\nprint(eco)  # Hola Hola Hola\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#longitud-de-un-string","title":"Longitud de un String","text":"<pre><code>mensaje = \"Python es genial\"\nprint(len(mensaje))  # 16 (incluye espacios)\n\npassword = \"secreto123\"\nprint(f\"Tu contrase\u00f1a tiene {len(password)} caracteres\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#acceso-a-caracteres-indexacion","title":"Acceso a Caracteres (Indexaci\u00f3n)","text":"<p>Los strings son secuencias, cada car\u00e1cter tiene una posici\u00f3n (\u00edndice):</p> <pre><code> P  y  t  h  o  n\n 0  1  2  3  4  5   (\u00edndices positivos)\n-6 -5 -4 -3 -2 -1   (\u00edndices negativos)\n</code></pre> <pre><code>texto = \"Python\"\n\n# \u00cdndices positivos (desde el inicio)\nprint(texto[0])   # P (primer car\u00e1cter)\nprint(texto[1])   # y\nprint(texto[5])   # n (\u00faltimo car\u00e1cter)\n\n# \u00cdndices negativos (desde el final)\nprint(texto[-1])  # n (\u00faltimo)\nprint(texto[-2])  # o (pen\u00faltimo)\nprint(texto[-6])  # P (primero)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#slicing-cortar-strings","title":"Slicing (Cortar Strings)","text":"<p>Sintaxis: <code>string[inicio:fin:paso]</code></p> <pre><code>texto = \"Python es genial\"\n\n# Desde \u00edndice 0 hasta 6 (sin incluir el 6)\nprint(texto[0:6])    # Python\n\n# Desde \u00edndice 7 hasta el final\nprint(texto[7:])     # es genial\n\n# Desde el inicio hasta \u00edndice 6\nprint(texto[:6])     # Python\n\n# Desde \u00edndice -6 hasta el final\nprint(texto[-6:])    # genial\n\n# Con paso\nprint(texto[::2])    # Pto sgna (cada 2 caracteres)\n\n# Invertir string\nprint(texto[::-1])   # laineg se nohtyP\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejemplos-de-slicing","title":"Ejemplos de Slicing","text":"<pre><code>fecha = \"25-01-2026\"\n\ndia = fecha[0:2]      # \"25\"\nmes = fecha[3:5]      # \"01\"\na\u00f1o = fecha[6:]       # \"2026\"\n\nprint(f\"D\u00eda: {dia}, Mes: {mes}, A\u00f1o: {a\u00f1o}\")\n\n# Obtener extensi\u00f3n de archivo\narchivo = \"documento.pdf\"\nextension = archivo[-3:]  # \"pdf\"\nnombre = archivo[:-4]     # \"documento\"\nprint(f\"Nombre: {nombre}, Extensi\u00f3n: {extension}\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#27-metodos-de-strings","title":"2.7. M\u00e9todos de Strings","text":"<p>Los strings tienen muchos m\u00e9todos (funciones) incorporados:</p>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#cambio-de-mayusculasminusculas","title":"Cambio de May\u00fasculas/Min\u00fasculas","text":"<pre><code>texto = \"Hola Mundo\"\n\nprint(texto.upper())       # HOLA MUNDO\nprint(texto.lower())       # hola mundo\nprint(texto.capitalize())  # Hola mundo (solo primera letra)\nprint(texto.title())       # Hola Mundo (cada palabra)\nprint(texto.swapcase())    # hOLA mUNDO (invierte)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#busqueda","title":"B\u00fasqueda","text":"<pre><code>frase = \"Python es un lenguaje de programaci\u00f3n\"\n\n# Encontrar posici\u00f3n\nprint(frase.find(\"es\"))       # 7 (\u00edndice donde empieza \"es\")\nprint(frase.find(\"Java\"))     # -1 (no encontrado)\n\n# Contar ocurrencias\nprint(frase.count(\"a\"))       # 3\n\n# Verificar inicio/fin\nprint(frase.startswith(\"Python\"))  # True\nprint(frase.endswith(\"ci\u00f3n\"))      # True\n\n# Verificar contenido\nprint(\"Python\" in frase)      # True\nprint(\"Java\" in frase)        # False\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#modificacion","title":"Modificaci\u00f3n","text":"<pre><code># Reemplazar\ntexto = \"Hola Mundo\"\nprint(texto.replace(\"Mundo\", \"Python\"))  # Hola Python\n\n# Eliminar espacios\nmensaje = \"   Hola   \"\nprint(mensaje.strip())   # \"Hola\" (elimina espacios inicio/fin)\nprint(mensaje.lstrip())  # \"Hola   \" (solo izquierda)\nprint(mensaje.rstrip())  # \"   Hola\" (solo derecha)\n\n# Tambi\u00e9n elimina otros caracteres\nurl = \"...www.ejemplo.com...\"\nprint(url.strip(\".\"))  # www.ejemplo.com\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#dividir-y-unir","title":"Dividir y Unir","text":"<pre><code># split() - Dividir string en lista\nfrase = \"Python es genial\"\npalabras = frase.split()  # Divide por espacios\nprint(palabras)  # ['Python', 'es', 'genial']\n\nfecha = \"25-01-2026\"\npartes = fecha.split(\"-\")\nprint(partes)  # ['25', '01', '2026']\n\n# join() - Unir lista en string\npalabras = ['Python', 'es', 'genial']\nfrase = \" \".join(palabras)\nprint(frase)  # Python es genial\n\nnumeros = ['1', '2', '3', '4']\nprint(\"-\".join(numeros))  # 1-2-3-4\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#verificacion","title":"Verificaci\u00f3n","text":"<pre><code># Verificar tipo de contenido\nprint(\"12345\".isdigit())     # True (solo d\u00edgitos)\nprint(\"Python\".isalpha())    # True (solo letras)\nprint(\"Python3\".isalnum())   # True (letras y n\u00fameros)\nprint(\"   \".isspace())       # True (solo espacios)\nprint(\"HOLA\".isupper())      # True (todo may\u00fasculas)\nprint(\"hola\".islower())      # True (todo min\u00fasculas)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#28-formateo-de-strings","title":"2.8. Formateo de Strings","text":""},{"location":"conocimientos-basicos/02-variables-tipos-datos/#f-strings-recomendado-python-36","title":"f-strings (Recomendado - Python 3.6+)","text":"<p>La forma m\u00e1s moderna y legible:</p> <pre><code>nombre = \"Ana\"\nedad = 25\n\n# Interpolaci\u00f3n b\u00e1sica\nprint(f\"Me llamo {nombre} y tengo {edad} a\u00f1os\")\n# Me llamo Ana y tengo 25 a\u00f1os\n\n# Expresiones dentro de las llaves\nprint(f\"En 5 a\u00f1os tendr\u00e9 {edad + 5} a\u00f1os\")\n# En 5 a\u00f1os tendr\u00e9 30 a\u00f1os\n\n# Llamar m\u00e9todos\nprint(f\"Mi nombre en may\u00fasculas: {nombre.upper()}\")\n# Mi nombre en may\u00fasculas: ANA\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#formateo-de-numeros","title":"Formateo de N\u00fameros","text":"<pre><code>precio = 1234.5678\nporcentaje = 0.756\n\n# Decimales\nprint(f\"Precio: {precio:.2f}\u20ac\")  # Precio: 1234.57\u20ac\n\n# Separador de miles\nprint(f\"Precio: {precio:,.2f}\u20ac\")  # Precio: 1,234.57\u20ac\n\n# Porcentaje\nprint(f\"Porcentaje: {porcentaje:.1%}\")  # Porcentaje: 75.6%\n\n# Alineaci\u00f3n y relleno\nnumero = 42\nprint(f\"{numero:05d}\")   # 00042 (rellena con ceros)\nprint(f\"{numero:&gt;10}\")   # \"        42\" (alinea derecha)\nprint(f\"{numero:&lt;10}\")   # \"42        \" (alinea izquierda)\nprint(f\"{numero:^10}\")   # \"    42    \" (centrado)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#metodo-format","title":"M\u00e9todo format()","text":"<pre><code># Alternativa a f-strings\nnombre = \"Carlos\"\nedad = 30\n\nprint(\"Me llamo {} y tengo {} a\u00f1os\".format(nombre, edad))\nprint(\"Me llamo {0} y tengo {1} a\u00f1os\".format(nombre, edad))\nprint(\"Me llamo {n} y tengo {e} a\u00f1os\".format(n=nombre, e=edad))\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#operador-estilo-antiguo","title":"Operador % (Estilo Antiguo)","text":"<pre><code># Menos recomendado, pero a\u00fan se ve en c\u00f3digo antiguo\nnombre = \"Mar\u00eda\"\nedad = 28\nprint(\"Me llamo %s y tengo %d a\u00f1os\" % (nombre, edad))\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#29-booleanos-bool","title":"2.9. Booleanos (bool)","text":"<p>Los booleanos solo tienen dos valores: <code>True</code> (verdadero) o <code>False</code> (falso).</p> <pre><code>es_mayor = True\ntiene_permiso = False\n\nprint(type(es_mayor))  # &lt;class 'bool'&gt;\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#valores-booleanos-en-contexto","title":"Valores Booleanos en Contexto","text":"<p>En Python, muchos valores se pueden evaluar como booleanos:</p> <pre><code># Valores \"falsos\" (Falsy)\nprint(bool(0))        # False\nprint(bool(0.0))      # False\nprint(bool(\"\"))       # False (string vac\u00edo)\nprint(bool([]))       # False (lista vac\u00eda)\nprint(bool(None))     # False\n\n# Valores \"verdaderos\" (Truthy)\nprint(bool(1))        # True\nprint(bool(-5))       # True (cualquier n\u00famero no cero)\nprint(bool(\"Hola\"))   # True (string no vac\u00edo)\nprint(bool([1, 2]))   # True (lista no vac\u00eda)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#operadores-de-comparacion","title":"Operadores de Comparaci\u00f3n","text":"<p>Devuelven valores booleanos:</p> <pre><code>x = 10\ny = 5\n\nprint(x == y)   # False (igual a)\nprint(x != y)   # True (diferente de)\nprint(x &gt; y)    # True (mayor que)\nprint(x &lt; y)    # False (menor que)\nprint(x &gt;= y)   # True (mayor o igual)\nprint(x &lt;= y)   # False (menor o igual)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#operadores-logicos","title":"Operadores L\u00f3gicos","text":"<pre><code>a = True\nb = False\n\nprint(a and b)  # False (Y l\u00f3gico: ambos deben ser True)\nprint(a or b)   # True (O l\u00f3gico: al menos uno True)\nprint(not a)    # False (negaci\u00f3n)\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#210-none-valor-nulo","title":"2.10. None (Valor Nulo)","text":"<p><code>None</code> representa la ausencia de valor:</p> <pre><code>resultado = None\n\nprint(resultado)        # None\nprint(type(resultado))  # &lt;class 'NoneType'&gt;\nprint(resultado is None)  # True\n\n# Uso com\u00fan: valor inicial antes de asignar\nusuario = None\n\n# Despu\u00e9s de alg\u00fan proceso\nusuario = \"admin\"\n\n# Verificar si tiene valor\nif usuario is not None:\n    print(f\"Usuario: {usuario}\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#211-conversion-de-tipos","title":"2.11. Conversi\u00f3n de Tipos","text":"<p>A veces necesitas convertir entre tipos de datos.</p>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#int-convertir-a-entero","title":"int() - Convertir a Entero","text":"<pre><code># Desde string\nedad_texto = \"25\"\nedad_numero = int(edad_texto)\nprint(edad_numero + 5)  # 30\n\n# Desde float (trunca decimales)\nprecio = 19.99\nprecio_entero = int(precio)\nprint(precio_entero)  # 19\n\n# Errores comunes\n# int(\"Hola\")  # Error: no se puede convertir texto\n# int(\"19.99\")  # Error: tiene decimal\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#float-convertir-a-decimal","title":"float() - Convertir a Decimal","text":"<pre><code># Desde string\ntemperatura = \"36.5\"\ntemp_numero = float(temperatura)\nprint(temp_numero + 0.5)  # 37.0\n\n# Desde entero\nnumero = 42\ndecimal = float(numero)\nprint(decimal)  # 42.0\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#str-convertir-a-texto","title":"str() - Convertir a Texto","text":"<pre><code>edad = 25\nedad_texto = str(edad)\nprint(\"Tengo \" + edad_texto + \" a\u00f1os\")\n\nprecio = 19.99\nprint(\"Precio: \" + str(precio) + \"\u20ac\")\n\n# \u00datil para concatenar con +\nnumero = 100\n# print(\"Valor: \" + numero)  # Error!\nprint(\"Valor: \" + str(numero))  # Correcto\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#bool-convertir-a-booleano","title":"bool() - Convertir a Booleano","text":"<pre><code>print(bool(1))      # True\nprint(bool(0))      # False\nprint(bool(\"S\u00ed\"))   # True\nprint(bool(\"\"))     # False\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejemplo-practico-calculadora","title":"Ejemplo Pr\u00e1ctico: Calculadora","text":"<pre><code>print(\"=== CALCULADORA ===\")\nprint()\n\n# input() siempre devuelve string\nnum1_texto = input(\"Primer n\u00famero: \")\nnum2_texto = input(\"Segundo n\u00famero: \")\n\n# Convertir a n\u00fameros\nnum1 = float(num1_texto)\nnum2 = float(num2_texto)\n\n# Ahora podemos hacer operaciones\nsuma = num1 + num2\nresta = num1 - num2\nmultiplicacion = num1 * num2\ndivision = num1 / num2\n\nprint()\nprint(f\"{num1} + {num2} = {suma}\")\nprint(f\"{num1} - {num2} = {resta}\")\nprint(f\"{num1} \u00d7 {num2} = {multiplicacion}\")\nprint(f\"{num1} \u00f7 {num2} = {division:.2f}\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#212-ejercicios-practicos","title":"2.12. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejercicio-1-intercambio-de-variables","title":"Ejercicio 1: Intercambio de Variables","text":"<pre><code># Intercambia los valores de a y b\na = 10\nb = 20\n\nprint(f\"Antes: a={a}, b={b}\")\n\n# Tu c\u00f3digo aqu\u00ed\na, b = b, a\n\nprint(f\"Despu\u00e9s: a={a}, b={b}\")\n# Debe mostrar: Despu\u00e9s: a=20, b=10\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejercicio-2-calculadora-de-propina","title":"Ejercicio 2: Calculadora de Propina","text":"<pre><code>print(\"=== Calculadora de Propina ===\")\n\ncuenta = float(input(\"Importe de la cuenta: \"))\nporcentaje_propina = float(input(\"Porcentaje de propina: \"))\n\npropina = cuenta * (porcentaje_propina / 100)\ntotal = cuenta + propina\n\nprint()\nprint(f\"Cuenta: {cuenta:.2f}\u20ac\")\nprint(f\"Propina ({porcentaje_propina}%): {propina:.2f}\u20ac\")\nprint(f\"Total a pagar: {total:.2f}\u20ac\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejercicio-3-extractor-de-informacion","title":"Ejercicio 3: Extractor de Informaci\u00f3n","text":"<pre><code>email = \"usuario@dominio.com\"\n\n# Extraer usuario y dominio\narroba_pos = email.find(\"@\")\nusuario = email[:arroba_pos]\ndominio = email[arroba_pos + 1:]\n\nprint(f\"Email: {email}\")\nprint(f\"Usuario: {usuario}\")\nprint(f\"Dominio: {dominio}\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejercicio-4-formateo-de-datos","title":"Ejercicio 4: Formateo de Datos","text":"<pre><code>nombre = \"ana garc\u00eda\"\nedad = 28\nsalario = 32500.50\n\n# Formatear correctamente\nnombre_formateado = nombre.title()\nprint(f\"Nombre: {nombre_formateado}\")\nprint(f\"Edad: {edad} a\u00f1os\")\nprint(f\"Salario: {salario:,.2f}\u20ac\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#ejercicio-5-validador-de-contrasena","title":"Ejercicio 5: Validador de Contrase\u00f1a","text":"<pre><code>password = input(\"Introduce una contrase\u00f1a: \")\n\nlongitud = len(password)\ntiene_numeros = any(c.isdigit() for c in password)\ntiene_letras = any(c.isalpha() for c in password)\n\nprint()\nprint(f\"Longitud: {longitud} caracteres\")\nprint(f\"Contiene n\u00fameros: {tiene_numeros}\")\nprint(f\"Contiene letras: {tiene_letras}\")\n\nif longitud &gt;= 8 and tiene_numeros and tiene_letras:\n    print(\"\u2713 Contrase\u00f1a v\u00e1lida\")\nelse:\n    print(\"\u2717 Contrase\u00f1a no v\u00e1lida\")\n</code></pre>"},{"location":"conocimientos-basicos/02-variables-tipos-datos/#213-resumen","title":"2.13. Resumen","text":"Tipo Descripci\u00f3n Ejemplo <code>int</code> N\u00fameros enteros <code>42</code>, <code>-7</code> <code>float</code> N\u00fameros decimales <code>3.14</code>, <code>-0.5</code> <code>str</code> Texto <code>\"Hola\"</code>, <code>'Python'</code> <code>bool</code> Verdadero/Falso <code>True</code>, <code>False</code> <code>None</code> Valor nulo <code>None</code> Conversi\u00f3n Funci\u00f3n A entero <code>int()</code> A decimal <code>float()</code> A texto <code>str()</code> A booleano <code>bool()</code> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/03-operadores/","title":"\u2795 Unidad 3. Operadores","text":"<p>Los operadores son s\u00edmbolos que realizan operaciones sobre valores y variables. Python tiene varios tipos de operadores que veremos en detalle.</p>"},{"location":"conocimientos-basicos/03-operadores/#31-operadores-aritmeticos","title":"3.1. Operadores Aritm\u00e9ticos","text":"<p>Realizan operaciones matem\u00e1ticas b\u00e1sicas:</p> Operador Nombre Ejemplo Resultado <code>+</code> Suma <code>5 + 3</code> <code>8</code> <code>-</code> Resta <code>5 - 3</code> <code>2</code> <code>*</code> Multiplicaci\u00f3n <code>5 * 3</code> <code>15</code> <code>/</code> Divisi\u00f3n <code>5 / 3</code> <code>1.666...</code> <code>//</code> Divisi\u00f3n entera <code>5 // 3</code> <code>1</code> <code>%</code> M\u00f3dulo (resto) <code>5 % 3</code> <code>2</code> <code>**</code> Potencia <code>5 ** 3</code> <code>125</code>"},{"location":"conocimientos-basicos/03-operadores/#ejemplos-detallados","title":"Ejemplos Detallados","text":"<pre><code># Suma\nprint(10 + 5)      # 15\nprint(3.5 + 2.5)   # 6.0\nprint(-5 + 10)     # 5\n\n# Resta\nprint(10 - 5)      # 5\nprint(3 - 8)       # -5\n\n# Multiplicaci\u00f3n\nprint(4 * 7)       # 28\nprint(2.5 * 4)     # 10.0\nprint(-3 * 5)      # -15\n\n# Divisi\u00f3n (siempre devuelve float)\nprint(10 / 2)      # 5.0\nprint(10 / 3)      # 3.3333...\nprint(7 / 2)       # 3.5\n\n# Divisi\u00f3n entera (descarta decimales)\nprint(10 // 3)     # 3\nprint(7 // 2)      # 3\nprint(-7 // 2)     # -4 (redondea hacia abajo)\n\n# M\u00f3dulo (resto de la divisi\u00f3n)\nprint(10 % 3)      # 1 (10 = 3*3 + 1)\nprint(15 % 5)      # 0 (divisi\u00f3n exacta)\nprint(7 % 2)       # 1 (impar)\nprint(8 % 2)       # 0 (par)\n\n# Potencia\nprint(2 ** 3)      # 8 (2\u00b3)\nprint(5 ** 2)      # 25 (5\u00b2)\nprint(4 ** 0.5)    # 2.0 (ra\u00edz cuadrada)\nprint(27 ** (1/3)) # 3.0 (ra\u00edz c\u00fabica)\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#usos-practicos-del-modulo","title":"Usos Pr\u00e1cticos del M\u00f3dulo","text":"<pre><code># Verificar si un n\u00famero es par o impar\nnumero = 17\nif numero % 2 == 0:\n    print(f\"{numero} es par\")\nelse:\n    print(f\"{numero} es impar\")\n# 17 es impar\n\n# Obtener el \u00faltimo d\u00edgito de un n\u00famero\nnumero = 12345\nultimo_digito = numero % 10\nprint(f\"\u00daltimo d\u00edgito de {numero}: {ultimo_digito}\")  # 5\n\n# Ciclos (volver al inicio despu\u00e9s de N)\n# \u00datil para \u00edndices circulares\nposicion = 0\ntotal_elementos = 5\n\nfor i in range(10):\n    indice = i % total_elementos\n    print(f\"Paso {i}: \u00edndice {indice}\")\n# 0,1,2,3,4,0,1,2,3,4\n\n# Convertir segundos a horas:minutos:segundos\nsegundos_totales = 3725\n\nhoras = segundos_totales // 3600\nminutos = (segundos_totales % 3600) // 60\nsegundos = segundos_totales % 60\n\nprint(f\"{segundos_totales} segundos = {horas}h {minutos}m {segundos}s\")\n# 3725 segundos = 1h 2m 5s\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#precedencia-de-operadores","title":"Precedencia de Operadores","text":"<p>Los operadores siguen un orden de evaluaci\u00f3n (de mayor a menor precedencia):</p> <ol> <li><code>**</code> (potencia)</li> <li><code>*</code>, <code>/</code>, <code>//</code>, <code>%</code></li> <li><code>+</code>, <code>-</code></li> </ol> <pre><code># Ejemplos de precedencia\nprint(2 + 3 * 4)       # 14 (no 20) - multiplicaci\u00f3n primero\nprint((2 + 3) * 4)     # 20 - par\u00e9ntesis primero\nprint(2 ** 3 ** 2)     # 512 (2^9) - potencia es de derecha a izquierda\nprint(10 - 3 - 2)      # 5 - de izquierda a derecha\n\n# Usar par\u00e9ntesis para claridad\nresultado = ((5 + 3) * 2) / 4\nprint(resultado)  # 4.0\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#32-operadores-de-comparacion","title":"3.2. Operadores de Comparaci\u00f3n","text":"<p>Comparan dos valores y devuelven <code>True</code> o <code>False</code>:</p> Operador Significado Ejemplo Resultado <code>==</code> Igual a <code>5 == 5</code> <code>True</code> <code>!=</code> Diferente de <code>5 != 3</code> <code>True</code> <code>&gt;</code> Mayor que <code>5 &gt; 3</code> <code>True</code> <code>&lt;</code> Menor que <code>5 &lt; 3</code> <code>False</code> <code>&gt;=</code> Mayor o igual <code>5 &gt;= 5</code> <code>True</code> <code>&lt;=</code> Menor o igual <code>5 &lt;= 3</code> <code>False</code>"},{"location":"conocimientos-basicos/03-operadores/#ejemplos-con-numeros","title":"Ejemplos con N\u00fameros","text":"<pre><code>a = 10\nb = 5\n\nprint(a == b)    # False (\u00bfson iguales?)\nprint(a != b)    # True (\u00bfson diferentes?)\nprint(a &gt; b)     # True (\u00bfa es mayor que b?)\nprint(a &lt; b)     # False (\u00bfa es menor que b?)\nprint(a &gt;= b)    # True (\u00bfa es mayor o igual que b?)\nprint(a &lt;= b)    # False (\u00bfa es menor o igual que b?)\n\n# Comparaciones encadenadas\nx = 15\nprint(10 &lt; x &lt; 20)     # True (x est\u00e1 entre 10 y 20)\nprint(0 &lt;= x &lt;= 100)   # True (x est\u00e1 entre 0 y 100)\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#comparacion-de-strings","title":"Comparaci\u00f3n de Strings","text":"<pre><code># Comparaci\u00f3n alfab\u00e9tica (orden lexicogr\u00e1fico)\nprint(\"abc\" == \"abc\")    # True\nprint(\"abc\" == \"ABC\")    # False (may\u00fasculas importan)\nprint(\"abc\" &lt; \"abd\")     # True (c &lt; d)\nprint(\"apple\" &lt; \"banana\")  # True (a &lt; b)\n\n# Comparar ignorando may\u00fasculas\nnombre1 = \"Ana\"\nnombre2 = \"ana\"\nprint(nombre1.lower() == nombre2.lower())  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#comparacion-de-otros-tipos","title":"Comparaci\u00f3n de Otros Tipos","text":"<pre><code># Listas\nprint([1, 2] == [1, 2])    # True\nprint([1, 2] == [2, 1])    # False (orden importa)\n\n# None\nx = None\nprint(x == None)    # True (funciona, pero...)\nprint(x is None)    # True (recomendado)\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#errores-comunes","title":"Errores Comunes","text":"<pre><code># MAL: usar = en lugar de ==\n# if x = 5:  # Error de sintaxis\n\n# BIEN: usar ==\nx = 5\nif x == 5:\n    print(\"x es 5\")\n\n# Cuidado con los floats\nprint(0.1 + 0.2 == 0.3)  # False (problema de precisi\u00f3n)\nprint(abs((0.1 + 0.2) - 0.3) &lt; 0.0001)  # True (comparar con tolerancia)\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#33-operadores-logicos","title":"3.3. Operadores L\u00f3gicos","text":"<p>Combinan expresiones booleanas:</p> Operador Descripci\u00f3n Ejemplo <code>and</code> Verdadero si ambos son verdaderos <code>True and True</code> \u2192 <code>True</code> <code>or</code> Verdadero si al menos uno es verdadero <code>True or False</code> \u2192 <code>True</code> <code>not</code> Invierte el valor <code>not True</code> \u2192 <code>False</code>"},{"location":"conocimientos-basicos/03-operadores/#tablas-de-verdad","title":"Tablas de Verdad","text":"<p>AND (Y l\u00f3gico):</p> A B A and B True True True True False False False True False False False False <p>OR (O l\u00f3gico):</p> A B A or B True True True True False True False True True False False False <p>NOT (Negaci\u00f3n):</p> A not A True False False True"},{"location":"conocimientos-basicos/03-operadores/#ejemplos-practicos","title":"Ejemplos Pr\u00e1cticos","text":"<pre><code>edad = 25\ntiene_dni = True\ntiene_dinero = True\n\n# AND: todas las condiciones deben cumplirse\npuede_comprar_alcohol = edad &gt;= 18 and tiene_dni\nprint(puede_comprar_alcohol)  # True\n\n# OR: al menos una condici\u00f3n debe cumplirse\nes_fin_de_semana = False\nes_festivo = True\ndia_libre = es_fin_de_semana or es_festivo\nprint(dia_libre)  # True\n\n# NOT: invierte la condici\u00f3n\nesta_cerrado = False\nesta_abierto = not esta_cerrado\nprint(esta_abierto)  # True\n\n# Combinaciones\npuede_entrar = edad &gt;= 18 and tiene_dni and not esta_cerrado\nprint(puede_entrar)  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#validacion-de-rangos","title":"Validaci\u00f3n de Rangos","text":"<pre><code># Verificar si un n\u00famero est\u00e1 en un rango\nnota = 7.5\n\nes_aprobado = nota &gt;= 5\nes_notable = nota &gt;= 7 and nota &lt; 9\nes_sobresaliente = nota &gt;= 9\n\nprint(f\"Nota: {nota}\")\nprint(f\"Aprobado: {es_aprobado}\")        # True\nprint(f\"Notable: {es_notable}\")          # True\nprint(f\"Sobresaliente: {es_sobresaliente}\")  # False\n\n# Forma m\u00e1s elegante con comparaciones encadenadas\nes_notable_v2 = 7 &lt;= nota &lt; 9\nprint(f\"Notable (v2): {es_notable_v2}\")  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#validacion-de-formularios","title":"Validaci\u00f3n de Formularios","text":"<pre><code>nombre = \"Juan\"\nemail = \"juan@email.com\"\nedad = 25\nacepta_terminos = True\n\n# Verificar que todos los campos est\u00e1n completos\nnombre_valido = len(nombre) &gt; 0\nemail_valido = \"@\" in email and \".\" in email\nedad_valida = edad &gt;= 18\n\n# Formulario completo\nformulario_valido = nombre_valido and email_valido and edad_valida and acepta_terminos\n\nprint(f\"Nombre v\u00e1lido: {nombre_valido}\")        # True\nprint(f\"Email v\u00e1lido: {email_valido}\")          # True\nprint(f\"Edad v\u00e1lida: {edad_valida}\")            # True\nprint(f\"Acepta t\u00e9rminos: {acepta_terminos}\")    # True\nprint(f\"Formulario v\u00e1lido: {formulario_valido}\")  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#evaluacion-de-cortocircuito","title":"Evaluaci\u00f3n de Cortocircuito","text":"<p>Python eval\u00faa expresiones de forma \"perezosa\":</p> <pre><code># Con AND: si el primero es False, no eval\u00faa el segundo\n# Con OR: si el primero es True, no eval\u00faa el segundo\n\ndef funcion_costosa():\n    print(\"Ejecutando funci\u00f3n costosa...\")\n    return True\n\n# No ejecuta funcion_costosa() porque False and X siempre es False\nresultado = False and funcion_costosa()\nprint(resultado)  # False (no imprime el mensaje)\n\n# S\u00ed ejecuta porque necesita evaluar el segundo\nresultado = True and funcion_costosa()\nprint(resultado)  # Imprime mensaje, luego True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#valores-truthy-y-falsy","title":"Valores Truthy y Falsy","text":"<pre><code># Valores que Python considera False (falsy)\nprint(bool(0))        # False\nprint(bool(0.0))      # False\nprint(bool(\"\"))       # False\nprint(bool([]))       # False\nprint(bool(None))     # False\n\n# Uso pr\u00e1ctico\nnombre = \"\"\nif nombre:\n    print(f\"Hola, {nombre}\")\nelse:\n    print(\"No has introducido nombre\")\n# No has introducido nombre\n\n# Valor por defecto con or\nnombre = \"\" or \"An\u00f3nimo\"\nprint(nombre)  # An\u00f3nimo\n\nusuario = None\nnombre_mostrar = usuario or \"Invitado\"\nprint(nombre_mostrar)  # Invitado\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#34-operadores-de-asignacion","title":"3.4. Operadores de Asignaci\u00f3n","text":"<p>Asignan valores a variables, con posibles operaciones:</p> Operador Equivalente a Ejemplo <code>=</code> Asignaci\u00f3n simple <code>x = 5</code> <code>+=</code> <code>x = x + valor</code> <code>x += 3</code> <code>-=</code> <code>x = x - valor</code> <code>x -= 3</code> <code>*=</code> <code>x = x * valor</code> <code>x *= 3</code> <code>/=</code> <code>x = x / valor</code> <code>x /= 3</code> <code>//=</code> <code>x = x // valor</code> <code>x //= 3</code> <code>%=</code> <code>x = x % valor</code> <code>x %= 3</code> <code>**=</code> <code>x = x ** valor</code> <code>x **= 3</code>"},{"location":"conocimientos-basicos/03-operadores/#ejemplos","title":"Ejemplos","text":"<pre><code># Asignaci\u00f3n simple\nx = 10\nprint(f\"x = {x}\")  # x = 10\n\n# Suma y asignaci\u00f3n\nx += 5  # equivale a x = x + 5\nprint(f\"x += 5 \u2192 {x}\")  # x = 15\n\n# Resta y asignaci\u00f3n\nx -= 3  # equivale a x = x - 3\nprint(f\"x -= 3 \u2192 {x}\")  # x = 12\n\n# Multiplicaci\u00f3n y asignaci\u00f3n\nx *= 2  # equivale a x = x * 2\nprint(f\"x *= 2 \u2192 {x}\")  # x = 24\n\n# Divisi\u00f3n y asignaci\u00f3n\nx /= 4  # equivale a x = x / 4\nprint(f\"x /= 4 \u2192 {x}\")  # x = 6.0\n\n# Divisi\u00f3n entera y asignaci\u00f3n\nx = 17\nx //= 3  # equivale a x = x // 3\nprint(f\"x //= 3 \u2192 {x}\")  # x = 5\n\n# M\u00f3dulo y asignaci\u00f3n\nx = 17\nx %= 5  # equivale a x = x % 5\nprint(f\"x %= 5 \u2192 {x}\")  # x = 2\n\n# Potencia y asignaci\u00f3n\nx = 2\nx **= 4  # equivale a x = x ** 4\nprint(f\"x **= 4 \u2192 {x}\")  # x = 16\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejemplo-contador","title":"Ejemplo: Contador","text":"<pre><code>contador = 0\n\ncontador += 1  # incrementar\nprint(contador)  # 1\n\ncontador += 1\nprint(contador)  # 2\n\ncontador += 1\nprint(contador)  # 3\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejemplo-acumulador","title":"Ejemplo: Acumulador","text":"<pre><code># Calcular suma de n\u00fameros\nsuma = 0\n\nsuma += 10\nsuma += 20\nsuma += 30\nsuma += 40\n\nprint(f\"Suma total: {suma}\")  # 100\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#con-strings","title":"Con Strings","text":"<pre><code>mensaje = \"Hola\"\nmensaje += \" \"\nmensaje += \"Mundo\"\nprint(mensaje)  # Hola Mundo\n\n# Construir una frase\nfrase = \"\"\nfrase += \"Python \"\nfrase += \"es \"\nfrase += \"genial\"\nprint(frase)  # Python es genial\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#35-operadores-de-identidad","title":"3.5. Operadores de Identidad","text":"<p>Verifican si dos variables apuntan al mismo objeto en memoria:</p> Operador Descripci\u00f3n <code>is</code> True si son el mismo objeto <code>is not</code> True si son objetos diferentes <pre><code># Comparar con None (uso m\u00e1s com\u00fan)\nx = None\nprint(x is None)      # True\nprint(x is not None)  # False\n\n# Diferencia entre == y is\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(a == b)    # True (mismo contenido)\nprint(a is b)    # False (objetos diferentes)\nprint(a is c)    # True (mismo objeto)\n\n# Verificar el id (direcci\u00f3n en memoria)\nprint(id(a))  # Por ejemplo: 140234567890\nprint(id(b))  # Diferente: 140234567891\nprint(id(c))  # Igual a id(a): 140234567890\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#cuando-usar-is-vs","title":"Cu\u00e1ndo Usar is vs ==","text":"<pre><code># Usar 'is' para comparar con None, True, False\nvalor = None\nif valor is None:\n    print(\"No tiene valor\")\n\n# Usar '==' para comparar contenido\nlista1 = [1, 2, 3]\nlista2 = [1, 2, 3]\nif lista1 == lista2:\n    print(\"Las listas tienen el mismo contenido\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#nota-sobre-enteros-pequenos","title":"Nota sobre Enteros Peque\u00f1os","text":"<p>Python optimiza los enteros peque\u00f1os (-5 a 256):</p> <pre><code>a = 100\nb = 100\nprint(a is b)  # True (Python reutiliza el objeto)\n\nc = 1000\nd = 1000\nprint(c is d)  # Puede ser False (objetos diferentes)\n\n# SIEMPRE usa == para comparar valores\nprint(c == d)  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#36-operadores-de-pertenencia","title":"3.6. Operadores de Pertenencia","text":"<p>Verifican si un elemento est\u00e1 dentro de una secuencia:</p> Operador Descripci\u00f3n <code>in</code> True si el elemento est\u00e1 en la secuencia <code>not in</code> True si el elemento NO est\u00e1 en la secuencia"},{"location":"conocimientos-basicos/03-operadores/#en-strings","title":"En Strings","text":"<pre><code>texto = \"Hola Mundo\"\n\nprint(\"Hola\" in texto)      # True\nprint(\"hola\" in texto)      # False (may\u00fasculas importan)\nprint(\"Python\" in texto)    # False\nprint(\"X\" not in texto)     # True\n\n# Verificar vocales\nletra = \"e\"\nvocales = \"aeiouAEIOU\"\nes_vocal = letra in vocales\nprint(f\"'{letra}' es vocal: {es_vocal}\")  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#en-listas","title":"En Listas","text":"<pre><code>frutas = [\"manzana\", \"naranja\", \"pl\u00e1tano\"]\n\nprint(\"manzana\" in frutas)     # True\nprint(\"uva\" in frutas)         # False\nprint(\"pera\" not in frutas)    # True\n\n# Verificar si el usuario eligi\u00f3 una opci\u00f3n v\u00e1lida\nopciones_validas = [1, 2, 3, 4]\nopcion_usuario = 2\nif opcion_usuario in opciones_validas:\n    print(\"Opci\u00f3n v\u00e1lida\")\nelse:\n    print(\"Opci\u00f3n no v\u00e1lida\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#en-diccionarios","title":"En Diccionarios","text":"<pre><code># 'in' verifica las CLAVES, no los valores\npersona = {\"nombre\": \"Ana\", \"edad\": 30, \"ciudad\": \"Madrid\"}\n\nprint(\"nombre\" in persona)    # True (es una clave)\nprint(\"Ana\" in persona)       # False (es un valor, no una clave)\nprint(\"apellido\" in persona)  # False\n\n# Para verificar valores\nprint(\"Ana\" in persona.values())  # True\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejemplo-validacion-de-input","title":"Ejemplo: Validaci\u00f3n de Input","text":"<pre><code>respuestas_validas = [\"s\", \"si\", \"s\u00ed\", \"n\", \"no\"]\n\nrespuesta = input(\"\u00bfDesea continuar? (s/n): \").lower()\n\nif respuesta in respuestas_validas:\n    print(\"Respuesta v\u00e1lida\")\nelse:\n    print(\"Por favor, responda 's' o 'n'\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#37-operadores-a-nivel-de-bits-bitwise","title":"3.7. Operadores a Nivel de Bits (Bitwise)","text":"<p>Operan directamente sobre los bits de los n\u00fameros. \u00datiles en programaci\u00f3n de bajo nivel:</p> Operador Nombre Descripci\u00f3n <code>&amp;</code> AND 1 si ambos bits son 1 <code>\\|</code> OR 1 si al menos un bit es 1 <code>^</code> XOR 1 si los bits son diferentes <code>~</code> NOT Invierte todos los bits <code>&lt;&lt;</code> Shift izquierda Desplaza bits a la izquierda <code>&gt;&gt;</code> Shift derecha Desplaza bits a la derecha <pre><code>a = 5   # En binario: 0101\nb = 3   # En binario: 0011\n\nprint(f\"a = {a} ({bin(a)})\")  # a = 5 (0b101)\nprint(f\"b = {b} ({bin(b)})\")  # b = 3 (0b11)\n\n# AND: 0101 &amp; 0011 = 0001\nprint(f\"a &amp; b = {a &amp; b} ({bin(a &amp; b)})\")  # 1 (0b1)\n\n# OR: 0101 | 0011 = 0111\nprint(f\"a | b = {a | b} ({bin(a | b)})\")  # 7 (0b111)\n\n# XOR: 0101 ^ 0011 = 0110\nprint(f\"a ^ b = {a ^ b} ({bin(a ^ b)})\")  # 6 (0b110)\n\n# Shift izquierda (multiplica por 2^n)\nprint(f\"a &lt;&lt; 1 = {a &lt;&lt; 1}\")  # 10 (5 * 2)\nprint(f\"a &lt;&lt; 2 = {a &lt;&lt; 2}\")  # 20 (5 * 4)\n\n# Shift derecha (divide por 2^n)\nprint(f\"a &gt;&gt; 1 = {a &gt;&gt; 1}\")  # 2 (5 // 2)\n</code></pre> <p>Nota: Los operadores bitwise no son comunes en IA/ML, pero pueden aparecer en optimizaciones de c\u00f3digo.</p>"},{"location":"conocimientos-basicos/03-operadores/#38-resumen-de-precedencia-de-operadores","title":"3.8. Resumen de Precedencia de Operadores","text":"<p>De mayor a menor precedencia:</p> Precedencia Operadores 1 (m\u00e1s alta) <code>()</code> Par\u00e9ntesis 2 <code>**</code> Potencia 3 <code>+x</code>, <code>-x</code>, <code>~x</code> Unarios 4 <code>*</code>, <code>/</code>, <code>//</code>, <code>%</code> 5 <code>+</code>, <code>-</code> 6 <code>&lt;&lt;</code>, <code>&gt;&gt;</code> 7 <code>&amp;</code> 8 <code>^</code> 9 <code>\\|</code> 10 <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>is</code>, <code>in</code> 11 <code>not</code> 12 <code>and</code> 13 (m\u00e1s baja) <code>or</code> <p>Consejo: Usa par\u00e9ntesis para hacer tu c\u00f3digo m\u00e1s legible:</p> <pre><code># Dif\u00edcil de leer\nresultado = 2 + 3 * 4 ** 2 / 8 - 1\n\n# M\u00e1s claro\nresultado = 2 + ((3 * (4 ** 2)) / 8) - 1\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#39-ejercicios-practicos","title":"3.9. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/03-operadores/#ejercicio-1-calculadora-completa","title":"Ejercicio 1: Calculadora Completa","text":"<pre><code>print(\"=== CALCULADORA ===\")\nprint()\n\nnum1 = float(input(\"Primer n\u00famero: \"))\nnum2 = float(input(\"Segundo n\u00famero: \"))\n\nprint()\nprint(f\"{num1} + {num2} = {num1 + num2}\")\nprint(f\"{num1} - {num2} = {num1 - num2}\")\nprint(f\"{num1} \u00d7 {num2} = {num1 * num2}\")\nprint(f\"{num1} \u00f7 {num2} = {num1 / num2:.4f}\")\nprint(f\"{num1} // {num2} = {num1 // num2}\")\nprint(f\"{num1} % {num2} = {num1 % num2}\")\nprint(f\"{num1} ^ {num2} = {num1 ** num2}\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejercicio-2-verificador-de-ano-bisiesto","title":"Ejercicio 2: Verificador de A\u00f1o Bisiesto","text":"<pre><code>a\u00f1o = int(input(\"Introduce un a\u00f1o: \"))\n\n# Un a\u00f1o es bisiesto si:\n# - Es divisible por 4 Y no es divisible por 100\n# - O es divisible por 400\nes_bisiesto = (a\u00f1o % 4 == 0 and a\u00f1o % 100 != 0) or (a\u00f1o % 400 == 0)\n\nif es_bisiesto:\n    print(f\"{a\u00f1o} es bisiesto\")\nelse:\n    print(f\"{a\u00f1o} no es bisiesto\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejercicio-3-clasificador-de-edad","title":"Ejercicio 3: Clasificador de Edad","text":"<pre><code>edad = int(input(\"Introduce tu edad: \"))\n\nes_bebe = edad &lt; 2\nes_ni\u00f1o = 2 &lt;= edad &lt; 12\nes_adolescente = 12 &lt;= edad &lt; 18\nes_adulto = 18 &lt;= edad &lt; 65\nes_senior = edad &gt;= 65\n\nprint()\nif es_bebe:\n    print(\"Eres un beb\u00e9\")\nelif es_ni\u00f1o:\n    print(\"Eres un ni\u00f1o/a\")\nelif es_adolescente:\n    print(\"Eres adolescente\")\nelif es_adulto:\n    print(\"Eres adulto/a\")\nelse:\n    print(\"Eres senior\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejercicio-4-verificador-de-triangulo","title":"Ejercicio 4: Verificador de Tri\u00e1ngulo","text":"<pre><code>print(\"=== Verificador de Tri\u00e1ngulo ===\")\nprint()\n\na = float(input(\"Lado a: \"))\nb = float(input(\"Lado b: \"))\nc = float(input(\"Lado c: \"))\n\n# Un tri\u00e1ngulo es v\u00e1lido si la suma de dos lados\n# es mayor que el tercero (para todos los pares)\nes_valido = (a + b &gt; c) and (a + c &gt; b) and (b + c &gt; a)\n\nprint()\nif es_valido:\n    print(\"Los lados forman un tri\u00e1ngulo v\u00e1lido\")\n\n    # Clasificar el tri\u00e1ngulo\n    if a == b == c:\n        print(\"Es un tri\u00e1ngulo equil\u00e1tero\")\n    elif a == b or b == c or a == c:\n        print(\"Es un tri\u00e1ngulo is\u00f3sceles\")\n    else:\n        print(\"Es un tri\u00e1ngulo escaleno\")\nelse:\n    print(\"Los lados NO forman un tri\u00e1ngulo v\u00e1lido\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#ejercicio-5-calculadora-de-descuentos","title":"Ejercicio 5: Calculadora de Descuentos","text":"<pre><code>print(\"=== Calculadora de Descuentos ===\")\nprint()\n\nprecio = float(input(\"Precio original: \"))\nes_cliente_vip = input(\"\u00bfEs cliente VIP? (s/n): \").lower() == \"s\"\ncantidad = int(input(\"Cantidad de productos: \"))\n\n# Calcular descuentos\ndescuento_vip = 0.10 if es_cliente_vip else 0\ndescuento_cantidad = 0.05 if cantidad &gt;= 3 else 0\ndescuento_total = descuento_vip + descuento_cantidad\n\nsubtotal = precio * cantidad\nahorro = subtotal * descuento_total\ntotal = subtotal - ahorro\n\nprint()\nprint(f\"Subtotal: {subtotal:.2f}\u20ac\")\nprint(f\"Descuento VIP: {descuento_vip*100:.0f}%\")\nprint(f\"Descuento cantidad: {descuento_cantidad*100:.0f}%\")\nprint(f\"Ahorro: {ahorro:.2f}\u20ac\")\nprint(f\"Total: {total:.2f}\u20ac\")\n</code></pre>"},{"location":"conocimientos-basicos/03-operadores/#310-resumen","title":"3.10. Resumen","text":"Tipo Operadores Aritm\u00e9ticos <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>//</code>, <code>%</code>, <code>**</code> Comparaci\u00f3n <code>==</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> L\u00f3gicos <code>and</code>, <code>or</code>, <code>not</code> Asignaci\u00f3n <code>=</code>, <code>+=</code>, <code>-=</code>, <code>*=</code>, <code>/=</code>, etc. Identidad <code>is</code>, <code>is not</code> Pertenencia <code>in</code>, <code>not in</code> Bitwise <code>&amp;</code>, <code>\\|</code>, <code>^</code>, <code>~</code>, <code>&lt;&lt;</code>, <code>&gt;&gt;</code> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/04-estructuras-control/","title":"\ud83d\udd00 Unidad 4. Estructuras de Control","text":"<p>Las estructuras de control permiten modificar el flujo de ejecuci\u00f3n de un programa. Incluyen condicionales (decisiones) y bucles (repeticiones).</p>"},{"location":"conocimientos-basicos/04-estructuras-control/#41-condicionales-if-elif-else","title":"4.1. Condicionales: if, elif, else","text":"<p>Los condicionales permiten ejecutar c\u00f3digo solo si se cumple una condici\u00f3n.</p>"},{"location":"conocimientos-basicos/04-estructuras-control/#estructura-if-simple","title":"Estructura if Simple","text":"<pre><code>edad = 18\n\nif edad &gt;= 18:\n    print(\"Eres mayor de edad\")\n    print(\"Puedes votar\")\n</code></pre> <p>Importante: El c\u00f3digo dentro del <code>if</code> debe estar indentado (con 4 espacios o 1 tabulaci\u00f3n).</p>"},{"location":"conocimientos-basicos/04-estructuras-control/#estructura-if-else","title":"Estructura if-else","text":"<pre><code>edad = 15\n\nif edad &gt;= 18:\n    print(\"Eres mayor de edad\")\nelse:\n    print(\"Eres menor de edad\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#estructura-if-elif-else","title":"Estructura if-elif-else","text":"<p>Para m\u00faltiples condiciones:</p> <pre><code>nota = 7.5\n\nif nota &gt;= 9:\n    print(\"Sobresaliente\")\nelif nota &gt;= 7:\n    print(\"Notable\")\nelif nota &gt;= 5:\n    print(\"Aprobado\")\nelse:\n    print(\"Suspenso\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejemplos-practicos","title":"Ejemplos Pr\u00e1cticos","text":"<pre><code># Ejemplo 1: Verificar n\u00famero positivo, negativo o cero\nnumero = float(input(\"Introduce un n\u00famero: \"))\n\nif numero &gt; 0:\n    print(\"El n\u00famero es positivo\")\nelif numero &lt; 0:\n    print(\"El n\u00famero es negativo\")\nelse:\n    print(\"El n\u00famero es cero\")\n</code></pre> <pre><code># Ejemplo 2: Calculadora de descuentos\nprecio = float(input(\"Precio del producto: \"))\ncantidad = int(input(\"Cantidad: \"))\n\ntotal = precio * cantidad\n\n# Aplicar descuento seg\u00fan el total\nif total &gt;= 100:\n    descuento = 0.15  # 15%\n    print(\"\u00a1Descuento del 15%!\")\nelif total &gt;= 50:\n    descuento = 0.10  # 10%\n    print(\"\u00a1Descuento del 10%!\")\nelif total &gt;= 25:\n    descuento = 0.05  # 5%\n    print(\"\u00a1Descuento del 5%!\")\nelse:\n    descuento = 0\n    print(\"Sin descuento\")\n\ntotal_final = total * (1 - descuento)\nprint(f\"Total a pagar: {total_final:.2f}\u20ac\")\n</code></pre> <pre><code># Ejemplo 3: Verificar d\u00eda de la semana\ndia = int(input(\"Introduce el n\u00famero del d\u00eda (1-7): \"))\n\nif dia == 1:\n    print(\"Lunes\")\nelif dia == 2:\n    print(\"Martes\")\nelif dia == 3:\n    print(\"Mi\u00e9rcoles\")\nelif dia == 4:\n    print(\"Jueves\")\nelif dia == 5:\n    print(\"Viernes\")\nelif dia == 6:\n    print(\"S\u00e1bado\")\nelif dia == 7:\n    print(\"Domingo\")\nelse:\n    print(\"D\u00eda no v\u00e1lido\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#condicionales-anidados","title":"Condicionales Anidados","text":"<p>Puedes poner un <code>if</code> dentro de otro:</p> <pre><code>edad = 25\ntiene_carnet = True\n\nif edad &gt;= 18:\n    print(\"Eres mayor de edad\")\n    if tiene_carnet:\n        print(\"Puedes conducir\")\n    else:\n        print(\"Necesitas sacarte el carnet\")\nelse:\n    print(\"Eres menor de edad\")\n    print(\"No puedes conducir\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#operador-ternario","title":"Operador Ternario","text":"<p>Una forma compacta de escribir if-else en una l\u00ednea:</p> <pre><code># Sintaxis: valor_si_verdadero if condicion else valor_si_falso\n\nedad = 20\nestado = \"mayor\" if edad &gt;= 18 else \"menor\"\nprint(f\"Eres {estado} de edad\")\n\n# Equivalente a:\nif edad &gt;= 18:\n    estado = \"mayor\"\nelse:\n    estado = \"menor\"\n\n# M\u00e1s ejemplos\nnumero = 7\nparidad = \"par\" if numero % 2 == 0 else \"impar\"\nprint(f\"{numero} es {paridad}\")\n\n# Con valores num\u00e9ricos\nx = 10\ny = 20\nmaximo = x if x &gt; y else y\nprint(f\"El mayor es: {maximo}\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#multiples-condiciones","title":"M\u00faltiples Condiciones","text":"<pre><code># Usando and, or, not\nedad = 25\ntiene_dni = True\nesta_sobrio = True\n\nif edad &gt;= 18 and tiene_dni and esta_sobrio:\n    print(\"Puede entrar al club\")\nelse:\n    print(\"No puede entrar\")\n\n# Verificar rango\ntemperatura = 25\nif 20 &lt;= temperatura &lt;= 30:\n    print(\"Temperatura agradable\")\n\n# Con or\ndia = \"s\u00e1bado\"\nif dia == \"s\u00e1bado\" or dia == \"domingo\":\n    print(\"\u00a1Es fin de semana!\")\n\n# Con not\nlloviendo = False\nif not lloviendo:\n    print(\"Podemos salir a pasear\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#42-bucle-while","title":"4.2. Bucle while","text":"<p>El bucle <code>while</code> repite un bloque de c\u00f3digo mientras una condici\u00f3n sea verdadera.</p>"},{"location":"conocimientos-basicos/04-estructuras-control/#sintaxis-basica","title":"Sintaxis B\u00e1sica","text":"<pre><code>contador = 1\n\nwhile contador &lt;= 5:\n    print(f\"Contador: {contador}\")\n    contador += 1\n\nprint(\"Fin del bucle\")\n</code></pre> <p>Salida:</p> <pre><code>Contador: 1\nContador: 2\nContador: 3\nContador: 4\nContador: 5\nFin del bucle\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejemplos-practicos_1","title":"Ejemplos Pr\u00e1cticos","text":"<pre><code># Ejemplo 1: Cuenta regresiva\nprint(\"Cuenta regresiva:\")\nn = 10\nwhile n &gt; 0:\n    print(n)\n    n -= 1\nprint(\"\u00a1Despegue!\")\n</code></pre> <pre><code># Ejemplo 2: Suma de n\u00fameros hasta que el usuario introduzca 0\nprint(\"Suma de n\u00fameros (introduce 0 para terminar)\")\nsuma = 0\n\nnumero = int(input(\"Introduce un n\u00famero: \"))\nwhile numero != 0:\n    suma += numero\n    numero = int(input(\"Introduce un n\u00famero: \"))\n\nprint(f\"La suma total es: {suma}\")\n</code></pre> <pre><code># Ejemplo 3: Adivinar un n\u00famero\nimport random\n\nnumero_secreto = random.randint(1, 100)\nintentos = 0\nadivinado = False\n\nprint(\"Adivina el n\u00famero (entre 1 y 100)\")\n\nwhile not adivinado:\n    intento = int(input(\"Tu intento: \"))\n    intentos += 1\n\n    if intento &lt; numero_secreto:\n        print(\"Demasiado bajo\")\n    elif intento &gt; numero_secreto:\n        print(\"Demasiado alto\")\n    else:\n        adivinado = True\n        print(f\"\u00a1Correcto! Lo adivinaste en {intentos} intentos\")\n</code></pre> <pre><code># Ejemplo 4: Validar entrada del usuario\nrespuesta = \"\"\n\nwhile respuesta not in [\"s\", \"n\"]:\n    respuesta = input(\"\u00bfDesea continuar? (s/n): \").lower()\n    if respuesta not in [\"s\", \"n\"]:\n        print(\"Por favor, introduce 's' o 'n'\")\n\nif respuesta == \"s\":\n    print(\"Continuando...\")\nelse:\n    print(\"Saliendo...\")\n</code></pre> <pre><code># Ejemplo 5: Men\u00fa interactivo\nopcion = 0\n\nwhile opcion != 4:\n    print(\"\\n=== MEN\u00da ===\")\n    print(\"1. Opci\u00f3n A\")\n    print(\"2. Opci\u00f3n B\")\n    print(\"3. Opci\u00f3n C\")\n    print(\"4. Salir\")\n\n    opcion = int(input(\"Elige una opci\u00f3n: \"))\n\n    if opcion == 1:\n        print(\"Has elegido la opci\u00f3n A\")\n    elif opcion == 2:\n        print(\"Has elegido la opci\u00f3n B\")\n    elif opcion == 3:\n        print(\"Has elegido la opci\u00f3n C\")\n    elif opcion == 4:\n        print(\"\u00a1Hasta luego!\")\n    else:\n        print(\"Opci\u00f3n no v\u00e1lida\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#bucle-infinito","title":"Bucle Infinito","text":"<p>\u00a1Cuidado! Si la condici\u00f3n nunca se hace falsa, el bucle nunca termina:</p> <pre><code># \u00a1BUCLE INFINITO! (No ejecutes esto)\n# while True:\n#     print(\"Esto se repite para siempre\")\n\n# Para salir de un bucle infinito, usa Ctrl+C\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#43-bucle-for","title":"4.3. Bucle for","text":"<p>El bucle <code>for</code> itera sobre una secuencia (lista, string, range, etc.).</p>"},{"location":"conocimientos-basicos/04-estructuras-control/#sintaxis-basica_1","title":"Sintaxis B\u00e1sica","text":"<pre><code># Iterar sobre una lista\nfrutas = [\"manzana\", \"naranja\", \"pl\u00e1tano\"]\n\nfor fruta in frutas:\n    print(fruta)\n</code></pre> <p>Salida:</p> <pre><code>manzana\nnaranja\npl\u00e1tano\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#iterar-sobre-strings","title":"Iterar sobre Strings","text":"<pre><code>palabra = \"Python\"\n\nfor letra in palabra:\n    print(letra)\n</code></pre> <p>Salida:</p> <pre><code>P\ny\nt\nh\no\nn\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#la-funcion-range","title":"La Funci\u00f3n range()","text":"<p><code>range()</code> genera una secuencia de n\u00fameros:</p> <pre><code># range(fin) - desde 0 hasta fin-1\nfor i in range(5):\n    print(i)  # 0, 1, 2, 3, 4\n\n# range(inicio, fin) - desde inicio hasta fin-1\nfor i in range(1, 6):\n    print(i)  # 1, 2, 3, 4, 5\n\n# range(inicio, fin, paso)\nfor i in range(0, 10, 2):\n    print(i)  # 0, 2, 4, 6, 8 (de 2 en 2)\n\n# Cuenta regresiva (paso negativo)\nfor i in range(10, 0, -1):\n    print(i)  # 10, 9, 8, ..., 1\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejemplos-practicos-con-for","title":"Ejemplos Pr\u00e1cticos con for","text":"<pre><code># Ejemplo 1: Tabla de multiplicar\nnumero = int(input(\"\u00bfDe qu\u00e9 n\u00famero quieres la tabla? \"))\n\nprint(f\"\\nTabla del {numero}:\")\nfor i in range(1, 11):\n    resultado = numero * i\n    print(f\"{numero} x {i} = {resultado}\")\n</code></pre> <pre><code># Ejemplo 2: Suma de los primeros N n\u00fameros\nn = int(input(\"\u00bfHasta qu\u00e9 n\u00famero sumar? \"))\n\nsuma = 0\nfor i in range(1, n + 1):\n    suma += i\n\nprint(f\"La suma de 1 a {n} es: {suma}\")\n\n# Tambi\u00e9n se puede calcular con la f\u00f3rmula: n * (n+1) / 2\n</code></pre> <pre><code># Ejemplo 3: Factorial\nn = int(input(\"Calcular factorial de: \"))\n\nfactorial = 1\nfor i in range(1, n + 1):\n    factorial *= i\n\nprint(f\"{n}! = {factorial}\")\n</code></pre> <pre><code># Ejemplo 4: Encontrar n\u00fameros pares\nprint(\"N\u00fameros pares del 1 al 20:\")\nfor num in range(1, 21):\n    if num % 2 == 0:\n        print(num, end=\" \")\nprint()  # Nueva l\u00ednea al final\n# Salida: 2 4 6 8 10 12 14 16 18 20\n</code></pre> <pre><code># Ejemplo 5: Dibujar un patr\u00f3n\nn = 5\nfor i in range(1, n + 1):\n    print(\"*\" * i)\n\n# Salida:\n# *\n# **\n# ***\n# ****\n# *****\n</code></pre> <pre><code># Ejemplo 6: Iterar con \u00edndice usando enumerate\nfrutas = [\"manzana\", \"naranja\", \"pl\u00e1tano\"]\n\nfor indice, fruta in enumerate(frutas):\n    print(f\"{indice + 1}. {fruta}\")\n\n# Salida:\n# 1. manzana\n# 2. naranja\n# 3. pl\u00e1tano\n</code></pre> <pre><code># Ejemplo 7: Iterar sobre diccionarios\npersona = {\"nombre\": \"Ana\", \"edad\": 30, \"ciudad\": \"Madrid\"}\n\n# Solo claves\nfor clave in persona:\n    print(clave)\n\n# Claves y valores\nfor clave, valor in persona.items():\n    print(f\"{clave}: {valor}\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#44-control-de-bucles-break-continue-else","title":"4.4. Control de Bucles: break, continue, else","text":""},{"location":"conocimientos-basicos/04-estructuras-control/#break-salir-del-bucle","title":"break - Salir del Bucle","text":"<pre><code># Buscar un n\u00famero en una lista\nnumeros = [1, 5, 8, 12, 15, 20]\nbuscar = 12\n\nfor num in numeros:\n    print(f\"Revisando {num}...\")\n    if num == buscar:\n        print(f\"\u00a1Encontrado: {buscar}!\")\n        break  # Sale del bucle\n\nprint(\"Fin de la b\u00fasqueda\")\n</code></pre> <pre><code># Salir de un bucle while\nwhile True:\n    respuesta = input(\"Escribe 'salir' para terminar: \")\n    if respuesta == \"salir\":\n        break\n    print(f\"Escribiste: {respuesta}\")\n\nprint(\"Has salido del bucle\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#continue-saltar-a-la-siguiente-iteracion","title":"continue - Saltar a la Siguiente Iteraci\u00f3n","text":"<pre><code># Imprimir solo n\u00fameros impares\nfor i in range(1, 11):\n    if i % 2 == 0:  # Si es par\n        continue    # Salta a la siguiente iteraci\u00f3n\n    print(i)\n\n# Salida: 1, 3, 5, 7, 9\n</code></pre> <pre><code># Procesar solo elementos v\u00e1lidos\ndatos = [10, -5, 20, 0, 15, -3]\n\nsuma = 0\nfor num in datos:\n    if num &lt;= 0:\n        print(f\"Ignorando {num} (no v\u00e1lido)\")\n        continue\n    suma += num\n    print(f\"Sumando {num}, total: {suma}\")\n\nprint(f\"Suma de positivos: {suma}\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#else-en-bucles","title":"else en Bucles","text":"<p>El <code>else</code> en un bucle se ejecuta si el bucle termina sin un <code>break</code>:</p> <pre><code># Buscar un n\u00famero primo\nnumero = 17\n\nfor i in range(2, numero):\n    if numero % i == 0:\n        print(f\"{numero} no es primo (divisible por {i})\")\n        break\nelse:\n    # Se ejecuta si no hubo break\n    print(f\"{numero} es primo\")\n</code></pre> <pre><code># Verificar si un elemento est\u00e1 en una lista\nlista = [1, 2, 3, 4, 5]\nbuscar = 10\n\nfor item in lista:\n    if item == buscar:\n        print(f\"\u00a1Encontrado {buscar}!\")\n        break\nelse:\n    print(f\"{buscar} no est\u00e1 en la lista\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#45-bucles-anidados","title":"4.5. Bucles Anidados","text":"<p>Un bucle dentro de otro:</p> <pre><code># Tabla de multiplicar completa\nfor i in range(1, 6):\n    print(f\"\\n--- Tabla del {i} ---\")\n    for j in range(1, 11):\n        print(f\"{i} x {j} = {i * j}\")\n</code></pre> <pre><code># Matriz de asteriscos\nfilas = 4\ncolumnas = 6\n\nfor i in range(filas):\n    for j in range(columnas):\n        print(\"*\", end=\" \")\n    print()  # Nueva l\u00ednea al final de cada fila\n\n# Salida:\n# * * * * * *\n# * * * * * *\n# * * * * * *\n# * * * * * *\n</code></pre> <pre><code># Tri\u00e1ngulo rect\u00e1ngulo\nn = 5\nfor i in range(1, n + 1):\n    for j in range(i):\n        print(\"*\", end=\"\")\n    print()\n\n# Salida:\n# *\n# **\n# ***\n# ****\n# *****\n</code></pre> <pre><code># Pir\u00e1mide centrada\nn = 5\nfor i in range(1, n + 1):\n    espacios = \" \" * (n - i)\n    asteriscos = \"*\" * (2 * i - 1)\n    print(espacios + asteriscos)\n\n# Salida:\n#     *\n#    ***\n#   *****\n#  *******\n# *********\n</code></pre> <pre><code># Buscar en una matriz\nmatriz = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nbuscar = 5\nencontrado = False\n\nfor i in range(len(matriz)):\n    for j in range(len(matriz[i])):\n        if matriz[i][j] == buscar:\n            print(f\"Encontrado {buscar} en posici\u00f3n [{i}][{j}]\")\n            encontrado = True\n            break\n    if encontrado:\n        break\n\nif not encontrado:\n    print(f\"{buscar} no encontrado\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#46-comprension-de-listas-list-comprehension","title":"4.6. Comprensi\u00f3n de Listas (List Comprehension)","text":"<p>Una forma concisa de crear listas con bucles:</p> <pre><code># Forma tradicional\ncuadrados = []\nfor x in range(10):\n    cuadrados.append(x ** 2)\nprint(cuadrados)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n# Con list comprehension\ncuadrados = [x ** 2 for x in range(10)]\nprint(cuadrados)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#sintaxis","title":"Sintaxis","text":"<pre><code>[expresion for elemento in iterable]\n[expresion for elemento in iterable if condicion]\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejemplos","title":"Ejemplos","text":"<pre><code># N\u00fameros pares\npares = [x for x in range(20) if x % 2 == 0]\nprint(pares)  # [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n\n# Convertir a may\u00fasculas\npalabras = [\"hola\", \"mundo\", \"python\"]\nmayusculas = [p.upper() for p in palabras]\nprint(mayusculas)  # ['HOLA', 'MUNDO', 'PYTHON']\n\n# Filtrar y transformar\nnumeros = [1, -2, 3, -4, 5, -6]\npositivos_dobles = [x * 2 for x in numeros if x &gt; 0]\nprint(positivos_dobles)  # [2, 6, 10]\n\n# Con if-else\nnumeros = [1, 2, 3, 4, 5]\nresultado = [\"par\" if x % 2 == 0 else \"impar\" for x in numeros]\nprint(resultado)  # ['impar', 'par', 'impar', 'par', 'impar']\n\n# Aplanar una lista de listas\nmatriz = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nplana = [num for fila in matriz for num in fila]\nprint(plana)  # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#47-ejercicios-practicos","title":"4.7. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/04-estructuras-control/#ejercicio-1-calculadora-con-menu","title":"Ejercicio 1: Calculadora con Men\u00fa","text":"<pre><code>while True:\n    print(\"\\n=== CALCULADORA ===\")\n    print(\"1. Sumar\")\n    print(\"2. Restar\")\n    print(\"3. Multiplicar\")\n    print(\"4. Dividir\")\n    print(\"5. Salir\")\n\n    opcion = input(\"Elige una opci\u00f3n: \")\n\n    if opcion == \"5\":\n        print(\"\u00a1Hasta luego!\")\n        break\n\n    if opcion in [\"1\", \"2\", \"3\", \"4\"]:\n        num1 = float(input(\"Primer n\u00famero: \"))\n        num2 = float(input(\"Segundo n\u00famero: \"))\n\n        if opcion == \"1\":\n            resultado = num1 + num2\n            print(f\"{num1} + {num2} = {resultado}\")\n        elif opcion == \"2\":\n            resultado = num1 - num2\n            print(f\"{num1} - {num2} = {resultado}\")\n        elif opcion == \"3\":\n            resultado = num1 * num2\n            print(f\"{num1} \u00d7 {num2} = {resultado}\")\n        elif opcion == \"4\":\n            if num2 != 0:\n                resultado = num1 / num2\n                print(f\"{num1} \u00f7 {num2} = {resultado}\")\n            else:\n                print(\"Error: No se puede dividir por cero\")\n    else:\n        print(\"Opci\u00f3n no v\u00e1lida\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejercicio-2-numeros-primos","title":"Ejercicio 2: N\u00fameros Primos","text":"<pre><code>n = int(input(\"Mostrar primos hasta: \"))\n\nprint(f\"N\u00fameros primos hasta {n}:\")\n\nfor num in range(2, n + 1):\n    es_primo = True\n    for i in range(2, int(num ** 0.5) + 1):\n        if num % i == 0:\n            es_primo = False\n            break\n    if es_primo:\n        print(num, end=\" \")\n\nprint()\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejercicio-3-fizzbuzz","title":"Ejercicio 3: FizzBuzz","text":"<pre><code># Cl\u00e1sico ejercicio de programaci\u00f3n\nfor i in range(1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"FizzBuzz\")\n    elif i % 3 == 0:\n        print(\"Fizz\")\n    elif i % 5 == 0:\n        print(\"Buzz\")\n    else:\n        print(i)\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejercicio-4-validacion-de-contrasena","title":"Ejercicio 4: Validaci\u00f3n de Contrase\u00f1a","text":"<pre><code>while True:\n    password = input(\"Crea una contrase\u00f1a: \")\n\n    es_valida = True\n    errores = []\n\n    if len(password) &lt; 8:\n        es_valida = False\n        errores.append(\"- Debe tener al menos 8 caracteres\")\n\n    tiene_mayuscula = False\n    tiene_minuscula = False\n    tiene_numero = False\n\n    for caracter in password:\n        if caracter.isupper():\n            tiene_mayuscula = True\n        elif caracter.islower():\n            tiene_minuscula = True\n        elif caracter.isdigit():\n            tiene_numero = True\n\n    if not tiene_mayuscula:\n        es_valida = False\n        errores.append(\"- Debe tener al menos una may\u00fascula\")\n    if not tiene_minuscula:\n        es_valida = False\n        errores.append(\"- Debe tener al menos una min\u00fascula\")\n    if not tiene_numero:\n        es_valida = False\n        errores.append(\"- Debe tener al menos un n\u00famero\")\n\n    if es_valida:\n        print(\"\u2713 Contrase\u00f1a v\u00e1lida\")\n        break\n    else:\n        print(\"\u2717 Contrase\u00f1a no v\u00e1lida:\")\n        for error in errores:\n            print(error)\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#ejercicio-5-piedra-papel-tijeras","title":"Ejercicio 5: Piedra, Papel, Tijeras","text":"<pre><code>import random\n\nopciones = [\"piedra\", \"papel\", \"tijeras\"]\npuntos_jugador = 0\npuntos_computadora = 0\n\nprint(\"=== PIEDRA, PAPEL, TIJERAS ===\")\nprint(\"(Escribe 'salir' para terminar)\")\n\nwhile True:\n    jugador = input(\"\\nTu elecci\u00f3n (piedra/papel/tijeras): \").lower()\n\n    if jugador == \"salir\":\n        break\n\n    if jugador not in opciones:\n        print(\"Opci\u00f3n no v\u00e1lida\")\n        continue\n\n    computadora = random.choice(opciones)\n    print(f\"Computadora eligi\u00f3: {computadora}\")\n\n    if jugador == computadora:\n        print(\"\u00a1Empate!\")\n    elif (jugador == \"piedra\" and computadora == \"tijeras\") or \\\n         (jugador == \"papel\" and computadora == \"piedra\") or \\\n         (jugador == \"tijeras\" and computadora == \"papel\"):\n        print(\"\u00a1Ganaste!\")\n        puntos_jugador += 1\n    else:\n        print(\"Perdiste...\")\n        puntos_computadora += 1\n\n    print(f\"Marcador: T\u00fa {puntos_jugador} - {puntos_computadora} Computadora\")\n\nprint(f\"\\nResultado final: T\u00fa {puntos_jugador} - {puntos_computadora} Computadora\")\n</code></pre>"},{"location":"conocimientos-basicos/04-estructuras-control/#48-resumen","title":"4.8. Resumen","text":"Estructura Uso <code>if</code> Ejecutar c\u00f3digo si se cumple una condici\u00f3n <code>elif</code> Condici\u00f3n alternativa <code>else</code> Si ninguna condici\u00f3n anterior se cumple <code>while</code> Repetir mientras una condici\u00f3n sea verdadera <code>for</code> Iterar sobre una secuencia <code>break</code> Salir del bucle <code>continue</code> Saltar a la siguiente iteraci\u00f3n <code>range()</code> Generar secuencia de n\u00fameros <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/05-estructuras-datos/","title":"\ud83d\udcda Unidad 5. Estructuras de Datos","text":"<p>Python ofrece varias estructuras de datos incorporadas para almacenar colecciones de elementos. Las principales son: listas, tuplas, diccionarios y conjuntos (sets).</p>"},{"location":"conocimientos-basicos/05-estructuras-datos/#51-listas","title":"5.1. Listas","text":"<p>Las listas son colecciones ordenadas y modificables de elementos. Pueden contener elementos de diferentes tipos.</p>"},{"location":"conocimientos-basicos/05-estructuras-datos/#creacion-de-listas","title":"Creaci\u00f3n de Listas","text":"<pre><code># Lista vac\u00eda\nlista_vacia = []\notra_lista_vacia = list()\n\n# Lista con elementos\nnumeros = [1, 2, 3, 4, 5]\nfrutas = [\"manzana\", \"naranja\", \"pl\u00e1tano\"]\nmixta = [1, \"hola\", 3.14, True, None]\n\n# Lista con list()\nletras = list(\"Python\")  # ['P', 'y', 't', 'h', 'o', 'n']\nrango = list(range(5))   # [0, 1, 2, 3, 4]\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#acceso-a-elementos","title":"Acceso a Elementos","text":"<pre><code>frutas = [\"manzana\", \"naranja\", \"pl\u00e1tano\", \"uva\", \"pera\"]\n\n# \u00cdndices positivos (desde el inicio)\nprint(frutas[0])   # manzana (primero)\nprint(frutas[1])   # naranja\nprint(frutas[4])   # pera (\u00faltimo)\n\n# \u00cdndices negativos (desde el final)\nprint(frutas[-1])  # pera (\u00faltimo)\nprint(frutas[-2])  # uva (pen\u00faltimo)\n\n# Slicing (cortar)\nprint(frutas[1:4])    # ['naranja', 'pl\u00e1tano', 'uva']\nprint(frutas[:3])     # ['manzana', 'naranja', 'pl\u00e1tano']\nprint(frutas[2:])     # ['pl\u00e1tano', 'uva', 'pera']\nprint(frutas[::2])    # ['manzana', 'pl\u00e1tano', 'pera'] (cada 2)\nprint(frutas[::-1])   # Lista invertida\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#modificar-elementos","title":"Modificar Elementos","text":"<pre><code>numeros = [10, 20, 30, 40, 50]\n\n# Cambiar un elemento\nnumeros[0] = 100\nprint(numeros)  # [100, 20, 30, 40, 50]\n\n# Cambiar varios elementos con slicing\nnumeros[1:3] = [200, 300]\nprint(numeros)  # [100, 200, 300, 40, 50]\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#metodos-de-listas","title":"M\u00e9todos de Listas","text":""},{"location":"conocimientos-basicos/05-estructuras-datos/#anadir-elementos","title":"A\u00f1adir Elementos","text":"<pre><code>frutas = [\"manzana\", \"naranja\"]\n\n# append() - A\u00f1adir al final\nfrutas.append(\"pl\u00e1tano\")\nprint(frutas)  # ['manzana', 'naranja', 'pl\u00e1tano']\n\n# insert() - A\u00f1adir en posici\u00f3n espec\u00edfica\nfrutas.insert(1, \"uva\")\nprint(frutas)  # ['manzana', 'uva', 'naranja', 'pl\u00e1tano']\n\n# extend() - A\u00f1adir varios elementos\nfrutas.extend([\"pera\", \"kiwi\"])\nprint(frutas)  # ['manzana', 'uva', 'naranja', 'pl\u00e1tano', 'pera', 'kiwi']\n\n# Tambi\u00e9n con el operador +\nfrutas = frutas + [\"mango\"]\nprint(frutas)\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#eliminar-elementos","title":"Eliminar Elementos","text":"<pre><code>frutas = [\"manzana\", \"naranja\", \"pl\u00e1tano\", \"naranja\", \"uva\"]\n\n# remove() - Eliminar por valor (primera ocurrencia)\nfrutas.remove(\"naranja\")\nprint(frutas)  # ['manzana', 'pl\u00e1tano', 'naranja', 'uva']\n\n# pop() - Eliminar por \u00edndice y devolver el valor\neliminado = frutas.pop(1)  # Elimina 'pl\u00e1tano'\nprint(eliminado)  # pl\u00e1tano\nprint(frutas)     # ['manzana', 'naranja', 'uva']\n\n# pop() sin \u00edndice - Eliminar el \u00faltimo\nultimo = frutas.pop()\nprint(ultimo)  # uva\nprint(frutas)  # ['manzana', 'naranja']\n\n# del - Eliminar por \u00edndice\ndel frutas[0]\nprint(frutas)  # ['naranja']\n\n# clear() - Vaciar la lista\nfrutas.clear()\nprint(frutas)  # []\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#busqueda-y-conteo","title":"B\u00fasqueda y Conteo","text":"<pre><code>numeros = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\n\n# index() - Encontrar \u00edndice de un elemento\nindice = numeros.index(9)\nprint(f\"El 9 est\u00e1 en el \u00edndice {indice}\")  # 5\n\n# count() - Contar ocurrencias\ncantidad = numeros.count(5)\nprint(f\"El 5 aparece {cantidad} veces\")  # 3\n\n# in - Verificar si existe\nprint(7 in numeros)   # False\nprint(5 in numeros)   # True\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#ordenacion","title":"Ordenaci\u00f3n","text":"<pre><code>numeros = [3, 1, 4, 1, 5, 9, 2, 6]\n\n# sort() - Ordenar la lista (modifica la original)\nnumeros.sort()\nprint(numeros)  # [1, 1, 2, 3, 4, 5, 6, 9]\n\n# Orden descendente\nnumeros.sort(reverse=True)\nprint(numeros)  # [9, 6, 5, 4, 3, 2, 1, 1]\n\n# sorted() - Devuelve una nueva lista ordenada (no modifica original)\noriginal = [3, 1, 4, 1, 5]\nordenada = sorted(original)\nprint(original)  # [3, 1, 4, 1, 5]\nprint(ordenada)  # [1, 1, 3, 4, 5]\n\n# Ordenar strings\npalabras = [\"banana\", \"Apple\", \"cherry\"]\npalabras.sort()  # Orden ASCII (may\u00fasculas primero)\nprint(palabras)  # ['Apple', 'banana', 'cherry']\n\npalabras.sort(key=str.lower)  # Ignorar may\u00fasculas\nprint(palabras)  # ['Apple', 'banana', 'cherry']\n\n# reverse() - Invertir la lista\nnumeros = [1, 2, 3, 4, 5]\nnumeros.reverse()\nprint(numeros)  # [5, 4, 3, 2, 1]\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#otras-operaciones","title":"Otras Operaciones","text":"<pre><code># copy() - Copiar lista\noriginal = [1, 2, 3]\ncopia = original.copy()\n# Tambi\u00e9n: copia = list(original) o copia = original[:]\n\n# Longitud\nprint(len(numeros))  # 5\n\n# Suma, m\u00ednimo, m\u00e1ximo (para n\u00fameros)\nnumeros = [10, 5, 20, 15, 3]\nprint(sum(numeros))  # 53\nprint(min(numeros))  # 3\nprint(max(numeros))  # 20\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#listas-anidadas-matrices","title":"Listas Anidadas (Matrices)","text":"<pre><code># Matriz 3x3\nmatriz = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Acceder a elementos\nprint(matriz[0])      # [1, 2, 3] (primera fila)\nprint(matriz[1][2])   # 6 (fila 1, columna 2)\n\n# Recorrer matriz\nfor fila in matriz:\n    for elemento in fila:\n        print(elemento, end=\" \")\n    print()\n\n# Con \u00edndices\nfor i in range(len(matriz)):\n    for j in range(len(matriz[i])):\n        print(f\"[{i}][{j}] = {matriz[i][j]}\")\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#52-tuplas","title":"5.2. Tuplas","text":"<p>Las tuplas son colecciones ordenadas e inmutables (no se pueden modificar).</p>"},{"location":"conocimientos-basicos/05-estructuras-datos/#creacion-de-tuplas","title":"Creaci\u00f3n de Tuplas","text":"<pre><code># Tupla vac\u00eda\ntupla_vacia = ()\notra_vacia = tuple()\n\n# Tupla con elementos\ncoordenadas = (10, 20)\ndatos = (\"Ana\", 25, \"Madrid\")\nmixta = (1, \"hola\", 3.14)\n\n# Tupla de un solo elemento (necesita coma)\nun_elemento = (42,)  # Con coma\nno_tupla = (42)      # Esto es solo un int\n\n# Sin par\u00e9ntesis (empaquetado)\npunto = 5, 10\nprint(punto)  # (5, 10)\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#acceso-a-elementos_1","title":"Acceso a Elementos","text":"<pre><code>colores = (\"rojo\", \"verde\", \"azul\", \"amarillo\")\n\n# Igual que las listas\nprint(colores[0])     # rojo\nprint(colores[-1])    # amarillo\nprint(colores[1:3])   # ('verde', 'azul')\n\n# Desempaquetado\nx, y, z = (1, 2, 3)\nprint(x, y, z)  # 1 2 3\n\n# Desempaquetado con *\nprimero, *resto = (1, 2, 3, 4, 5)\nprint(primero)  # 1\nprint(resto)    # [2, 3, 4, 5]\n\n*inicio, ultimo = (1, 2, 3, 4, 5)\nprint(inicio)  # [1, 2, 3, 4]\nprint(ultimo)  # 5\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#inmutabilidad","title":"Inmutabilidad","text":"<pre><code>tupla = (1, 2, 3)\n\n# NO se puede modificar\n# tupla[0] = 100  # Error: TypeError\n\n# Pero se puede crear una nueva\nnueva = tupla + (4, 5)\nprint(nueva)  # (1, 2, 3, 4, 5)\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#metodos-de-tuplas","title":"M\u00e9todos de Tuplas","text":"<p>Las tuplas tienen menos m\u00e9todos que las listas (por ser inmutables):</p> <pre><code>numeros = (1, 2, 3, 2, 4, 2, 5)\n\n# count() - Contar ocurrencias\nprint(numeros.count(2))  # 3\n\n# index() - Encontrar \u00edndice\nprint(numeros.index(4))  # 4\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#cuando-usar-tuplas","title":"Cu\u00e1ndo Usar Tuplas","text":"<ul> <li>Datos que no deben cambiar (coordenadas, configuraci\u00f3n).</li> <li>Claves de diccionarios (las listas no pueden ser claves).</li> <li>Retornar m\u00faltiples valores de una funci\u00f3n.</li> <li>Son m\u00e1s eficientes que las listas.</li> </ul> <pre><code># Retornar m\u00faltiples valores\ndef dividir(a, b):\n    cociente = a // b\n    resto = a % b\n    return cociente, resto  # Devuelve tupla\n\nresultado = dividir(17, 5)\nprint(resultado)  # (3, 2)\n\n# Desempaquetando directamente\nc, r = dividir(17, 5)\nprint(f\"Cociente: {c}, Resto: {r}\")\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#53-diccionarios","title":"5.3. Diccionarios","text":"<p>Los diccionarios almacenan pares clave-valor. Son muy \u00fatiles para representar datos estructurados.</p>"},{"location":"conocimientos-basicos/05-estructuras-datos/#creacion-de-diccionarios","title":"Creaci\u00f3n de Diccionarios","text":"<pre><code># Diccionario vac\u00edo\nvacio = {}\notro_vacio = dict()\n\n# Con elementos\npersona = {\n    \"nombre\": \"Ana\",\n    \"edad\": 30,\n    \"ciudad\": \"Madrid\"\n}\n\n# Con dict()\ndatos = dict(nombre=\"Luis\", edad=25)\nprint(datos)  # {'nombre': 'Luis', 'edad': 25}\n\n# Desde lista de tuplas\nitems = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\ndiccionario = dict(items)\nprint(diccionario)  # {'a': 1, 'b': 2, 'c': 3}\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#acceso-a-valores","title":"Acceso a Valores","text":"<pre><code>persona = {\"nombre\": \"Ana\", \"edad\": 30, \"ciudad\": \"Madrid\"}\n\n# Con corchetes\nprint(persona[\"nombre\"])  # Ana\n# print(persona[\"email\"])  # KeyError si no existe\n\n# Con get() (m\u00e1s seguro)\nprint(persona.get(\"nombre\"))    # Ana\nprint(persona.get(\"email\"))     # None (no error)\nprint(persona.get(\"email\", \"No especificado\"))  # Valor por defecto\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#modificar-diccionarios","title":"Modificar Diccionarios","text":"<pre><code>persona = {\"nombre\": \"Ana\", \"edad\": 30}\n\n# Modificar valor existente\npersona[\"edad\"] = 31\nprint(persona)  # {'nombre': 'Ana', 'edad': 31}\n\n# A\u00f1adir nuevo par clave-valor\npersona[\"ciudad\"] = \"Madrid\"\npersona[\"email\"] = \"ana@email.com\"\nprint(persona)\n\n# update() - A\u00f1adir/actualizar m\u00faltiples\npersona.update({\"edad\": 32, \"telefono\": \"123456\"})\nprint(persona)\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#eliminar-elementos_1","title":"Eliminar Elementos","text":"<pre><code>persona = {\n    \"nombre\": \"Ana\",\n    \"edad\": 30,\n    \"ciudad\": \"Madrid\",\n    \"email\": \"ana@email.com\"\n}\n\n# del - Eliminar por clave\ndel persona[\"email\"]\nprint(persona)\n\n# pop() - Eliminar y devolver valor\nciudad = persona.pop(\"ciudad\")\nprint(f\"Ciudad eliminada: {ciudad}\")\nprint(persona)\n\n# pop() con valor por defecto\ntelefono = persona.pop(\"telefono\", \"No exist\u00eda\")\nprint(telefono)  # No exist\u00eda\n\n# popitem() - Eliminar \u00faltimo elemento\npersona[\"pais\"] = \"Espa\u00f1a\"\nitem = persona.popitem()\nprint(f\"Eliminado: {item}\")  # ('pais', 'Espa\u00f1a')\n\n# clear() - Vaciar diccionario\npersona.clear()\nprint(persona)  # {}\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#metodos-de-diccionarios","title":"M\u00e9todos de Diccionarios","text":"<pre><code>persona = {\"nombre\": \"Ana\", \"edad\": 30, \"ciudad\": \"Madrid\"}\n\n# keys() - Obtener claves\nprint(persona.keys())    # dict_keys(['nombre', 'edad', 'ciudad'])\nprint(list(persona.keys()))  # ['nombre', 'edad', 'ciudad']\n\n# values() - Obtener valores\nprint(persona.values())  # dict_values(['Ana', 30, 'Madrid'])\nprint(list(persona.values()))  # ['Ana', 30, 'Madrid']\n\n# items() - Obtener pares clave-valor\nprint(persona.items())  # dict_items([('nombre', 'Ana'), ...])\n\n# Verificar si existe una clave\nprint(\"nombre\" in persona)  # True\nprint(\"email\" in persona)   # False\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#iterar-sobre-diccionarios","title":"Iterar sobre Diccionarios","text":"<pre><code>persona = {\"nombre\": \"Ana\", \"edad\": 30, \"ciudad\": \"Madrid\"}\n\n# Iterar claves (por defecto)\nfor clave in persona:\n    print(clave)\n\n# Iterar valores\nfor valor in persona.values():\n    print(valor)\n\n# Iterar claves y valores\nfor clave, valor in persona.items():\n    print(f\"{clave}: {valor}\")\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#diccionarios-anidados","title":"Diccionarios Anidados","text":"<pre><code>estudiantes = {\n    \"ana\": {\n        \"edad\": 20,\n        \"notas\": [8, 9, 7],\n        \"ciudad\": \"Madrid\"\n    },\n    \"luis\": {\n        \"edad\": 22,\n        \"notas\": [6, 7, 8],\n        \"ciudad\": \"Barcelona\"\n    }\n}\n\n# Acceder a datos anidados\nprint(estudiantes[\"ana\"][\"edad\"])        # 20\nprint(estudiantes[\"ana\"][\"notas\"][0])    # 8\n\n# Recorrer\nfor nombre, datos in estudiantes.items():\n    promedio = sum(datos[\"notas\"]) / len(datos[\"notas\"])\n    print(f\"{nombre}: promedio = {promedio:.1f}\")\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#comprension-de-diccionarios","title":"Comprensi\u00f3n de Diccionarios","text":"<pre><code># Crear diccionario con comprensi\u00f3n\ncuadrados = {x: x**2 for x in range(6)}\nprint(cuadrados)  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n# Con condici\u00f3n\npares = {x: x**2 for x in range(10) if x % 2 == 0}\nprint(pares)  # {0: 0, 2: 4, 4: 16, 6: 36, 8: 64}\n\n# Invertir claves y valores\noriginal = {\"a\": 1, \"b\": 2, \"c\": 3}\ninvertido = {v: k for k, v in original.items()}\nprint(invertido)  # {1: 'a', 2: 'b', 3: 'c'}\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#54-conjuntos-sets","title":"5.4. Conjuntos (Sets)","text":"<p>Los sets son colecciones desordenadas de elementos \u00fanicos.</p>"},{"location":"conocimientos-basicos/05-estructuras-datos/#creacion-de-sets","title":"Creaci\u00f3n de Sets","text":"<pre><code># Set vac\u00edo (no se puede usar {}, eso crea un diccionario)\nvacio = set()\n\n# Con elementos\nnumeros = {1, 2, 3, 4, 5}\nletras = {\"a\", \"b\", \"c\"}\n\n# Los duplicados se eliminan autom\u00e1ticamente\ncon_duplicados = {1, 2, 2, 3, 3, 3}\nprint(con_duplicados)  # {1, 2, 3}\n\n# Desde lista (elimina duplicados)\nlista = [1, 2, 2, 3, 3, 4]\nconjunto = set(lista)\nprint(conjunto)  # {1, 2, 3, 4}\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#operaciones-basicas","title":"Operaciones B\u00e1sicas","text":"<pre><code>frutas = {\"manzana\", \"naranja\", \"pl\u00e1tano\"}\n\n# A\u00f1adir elemento\nfrutas.add(\"uva\")\nprint(frutas)\n\n# A\u00f1adir varios elementos\nfrutas.update([\"pera\", \"kiwi\"])\nprint(frutas)\n\n# Eliminar elemento\nfrutas.remove(\"naranja\")  # Error si no existe\nfrutas.discard(\"mango\")   # No error si no existe\n\n# pop() - Eliminar elemento aleatorio\nelemento = frutas.pop()\nprint(f\"Eliminado: {elemento}\")\n\n# Verificar pertenencia\nprint(\"manzana\" in frutas)  # True o False\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#operaciones-de-conjuntos","title":"Operaciones de Conjuntos","text":"<pre><code>A = {1, 2, 3, 4, 5}\nB = {4, 5, 6, 7, 8}\n\n# Uni\u00f3n (elementos en A o B)\nunion = A | B  # o A.union(B)\nprint(union)   # {1, 2, 3, 4, 5, 6, 7, 8}\n\n# Intersecci\u00f3n (elementos en A y B)\ninterseccion = A &amp; B  # o A.intersection(B)\nprint(interseccion)   # {4, 5}\n\n# Diferencia (elementos en A pero no en B)\ndiferencia = A - B  # o A.difference(B)\nprint(diferencia)   # {1, 2, 3}\n\n# Diferencia sim\u00e9trica (elementos en A o B pero no en ambos)\nsim_diferencia = A ^ B  # o A.symmetric_difference(B)\nprint(sim_diferencia)   # {1, 2, 3, 6, 7, 8}\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#verificar-subconjuntos","title":"Verificar Subconjuntos","text":"<pre><code>A = {1, 2, 3}\nB = {1, 2, 3, 4, 5}\nC = {1, 2, 3}\n\n# Subconjunto\nprint(A &lt;= B)  # True (A es subconjunto de B)\nprint(A.issubset(B))  # True\n\n# Subconjunto propio (m\u00e1s peque\u00f1o)\nprint(A &lt; B)   # True\nprint(A &lt; C)   # False (son iguales)\n\n# Superconjunto\nprint(B &gt;= A)  # True\nprint(B.issuperset(A))  # True\n\n# Conjuntos disjuntos (sin elementos comunes)\nX = {1, 2, 3}\nY = {4, 5, 6}\nprint(X.isdisjoint(Y))  # True\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#ejemplo-eliminar-duplicados","title":"Ejemplo: Eliminar Duplicados","text":"<pre><code># Eliminar duplicados de una lista\nlista_con_duplicados = [1, 2, 2, 3, 4, 4, 4, 5]\nlista_sin_duplicados = list(set(lista_con_duplicados))\nprint(lista_sin_duplicados)  # [1, 2, 3, 4, 5]\n\n# Nota: el orden puede no conservarse\n# Para conservar orden (Python 3.7+):\nfrom collections import OrderedDict\nlista_ordenada = list(dict.fromkeys(lista_con_duplicados))\nprint(lista_ordenada)  # [1, 2, 2, 3, 4, 5] (mantiene orden)\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#55-comparacion-de-estructuras-de-datos","title":"5.5. Comparaci\u00f3n de Estructuras de Datos","text":"Caracter\u00edstica Lista Tupla Diccionario Set Sintaxis <code>[1, 2]</code> <code>(1, 2)</code> <code>{\"a\": 1}</code> <code>{1, 2}</code> Ordenada \u2713 \u2713 \u2713 (3.7+) \u2717 Modificable \u2713 \u2717 \u2713 \u2713 Duplicados \u2713 \u2713 Claves \u00fanicas \u2717 Indexable \u2713 \u2713 Por clave \u2717 Uso t\u00edpico Colecci\u00f3n general Datos fijos Mapeo clave-valor Elementos \u00fanicos"},{"location":"conocimientos-basicos/05-estructuras-datos/#56-ejercicios-practicos","title":"5.6. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/05-estructuras-datos/#ejercicio-1-gestion-de-lista-de-tareas","title":"Ejercicio 1: Gesti\u00f3n de Lista de Tareas","text":"<pre><code>tareas = []\n\nwhile True:\n    print(\"\\n=== GESTOR DE TAREAS ===\")\n    print(\"1. Ver tareas\")\n    print(\"2. A\u00f1adir tarea\")\n    print(\"3. Completar tarea\")\n    print(\"4. Eliminar tarea\")\n    print(\"5. Salir\")\n\n    opcion = input(\"Opci\u00f3n: \")\n\n    if opcion == \"1\":\n        if not tareas:\n            print(\"No hay tareas\")\n        else:\n            for i, tarea in enumerate(tareas, 1):\n                print(f\"{i}. {tarea}\")\n\n    elif opcion == \"2\":\n        nueva = input(\"Nueva tarea: \")\n        tareas.append(nueva)\n        print(\"Tarea a\u00f1adida\")\n\n    elif opcion == \"3\":\n        indice = int(input(\"N\u00famero de tarea completada: \")) - 1\n        if 0 &lt;= indice &lt; len(tareas):\n            completada = tareas.pop(indice)\n            print(f\"\u00a1Completada: {completada}!\")\n        else:\n            print(\"\u00cdndice no v\u00e1lido\")\n\n    elif opcion == \"4\":\n        indice = int(input(\"N\u00famero de tarea a eliminar: \")) - 1\n        if 0 &lt;= indice &lt; len(tareas):\n            eliminada = tareas.pop(indice)\n            print(f\"Eliminada: {eliminada}\")\n        else:\n            print(\"\u00cdndice no v\u00e1lido\")\n\n    elif opcion == \"5\":\n        print(\"\u00a1Hasta luego!\")\n        break\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#ejercicio-2-agenda-de-contactos","title":"Ejercicio 2: Agenda de Contactos","text":"<pre><code>contactos = {}\n\nwhile True:\n    print(\"\\n=== AGENDA DE CONTACTOS ===\")\n    print(\"1. Ver contactos\")\n    print(\"2. A\u00f1adir contacto\")\n    print(\"3. Buscar contacto\")\n    print(\"4. Eliminar contacto\")\n    print(\"5. Salir\")\n\n    opcion = input(\"Opci\u00f3n: \")\n\n    if opcion == \"1\":\n        if not contactos:\n            print(\"No hay contactos\")\n        else:\n            for nombre, telefono in contactos.items():\n                print(f\"  {nombre}: {telefono}\")\n\n    elif opcion == \"2\":\n        nombre = input(\"Nombre: \")\n        telefono = input(\"Tel\u00e9fono: \")\n        contactos[nombre] = telefono\n        print(f\"Contacto '{nombre}' a\u00f1adido\")\n\n    elif opcion == \"3\":\n        nombre = input(\"Nombre a buscar: \")\n        telefono = contactos.get(nombre)\n        if telefono:\n            print(f\"{nombre}: {telefono}\")\n        else:\n            print(\"Contacto no encontrado\")\n\n    elif opcion == \"4\":\n        nombre = input(\"Nombre a eliminar: \")\n        if nombre in contactos:\n            del contactos[nombre]\n            print(f\"Contacto '{nombre}' eliminado\")\n        else:\n            print(\"Contacto no encontrado\")\n\n    elif opcion == \"5\":\n        break\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#ejercicio-3-analisis-de-texto","title":"Ejercicio 3: An\u00e1lisis de Texto","text":"<pre><code>texto = input(\"Introduce un texto: \")\n\n# Contar caracteres (sin espacios)\ncaracteres = len(texto.replace(\" \", \"\"))\n\n# Contar palabras\npalabras = texto.split()\nnum_palabras = len(palabras)\n\n# Palabra m\u00e1s larga\npalabra_larga = max(palabras, key=len) if palabras else \"\"\n\n# Caracteres \u00fanicos\ncaracteres_unicos = set(texto.lower().replace(\" \", \"\"))\n\n# Frecuencia de palabras\nfrecuencia = {}\nfor palabra in palabras:\n    palabra = palabra.lower()\n    frecuencia[palabra] = frecuencia.get(palabra, 0) + 1\n\nprint(f\"\\nCaracteres: {caracteres}\")\nprint(f\"Palabras: {num_palabras}\")\nprint(f\"Palabra m\u00e1s larga: '{palabra_larga}'\")\nprint(f\"Caracteres \u00fanicos: {len(caracteres_unicos)}\")\nprint(f\"\\nFrecuencia de palabras:\")\nfor palabra, cuenta in sorted(frecuencia.items(), key=lambda x: x[1], reverse=True):\n    print(f\"  {palabra}: {cuenta}\")\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#ejercicio-4-operaciones-con-conjuntos","title":"Ejercicio 4: Operaciones con Conjuntos","text":"<pre><code>print(\"=== COMPARADOR DE LISTAS ===\")\n\n# Pedir dos listas de n\u00fameros\nentrada1 = input(\"Lista 1 (n\u00fameros separados por comas): \")\nentrada2 = input(\"Lista 2 (n\u00fameros separados por comas): \")\n\n# Convertir a sets\nset1 = set(map(int, entrada1.split(\",\")))\nset2 = set(map(int, entrada2.split(\",\")))\n\nprint(f\"\\nConjunto 1: {set1}\")\nprint(f\"Conjunto 2: {set2}\")\n\nprint(f\"\\nElementos en ambos: {set1 &amp; set2}\")\nprint(f\"Elementos en al menos uno: {set1 | set2}\")\nprint(f\"Solo en conjunto 1: {set1 - set2}\")\nprint(f\"Solo en conjunto 2: {set2 - set1}\")\nprint(f\"En uno pero no en ambos: {set1 ^ set2}\")\n</code></pre>"},{"location":"conocimientos-basicos/05-estructuras-datos/#57-resumen","title":"5.7. Resumen","text":"Estructura Creaci\u00f3n Caracter\u00edsticas Lista <code>[]</code> Ordenada, mutable, permite duplicados Tupla <code>()</code> Ordenada, inmutable, permite duplicados Dict <code>{}</code> Pares clave-valor, claves \u00fanicas Set <code>set()</code> No ordenado, mutable, sin duplicados <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/06-funciones/","title":"\ud83d\udcda Unidad 6. Funciones","text":"<p>Las funciones son bloques de c\u00f3digo reutilizables que realizan una tarea espec\u00edfica. Permiten organizar el c\u00f3digo, evitar repeticiones y facilitar el mantenimiento.</p>"},{"location":"conocimientos-basicos/06-funciones/#61-definicion-de-funciones","title":"6.1. Definici\u00f3n de Funciones","text":""},{"location":"conocimientos-basicos/06-funciones/#sintaxis-basica","title":"Sintaxis B\u00e1sica","text":"<pre><code>def nombre_funcion():\n    # C\u00f3digo de la funci\u00f3n\n    pass\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#primera-funcion","title":"Primera Funci\u00f3n","text":"<pre><code>def saludar():\n    print(\"\u00a1Hola, mundo!\")\n\n# Llamar a la funci\u00f3n\nsaludar()  # \u00a1Hola, mundo!\nsaludar()  # \u00a1Hola, mundo!\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#funciones-con-parametros","title":"Funciones con Par\u00e1metros","text":"<pre><code>def saludar(nombre):\n    print(f\"\u00a1Hola, {nombre}!\")\n\nsaludar(\"Ana\")    # \u00a1Hola, Ana!\nsaludar(\"Luis\")   # \u00a1Hola, Luis!\n\n# Varios par\u00e1metros\ndef sumar(a, b):\n    print(f\"{a} + {b} = {a + b}\")\n\nsumar(5, 3)   # 5 + 3 = 8\nsumar(10, 20) # 10 + 20 = 30\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#62-retorno-de-valores","title":"6.2. Retorno de Valores","text":"<p>Las funciones pueden devolver valores usando <code>return</code>.</p> <pre><code>def sumar(a, b):\n    return a + b\n\nresultado = sumar(5, 3)\nprint(resultado)  # 8\n\n# Usar directamente en expresiones\nprint(sumar(10, 20))  # 30\ntotal = sumar(1, 2) + sumar(3, 4)  # 3 + 7 = 10\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#return-detiene-la-funcion","title":"Return Detiene la Funci\u00f3n","text":"<pre><code>def dividir(a, b):\n    if b == 0:\n        return \"Error: divisi\u00f3n por cero\"\n    return a / b\n\nprint(dividir(10, 2))  # 5.0\nprint(dividir(10, 0))  # Error: divisi\u00f3n por cero\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#retornar-multiples-valores","title":"Retornar M\u00faltiples Valores","text":"<pre><code>def operaciones(a, b):\n    suma = a + b\n    resta = a - b\n    multiplicacion = a * b\n    return suma, resta, multiplicacion  # Retorna tupla\n\nresultado = operaciones(10, 3)\nprint(resultado)  # (13, 7, 30)\n\n# Desempaquetando\ns, r, m = operaciones(10, 3)\nprint(f\"Suma: {s}, Resta: {r}, Mult: {m}\")\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#funciones-sin-return","title":"Funciones sin Return","text":"<pre><code>def mostrar_info(nombre, edad):\n    print(f\"Nombre: {nombre}\")\n    print(f\"Edad: {edad}\")\n\nresultado = mostrar_info(\"Ana\", 25)\nprint(resultado)  # None (no hay return expl\u00edcito)\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#63-tipos-de-parametros","title":"6.3. Tipos de Par\u00e1metros","text":""},{"location":"conocimientos-basicos/06-funciones/#parametros-posicionales","title":"Par\u00e1metros Posicionales","text":"<pre><code>def presentar(nombre, edad, ciudad):\n    print(f\"{nombre}, {edad} a\u00f1os, de {ciudad}\")\n\n# El orden importa\npresentar(\"Ana\", 25, \"Madrid\")\n# presentar(25, \"Ana\", \"Madrid\")  # Incorrecto\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#parametros-con-nombre-keyword-arguments","title":"Par\u00e1metros con Nombre (Keyword Arguments)","text":"<pre><code>def presentar(nombre, edad, ciudad):\n    print(f\"{nombre}, {edad} a\u00f1os, de {ciudad}\")\n\n# Se pueden pasar en cualquier orden\npresentar(edad=25, ciudad=\"Madrid\", nombre=\"Ana\")\npresentar(\"Ana\", ciudad=\"Madrid\", edad=25)\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#parametros-con-valores-por-defecto","title":"Par\u00e1metros con Valores por Defecto","text":"<pre><code>def saludar(nombre, saludo=\"Hola\"):\n    print(f\"{saludo}, {nombre}!\")\n\nsaludar(\"Ana\")           # Hola, Ana!\nsaludar(\"Luis\", \"Buenos d\u00edas\")  # Buenos d\u00edas, Luis!\n\n# Otro ejemplo\ndef potencia(base, exponente=2):\n    return base ** exponente\n\nprint(potencia(5))      # 25 (5\u00b2)\nprint(potencia(5, 3))   # 125 (5\u00b3)\n</code></pre> <p>\u26a0\ufe0f Importante: Los par\u00e1metros con valor por defecto deben ir al final.</p> <pre><code># Correcto\ndef funcion(a, b, c=10):\n    pass\n\n# Incorrecto\n# def funcion(a, b=10, c):  # SyntaxError\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#64-parametros-arbitrarios","title":"6.4. Par\u00e1metros Arbitrarios","text":""},{"location":"conocimientos-basicos/06-funciones/#args-argumentos-posicionales-variables","title":"*args (Argumentos Posicionales Variables)","text":"<pre><code>def sumar_todos(*numeros):\n    print(f\"Tipo: {type(numeros)}\")  # &lt;class 'tuple'&gt;\n    return sum(numeros)\n\nprint(sumar_todos(1, 2))           # 3\nprint(sumar_todos(1, 2, 3, 4, 5))  # 15\nprint(sumar_todos())               # 0\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#kwargs-argumentos-con-nombre-variables","title":"**kwargs (Argumentos con Nombre Variables)","text":"<pre><code>def mostrar_datos(**datos):\n    print(f\"Tipo: {type(datos)}\")  # &lt;class 'dict'&gt;\n    for clave, valor in datos.items():\n        print(f\"  {clave}: {valor}\")\n\nmostrar_datos(nombre=\"Ana\", edad=25)\n# nombre: Ana\n# edad: 25\n\nmostrar_datos(ciudad=\"Madrid\", pais=\"Espa\u00f1a\", codigo=28001)\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#combinando-todos-los-tipos","title":"Combinando Todos los Tipos","text":"<pre><code>def funcion_completa(a, b, *args, opcion=True, **kwargs):\n    print(f\"a = {a}\")\n    print(f\"b = {b}\")\n    print(f\"args = {args}\")\n    print(f\"opcion = {opcion}\")\n    print(f\"kwargs = {kwargs}\")\n\nfuncion_completa(1, 2, 3, 4, 5, opcion=False, x=10, y=20)\n# a = 1\n# b = 2\n# args = (3, 4, 5)\n# opcion = False\n# kwargs = {'x': 10, 'y': 20}\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#desempaquetado-de-argumentos","title":"Desempaquetado de Argumentos","text":"<pre><code>def sumar(a, b, c):\n    return a + b + c\n\n# Desempaquetar lista/tupla con *\nnumeros = [1, 2, 3]\nprint(sumar(*numeros))  # 6\n\n# Desempaquetar diccionario con **\ndatos = {\"a\": 10, \"b\": 20, \"c\": 30}\nprint(sumar(**datos))  # 60\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#65-ambito-de-variables-scope","title":"6.5. \u00c1mbito de Variables (Scope)","text":""},{"location":"conocimientos-basicos/06-funciones/#variables-locales-y-globales","title":"Variables Locales y Globales","text":"<pre><code># Variable global\nmensaje = \"Soy global\"\n\ndef funcion():\n    # Variable local\n    mensaje_local = \"Soy local\"\n    print(mensaje)        # Accede a global\n    print(mensaje_local)  # Accede a local\n\nfuncion()\nprint(mensaje)  # \"Soy global\"\n# print(mensaje_local)  # Error: no existe fuera de la funci\u00f3n\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#modificar-variables-globales","title":"Modificar Variables Globales","text":"<pre><code>contador = 0\n\ndef incrementar():\n    global contador  # Indicamos que usamos la global\n    contador += 1\n\nprint(contador)  # 0\nincrementar()\nprint(contador)  # 1\nincrementar()\nprint(contador)  # 2\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#la-regla-legb","title":"La Regla LEGB","text":"<p>Python busca variables en este orden:</p> <ol> <li>Local - Dentro de la funci\u00f3n actual</li> <li>Enclosing - Funciones que contienen a la actual</li> <li>Global - M\u00f3dulo actual</li> <li>Built-in - Funciones integradas de Python</li> </ol> <pre><code>x = \"global\"\n\ndef externa():\n    x = \"enclosing\"\n\n    def interna():\n        x = \"local\"\n        print(x)  # local\n\n    interna()\n    print(x)  # enclosing\n\nexterna()\nprint(x)  # global\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#66-funciones-lambda","title":"6.6. Funciones Lambda","text":"<p>Las funciones lambda son funciones an\u00f3nimas de una sola expresi\u00f3n.</p>"},{"location":"conocimientos-basicos/06-funciones/#sintaxis","title":"Sintaxis","text":"<pre><code>lambda argumentos: expresi\u00f3n\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#ejemplos-basicos","title":"Ejemplos B\u00e1sicos","text":"<pre><code># Funci\u00f3n normal\ndef cuadrado(x):\n    return x ** 2\n\n# Equivalente con lambda\ncuadrado_lambda = lambda x: x ** 2\n\nprint(cuadrado(5))        # 25\nprint(cuadrado_lambda(5)) # 25\n\n# Lambda con varios argumentos\nsumar = lambda a, b: a + b\nprint(sumar(3, 4))  # 7\n\n# Lambda sin argumentos\nsaludar = lambda: \"\u00a1Hola!\"\nprint(saludar())  # \u00a1Hola!\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#lambda-con-funciones-de-orden-superior","title":"Lambda con Funciones de Orden Superior","text":"<pre><code>numeros = [1, 2, 3, 4, 5]\n\n# map() - Aplicar funci\u00f3n a cada elemento\ncuadrados = list(map(lambda x: x**2, numeros))\nprint(cuadrados)  # [1, 4, 9, 16, 25]\n\n# filter() - Filtrar elementos\npares = list(filter(lambda x: x % 2 == 0, numeros))\nprint(pares)  # [2, 4]\n\n# sorted() con key\npalabras = [\"Python\", \"es\", \"genial\"]\nordenadas = sorted(palabras, key=lambda x: len(x))\nprint(ordenadas)  # ['es', 'Python', 'genial']\n\n# Ordenar diccionarios\nestudiantes = [\n    {\"nombre\": \"Ana\", \"nota\": 8},\n    {\"nombre\": \"Luis\", \"nota\": 9},\n    {\"nombre\": \"Mar\u00eda\", \"nota\": 7}\n]\npor_nota = sorted(estudiantes, key=lambda x: x[\"nota\"], reverse=True)\nprint(por_nota)  # Ordenados por nota descendente\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#67-funciones-de-orden-superior","title":"6.7. Funciones de Orden Superior","text":"<p>Son funciones que reciben otras funciones como argumentos o las devuelven.</p>"},{"location":"conocimientos-basicos/06-funciones/#map","title":"map()","text":"<pre><code># Aplicar funci\u00f3n a cada elemento\nnumeros = [1, 2, 3, 4, 5]\n\ndef duplicar(x):\n    return x * 2\n\nduplicados = list(map(duplicar, numeros))\nprint(duplicados)  # [2, 4, 6, 8, 10]\n\n# Con lambda\ntriplicados = list(map(lambda x: x * 3, numeros))\nprint(triplicados)  # [3, 6, 9, 12, 15]\n\n# Con m\u00faltiples iterables\nlista1 = [1, 2, 3]\nlista2 = [10, 20, 30]\nsumas = list(map(lambda a, b: a + b, lista1, lista2))\nprint(sumas)  # [11, 22, 33]\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#filter","title":"filter()","text":"<pre><code># Filtrar elementos que cumplen condici\u00f3n\nnumeros = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\ndef es_par(x):\n    return x % 2 == 0\n\npares = list(filter(es_par, numeros))\nprint(pares)  # [2, 4, 6, 8, 10]\n\n# Con lambda\nimpares = list(filter(lambda x: x % 2 != 0, numeros))\nprint(impares)  # [1, 3, 5, 7, 9]\n\n# Filtrar strings\npalabras = [\"hola\", \"\", \"mundo\", \"\", \"python\"]\nno_vacias = list(filter(None, palabras))  # None filtra valores falsy\nprint(no_vacias)  # ['hola', 'mundo', 'python']\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#reduce","title":"reduce()","text":"<pre><code>from functools import reduce\n\n# Acumular valores\nnumeros = [1, 2, 3, 4, 5]\n\n# Suma acumulada\nsuma = reduce(lambda a, b: a + b, numeros)\nprint(suma)  # 15\n\n# Producto\nproducto = reduce(lambda a, b: a * b, numeros)\nprint(producto)  # 120\n\n# Encontrar m\u00e1ximo (manualmente)\nmaximo = reduce(lambda a, b: a if a &gt; b else b, numeros)\nprint(maximo)  # 5\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#zip","title":"zip()","text":"<pre><code>nombres = [\"Ana\", \"Luis\", \"Mar\u00eda\"]\nedades = [25, 30, 28]\nciudades = [\"Madrid\", \"Barcelona\", \"Valencia\"]\n\n# Combinar listas\ncombinados = list(zip(nombres, edades, ciudades))\nprint(combinados)\n# [('Ana', 25, 'Madrid'), ('Luis', 30, 'Barcelona'), ('Mar\u00eda', 28, 'Valencia')]\n\n# Iterar\nfor nombre, edad, ciudad in zip(nombres, edades, ciudades):\n    print(f\"{nombre}, {edad} a\u00f1os, de {ciudad}\")\n\n# Crear diccionario\ndatos = dict(zip(nombres, edades))\nprint(datos)  # {'Ana': 25, 'Luis': 30, 'Mar\u00eda': 28}\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#68-funciones-recursivas","title":"6.8. Funciones Recursivas","text":"<p>Una funci\u00f3n recursiva es aquella que se llama a s\u00ed misma.</p>"},{"location":"conocimientos-basicos/06-funciones/#factorial","title":"Factorial","text":"<pre><code># n! = n \u00d7 (n-1) \u00d7 (n-2) \u00d7 ... \u00d7 1\ndef factorial(n):\n    if n == 0 or n == 1:  # Caso base\n        return 1\n    return n * factorial(n - 1)  # Caso recursivo\n\nprint(factorial(5))  # 120 (5\u00d74\u00d73\u00d72\u00d71)\nprint(factorial(0))  # 1\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#fibonacci","title":"Fibonacci","text":"<pre><code># 0, 1, 1, 2, 3, 5, 8, 13, 21, ...\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n - 1) + fibonacci(n - 2)\n\nfor i in range(10):\n    print(fibonacci(i), end=\" \")\n# 0 1 1 2 3 5 8 13 21 34\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#suma-de-lista","title":"Suma de Lista","text":"<pre><code>def suma_recursiva(lista):\n    if len(lista) == 0:  # Caso base\n        return 0\n    return lista[0] + suma_recursiva(lista[1:])\n\nprint(suma_recursiva([1, 2, 3, 4, 5]))  # 15\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#busqueda-binaria","title":"B\u00fasqueda Binaria","text":"<pre><code>def busqueda_binaria(lista, objetivo, inicio=0, fin=None):\n    if fin is None:\n        fin = len(lista) - 1\n\n    if inicio &gt; fin:\n        return -1  # No encontrado\n\n    medio = (inicio + fin) // 2\n\n    if lista[medio] == objetivo:\n        return medio\n    elif lista[medio] &lt; objetivo:\n        return busqueda_binaria(lista, objetivo, medio + 1, fin)\n    else:\n        return busqueda_binaria(lista, objetivo, inicio, medio - 1)\n\nnumeros = [1, 3, 5, 7, 9, 11, 13, 15]\nprint(busqueda_binaria(numeros, 7))   # 3 (\u00edndice)\nprint(busqueda_binaria(numeros, 6))   # -1 (no est\u00e1)\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#69-documentacion-de-funciones","title":"6.9. Documentaci\u00f3n de Funciones","text":""},{"location":"conocimientos-basicos/06-funciones/#docstrings","title":"Docstrings","text":"<pre><code>def calcular_area_rectangulo(base, altura):\n    \"\"\"\n    Calcula el \u00e1rea de un rect\u00e1ngulo.\n\n    Args:\n        base (float): La base del rect\u00e1ngulo.\n        altura (float): La altura del rect\u00e1ngulo.\n\n    Returns:\n        float: El \u00e1rea del rect\u00e1ngulo.\n\n    Example:\n        &gt;&gt;&gt; calcular_area_rectangulo(5, 3)\n        15\n    \"\"\"\n    return base * altura\n\n# Acceder a la documentaci\u00f3n\nprint(calcular_area_rectangulo.__doc__)\nhelp(calcular_area_rectangulo)\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#type-hints-anotaciones-de-tipo","title":"Type Hints (Anotaciones de Tipo)","text":"<pre><code>def saludar(nombre: str) -&gt; str:\n    return f\"Hola, {nombre}\"\n\ndef sumar(a: int, b: int) -&gt; int:\n    return a + b\n\ndef procesar_datos(\n    datos: list[dict],\n    filtro: str = None\n) -&gt; list[dict]:\n    \"\"\"Procesa una lista de diccionarios.\"\"\"\n    if filtro:\n        return [d for d in datos if filtro in str(d)]\n    return datos\n\n# Los type hints son solo indicaciones, no obligan\nresultado = sumar(3.5, 2.5)  # Funciona aunque sean floats\nprint(resultado)  # 6.0\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#610-decoradores-introduccion","title":"6.10. Decoradores (Introducci\u00f3n)","text":"<p>Los decoradores modifican el comportamiento de funciones.</p> <pre><code># Decorador simple\ndef mi_decorador(funcion):\n    def wrapper():\n        print(\"Antes de la funci\u00f3n\")\n        funcion()\n        print(\"Despu\u00e9s de la funci\u00f3n\")\n    return wrapper\n\n@mi_decorador\ndef saludar():\n    print(\"\u00a1Hola!\")\n\nsaludar()\n# Antes de la funci\u00f3n\n# \u00a1Hola!\n# Despu\u00e9s de la funci\u00f3n\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#decorador-con-argumentos","title":"Decorador con Argumentos","text":"<pre><code>def mi_decorador(funcion):\n    def wrapper(*args, **kwargs):\n        print(\"Inicio\")\n        resultado = funcion(*args, **kwargs)\n        print(\"Fin\")\n        return resultado\n    return wrapper\n\n@mi_decorador\ndef sumar(a, b):\n    return a + b\n\nresultado = sumar(3, 4)\n# Inicio\n# Fin\nprint(resultado)  # 7\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#decorador-para-medir-tiempo","title":"Decorador para Medir Tiempo","text":"<pre><code>import time\n\ndef medir_tiempo(funcion):\n    def wrapper(*args, **kwargs):\n        inicio = time.time()\n        resultado = funcion(*args, **kwargs)\n        fin = time.time()\n        print(f\"{funcion.__name__} tard\u00f3 {fin - inicio:.4f} segundos\")\n        return resultado\n    return wrapper\n\n@medir_tiempo\ndef operacion_lenta():\n    time.sleep(1)\n    return \"Completado\"\n\nresultado = operacion_lenta()\n# operacion_lenta tard\u00f3 1.0012 segundos\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#611-ejercicios-practicos","title":"6.11. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/06-funciones/#ejercicio-1-calculadora-con-funciones","title":"Ejercicio 1: Calculadora con Funciones","text":"<pre><code>def sumar(a, b):\n    return a + b\n\ndef restar(a, b):\n    return a - b\n\ndef multiplicar(a, b):\n    return a * b\n\ndef dividir(a, b):\n    if b == 0:\n        return \"Error: Divisi\u00f3n por cero\"\n    return a / b\n\ndef potencia(a, b):\n    return a ** b\n\ndef calculadora():\n    while True:\n        print(\"\\n=== CALCULADORA ===\")\n        print(\"1. Sumar\")\n        print(\"2. Restar\")\n        print(\"3. Multiplicar\")\n        print(\"4. Dividir\")\n        print(\"5. Potencia\")\n        print(\"6. Salir\")\n\n        opcion = input(\"Opci\u00f3n: \")\n\n        if opcion == \"6\":\n            print(\"\u00a1Adi\u00f3s!\")\n            break\n\n        if opcion not in \"12345\":\n            print(\"Opci\u00f3n no v\u00e1lida\")\n            continue\n\n        try:\n            a = float(input(\"Primer n\u00famero: \"))\n            b = float(input(\"Segundo n\u00famero: \"))\n        except ValueError:\n            print(\"Error: Introduce n\u00fameros v\u00e1lidos\")\n            continue\n\n        operaciones = {\n            \"1\": (\"Suma\", sumar),\n            \"2\": (\"Resta\", restar),\n            \"3\": (\"Multiplicaci\u00f3n\", multiplicar),\n            \"4\": (\"Divisi\u00f3n\", dividir),\n            \"5\": (\"Potencia\", potencia)\n        }\n\n        nombre, funcion = operaciones[opcion]\n        resultado = funcion(a, b)\n        print(f\"{nombre}: {resultado}\")\n\ncalculadora()\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#ejercicio-2-validador-de-datos","title":"Ejercicio 2: Validador de Datos","text":"<pre><code>def validar_email(email):\n    \"\"\"Valida formato b\u00e1sico de email.\"\"\"\n    if \"@\" not in email:\n        return False, \"Falta @\"\n    partes = email.split(\"@\")\n    if len(partes) != 2:\n        return False, \"Formato incorrecto\"\n    if \".\" not in partes[1]:\n        return False, \"Dominio inv\u00e1lido\"\n    return True, \"Email v\u00e1lido\"\n\ndef validar_telefono(telefono):\n    \"\"\"Valida tel\u00e9fono espa\u00f1ol (9 d\u00edgitos).\"\"\"\n    numeros = telefono.replace(\" \", \"\").replace(\"-\", \"\")\n    if not numeros.isdigit():\n        return False, \"Solo d\u00edgitos permitidos\"\n    if len(numeros) != 9:\n        return False, \"Debe tener 9 d\u00edgitos\"\n    return True, \"Tel\u00e9fono v\u00e1lido\"\n\ndef validar_edad(edad):\n    \"\"\"Valida que la edad sea razonable.\"\"\"\n    try:\n        edad = int(edad)\n        if edad &lt; 0:\n            return False, \"La edad no puede ser negativa\"\n        if edad &gt; 150:\n            return False, \"Edad no realista\"\n        return True, \"Edad v\u00e1lida\"\n    except ValueError:\n        return False, \"Debe ser un n\u00famero\"\n\n# Pruebas\nprint(validar_email(\"usuario@ejemplo.com\"))  # (True, 'Email v\u00e1lido')\nprint(validar_email(\"sin_arroba.com\"))       # (False, 'Falta @')\nprint(validar_telefono(\"612 345 678\"))       # (True, 'Tel\u00e9fono v\u00e1lido')\nprint(validar_edad(\"25\"))                    # (True, 'Edad v\u00e1lida')\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#ejercicio-3-funciones-estadisticas","title":"Ejercicio 3: Funciones Estad\u00edsticas","text":"<pre><code>def media(numeros):\n    \"\"\"Calcula la media aritm\u00e9tica.\"\"\"\n    if not numeros:\n        return None\n    return sum(numeros) / len(numeros)\n\ndef mediana(numeros):\n    \"\"\"Calcula la mediana.\"\"\"\n    if not numeros:\n        return None\n    ordenados = sorted(numeros)\n    n = len(ordenados)\n    medio = n // 2\n    if n % 2 == 0:\n        return (ordenados[medio - 1] + ordenados[medio]) / 2\n    return ordenados[medio]\n\ndef moda(numeros):\n    \"\"\"Calcula la moda (valor m\u00e1s frecuente).\"\"\"\n    if not numeros:\n        return None\n    frecuencias = {}\n    for n in numeros:\n        frecuencias[n] = frecuencias.get(n, 0) + 1\n    max_freq = max(frecuencias.values())\n    modas = [k for k, v in frecuencias.items() if v == max_freq]\n    return modas[0] if len(modas) == 1 else modas\n\ndef desviacion_estandar(numeros):\n    \"\"\"Calcula la desviaci\u00f3n est\u00e1ndar.\"\"\"\n    if not numeros:\n        return None\n    m = media(numeros)\n    varianza = sum((x - m) ** 2 for x in numeros) / len(numeros)\n    return varianza ** 0.5\n\n# Pruebas\ndatos = [4, 7, 2, 9, 4, 1, 4, 8, 3, 4]\nprint(f\"Media: {media(datos):.2f}\")            # 4.60\nprint(f\"Mediana: {mediana(datos)}\")            # 4.0\nprint(f\"Moda: {moda(datos)}\")                  # 4\nprint(f\"Desv. Est\u00e1ndar: {desviacion_estandar(datos):.2f}\")  # 2.33\n</code></pre>"},{"location":"conocimientos-basicos/06-funciones/#612-resumen","title":"6.12. Resumen","text":"Concepto Descripci\u00f3n <code>def funcion():</code> Definir funci\u00f3n <code>return valor</code> Devolver valor <code>param=valor</code> Par\u00e1metro con valor por defecto <code>*args</code> Argumentos posicionales variables <code>**kwargs</code> Argumentos con nombre variables <code>lambda x: x*2</code> Funci\u00f3n an\u00f3nima <code>map()</code> Aplicar funci\u00f3n a iterables <code>filter()</code> Filtrar elementos <code>reduce()</code> Reducir a un valor <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/","title":"\ud83d\udcda Unidad 7. M\u00f3dulos y Paquetes","text":"<p>Los m\u00f3dulos y paquetes permiten organizar y reutilizar c\u00f3digo en Python. Un m\u00f3dulo es un archivo <code>.py</code> con c\u00f3digo, y un paquete es una carpeta con m\u00f3dulos.</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#71-importar-modulos","title":"7.1. Importar M\u00f3dulos","text":""},{"location":"conocimientos-basicos/07-modulos-paquetes/#import-basico","title":"Import B\u00e1sico","text":"<pre><code># Importar m\u00f3dulo completo\nimport math\n\n# Usar funciones del m\u00f3dulo\nprint(math.sqrt(16))    # 4.0\nprint(math.pi)          # 3.141592653589793\nprint(math.ceil(4.2))   # 5\nprint(math.floor(4.8))  # 4\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#import-con-alias","title":"Import con Alias","text":"<pre><code>import math as m\n\nprint(m.sqrt(25))  # 5.0\nprint(m.pi)        # 3.14159...\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#importar-elementos-especificos","title":"Importar Elementos Espec\u00edficos","text":"<pre><code>from math import sqrt, pi, cos\n\nprint(sqrt(36))  # 6.0\nprint(pi)        # 3.14159...\nprint(cos(0))    # 1.0\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#importar-con-alias-especifico","title":"Importar con Alias Espec\u00edfico","text":"<pre><code>from math import sqrt as raiz_cuadrada\n\nprint(raiz_cuadrada(49))  # 7.0\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#importar-todo-no-recomendado","title":"Importar Todo (No Recomendado)","text":"<pre><code>from math import *\n\nprint(sqrt(64))  # 8.0\nprint(sin(0))    # 0.0\n\n# No se recomienda porque puede causar conflictos de nombres\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#72-modulos-de-la-biblioteca-estandar","title":"7.2. M\u00f3dulos de la Biblioteca Est\u00e1ndar","text":"<p>Python incluye muchos m\u00f3dulos \u00fatiles.</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-math-matematicas","title":"M\u00f3dulo <code>math</code> - Matem\u00e1ticas","text":"<pre><code>import math\n\n# Constantes\nprint(math.pi)   # 3.141592653589793\nprint(math.e)    # 2.718281828459045\nprint(math.tau)  # 6.283185307179586 (2\u03c0)\n\n# Funciones b\u00e1sicas\nprint(math.sqrt(16))     # 4.0 (ra\u00edz cuadrada)\nprint(math.pow(2, 8))    # 256.0 (potencia)\nprint(math.factorial(5)) # 120 (5!)\n\n# Redondeo\nprint(math.ceil(4.1))    # 5 (hacia arriba)\nprint(math.floor(4.9))   # 4 (hacia abajo)\nprint(math.trunc(4.7))   # 4 (truncar)\n\n# Trigonometr\u00eda (en radianes)\nprint(math.sin(math.pi/2))  # 1.0\nprint(math.cos(0))          # 1.0\nprint(math.degrees(math.pi))  # 180.0 (radianes a grados)\nprint(math.radians(180))      # 3.14159... (grados a radianes)\n\n# Logaritmos\nprint(math.log(10))      # 2.302... (ln)\nprint(math.log10(100))   # 2.0\nprint(math.log2(8))      # 3.0\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-random-numeros-aleatorios","title":"M\u00f3dulo <code>random</code> - N\u00fameros Aleatorios","text":"<pre><code>import random\n\n# N\u00famero aleatorio\nprint(random.random())        # Float entre 0 y 1\nprint(random.uniform(1, 10))  # Float entre 1 y 10\nprint(random.randint(1, 100)) # Entero entre 1 y 100 (incluidos)\n\n# Elegir de una secuencia\ncolores = [\"rojo\", \"verde\", \"azul\"]\nprint(random.choice(colores))  # Un elemento al azar\n\n# M\u00faltiples elecciones\nprint(random.choices(colores, k=5))  # 5 elementos (con repetici\u00f3n)\nprint(random.sample(range(1, 50), k=6))  # 6 elementos (sin repetici\u00f3n)\n\n# Mezclar lista\nnumeros = [1, 2, 3, 4, 5]\nrandom.shuffle(numeros)  # Mezcla in-place\nprint(numeros)\n\n# Semilla para reproducibilidad\nrandom.seed(42)\nprint(random.randint(1, 100))  # Siempre da el mismo resultado\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-datetime-fechas-y-horas","title":"M\u00f3dulo <code>datetime</code> - Fechas y Horas","text":"<pre><code>from datetime import datetime, date, time, timedelta\n\n# Fecha y hora actual\nahora = datetime.now()\nprint(ahora)  # 2026-01-15 10:30:45.123456\n\n# Solo fecha\nhoy = date.today()\nprint(hoy)  # 2026-01-15\n\n# Crear fecha espec\u00edfica\nmi_fecha = date(2026, 12, 25)\nprint(mi_fecha)  # 2026-12-25\n\n# Crear datetime espec\u00edfico\nmi_datetime = datetime(2026, 12, 25, 18, 30)\nprint(mi_datetime)  # 2026-12-25 18:30:00\n\n# Acceder a componentes\nprint(ahora.year)   # 2026\nprint(ahora.month)  # 1\nprint(ahora.day)    # 15\nprint(ahora.hour)   # 10\nprint(ahora.minute) # 30\n\n# Formatear fechas\nprint(ahora.strftime(\"%d/%m/%Y\"))        # 15/01/2026\nprint(ahora.strftime(\"%H:%M:%S\"))        # 10:30:45\nprint(ahora.strftime(\"%A, %d de %B\"))    # Wednesday, 15 de January\n\n# Parsear strings a fechas\nfecha_str = \"25/12/2026\"\nfecha = datetime.strptime(fecha_str, \"%d/%m/%Y\")\nprint(fecha)  # 2026-12-25 00:00:00\n\n# Operaciones con fechas\nmanana = hoy + timedelta(days=1)\nhace_una_semana = hoy - timedelta(weeks=1)\nen_dos_horas = ahora + timedelta(hours=2)\n\n# Diferencia entre fechas\nnavidad = date(2026, 12, 25)\ndias_restantes = navidad - hoy\nprint(f\"Faltan {dias_restantes.days} d\u00edas para Navidad\")\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-os-sistema-operativo","title":"M\u00f3dulo <code>os</code> - Sistema Operativo","text":"<pre><code>import os\n\n# Directorio actual\nprint(os.getcwd())  # C:\\Users\\...\n\n# Cambiar directorio\n# os.chdir(\"/ruta/nueva\")\n\n# Listar archivos\narchivos = os.listdir(\".\")\nprint(archivos)\n\n# Verificar existencia\nprint(os.path.exists(\"archivo.txt\"))\nprint(os.path.isfile(\"archivo.txt\"))\nprint(os.path.isdir(\"carpeta\"))\n\n# Crear directorio\n# os.mkdir(\"nueva_carpeta\")      # Una carpeta\n# os.makedirs(\"ruta/nueva/completa\")  # Varias carpetas\n\n# Eliminar\n# os.remove(\"archivo.txt\")       # Archivo\n# os.rmdir(\"carpeta_vacia\")      # Carpeta vac\u00eda\n\n# Variables de entorno\nprint(os.environ.get(\"PATH\"))\nprint(os.environ.get(\"HOME\"))\n\n# Informaci\u00f3n del sistema\nprint(os.name)       # 'nt' (Windows) o 'posix' (Linux/Mac)\nprint(os.sep)        # '\\' (Windows) o '/' (Linux/Mac)\n\n# Rutas\nruta = os.path.join(\"carpeta\", \"subcarpeta\", \"archivo.txt\")\nprint(ruta)  # carpeta\\subcarpeta\\archivo.txt\n\nprint(os.path.basename(\"/ruta/al/archivo.txt\"))  # archivo.txt\nprint(os.path.dirname(\"/ruta/al/archivo.txt\"))   # /ruta/al\nprint(os.path.splitext(\"documento.pdf\"))         # ('documento', '.pdf')\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-sys-sistema","title":"M\u00f3dulo <code>sys</code> - Sistema","text":"<pre><code>import sys\n\n# Versi\u00f3n de Python\nprint(sys.version)         # 3.12.0 ...\nprint(sys.version_info)    # (3, 12, 0, 'final', 0)\n\n# Argumentos de l\u00ednea de comandos\n# python script.py arg1 arg2\nprint(sys.argv)  # ['script.py', 'arg1', 'arg2']\n\n# Rutas de b\u00fasqueda de m\u00f3dulos\nprint(sys.path)\n\n# Salir del programa\n# sys.exit(0)  # 0 = \u00e9xito, otro n\u00famero = error\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-json-json","title":"M\u00f3dulo <code>json</code> - JSON","text":"<pre><code>import json\n\n# Diccionario a JSON (serializar)\ndatos = {\n    \"nombre\": \"Ana\",\n    \"edad\": 30,\n    \"ciudades\": [\"Madrid\", \"Barcelona\"]\n}\n\njson_str = json.dumps(datos)\nprint(json_str)  # {\"nombre\": \"Ana\", \"edad\": 30, ...}\n\n# Con formato legible\njson_bonito = json.dumps(datos, indent=4, ensure_ascii=False)\nprint(json_bonito)\n\n# JSON a diccionario (deserializar)\njson_texto = '{\"nombre\": \"Luis\", \"edad\": 25}'\ndatos = json.loads(json_texto)\nprint(datos[\"nombre\"])  # Luis\n\n# Guardar JSON en archivo\nwith open(\"datos.json\", \"w\") as f:\n    json.dump(datos, f, indent=4)\n\n# Leer JSON de archivo\nwith open(\"datos.json\", \"r\") as f:\n    datos_leidos = json.load(f)\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-re-expresiones-regulares","title":"M\u00f3dulo <code>re</code> - Expresiones Regulares","text":"<pre><code>import re\n\ntexto = \"Mi email es usuario@ejemplo.com y mi tel\u00e9fono 612345678\"\n\n# Buscar patr\u00f3n\nresultado = re.search(r\"\\d+\", texto)  # Buscar d\u00edgitos\nif resultado:\n    print(resultado.group())  # 612345678\n\n# Encontrar todos\nemails = \"contacto@ejemplo.com, info@empresa.es\"\npatron_email = r\"\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b\"\nencontrados = re.findall(patron_email, emails)\nprint(encontrados)  # ['contacto@ejemplo.com', 'info@empresa.es']\n\n# Reemplazar\ntexto = \"Hola 123 Mundo 456\"\nlimpio = re.sub(r\"\\d+\", \"XXX\", texto)\nprint(limpio)  # Hola XXX Mundo XXX\n\n# Validar formato\ndef validar_email(email):\n    patron = r\"^[\\w.-]+@[\\w.-]+\\.\\w{2,}$\"\n    return bool(re.match(patron, email))\n\nprint(validar_email(\"test@ejemplo.com\"))  # True\nprint(validar_email(\"invalido\"))          # False\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-collections-colecciones-especiales","title":"M\u00f3dulo <code>collections</code> - Colecciones Especiales","text":"<pre><code>from collections import Counter, defaultdict, namedtuple, deque\n\n# Counter - Contar elementos\npalabras = [\"a\", \"b\", \"a\", \"c\", \"a\", \"b\"]\ncontador = Counter(palabras)\nprint(contador)  # Counter({'a': 3, 'b': 2, 'c': 1})\nprint(contador.most_common(2))  # [('a', 3), ('b', 2)]\n\n# defaultdict - Diccionario con valor por defecto\ndd = defaultdict(list)\ndd[\"frutas\"].append(\"manzana\")\ndd[\"frutas\"].append(\"naranja\")\ndd[\"verduras\"].append(\"zanahoria\")\nprint(dict(dd))  # {'frutas': ['manzana', 'naranja'], 'verduras': ['zanahoria']}\n\n# namedtuple - Tupla con nombres\nPunto = namedtuple(\"Punto\", [\"x\", \"y\"])\np = Punto(10, 20)\nprint(p.x, p.y)  # 10 20\n\n# deque - Cola de doble extremo (eficiente)\ncola = deque([1, 2, 3])\ncola.append(4)      # A\u00f1adir al final\ncola.appendleft(0)  # A\u00f1adir al inicio\nprint(cola)  # deque([0, 1, 2, 3, 4])\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#73-crear-modulos-propios","title":"7.3. Crear M\u00f3dulos Propios","text":""},{"location":"conocimientos-basicos/07-modulos-paquetes/#modulo-simple","title":"M\u00f3dulo Simple","text":"<p>Crea un archivo <code>mi_modulo.py</code>:</p> <pre><code># mi_modulo.py\n\"\"\"Mi m\u00f3dulo personalizado con funciones \u00fatiles.\"\"\"\n\nPI = 3.14159\n\ndef saludar(nombre):\n    \"\"\"Saluda a una persona.\"\"\"\n    return f\"\u00a1Hola, {nombre}!\"\n\ndef area_circulo(radio):\n    \"\"\"Calcula el \u00e1rea de un c\u00edrculo.\"\"\"\n    return PI * radio ** 2\n\ndef es_par(numero):\n    \"\"\"Verifica si un n\u00famero es par.\"\"\"\n    return numero % 2 == 0\n</code></pre> <p>Usa el m\u00f3dulo en otro archivo:</p> <pre><code># main.py\nimport mi_modulo\n\nprint(mi_modulo.saludar(\"Ana\"))\nprint(mi_modulo.area_circulo(5))\nprint(mi_modulo.PI)\n\n# O importar espec\u00edfico\nfrom mi_modulo import saludar, es_par\nprint(saludar(\"Luis\"))\nprint(es_par(7))  # False\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#bloque-__name__","title":"Bloque <code>__name__</code>","text":"<pre><code># calculadora.py\ndef sumar(a, b):\n    return a + b\n\ndef restar(a, b):\n    return a - b\n\n# C\u00f3digo que solo se ejecuta si se ejecuta este archivo directamente\nif __name__ == \"__main__\":\n    # Pruebas\n    print(\"Probando calculadora...\")\n    print(f\"5 + 3 = {sumar(5, 3)}\")\n    print(f\"10 - 4 = {restar(10, 4)}\")\n</code></pre> <p>Si ejecutas <code>python calculadora.py</code>, se ejecutan las pruebas. Si haces <code>import calculadora</code>, las pruebas NO se ejecutan.</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#74-crear-paquetes","title":"7.4. Crear Paquetes","text":"<p>Un paquete es una carpeta con un archivo <code>__init__.py</code>.</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#estructura-de-paquete","title":"Estructura de Paquete","text":"<pre><code>mi_paquete/\n    __init__.py\n    matematicas.py\n    texto.py\n    utilidades/\n        __init__.py\n        archivos.py\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#contenido-de-los-archivos","title":"Contenido de los Archivos","text":"<p><code>mi_paquete/__init__.py</code>: <pre><code>\"\"\"Mi paquete de utilidades.\"\"\"\nfrom .matematicas import sumar, restar\nfrom .texto import mayusculas\n</code></pre></p> <p><code>mi_paquete/matematicas.py</code>: <pre><code>def sumar(a, b):\n    return a + b\n\ndef restar(a, b):\n    return a - b\n\ndef multiplicar(a, b):\n    return a * b\n</code></pre></p> <p><code>mi_paquete/texto.py</code>: <pre><code>def mayusculas(texto):\n    return texto.upper()\n\ndef minusculas(texto):\n    return texto.lower()\n\ndef invertir(texto):\n    return texto[::-1]\n</code></pre></p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#usar-el-paquete","title":"Usar el Paquete","text":"<pre><code># Importar desde el paquete\nimport mi_paquete\nprint(mi_paquete.sumar(5, 3))\n\n# Importar m\u00f3dulo espec\u00edfico\nfrom mi_paquete import matematicas\nprint(matematicas.multiplicar(4, 5))\n\n# Importar funci\u00f3n espec\u00edfica\nfrom mi_paquete.texto import invertir\nprint(invertir(\"Python\"))  # nohtyP\n\n# Importar subpaquete\nfrom mi_paquete.utilidades import archivos\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#75-gestion-de-paquetes-con-pip","title":"7.5. Gesti\u00f3n de Paquetes con pip","text":"<p>pip es el gestor de paquetes de Python.</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#comandos-basicos","title":"Comandos B\u00e1sicos","text":"<pre><code># Ver versi\u00f3n de pip\npip --version\n\n# Instalar paquete\npip install nombre_paquete\npip install numpy\npip install pandas==1.5.0  # Versi\u00f3n espec\u00edfica\n\n# Desinstalar paquete\npip uninstall nombre_paquete\n\n# Actualizar paquete\npip install --upgrade nombre_paquete\n\n# Ver paquetes instalados\npip list\n\n# Informaci\u00f3n de un paquete\npip show numpy\n\n# Buscar paquetes (en PyPI)\npip search nombre  # Nota: puede estar deshabilitado\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#archivo-requirementstxt","title":"Archivo requirements.txt","text":"<pre><code># Crear archivo con dependencias actuales\npip freeze &gt; requirements.txt\n\n# Instalar desde requirements.txt\npip install -r requirements.txt\n</code></pre> <p>Ejemplo de <code>requirements.txt</code>:</p> <pre><code>numpy==1.24.0\npandas&gt;=2.0.0\nmatplotlib\nscikit-learn~=1.2.0\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#76-entornos-virtuales","title":"7.6. Entornos Virtuales","text":"<p>Los entornos virtuales a\u00edslan las dependencias de cada proyecto.</p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#crear-entorno-virtual","title":"Crear Entorno Virtual","text":"<pre><code># Windows\npython -m venv mi_entorno\n\n# Linux/Mac\npython3 -m venv mi_entorno\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#activar-entorno","title":"Activar Entorno","text":"<pre><code># Windows\nmi_entorno\\Scripts\\activate\n\n# Linux/Mac\nsource mi_entorno/bin/activate\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#desactivar-entorno","title":"Desactivar Entorno","text":"<pre><code>deactivate\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#flujo-de-trabajo-tipico","title":"Flujo de Trabajo T\u00edpico","text":"<pre><code># 1. Crear entorno\npython -m venv venv\n\n# 2. Activar\nvenv\\Scripts\\activate  # Windows\n\n# 3. Instalar dependencias\npip install numpy pandas matplotlib\n\n# 4. Guardar dependencias\npip freeze &gt; requirements.txt\n\n# 5. Trabajar en el proyecto...\n\n# 6. Desactivar cuando termines\ndeactivate\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#estructura-de-proyecto-recomendada","title":"Estructura de Proyecto Recomendada","text":"<pre><code>mi_proyecto/\n    venv/                 # Entorno virtual (no subir a Git)\n    src/                  # C\u00f3digo fuente\n        __init__.py\n        main.py\n        utils.py\n    tests/                # Pruebas\n        test_main.py\n    requirements.txt      # Dependencias\n    README.md\n    .gitignore\n</code></pre> <p><code>.gitignore</code> t\u00edpico: <pre><code>venv/\n__pycache__/\n*.pyc\n.env\n</code></pre></p>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#77-ejercicios-practicos","title":"7.7. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/07-modulos-paquetes/#ejercicio-1-generador-de-contrasenas","title":"Ejercicio 1: Generador de Contrase\u00f1as","text":"<pre><code>import random\nimport string\n\ndef generar_contrasena(longitud=12, incluir_mayusculas=True, \n                       incluir_numeros=True, incluir_simbolos=True):\n    \"\"\"\n    Genera una contrase\u00f1a aleatoria.\n\n    Args:\n        longitud: Longitud de la contrase\u00f1a\n        incluir_mayusculas: Si incluir letras may\u00fasculas\n        incluir_numeros: Si incluir d\u00edgitos\n        incluir_simbolos: Si incluir s\u00edmbolos especiales\n\n    Returns:\n        str: Contrase\u00f1a generada\n    \"\"\"\n    caracteres = string.ascii_lowercase  # a-z\n\n    if incluir_mayusculas:\n        caracteres += string.ascii_uppercase  # A-Z\n    if incluir_numeros:\n        caracteres += string.digits  # 0-9\n    if incluir_simbolos:\n        caracteres += string.punctuation  # !@#$%...\n\n    contrasena = ''.join(random.choice(caracteres) for _ in range(longitud))\n    return contrasena\n\n# Generar varias contrase\u00f1as\nprint(\"Contrase\u00f1as generadas:\")\nfor i in range(5):\n    print(f\"  {i+1}. {generar_contrasena()}\")\n\n# Contrase\u00f1a solo con letras y n\u00fameros\nprint(f\"\\nSin s\u00edmbolos: {generar_contrasena(16, incluir_simbolos=False)}\")\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#ejercicio-2-analisis-de-fechas","title":"Ejercicio 2: An\u00e1lisis de Fechas","text":"<pre><code>from datetime import datetime, timedelta\n\ndef dias_entre_fechas(fecha1_str, fecha2_str, formato=\"%d/%m/%Y\"):\n    \"\"\"Calcula d\u00edas entre dos fechas.\"\"\"\n    fecha1 = datetime.strptime(fecha1_str, formato)\n    fecha2 = datetime.strptime(fecha2_str, formato)\n    diferencia = abs((fecha2 - fecha1).days)\n    return diferencia\n\ndef edad(fecha_nacimiento_str, formato=\"%d/%m/%Y\"):\n    \"\"\"Calcula la edad en a\u00f1os.\"\"\"\n    nacimiento = datetime.strptime(fecha_nacimiento_str, formato)\n    hoy = datetime.now()\n    edad = hoy.year - nacimiento.year\n    # Ajustar si no ha llegado el cumplea\u00f1os este a\u00f1o\n    if (hoy.month, hoy.day) &lt; (nacimiento.month, nacimiento.day):\n        edad -= 1\n    return edad\n\ndef siguiente_cumpleanos(fecha_nacimiento_str, formato=\"%d/%m/%Y\"):\n    \"\"\"Calcula d\u00edas hasta el pr\u00f3ximo cumplea\u00f1os.\"\"\"\n    nacimiento = datetime.strptime(fecha_nacimiento_str, formato)\n    hoy = datetime.now()\n\n    # Cumplea\u00f1os este a\u00f1o\n    cumple_este_ano = nacimiento.replace(year=hoy.year)\n\n    # Si ya pas\u00f3, calcular para el pr\u00f3ximo a\u00f1o\n    if cumple_este_ano.date() &lt; hoy.date():\n        cumple_este_ano = nacimiento.replace(year=hoy.year + 1)\n\n    dias = (cumple_este_ano.date() - hoy.date()).days\n    return dias\n\n# Pruebas\nprint(f\"D\u00edas entre fechas: {dias_entre_fechas('01/01/2020', '31/12/2020')}\")\nprint(f\"Mi edad: {edad('15/06/1990')}\")\nprint(f\"D\u00edas hasta mi cumplea\u00f1os: {siguiente_cumpleanos('15/06/1990')}\")\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#ejercicio-3-modulo-de-estadisticas","title":"Ejercicio 3: M\u00f3dulo de Estad\u00edsticas","text":"<p>Crea <code>estadisticas.py</code>:</p> <pre><code>\"\"\"M\u00f3dulo de estad\u00edsticas b\u00e1sicas.\"\"\"\n\ndef media(datos):\n    \"\"\"Calcula la media aritm\u00e9tica.\"\"\"\n    if not datos:\n        return None\n    return sum(datos) / len(datos)\n\ndef mediana(datos):\n    \"\"\"Calcula la mediana.\"\"\"\n    if not datos:\n        return None\n    ordenados = sorted(datos)\n    n = len(ordenados)\n    medio = n // 2\n    if n % 2 == 0:\n        return (ordenados[medio - 1] + ordenados[medio]) / 2\n    return ordenados[medio]\n\ndef moda(datos):\n    \"\"\"Calcula la moda.\"\"\"\n    if not datos:\n        return None\n    from collections import Counter\n    contador = Counter(datos)\n    max_freq = max(contador.values())\n    modas = [k for k, v in contador.items() if v == max_freq]\n    return modas[0] if len(modas) == 1 else modas\n\ndef varianza(datos):\n    \"\"\"Calcula la varianza.\"\"\"\n    if not datos:\n        return None\n    m = media(datos)\n    return sum((x - m) ** 2 for x in datos) / len(datos)\n\ndef desviacion_estandar(datos):\n    \"\"\"Calcula la desviaci\u00f3n est\u00e1ndar.\"\"\"\n    var = varianza(datos)\n    return var ** 0.5 if var is not None else None\n\ndef resumen(datos):\n    \"\"\"Genera un resumen estad\u00edstico.\"\"\"\n    return {\n        \"n\": len(datos),\n        \"min\": min(datos),\n        \"max\": max(datos),\n        \"media\": media(datos),\n        \"mediana\": mediana(datos),\n        \"moda\": moda(datos),\n        \"desv_std\": desviacion_estandar(datos)\n    }\n\nif __name__ == \"__main__\":\n    # Pruebas\n    datos = [4, 7, 2, 9, 4, 1, 4, 8, 3, 4]\n    print(\"Datos:\", datos)\n    for clave, valor in resumen(datos).items():\n        print(f\"  {clave}: {valor}\")\n</code></pre>"},{"location":"conocimientos-basicos/07-modulos-paquetes/#78-resumen","title":"7.8. Resumen","text":"Concepto Ejemplo Import m\u00f3dulo <code>import math</code> Import con alias <code>import numpy as np</code> Import espec\u00edfico <code>from math import sqrt</code> Crear m\u00f3dulo Archivo <code>.py</code> con funciones Crear paquete Carpeta con <code>__init__.py</code> Instalar paquete <code>pip install nombre</code> Entorno virtual <code>python -m venv venv</code> Activar entorno <code>venv\\Scripts\\activate</code> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/08-manejo-archivos/","title":"\ud83d\udcda Unidad 8. Manejo de Archivos","text":"<p>El manejo de archivos es fundamental para leer datos, guardar resultados y trabajar con informaci\u00f3n persistente.</p>"},{"location":"conocimientos-basicos/08-manejo-archivos/#81-abrir-y-cerrar-archivos","title":"8.1. Abrir y Cerrar Archivos","text":""},{"location":"conocimientos-basicos/08-manejo-archivos/#funcion-open","title":"Funci\u00f3n <code>open()</code>","text":"<pre><code># Sintaxis b\u00e1sica\narchivo = open(\"nombre_archivo.txt\", \"modo\")\n# ... operaciones ...\narchivo.close()  # Importante cerrar\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#modos-de-apertura","title":"Modos de Apertura","text":"Modo Descripci\u00f3n <code>\"r\"</code> Lectura (por defecto). Error si no existe <code>\"w\"</code> Escritura. Crea archivo o sobrescribe <code>\"a\"</code> A\u00f1adir. Escribe al final <code>\"x\"</code> Creaci\u00f3n exclusiva. Error si existe <code>\"r+\"</code> Lectura y escritura <code>\"w+\"</code> Escritura y lectura (sobrescribe) <code>\"a+\"</code> A\u00f1adir y lectura <code>\"b\"</code> Modo binario (a\u00f1adir a otros: <code>\"rb\"</code>, <code>\"wb\"</code>)"},{"location":"conocimientos-basicos/08-manejo-archivos/#uso-con-with-recomendado","title":"Uso con <code>with</code> (Recomendado)","text":"<pre><code># El archivo se cierra autom\u00e1ticamente al salir del bloque\nwith open(\"archivo.txt\", \"r\") as archivo:\n    contenido = archivo.read()\n    print(contenido)\n# Aqu\u00ed el archivo ya est\u00e1 cerrado\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#82-lectura-de-archivos","title":"8.2. Lectura de Archivos","text":""},{"location":"conocimientos-basicos/08-manejo-archivos/#leer-todo-el-contenido","title":"Leer Todo el Contenido","text":"<pre><code># Leer todo como string\nwith open(\"archivo.txt\", \"r\") as f:\n    contenido = f.read()\n    print(contenido)\n\n# Leer con codificaci\u00f3n espec\u00edfica\nwith open(\"archivo.txt\", \"r\", encoding=\"utf-8\") as f:\n    contenido = f.read()\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#leer-linea-por-linea","title":"Leer L\u00ednea por L\u00ednea","text":"<pre><code># Leer una l\u00ednea\nwith open(\"archivo.txt\", \"r\") as f:\n    primera_linea = f.readline()\n    segunda_linea = f.readline()\n    print(primera_linea)\n    print(segunda_linea)\n\n# Leer todas las l\u00edneas como lista\nwith open(\"archivo.txt\", \"r\") as f:\n    lineas = f.readlines()\n    print(lineas)  # ['l\u00ednea1\\n', 'l\u00ednea2\\n', ...]\n\n# Iterar sobre l\u00edneas (m\u00e1s eficiente para archivos grandes)\nwith open(\"archivo.txt\", \"r\") as f:\n    for linea in f:\n        print(linea.strip())  # strip() quita \\n\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#leer-cantidad-especifica","title":"Leer Cantidad Espec\u00edfica","text":"<pre><code>with open(\"archivo.txt\", \"r\") as f:\n    # Leer primeros 100 caracteres\n    primeros_100 = f.read(100)\n\n    # Leer siguientes 50\n    siguientes_50 = f.read(50)\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejemplos-practicos-de-lectura","title":"Ejemplos Pr\u00e1cticos de Lectura","text":"<pre><code># Contar l\u00edneas de un archivo\nwith open(\"datos.txt\", \"r\") as f:\n    num_lineas = sum(1 for _ in f)\nprint(f\"El archivo tiene {num_lineas} l\u00edneas\")\n\n# Buscar palabra en archivo\ndef buscar_palabra(archivo, palabra):\n    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n        for num_linea, linea in enumerate(f, 1):\n            if palabra.lower() in linea.lower():\n                print(f\"L\u00ednea {num_linea}: {linea.strip()}\")\n\nbuscar_palabra(\"texto.txt\", \"python\")\n\n# Leer archivo y procesar datos\nwith open(\"numeros.txt\", \"r\") as f:\n    numeros = [int(linea.strip()) for linea in f if linea.strip()]\n    print(f\"Suma: {sum(numeros)}\")\n    print(f\"Media: {sum(numeros)/len(numeros)}\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#83-escritura-de-archivos","title":"8.3. Escritura de Archivos","text":""},{"location":"conocimientos-basicos/08-manejo-archivos/#escribir-texto","title":"Escribir Texto","text":"<pre><code># Sobrescribir archivo (o crear si no existe)\nwith open(\"salida.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Primera l\u00ednea\\n\")\n    f.write(\"Segunda l\u00ednea\\n\")\n\n# A\u00f1adir al final\nwith open(\"salida.txt\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\"L\u00ednea a\u00f1adida\\n\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#escribir-multiples-lineas","title":"Escribir M\u00faltiples L\u00edneas","text":"<pre><code>lineas = [\"L\u00ednea 1\", \"L\u00ednea 2\", \"L\u00ednea 3\"]\n\n# Con writelines (no a\u00f1ade \\n autom\u00e1ticamente)\nwith open(\"archivo.txt\", \"w\") as f:\n    f.writelines(linea + \"\\n\" for linea in lineas)\n\n# Con join\nwith open(\"archivo.txt\", \"w\") as f:\n    f.write(\"\\n\".join(lineas))\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejemplos-practicos-de-escritura","title":"Ejemplos Pr\u00e1cticos de Escritura","text":"<pre><code># Crear informe\ndef crear_informe(datos, archivo_salida):\n    with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"=\" * 40 + \"\\n\")\n        f.write(\"        INFORME DE DATOS\\n\")\n        f.write(\"=\" * 40 + \"\\n\\n\")\n\n        for i, dato in enumerate(datos, 1):\n            f.write(f\"{i}. {dato}\\n\")\n\n        f.write(f\"\\nTotal: {len(datos)} elementos\\n\")\n\ndatos = [\"Manzana\", \"Naranja\", \"Pl\u00e1tano\"]\ncrear_informe(datos, \"informe.txt\")\n\n# Registro de log\nfrom datetime import datetime\n\ndef log(mensaje, archivo=\"app.log\"):\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    with open(archivo, \"a\", encoding=\"utf-8\") as f:\n        f.write(f\"[{timestamp}] {mensaje}\\n\")\n\nlog(\"Aplicaci\u00f3n iniciada\")\nlog(\"Usuario conectado\")\nlog(\"Error: archivo no encontrado\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#84-archivos-csv","title":"8.4. Archivos CSV","text":"<p>CSV (Comma-Separated Values) es un formato com\u00fan para datos tabulares.</p>"},{"location":"conocimientos-basicos/08-manejo-archivos/#modulo-csv","title":"M\u00f3dulo <code>csv</code>","text":"<pre><code>import csv\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#leer-csv","title":"Leer CSV","text":"<pre><code>import csv\n\n# Leer como lista de listas\nwith open(\"datos.csv\", \"r\", encoding=\"utf-8\") as f:\n    lector = csv.reader(f)\n    for fila in lector:\n        print(fila)  # ['valor1', 'valor2', ...]\n\n# Leer como diccionarios (con encabezados)\nwith open(\"datos.csv\", \"r\", encoding=\"utf-8\") as f:\n    lector = csv.DictReader(f)\n    for fila in lector:\n        print(fila)  # {'columna1': 'valor1', ...}\n        print(fila[\"nombre\"])  # Acceder por nombre de columna\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#escribir-csv","title":"Escribir CSV","text":"<pre><code>import csv\n\n# Escribir lista de listas\ndatos = [\n    [\"Nombre\", \"Edad\", \"Ciudad\"],\n    [\"Ana\", 25, \"Madrid\"],\n    [\"Luis\", 30, \"Barcelona\"],\n    [\"Mar\u00eda\", 28, \"Valencia\"]\n]\n\nwith open(\"personas.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    escritor = csv.writer(f)\n    escritor.writerows(datos)  # Escribir todas las filas\n\n# Escribir fila por fila\nwith open(\"personas.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    escritor = csv.writer(f)\n    escritor.writerow([\"Nombre\", \"Edad\", \"Ciudad\"])  # Encabezado\n    escritor.writerow([\"Ana\", 25, \"Madrid\"])\n    escritor.writerow([\"Luis\", 30, \"Barcelona\"])\n\n# Escribir desde diccionarios\npersonas = [\n    {\"nombre\": \"Ana\", \"edad\": 25, \"ciudad\": \"Madrid\"},\n    {\"nombre\": \"Luis\", \"edad\": 30, \"ciudad\": \"Barcelona\"}\n]\n\nwith open(\"personas.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    campos = [\"nombre\", \"edad\", \"ciudad\"]\n    escritor = csv.DictWriter(f, fieldnames=campos)\n    escritor.writeheader()  # Escribir encabezados\n    escritor.writerows(personas)\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#delimitadores-personalizados","title":"Delimitadores Personalizados","text":"<pre><code># Usar punto y coma como delimitador\nwith open(\"datos.csv\", \"r\", encoding=\"utf-8\") as f:\n    lector = csv.reader(f, delimiter=\";\")\n    for fila in lector:\n        print(fila)\n\n# Usar tabulador\nwith open(\"datos.tsv\", \"w\", newline=\"\") as f:\n    escritor = csv.writer(f, delimiter=\"\\t\")\n    escritor.writerow([\"Col1\", \"Col2\", \"Col3\"])\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejemplo-completo-sistema-de-inventario","title":"Ejemplo Completo: Sistema de Inventario","text":"<pre><code>import csv\nimport os\n\nARCHIVO = \"inventario.csv\"\n\ndef inicializar():\n    \"\"\"Crea el archivo si no existe.\"\"\"\n    if not os.path.exists(ARCHIVO):\n        with open(ARCHIVO, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            escritor = csv.writer(f)\n            escritor.writerow([\"ID\", \"Producto\", \"Cantidad\", \"Precio\"])\n\ndef leer_inventario():\n    \"\"\"Lee y muestra el inventario.\"\"\"\n    with open(ARCHIVO, \"r\", encoding=\"utf-8\") as f:\n        lector = csv.DictReader(f)\n        productos = list(lector)\n\n    if not productos:\n        print(\"Inventario vac\u00edo\")\n        return\n\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"{'ID':&lt;5} {'Producto':&lt;20} {'Cantidad':&lt;10} {'Precio':&lt;10}\")\n    print(\"-\" * 50)\n    for p in productos:\n        print(f\"{p['ID']:&lt;5} {p['Producto']:&lt;20} {p['Cantidad']:&lt;10} {p['Precio']:&lt;10}\")\n    print(\"=\" * 50)\n\ndef agregar_producto(id, nombre, cantidad, precio):\n    \"\"\"A\u00f1ade un producto al inventario.\"\"\"\n    with open(ARCHIVO, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        escritor = csv.writer(f)\n        escritor.writerow([id, nombre, cantidad, precio])\n    print(f\"Producto '{nombre}' a\u00f1adido\")\n\n# Uso\ninicializar()\nagregar_producto(\"001\", \"Laptop\", 10, 999.99)\nagregar_producto(\"002\", \"Mouse\", 50, 29.99)\nagregar_producto(\"003\", \"Teclado\", 30, 79.99)\nleer_inventario()\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#85-archivos-json","title":"8.5. Archivos JSON","text":"<p>JSON (JavaScript Object Notation) es ideal para datos estructurados.</p> <pre><code>import json\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#leer-json","title":"Leer JSON","text":"<pre><code>import json\n\n# Desde archivo\nwith open(\"datos.json\", \"r\", encoding=\"utf-8\") as f:\n    datos = json.load(f)\n    print(datos)\n\n# Desde string\njson_str = '{\"nombre\": \"Ana\", \"edad\": 25}'\ndatos = json.loads(json_str)\nprint(datos[\"nombre\"])  # Ana\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#escribir-json","title":"Escribir JSON","text":"<pre><code>import json\n\ndatos = {\n    \"nombre\": \"Ana\",\n    \"edad\": 25,\n    \"ciudades\": [\"Madrid\", \"Barcelona\"],\n    \"activo\": True\n}\n\n# A archivo\nwith open(\"datos.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(datos, f, indent=4, ensure_ascii=False)\n\n# A string\njson_str = json.dumps(datos, indent=4, ensure_ascii=False)\nprint(json_str)\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejemplo-configuracion-de-aplicacion","title":"Ejemplo: Configuraci\u00f3n de Aplicaci\u00f3n","text":"<pre><code>import json\nimport os\n\nARCHIVO_CONFIG = \"config.json\"\n\nCONFIG_DEFAULT = {\n    \"idioma\": \"es\",\n    \"tema\": \"claro\",\n    \"notificaciones\": True,\n    \"max_resultados\": 50\n}\n\ndef cargar_config():\n    \"\"\"Carga configuraci\u00f3n o crea archivo por defecto.\"\"\"\n    if os.path.exists(ARCHIVO_CONFIG):\n        with open(ARCHIVO_CONFIG, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    else:\n        guardar_config(CONFIG_DEFAULT)\n        return CONFIG_DEFAULT.copy()\n\ndef guardar_config(config):\n    \"\"\"Guarda configuraci\u00f3n en archivo.\"\"\"\n    with open(ARCHIVO_CONFIG, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=4, ensure_ascii=False)\n\ndef actualizar_config(clave, valor):\n    \"\"\"Actualiza una opci\u00f3n de configuraci\u00f3n.\"\"\"\n    config = cargar_config()\n    config[clave] = valor\n    guardar_config(config)\n    print(f\"Configuraci\u00f3n actualizada: {clave} = {valor}\")\n\n# Uso\nconfig = cargar_config()\nprint(f\"Idioma actual: {config['idioma']}\")\n\nactualizar_config(\"tema\", \"oscuro\")\nactualizar_config(\"max_resultados\", 100)\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejemplo-base-de-datos-simple","title":"Ejemplo: Base de Datos Simple","text":"<pre><code>import json\nimport os\n\nclass BaseDatosJSON:\n    def __init__(self, archivo):\n        self.archivo = archivo\n        self._inicializar()\n\n    def _inicializar(self):\n        if not os.path.exists(self.archivo):\n            self._guardar([])\n\n    def _cargar(self):\n        with open(self.archivo, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    def _guardar(self, datos):\n        with open(self.archivo, \"w\", encoding=\"utf-8\") as f:\n            json.dump(datos, f, indent=2, ensure_ascii=False)\n\n    def insertar(self, registro):\n        datos = self._cargar()\n        registro[\"id\"] = len(datos) + 1\n        datos.append(registro)\n        self._guardar(datos)\n        return registro[\"id\"]\n\n    def buscar(self, **criterios):\n        datos = self._cargar()\n        resultados = []\n        for registro in datos:\n            if all(registro.get(k) == v for k, v in criterios.items()):\n                resultados.append(registro)\n        return resultados\n\n    def actualizar(self, id, **campos):\n        datos = self._cargar()\n        for registro in datos:\n            if registro.get(\"id\") == id:\n                registro.update(campos)\n                self._guardar(datos)\n                return True\n        return False\n\n    def eliminar(self, id):\n        datos = self._cargar()\n        datos = [r for r in datos if r.get(\"id\") != id]\n        self._guardar(datos)\n\n    def todos(self):\n        return self._cargar()\n\n# Uso\ndb = BaseDatosJSON(\"usuarios.json\")\n\n# Insertar\ndb.insertar({\"nombre\": \"Ana\", \"email\": \"ana@email.com\", \"edad\": 25})\ndb.insertar({\"nombre\": \"Luis\", \"email\": \"luis@email.com\", \"edad\": 30})\ndb.insertar({\"nombre\": \"Mar\u00eda\", \"email\": \"maria@email.com\", \"edad\": 25})\n\n# Buscar\nprint(\"Usuarios de 25 a\u00f1os:\", db.buscar(edad=25))\n\n# Actualizar\ndb.actualizar(1, edad=26)\n\n# Ver todos\nprint(\"Todos los usuarios:\", db.todos())\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#86-manejo-de-rutas","title":"8.6. Manejo de Rutas","text":""},{"location":"conocimientos-basicos/08-manejo-archivos/#modulo-ospath","title":"M\u00f3dulo <code>os.path</code>","text":"<pre><code>import os\n\n# Obtener directorio actual\nprint(os.getcwd())\n\n# Construir rutas\nruta = os.path.join(\"carpeta\", \"subcarpeta\", \"archivo.txt\")\nprint(ruta)  # carpeta\\subcarpeta\\archivo.txt (Windows)\n\n# Obtener partes de la ruta\nruta = \"/home/usuario/documentos/archivo.txt\"\nprint(os.path.basename(ruta))  # archivo.txt\nprint(os.path.dirname(ruta))   # /home/usuario/documentos\nprint(os.path.splitext(ruta))  # ('/home/.../archivo', '.txt')\n\n# Verificar existencia\nprint(os.path.exists(\"archivo.txt\"))\nprint(os.path.isfile(\"archivo.txt\"))\nprint(os.path.isdir(\"carpeta\"))\n\n# Ruta absoluta\nprint(os.path.abspath(\"archivo.txt\"))\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#modulo-pathlib-moderno","title":"M\u00f3dulo <code>pathlib</code> (Moderno)","text":"<pre><code>from pathlib import Path\n\n# Crear objeto Path\nruta = Path(\"documentos/archivo.txt\")\n\n# Propiedades\nprint(ruta.name)        # archivo.txt\nprint(ruta.stem)        # archivo\nprint(ruta.suffix)      # .txt\nprint(ruta.parent)      # documentos\nprint(ruta.parts)       # ('documentos', 'archivo.txt')\n\n# Verificar\nprint(ruta.exists())\nprint(ruta.is_file())\nprint(ruta.is_dir())\n\n# Construir rutas\nnueva_ruta = Path(\"carpeta\") / \"subcarpeta\" / \"archivo.txt\"\nprint(nueva_ruta)\n\n# Listar archivos\ncarpeta = Path(\".\")\nfor archivo in carpeta.iterdir():\n    print(archivo)\n\n# Buscar archivos por patr\u00f3n\nfor py_file in Path(\".\").glob(\"*.py\"):\n    print(py_file)\n\n# Buscar recursivamente\nfor txt_file in Path(\".\").rglob(\"*.txt\"):\n    print(txt_file)\n\n# Crear directorios\nPath(\"nueva_carpeta/subcarpeta\").mkdir(parents=True, exist_ok=True)\n\n# Leer/escribir directamente\narchivo = Path(\"archivo.txt\")\narchivo.write_text(\"Hola mundo\", encoding=\"utf-8\")\ncontenido = archivo.read_text(encoding=\"utf-8\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#87-operaciones-con-archivos","title":"8.7. Operaciones con Archivos","text":""},{"location":"conocimientos-basicos/08-manejo-archivos/#copiar-mover-eliminar","title":"Copiar, Mover, Eliminar","text":"<pre><code>import shutil\nimport os\n\n# Copiar archivo\nshutil.copy(\"origen.txt\", \"destino.txt\")\nshutil.copy2(\"origen.txt\", \"destino.txt\")  # Preserva metadata\n\n# Copiar directorio\nshutil.copytree(\"carpeta_origen\", \"carpeta_destino\")\n\n# Mover archivo/directorio\nshutil.move(\"archivo.txt\", \"nueva_ubicacion/archivo.txt\")\n\n# Eliminar archivo\nos.remove(\"archivo.txt\")\n\n# Eliminar directorio vac\u00edo\nos.rmdir(\"carpeta_vacia\")\n\n# Eliminar directorio con contenido\nshutil.rmtree(\"carpeta_con_archivos\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#renombrar","title":"Renombrar","text":"<pre><code>import os\n\n# Renombrar archivo\nos.rename(\"nombre_viejo.txt\", \"nombre_nuevo.txt\")\n\n# Renombrar m\u00faltiples archivos\nfor archivo in os.listdir(\".\"):\n    if archivo.endswith(\".txt\"):\n        nuevo_nombre = archivo.replace(\".txt\", \"_backup.txt\")\n        os.rename(archivo, nuevo_nombre)\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#88-archivos-binarios","title":"8.8. Archivos Binarios","text":"<p>Para im\u00e1genes, audio, ejecutables, etc.</p> <pre><code># Leer archivo binario\nwith open(\"imagen.png\", \"rb\") as f:\n    datos = f.read()\n    print(f\"Tama\u00f1o: {len(datos)} bytes\")\n\n# Escribir archivo binario\nwith open(\"copia.png\", \"wb\") as f:\n    f.write(datos)\n\n# Copiar archivo binario\nwith open(\"original.pdf\", \"rb\") as origen:\n    with open(\"copia.pdf\", \"wb\") as destino:\n        destino.write(origen.read())\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#89-ejercicios-practicos","title":"8.9. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/08-manejo-archivos/#ejercicio-1-analisis-de-texto","title":"Ejercicio 1: An\u00e1lisis de Texto","text":"<pre><code>def analizar_archivo(ruta):\n    \"\"\"Analiza un archivo de texto.\"\"\"\n    try:\n        with open(ruta, \"r\", encoding=\"utf-8\") as f:\n            contenido = f.read()\n    except FileNotFoundError:\n        return \"Error: Archivo no encontrado\"\n    except Exception as e:\n        return f\"Error: {e}\"\n\n    lineas = contenido.split(\"\\n\")\n    palabras = contenido.split()\n    caracteres = len(contenido)\n    caracteres_sin_espacios = len(contenido.replace(\" \", \"\").replace(\"\\n\", \"\"))\n\n    # Palabra m\u00e1s frecuente\n    frecuencias = {}\n    for palabra in palabras:\n        palabra = palabra.lower().strip(\".,;:!?\\\"'\")\n        if palabra:\n            frecuencias[palabra] = frecuencias.get(palabra, 0) + 1\n\n    palabra_mas_comun = max(frecuencias.items(), key=lambda x: x[1]) if frecuencias else None\n\n    return {\n        \"lineas\": len(lineas),\n        \"palabras\": len(palabras),\n        \"caracteres\": caracteres,\n        \"caracteres_sin_espacios\": caracteres_sin_espacios,\n        \"palabra_mas_comun\": palabra_mas_comun\n    }\n\n# Uso\nresultado = analizar_archivo(\"texto.txt\")\nfor clave, valor in resultado.items():\n    print(f\"{clave}: {valor}\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejercicio-2-procesador-de-notas-csv","title":"Ejercicio 2: Procesador de Notas CSV","text":"<pre><code>import csv\n\ndef procesar_notas(archivo_entrada, archivo_salida):\n    \"\"\"Procesa notas de estudiantes y genera informe.\"\"\"\n    estudiantes = []\n\n    # Leer datos\n    with open(archivo_entrada, \"r\", encoding=\"utf-8\") as f:\n        lector = csv.DictReader(f)\n        for fila in lector:\n            nombre = fila[\"nombre\"]\n            notas = [float(fila[k]) for k in fila if k.startswith(\"nota\")]\n            media = sum(notas) / len(notas)\n            aprobado = media &gt;= 5\n            estudiantes.append({\n                \"nombre\": nombre,\n                \"notas\": notas,\n                \"media\": round(media, 2),\n                \"aprobado\": aprobado\n            })\n\n    # Escribir informe\n    with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"INFORME DE CALIFICACIONES\\n\")\n        f.write(\"=\" * 40 + \"\\n\\n\")\n\n        for e in estudiantes:\n            estado = \"APROBADO\" if e[\"aprobado\"] else \"SUSPENSO\"\n            f.write(f\"Estudiante: {e['nombre']}\\n\")\n            f.write(f\"  Notas: {e['notas']}\\n\")\n            f.write(f\"  Media: {e['media']}\\n\")\n            f.write(f\"  Estado: {estado}\\n\\n\")\n\n        # Estad\u00edsticas\n        medias = [e[\"media\"] for e in estudiantes]\n        aprobados = sum(1 for e in estudiantes if e[\"aprobado\"])\n\n        f.write(\"-\" * 40 + \"\\n\")\n        f.write(\"ESTAD\u00cdSTICAS\\n\")\n        f.write(f\"  Total estudiantes: {len(estudiantes)}\\n\")\n        f.write(f\"  Aprobados: {aprobados}\\n\")\n        f.write(f\"  Suspensos: {len(estudiantes) - aprobados}\\n\")\n        f.write(f\"  Media general: {sum(medias)/len(medias):.2f}\\n\")\n\n    print(f\"Informe generado en {archivo_salida}\")\n\n# Crear CSV de ejemplo\nwith open(\"notas.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n    escritor = csv.writer(f)\n    escritor.writerow([\"nombre\", \"nota1\", \"nota2\", \"nota3\"])\n    escritor.writerow([\"Ana\", 8, 7, 9])\n    escritor.writerow([\"Luis\", 5, 4, 6])\n    escritor.writerow([\"Mar\u00eda\", 9, 10, 8])\n    escritor.writerow([\"Pedro\", 3, 4, 2])\n\nprocesar_notas(\"notas.csv\", \"informe_notas.txt\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#ejercicio-3-organizador-de-archivos","title":"Ejercicio 3: Organizador de Archivos","text":"<pre><code>import os\nimport shutil\nfrom pathlib import Path\n\ndef organizar_por_extension(directorio):\n    \"\"\"Organiza archivos en carpetas por extensi\u00f3n.\"\"\"\n\n    carpetas = {\n        \"Im\u00e1genes\": [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".svg\"],\n        \"Documentos\": [\".pdf\", \".doc\", \".docx\", \".txt\", \".xlsx\", \".pptx\"],\n        \"Audio\": [\".mp3\", \".wav\", \".flac\", \".aac\"],\n        \"Video\": [\".mp4\", \".avi\", \".mkv\", \".mov\"],\n        \"C\u00f3digo\": [\".py\", \".js\", \".html\", \".css\", \".java\", \".cpp\"],\n        \"Comprimidos\": [\".zip\", \".rar\", \".7z\", \".tar\", \".gz\"]\n    }\n\n    path = Path(directorio)\n\n    for archivo in path.iterdir():\n        if archivo.is_file():\n            extension = archivo.suffix.lower()\n\n            # Encontrar carpeta destino\n            destino = \"Otros\"\n            for carpeta, extensiones in carpetas.items():\n                if extension in extensiones:\n                    destino = carpeta\n                    break\n\n            # Crear carpeta si no existe\n            carpeta_destino = path / destino\n            carpeta_destino.mkdir(exist_ok=True)\n\n            # Mover archivo\n            try:\n                shutil.move(str(archivo), str(carpeta_destino / archivo.name))\n                print(f\"Movido: {archivo.name} -&gt; {destino}/\")\n            except Exception as e:\n                print(f\"Error moviendo {archivo.name}: {e}\")\n\n# Uso (\u00a1cuidado con el directorio que uses!)\n# organizar_por_extension(\"./descargas\")\n</code></pre>"},{"location":"conocimientos-basicos/08-manejo-archivos/#810-resumen","title":"8.10. Resumen","text":"Operaci\u00f3n C\u00f3digo Abrir archivo <code>with open(\"archivo.txt\", \"r\") as f:</code> Leer todo <code>contenido = f.read()</code> Leer l\u00edneas <code>lineas = f.readlines()</code> Escribir <code>f.write(\"texto\")</code> Modo escritura <code>\"w\"</code> (sobrescribe), <code>\"a\"</code> (a\u00f1ade) CSV lectura <code>csv.reader(f)</code> o <code>csv.DictReader(f)</code> CSV escritura <code>csv.writer(f)</code> o <code>csv.DictWriter(f)</code> JSON lectura <code>json.load(f)</code> JSON escritura <code>json.dump(datos, f)</code> Rutas modernas <code>from pathlib import Path</code> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/09-excepciones/","title":"\ud83d\udcda Unidad 9. Manejo de Excepciones","text":"<p>Las excepciones son errores que ocurren durante la ejecuci\u00f3n del programa. El manejo de excepciones permite controlar estos errores de forma elegante.</p>"},{"location":"conocimientos-basicos/09-excepciones/#91-que-son-las-excepciones","title":"9.1. \u00bfQu\u00e9 son las Excepciones?","text":"<p>Cuando ocurre un error en Python, se genera una excepci\u00f3n que detiene el programa.</p> <pre><code># Error de divisi\u00f3n por cero\nresultado = 10 / 0  # ZeroDivisionError\n\n# Error de \u00edndice\nlista = [1, 2, 3]\nprint(lista[10])  # IndexError\n\n# Error de tipo\nnumero = \"texto\" + 5  # TypeError\n\n# Error de clave\ndiccionario = {\"a\": 1}\nprint(diccionario[\"b\"])  # KeyError\n\n# Error de archivo\narchivo = open(\"no_existe.txt\")  # FileNotFoundError\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#92-try-except","title":"9.2. Try / Except","text":""},{"location":"conocimientos-basicos/09-excepciones/#sintaxis-basica","title":"Sintaxis B\u00e1sica","text":"<pre><code>try:\n    # C\u00f3digo que puede causar error\n    resultado = 10 / 0\nexcept:\n    # Se ejecuta si hay error\n    print(\"\u00a1Ocurri\u00f3 un error!\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#capturar-excepciones-especificas","title":"Capturar Excepciones Espec\u00edficas","text":"<pre><code>try:\n    numero = int(input(\"Introduce un n\u00famero: \"))\n    resultado = 100 / numero\n    print(f\"Resultado: {resultado}\")\nexcept ValueError:\n    print(\"Error: Debes introducir un n\u00famero v\u00e1lido\")\nexcept ZeroDivisionError:\n    print(\"Error: No se puede dividir por cero\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#capturar-multiples-excepciones","title":"Capturar M\u00faltiples Excepciones","text":"<pre><code>try:\n    lista = [1, 2, 3]\n    indice = int(input(\"\u00cdndice: \"))\n    valor = lista[indice]\n    resultado = 10 / valor\nexcept (ValueError, IndexError) as e:\n    print(f\"Error de entrada: {e}\")\nexcept ZeroDivisionError:\n    print(\"Error: Divisi\u00f3n por cero\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#obtener-informacion-del-error","title":"Obtener Informaci\u00f3n del Error","text":"<pre><code>try:\n    resultado = 10 / 0\nexcept ZeroDivisionError as e:\n    print(f\"Tipo de error: {type(e).__name__}\")\n    print(f\"Mensaje: {e}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#93-else-y-finally","title":"9.3. Else y Finally","text":""},{"location":"conocimientos-basicos/09-excepciones/#bloque-else","title":"Bloque <code>else</code>","text":"<p>Se ejecuta si NO hubo excepciones.</p> <pre><code>try:\n    numero = int(input(\"N\u00famero: \"))\n    resultado = 100 / numero\nexcept ValueError:\n    print(\"Error: No es un n\u00famero v\u00e1lido\")\nexcept ZeroDivisionError:\n    print(\"Error: No se puede dividir por cero\")\nelse:\n    # Se ejecuta solo si no hubo errores\n    print(f\"El resultado es: {resultado}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#bloque-finally","title":"Bloque <code>finally</code>","text":"<p>Se ejecuta SIEMPRE, haya o no excepciones.</p> <pre><code>try:\n    archivo = open(\"datos.txt\", \"r\")\n    contenido = archivo.read()\nexcept FileNotFoundError:\n    print(\"Error: Archivo no encontrado\")\nfinally:\n    # Siempre se ejecuta\n    print(\"Operaci\u00f3n finalizada\")\n    # Aqu\u00ed se suelen cerrar recursos\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#ejemplo-completo","title":"Ejemplo Completo","text":"<pre><code>def dividir(a, b):\n    try:\n        resultado = a / b\n    except ZeroDivisionError:\n        print(\"Error: Divisi\u00f3n por cero\")\n        return None\n    except TypeError:\n        print(\"Error: Los valores deben ser num\u00e9ricos\")\n        return None\n    else:\n        print(\"Divisi\u00f3n exitosa\")\n        return resultado\n    finally:\n        print(\"Funci\u00f3n dividir() finalizada\")\n\nprint(dividir(10, 2))   # Divisi\u00f3n exitosa, 5.0\nprint(dividir(10, 0))   # Error, None\nprint(dividir(\"a\", 2))  # Error, None\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#94-tipos-comunes-de-excepciones","title":"9.4. Tipos Comunes de Excepciones","text":"Excepci\u00f3n Descripci\u00f3n <code>ValueError</code> Valor incorrecto <code>TypeError</code> Tipo de dato incorrecto <code>KeyError</code> Clave no existe en diccionario <code>IndexError</code> \u00cdndice fuera de rango <code>ZeroDivisionError</code> Divisi\u00f3n por cero <code>FileNotFoundError</code> Archivo no encontrado <code>AttributeError</code> Atributo/m\u00e9todo no existe <code>ImportError</code> Error al importar m\u00f3dulo <code>NameError</code> Variable no definida <code>PermissionError</code> Sin permisos <code>ConnectionError</code> Error de conexi\u00f3n <code>TimeoutError</code> Tiempo de espera agotado"},{"location":"conocimientos-basicos/09-excepciones/#ejemplos-de-cada-tipo","title":"Ejemplos de Cada Tipo","text":"<pre><code># ValueError\nint(\"texto\")  # No se puede convertir\n\n# TypeError\n\"texto\" + 5  # No se puede sumar string con int\nlen(123)     # int no tiene longitud\n\n# KeyError\nd = {\"a\": 1}\nd[\"b\"]  # Clave no existe\n\n# IndexError\nlista = [1, 2, 3]\nlista[10]  # \u00cdndice fuera de rango\n\n# AttributeError\nnumero = 5\nnumero.append(6)  # int no tiene m\u00e9todo append\n\n# NameError\nprint(variable_no_definida)\n\n# FileNotFoundError\nopen(\"archivo_inexistente.txt\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#95-raise-lanzar-excepciones","title":"9.5. Raise - Lanzar Excepciones","text":"<p>Podemos lanzar excepciones manualmente con <code>raise</code>.</p> <pre><code>def validar_edad(edad):\n    if edad &lt; 0:\n        raise ValueError(\"La edad no puede ser negativa\")\n    if edad &gt; 150:\n        raise ValueError(\"La edad no es realista\")\n    return True\n\ntry:\n    validar_edad(-5)\nexcept ValueError as e:\n    print(f\"Error de validaci\u00f3n: {e}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#ejemplos-practicos","title":"Ejemplos Pr\u00e1cticos","text":"<pre><code>def dividir(a, b):\n    if b == 0:\n        raise ZeroDivisionError(\"El divisor no puede ser cero\")\n    return a / b\n\ndef procesar_lista(lista):\n    if not isinstance(lista, list):\n        raise TypeError(\"Se esperaba una lista\")\n    if len(lista) == 0:\n        raise ValueError(\"La lista no puede estar vac\u00eda\")\n    return sum(lista) / len(lista)\n\n# Uso\ntry:\n    resultado = dividir(10, 0)\nexcept ZeroDivisionError as e:\n    print(e)\n\ntry:\n    media = procesar_lista([])\nexcept ValueError as e:\n    print(e)\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#re-lanzar-excepciones","title":"Re-lanzar Excepciones","text":"<pre><code>def procesar_datos(datos):\n    try:\n        resultado = datos[0] / datos[1]\n        return resultado\n    except Exception as e:\n        print(f\"Error capturado: {e}\")\n        raise  # Re-lanza la misma excepci\u00f3n\n\ntry:\n    procesar_datos([10, 0])\nexcept ZeroDivisionError:\n    print(\"Error manejado en el nivel superior\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#96-excepciones-personalizadas","title":"9.6. Excepciones Personalizadas","text":"<p>Podemos crear nuestras propias excepciones.</p> <pre><code>class MiError(Exception):\n    \"\"\"Excepci\u00f3n personalizada b\u00e1sica.\"\"\"\n    pass\n\nclass EdadInvalidaError(Exception):\n    \"\"\"Error cuando la edad no es v\u00e1lida.\"\"\"\n    def __init__(self, edad, mensaje=\"Edad no v\u00e1lida\"):\n        self.edad = edad\n        self.mensaje = mensaje\n        super().__init__(self.mensaje)\n\n    def __str__(self):\n        return f\"{self.mensaje}: {self.edad}\"\n\n# Uso\ndef verificar_edad(edad):\n    if edad &lt; 0 or edad &gt; 150:\n        raise EdadInvalidaError(edad)\n    if edad &lt; 18:\n        raise EdadInvalidaError(edad, \"Debe ser mayor de edad\")\n    return True\n\ntry:\n    verificar_edad(-5)\nexcept EdadInvalidaError as e:\n    print(e)  # Edad no v\u00e1lida: -5\n\ntry:\n    verificar_edad(15)\nexcept EdadInvalidaError as e:\n    print(e)  # Debe ser mayor de edad: 15\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#jerarquia-de-excepciones","title":"Jerarqu\u00eda de Excepciones","text":"<pre><code>class ErrorAplicacion(Exception):\n    \"\"\"Clase base para errores de la aplicaci\u00f3n.\"\"\"\n    pass\n\nclass ErrorValidacion(ErrorAplicacion):\n    \"\"\"Error en validaci\u00f3n de datos.\"\"\"\n    pass\n\nclass ErrorBaseDatos(ErrorAplicacion):\n    \"\"\"Error en operaciones de base de datos.\"\"\"\n    pass\n\nclass ErrorConexion(ErrorBaseDatos):\n    \"\"\"Error de conexi\u00f3n a la base de datos.\"\"\"\n    pass\n\n# Uso\ndef conectar_db():\n    raise ErrorConexion(\"No se pudo conectar al servidor\")\n\ntry:\n    conectar_db()\nexcept ErrorBaseDatos as e:\n    print(f\"Error de BD: {e}\")\nexcept ErrorAplicacion as e:\n    print(f\"Error de aplicaci\u00f3n: {e}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#97-context-managers","title":"9.7. Context Managers","text":"<p>Los context managers garantizan que los recursos se liberen correctamente.</p>"},{"location":"conocimientos-basicos/09-excepciones/#el-statement-with","title":"El Statement <code>with</code>","text":"<pre><code># Sin context manager (puede haber problemas)\narchivo = open(\"datos.txt\", \"r\")\ncontenido = archivo.read()\narchivo.close()  # Puede no ejecutarse si hay error\n\n# Con context manager (recomendado)\nwith open(\"datos.txt\", \"r\") as archivo:\n    contenido = archivo.read()\n# El archivo se cierra autom\u00e1ticamente\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#multiples-context-managers","title":"M\u00faltiples Context Managers","text":"<pre><code>with open(\"entrada.txt\", \"r\") as entrada, open(\"salida.txt\", \"w\") as salida:\n    for linea in entrada:\n        salida.write(linea.upper())\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#crear-context-manager-con-contextlib","title":"Crear Context Manager con <code>contextlib</code>","text":"<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef temporizador():\n    import time\n    inicio = time.time()\n    print(\"Iniciando...\")\n    yield  # Aqu\u00ed se ejecuta el c\u00f3digo del bloque with\n    fin = time.time()\n    print(f\"Tiempo transcurrido: {fin - inicio:.2f} segundos\")\n\n# Uso\nwith temporizador():\n    suma = sum(range(1000000))\n    print(f\"Suma: {suma}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#98-buenas-practicas","title":"9.8. Buenas Pr\u00e1cticas","text":""},{"location":"conocimientos-basicos/09-excepciones/#1-ser-especifico-con-las-excepciones","title":"1. Ser Espec\u00edfico con las Excepciones","text":"<pre><code># MAL - Captura todo\ntry:\n    # c\u00f3digo\n    pass\nexcept:\n    print(\"Error\")\n\n# BIEN - Captura espec\u00edfica\ntry:\n    # c\u00f3digo\n    pass\nexcept ValueError as e:\n    print(f\"Error de valor: {e}\")\nexcept TypeError as e:\n    print(f\"Error de tipo: {e}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#2-no-silenciar-excepciones","title":"2. No Silenciar Excepciones","text":"<pre><code># MAL - Ignora el error completamente\ntry:\n    resultado = 10 / 0\nexcept:\n    pass\n\n# BIEN - Al menos registrar el error\ntry:\n    resultado = 10 / 0\nexcept ZeroDivisionError as e:\n    print(f\"Advertencia: {e}\")\n    resultado = 0\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#3-usar-finally-para-limpieza","title":"3. Usar Finally para Limpieza","text":"<pre><code>conexion = None\ntry:\n    conexion = abrir_conexion()\n    # usar conexi\u00f3n\nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    if conexion:\n        conexion.cerrar()\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#4-validar-antes-cuando-sea-posible","title":"4. Validar Antes cuando sea Posible","text":"<pre><code># En lugar de capturar excepci\u00f3n\ndef obtener_elemento_v1(lista, indice):\n    try:\n        return lista[indice]\n    except IndexError:\n        return None\n\n# Validar antes (EAFP vs LBYL)\ndef obtener_elemento_v2(lista, indice):\n    if 0 &lt;= indice &lt; len(lista):\n        return lista[indice]\n    return None\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#99-ejercicios-practicos","title":"9.9. Ejercicios Pr\u00e1cticos","text":""},{"location":"conocimientos-basicos/09-excepciones/#ejercicio-1-calculadora-segura","title":"Ejercicio 1: Calculadora Segura","text":"<pre><code>class CalculadoraError(Exception):\n    \"\"\"Error de la calculadora.\"\"\"\n    pass\n\nclass DivisionPorCeroError(CalculadoraError):\n    \"\"\"Error de divisi\u00f3n por cero.\"\"\"\n    pass\n\nclass OperacionInvalidaError(CalculadoraError):\n    \"\"\"Operaci\u00f3n no reconocida.\"\"\"\n    pass\n\ndef calculadora_segura():\n    operaciones = {\n        \"+\": lambda a, b: a + b,\n        \"-\": lambda a, b: a - b,\n        \"*\": lambda a, b: a * b,\n        \"/\": lambda a, b: a / b if b != 0 else None,\n        \"**\": lambda a, b: a ** b\n    }\n\n    while True:\n        print(\"\\n=== CALCULADORA ===\")\n        entrada = input(\"Operaci\u00f3n (ej: 5 + 3) o 'salir': \").strip()\n\n        if entrada.lower() == \"salir\":\n            print(\"\u00a1Hasta luego!\")\n            break\n\n        try:\n            partes = entrada.split()\n            if len(partes) != 3:\n                raise OperacionInvalidaError(\"Formato: n\u00famero operador n\u00famero\")\n\n            num1, op, num2 = partes\n            num1 = float(num1)\n            num2 = float(num2)\n\n            if op not in operaciones:\n                raise OperacionInvalidaError(f\"Operador '{op}' no v\u00e1lido\")\n\n            if op == \"/\" and num2 == 0:\n                raise DivisionPorCeroError(\"No se puede dividir por cero\")\n\n            resultado = operaciones[op](num1, num2)\n            print(f\"Resultado: {resultado}\")\n\n        except ValueError:\n            print(\"Error: Introduce n\u00fameros v\u00e1lidos\")\n        except CalculadoraError as e:\n            print(f\"Error: {e}\")\n        except Exception as e:\n            print(f\"Error inesperado: {e}\")\n\ncalculadora_segura()\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#ejercicio-2-gestor-de-archivos-seguro","title":"Ejercicio 2: Gestor de Archivos Seguro","text":"<pre><code>import os\nimport json\n\nclass ArchivoError(Exception):\n    \"\"\"Error relacionado con archivos.\"\"\"\n    pass\n\nclass ArchivoNoEncontradoError(ArchivoError):\n    \"\"\"El archivo no existe.\"\"\"\n    pass\n\nclass FormatoInvalidoError(ArchivoError):\n    \"\"\"El formato del archivo no es v\u00e1lido.\"\"\"\n    pass\n\ndef leer_json_seguro(ruta):\n    \"\"\"Lee un archivo JSON de forma segura.\"\"\"\n    if not os.path.exists(ruta):\n        raise ArchivoNoEncontradoError(f\"No existe: {ruta}\")\n\n    try:\n        with open(ruta, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except json.JSONDecodeError as e:\n        raise FormatoInvalidoError(f\"JSON inv\u00e1lido: {e}\")\n    except PermissionError:\n        raise ArchivoError(f\"Sin permisos para leer: {ruta}\")\n\ndef escribir_json_seguro(datos, ruta, crear_backup=True):\n    \"\"\"Escribe datos en un archivo JSON de forma segura.\"\"\"\n    # Crear backup si existe\n    if crear_backup and os.path.exists(ruta):\n        backup = ruta + \".backup\"\n        try:\n            import shutil\n            shutil.copy(ruta, backup)\n        except Exception as e:\n            print(f\"Advertencia: No se pudo crear backup: {e}\")\n\n    try:\n        with open(ruta, \"w\", encoding=\"utf-8\") as f:\n            json.dump(datos, f, indent=2, ensure_ascii=False)\n        return True\n    except PermissionError:\n        raise ArchivoError(f\"Sin permisos para escribir: {ruta}\")\n    except Exception as e:\n        raise ArchivoError(f\"Error al escribir: {e}\")\n\n# Uso\ntry:\n    datos = leer_json_seguro(\"config.json\")\n    print(\"Datos cargados:\", datos)\nexcept ArchivoNoEncontradoError:\n    print(\"Archivo no encontrado, creando uno nuevo...\")\n    datos = {\"configuracion\": \"default\"}\n    escribir_json_seguro(datos, \"config.json\")\nexcept FormatoInvalidoError as e:\n    print(f\"Error de formato: {e}\")\nexcept ArchivoError as e:\n    print(f\"Error de archivo: {e}\")\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#ejercicio-3-validador-de-formulario","title":"Ejercicio 3: Validador de Formulario","text":"<pre><code>class ValidacionError(Exception):\n    \"\"\"Error de validaci\u00f3n.\"\"\"\n    def __init__(self, campo, mensaje):\n        self.campo = campo\n        self.mensaje = mensaje\n        super().__init__(f\"{campo}: {mensaje}\")\n\nclass Validador:\n    @staticmethod\n    def validar_email(email):\n        if not email:\n            raise ValidacionError(\"email\", \"El email es obligatorio\")\n        if \"@\" not in email or \".\" not in email:\n            raise ValidacionError(\"email\", \"Formato de email inv\u00e1lido\")\n        return True\n\n    @staticmethod\n    def validar_edad(edad):\n        try:\n            edad = int(edad)\n        except (ValueError, TypeError):\n            raise ValidacionError(\"edad\", \"Debe ser un n\u00famero\")\n\n        if edad &lt; 0:\n            raise ValidacionError(\"edad\", \"No puede ser negativa\")\n        if edad &lt; 18:\n            raise ValidacionError(\"edad\", \"Debe ser mayor de 18 a\u00f1os\")\n        if edad &gt; 120:\n            raise ValidacionError(\"edad\", \"Edad no realista\")\n        return True\n\n    @staticmethod\n    def validar_telefono(telefono):\n        numeros = telefono.replace(\" \", \"\").replace(\"-\", \"\")\n        if not numeros.isdigit():\n            raise ValidacionError(\"tel\u00e9fono\", \"Solo se permiten d\u00edgitos\")\n        if len(numeros) != 9:\n            raise ValidacionError(\"tel\u00e9fono\", \"Debe tener 9 d\u00edgitos\")\n        return True\n\n    @staticmethod\n    def validar_password(password):\n        if len(password) &lt; 8:\n            raise ValidacionError(\"contrase\u00f1a\", \"M\u00ednimo 8 caracteres\")\n        if not any(c.isupper() for c in password):\n            raise ValidacionError(\"contrase\u00f1a\", \"Debe contener may\u00fasculas\")\n        if not any(c.isdigit() for c in password):\n            raise ValidacionError(\"contrase\u00f1a\", \"Debe contener n\u00fameros\")\n        return True\n\ndef procesar_formulario(datos):\n    \"\"\"Procesa y valida un formulario.\"\"\"\n    errores = []\n\n    # Validar cada campo\n    validaciones = [\n        (Validador.validar_email, datos.get(\"email\", \"\")),\n        (Validador.validar_edad, datos.get(\"edad\", \"\")),\n        (Validador.validar_telefono, datos.get(\"telefono\", \"\")),\n        (Validador.validar_password, datos.get(\"password\", \"\"))\n    ]\n\n    for validar, valor in validaciones:\n        try:\n            validar(valor)\n        except ValidacionError as e:\n            errores.append(str(e))\n\n    if errores:\n        print(\"Errores de validaci\u00f3n:\")\n        for error in errores:\n            print(f\"  - {error}\")\n        return False\n\n    print(\"Formulario v\u00e1lido \u2713\")\n    return True\n\n# Probar\nformulario = {\n    \"email\": \"usuario@ejemplo.com\",\n    \"edad\": \"25\",\n    \"telefono\": \"612 345 678\",\n    \"password\": \"MiPassword123\"\n}\nprocesar_formulario(formulario)\n\nformulario_invalido = {\n    \"email\": \"invalido\",\n    \"edad\": \"quince\",\n    \"telefono\": \"123\",\n    \"password\": \"corta\"\n}\nprocesar_formulario(formulario_invalido)\n</code></pre>"},{"location":"conocimientos-basicos/09-excepciones/#910-resumen","title":"9.10. Resumen","text":"Concepto Descripci\u00f3n <code>try</code> Bloque que puede causar excepciones <code>except</code> Captura y maneja excepciones <code>else</code> Se ejecuta si no hay excepciones <code>finally</code> Se ejecuta siempre <code>raise</code> Lanza una excepci\u00f3n <code>Exception</code> Clase base para excepciones <code>as e</code> Captura la excepci\u00f3n en variable <pre><code>try:\n    # C\u00f3digo que puede fallar\n    resultado = operacion_riesgosa()\nexcept TipoError as e:\n    # Manejar error espec\u00edfico\n    print(f\"Error: {e}\")\nexcept Exception as e:\n    # Manejar cualquier otro error\n    print(f\"Error inesperado: {e}\")\nelse:\n    # Se ejecuta si no hubo errores\n    print(\"\u00c9xito\")\nfinally:\n    # Limpieza, se ejecuta siempre\n    liberar_recursos()\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/10-poo/","title":"\ud83d\udcda Unidad 10. Programaci\u00f3n Orientada a Objetos","text":"<p>La Programaci\u00f3n Orientada a Objetos (POO) es un paradigma que organiza el c\u00f3digo en torno a objetos que combinan datos (atributos) y comportamiento (m\u00e9todos).</p>"},{"location":"conocimientos-basicos/10-poo/#101-conceptos-fundamentales","title":"10.1. Conceptos Fundamentales","text":""},{"location":"conocimientos-basicos/10-poo/#que-es-un-objeto","title":"\u00bfQu\u00e9 es un Objeto?","text":"<p>Un objeto es una entidad que tiene:</p> <ul> <li>Atributos: Caracter\u00edsticas o datos (variables).</li> <li>M\u00e9todos: Comportamientos o acciones (funciones).</li> </ul> <p>Ejemplo del mundo real:</p> <p>Un coche tiene: *   Atributos: color, marca, velocidad, combustible *   M\u00e9todos: arrancar(), acelerar(), frenar(), apagar()</p>"},{"location":"conocimientos-basicos/10-poo/#que-es-una-clase","title":"\u00bfQu\u00e9 es una Clase?","text":"<p>Una clase es un molde o plantilla para crear objetos. Define qu\u00e9 atributos y m\u00e9todos tendr\u00e1n los objetos.</p> <pre><code>Clase: Coche (plantilla)\n    \u2193\nObjetos: mi_coche, tu_coche, coche_rojo (instancias)\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#102-crear-clases-y-objetos","title":"10.2. Crear Clases y Objetos","text":""},{"location":"conocimientos-basicos/10-poo/#sintaxis-basica","title":"Sintaxis B\u00e1sica","text":"<pre><code>class Coche:\n    \"\"\"Clase que representa un coche.\"\"\"\n    pass  # Clase vac\u00eda\n\n# Crear objetos (instancias)\nmi_coche = Coche()\ntu_coche = Coche()\n\nprint(type(mi_coche))  # &lt;class '__main__.Coche'&gt;\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#el-metodo-__init__","title":"El M\u00e9todo <code>__init__</code>","text":"<p>El m\u00e9todo <code>__init__</code> es el constructor. Se ejecuta autom\u00e1ticamente al crear un objeto.</p> <pre><code>class Coche:\n    def __init__(self, marca, modelo, a\u00f1o):\n        self.marca = marca     # Atributo de instancia\n        self.modelo = modelo\n        self.a\u00f1o = a\u00f1o\n\n# Crear objetos\nmi_coche = Coche(\"Toyota\", \"Corolla\", 2020)\ntu_coche = Coche(\"Honda\", \"Civic\", 2022)\n\n# Acceder a atributos\nprint(mi_coche.marca)   # Toyota\nprint(tu_coche.modelo)  # Civic\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#que-es-self","title":"\u00bfQu\u00e9 es <code>self</code>?","text":"<p><code>self</code> es una referencia al objeto actual. Permite acceder a sus atributos y m\u00e9todos.</p> <pre><code>class Persona:\n    def __init__(self, nombre):\n        self.nombre = nombre  # self.nombre es el atributo del objeto\n\n    def saludar(self):\n        print(f\"Hola, soy {self.nombre}\")  # Accede al atributo\n\nana = Persona(\"Ana\")\nana.saludar()  # Hola, soy Ana\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#103-atributos","title":"10.3. Atributos","text":""},{"location":"conocimientos-basicos/10-poo/#atributos-de-instancia","title":"Atributos de Instancia","text":"<p>Pertenecen a cada objeto individual.</p> <pre><code>class Estudiante:\n    def __init__(self, nombre, edad):\n        self.nombre = nombre  # Cada estudiante tiene su nombre\n        self.edad = edad\n        self.notas = []       # Lista vac\u00eda para cada estudiante\n\nana = Estudiante(\"Ana\", 20)\nluis = Estudiante(\"Luis\", 22)\n\nana.notas.append(9)\nprint(ana.notas)   # [9]\nprint(luis.notas)  # [] (independiente)\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#atributos-de-clase","title":"Atributos de Clase","text":"<p>Compartidos por todos los objetos de la clase.</p> <pre><code>class Estudiante:\n    # Atributo de clase\n    escuela = \"IES Python\"\n    total_estudiantes = 0\n\n    def __init__(self, nombre):\n        self.nombre = nombre  # Atributo de instancia\n        Estudiante.total_estudiantes += 1\n\nana = Estudiante(\"Ana\")\nluis = Estudiante(\"Luis\")\n\nprint(ana.escuela)              # IES Python\nprint(luis.escuela)             # IES Python\nprint(Estudiante.total_estudiantes)  # 2\n\n# Modificar atributo de clase\nEstudiante.escuela = \"IES Nuevo\"\nprint(ana.escuela)   # IES Nuevo\nprint(luis.escuela)  # IES Nuevo\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#104-metodos","title":"10.4. M\u00e9todos","text":""},{"location":"conocimientos-basicos/10-poo/#metodos-de-instancia","title":"M\u00e9todos de Instancia","text":"<p>Operan sobre un objeto espec\u00edfico usando <code>self</code>.</p> <pre><code>class CuentaBancaria:\n    def __init__(self, titular, saldo=0):\n        self.titular = titular\n        self.saldo = saldo\n\n    def depositar(self, cantidad):\n        if cantidad &gt; 0:\n            self.saldo += cantidad\n            print(f\"Depositados {cantidad}\u20ac. Saldo: {self.saldo}\u20ac\")\n\n    def retirar(self, cantidad):\n        if cantidad &gt; self.saldo:\n            print(\"Saldo insuficiente\")\n        else:\n            self.saldo -= cantidad\n            print(f\"Retirados {cantidad}\u20ac. Saldo: {self.saldo}\u20ac\")\n\n    def obtener_saldo(self):\n        return self.saldo\n\n# Uso\ncuenta = CuentaBancaria(\"Ana\", 1000)\ncuenta.depositar(500)   # Depositados 500\u20ac. Saldo: 1500\u20ac\ncuenta.retirar(200)     # Retirados 200\u20ac. Saldo: 1300\u20ac\nprint(cuenta.obtener_saldo())  # 1300\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#metodos-de-clase","title":"M\u00e9todos de Clase","text":"<p>Operan sobre la clase, no sobre instancias. Usan <code>@classmethod</code> y <code>cls</code>.</p> <pre><code>class Empleado:\n    aumento_anual = 1.05  # 5%\n\n    def __init__(self, nombre, salario):\n        self.nombre = nombre\n        self.salario = salario\n\n    @classmethod\n    def establecer_aumento(cls, porcentaje):\n        cls.aumento_anual = 1 + porcentaje / 100\n\n    @classmethod\n    def desde_string(cls, cadena):\n        \"\"\"Constructor alternativo.\"\"\"\n        nombre, salario = cadena.split(\"-\")\n        return cls(nombre, float(salario))\n\n    def aplicar_aumento(self):\n        self.salario *= self.aumento_anual\n\n# Uso\nEmpleado.establecer_aumento(10)  # 10% de aumento\n\nemp1 = Empleado(\"Ana\", 30000)\nemp2 = Empleado.desde_string(\"Luis-35000\")  # Constructor alternativo\n\nemp1.aplicar_aumento()\nprint(emp1.salario)  # 33000\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#metodos-estaticos","title":"M\u00e9todos Est\u00e1ticos","text":"<p>No reciben <code>self</code> ni <code>cls</code>. Son funciones relacionadas con la clase.</p> <pre><code>class Matematicas:\n    @staticmethod\n    def es_par(numero):\n        return numero % 2 == 0\n\n    @staticmethod\n    def factorial(n):\n        if n &lt;= 1:\n            return 1\n        return n * Matematicas.factorial(n - 1)\n\n    @staticmethod\n    def es_primo(n):\n        if n &lt; 2:\n            return False\n        for i in range(2, int(n**0.5) + 1):\n            if n % i == 0:\n                return False\n        return True\n\n# No necesitan instancia\nprint(Matematicas.es_par(4))      # True\nprint(Matematicas.factorial(5))   # 120\nprint(Matematicas.es_primo(17))   # True\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#105-encapsulamiento","title":"10.5. Encapsulamiento","text":"<p>El encapsulamiento oculta los detalles internos y protege los datos.</p>"},{"location":"conocimientos-basicos/10-poo/#convenciones-de-nombres","title":"Convenciones de Nombres","text":"<pre><code>class Ejemplo:\n    def __init__(self):\n        self.publico = \"Accesible\"      # P\u00fablico\n        self._protegido = \"Convenci\u00f3n\"  # Protegido (convenci\u00f3n)\n        self.__privado = \"Oculto\"       # Privado (name mangling)\n\nobj = Ejemplo()\nprint(obj.publico)       # Accesible\nprint(obj._protegido)    # Accesible (por convenci\u00f3n no deber\u00eda)\n# print(obj.__privado)   # AttributeError\nprint(obj._Ejemplo__privado)  # Accesible (name mangling)\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#propiedades-getters-y-setters","title":"Propiedades (Getters y Setters)","text":"<pre><code>class Temperatura:\n    def __init__(self, celsius=0):\n        self._celsius = celsius\n\n    @property\n    def celsius(self):\n        \"\"\"Getter para celsius.\"\"\"\n        return self._celsius\n\n    @celsius.setter\n    def celsius(self, valor):\n        \"\"\"Setter para celsius.\"\"\"\n        if valor &lt; -273.15:\n            raise ValueError(\"Temperatura por debajo del cero absoluto\")\n        self._celsius = valor\n\n    @property\n    def fahrenheit(self):\n        \"\"\"Propiedad calculada (solo lectura).\"\"\"\n        return self._celsius * 9/5 + 32\n\n    @fahrenheit.setter\n    def fahrenheit(self, valor):\n        self._celsius = (valor - 32) * 5/9\n\n# Uso\ntemp = Temperatura(25)\nprint(temp.celsius)     # 25\nprint(temp.fahrenheit)  # 77.0\n\ntemp.celsius = 30\nprint(temp.fahrenheit)  # 86.0\n\ntemp.fahrenheit = 100\nprint(temp.celsius)     # 37.77...\n\n# temp.celsius = -300  # ValueError\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#ejemplo-completo-de-encapsulamiento","title":"Ejemplo Completo de Encapsulamiento","text":"<pre><code>class CuentaBancaria:\n    def __init__(self, titular, saldo_inicial=0):\n        self._titular = titular\n        self.__saldo = saldo_inicial\n        self.__historial = []\n\n    @property\n    def titular(self):\n        return self._titular\n\n    @property\n    def saldo(self):\n        return self.__saldo\n\n    @property\n    def historial(self):\n        return self.__historial.copy()  # Devuelve copia\n\n    def depositar(self, cantidad):\n        if cantidad &lt;= 0:\n            raise ValueError(\"La cantidad debe ser positiva\")\n        self.__saldo += cantidad\n        self.__historial.append(f\"Dep\u00f3sito: +{cantidad}\u20ac\")\n\n    def retirar(self, cantidad):\n        if cantidad &lt;= 0:\n            raise ValueError(\"La cantidad debe ser positiva\")\n        if cantidad &gt; self.__saldo:\n            raise ValueError(\"Saldo insuficiente\")\n        self.__saldo -= cantidad\n        self.__historial.append(f\"Retiro: -{cantidad}\u20ac\")\n\n# Uso\ncuenta = CuentaBancaria(\"Ana\", 1000)\ncuenta.depositar(500)\ncuenta.retirar(200)\n\nprint(cuenta.saldo)      # 1300\nprint(cuenta.historial)  # ['Dep\u00f3sito: +500\u20ac', 'Retiro: -200\u20ac']\n\n# No se puede modificar directamente\n# cuenta.saldo = 999999  # AttributeError\n# cuenta.__saldo = 0     # No afecta al atributo real\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#106-herencia","title":"10.6. Herencia","text":"<p>La herencia permite crear nuevas clases basadas en clases existentes.</p>"},{"location":"conocimientos-basicos/10-poo/#sintaxis-basica_1","title":"Sintaxis B\u00e1sica","text":"<pre><code>class Animal:\n    def __init__(self, nombre):\n        self.nombre = nombre\n\n    def hablar(self):\n        print(\"El animal hace un sonido\")\n\nclass Perro(Animal):  # Perro hereda de Animal\n    def hablar(self):\n        print(f\"{self.nombre} dice: \u00a1Guau!\")\n\nclass Gato(Animal):\n    def hablar(self):\n        print(f\"{self.nombre} dice: \u00a1Miau!\")\n\n# Uso\nperro = Perro(\"Max\")\ngato = Gato(\"Luna\")\n\nperro.hablar()  # Max dice: \u00a1Guau!\ngato.hablar()   # Luna dice: \u00a1Miau!\n\n# Verificar herencia\nprint(isinstance(perro, Perro))   # True\nprint(isinstance(perro, Animal))  # True\nprint(issubclass(Perro, Animal))  # True\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#metodo-super","title":"M\u00e9todo <code>super()</code>","text":"<p><code>super()</code> permite acceder a la clase padre.</p> <pre><code>class Vehiculo:\n    def __init__(self, marca, modelo):\n        self.marca = marca\n        self.modelo = modelo\n\n    def info(self):\n        return f\"{self.marca} {self.modelo}\"\n\nclass Coche(Vehiculo):\n    def __init__(self, marca, modelo, puertas):\n        super().__init__(marca, modelo)  # Llama al __init__ del padre\n        self.puertas = puertas\n\n    def info(self):\n        return f\"{super().info()} - {self.puertas} puertas\"\n\ncoche = Coche(\"Toyota\", \"Corolla\", 4)\nprint(coche.info())  # Toyota Corolla - 4 puertas\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#herencia-multiple","title":"Herencia M\u00faltiple","text":"<p>Python permite heredar de m\u00faltiples clases.</p> <pre><code>class Volador:\n    def volar(self):\n        print(\"Estoy volando\")\n\nclass Nadador:\n    def nadar(self):\n        print(\"Estoy nadando\")\n\nclass Pato(Volador, Nadador):\n    def graznar(self):\n        print(\"\u00a1Cuac!\")\n\npato = Pato()\npato.volar()    # Estoy volando\npato.nadar()    # Estoy nadando\npato.graznar()  # \u00a1Cuac!\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#mro-method-resolution-order","title":"MRO (Method Resolution Order)","text":"<pre><code>class A:\n    def metodo(self):\n        print(\"A\")\n\nclass B(A):\n    def metodo(self):\n        print(\"B\")\n\nclass C(A):\n    def metodo(self):\n        print(\"C\")\n\nclass D(B, C):\n    pass\n\nd = D()\nd.metodo()  # B (busca en B antes que en C)\n\n# Ver orden de resoluci\u00f3n\nprint(D.__mro__)\n# (&lt;class 'D'&gt;, &lt;class 'B'&gt;, &lt;class 'C'&gt;, &lt;class 'A'&gt;, &lt;class 'object'&gt;)\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#107-polimorfismo","title":"10.7. Polimorfismo","text":"<p>El polimorfismo permite que objetos de diferentes clases respondan al mismo m\u00e9todo de forma diferente.</p> <pre><code>class Forma:\n    def area(self):\n        raise NotImplementedError(\"Subclases deben implementar este m\u00e9todo\")\n\nclass Rectangulo(Forma):\n    def __init__(self, base, altura):\n        self.base = base\n        self.altura = altura\n\n    def area(self):\n        return self.base * self.altura\n\nclass Circulo(Forma):\n    def __init__(self, radio):\n        self.radio = radio\n\n    def area(self):\n        import math\n        return math.pi * self.radio ** 2\n\nclass Triangulo(Forma):\n    def __init__(self, base, altura):\n        self.base = base\n        self.altura = altura\n\n    def area(self):\n        return self.base * self.altura / 2\n\n# Polimorfismo en acci\u00f3n\nformas = [\n    Rectangulo(4, 5),\n    Circulo(3),\n    Triangulo(4, 6)\n]\n\nfor forma in formas:\n    print(f\"{type(forma).__name__}: \u00e1rea = {forma.area():.2f}\")\n\n# Rectangulo: \u00e1rea = 20.00\n# Circulo: \u00e1rea = 28.27\n# Triangulo: \u00e1rea = 12.00\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#duck-typing","title":"Duck Typing","text":"<p>\"Si camina como un pato y hace cuac como un pato, es un pato.\"</p> <pre><code>class Archivo:\n    def leer(self):\n        return \"Contenido del archivo\"\n\nclass BaseDatos:\n    def leer(self):\n        return \"Datos de la base de datos\"\n\nclass API:\n    def leer(self):\n        return \"Respuesta de la API\"\n\ndef procesar_datos(fuente):\n    \"\"\"No importa el tipo, solo que tenga m\u00e9todo leer().\"\"\"\n    datos = fuente.leer()\n    print(f\"Procesando: {datos}\")\n\n# Funciona con cualquier objeto que tenga leer()\nprocesar_datos(Archivo())\nprocesar_datos(BaseDatos())\nprocesar_datos(API())\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#108-clases-abstractas","title":"10.8. Clases Abstractas","text":"<p>Las clases abstractas definen una interfaz que las subclases deben implementar.</p> <pre><code>from abc import ABC, abstractmethod\n\nclass FiguraGeometrica(ABC):\n    \"\"\"Clase abstracta para figuras geom\u00e9tricas.\"\"\"\n\n    @abstractmethod\n    def area(self):\n        \"\"\"Calcula el \u00e1rea de la figura.\"\"\"\n        pass\n\n    @abstractmethod\n    def perimetro(self):\n        \"\"\"Calcula el per\u00edmetro de la figura.\"\"\"\n        pass\n\n    def descripcion(self):\n        \"\"\"M\u00e9todo concreto (no abstracto).\"\"\"\n        return f\"Soy una {self.__class__.__name__}\"\n\nclass Cuadrado(FiguraGeometrica):\n    def __init__(self, lado):\n        self.lado = lado\n\n    def area(self):\n        return self.lado ** 2\n\n    def perimetro(self):\n        return 4 * self.lado\n\nclass Circulo(FiguraGeometrica):\n    def __init__(self, radio):\n        self.radio = radio\n\n    def area(self):\n        import math\n        return math.pi * self.radio ** 2\n\n    def perimetro(self):\n        import math\n        return 2 * math.pi * self.radio\n\n# No se puede instanciar clase abstracta\n# figura = FiguraGeometrica()  # TypeError\n\n# Pero s\u00ed las subclases concretas\ncuadrado = Cuadrado(5)\ncirculo = Circulo(3)\n\nprint(cuadrado.descripcion())  # Soy una Cuadrado\nprint(f\"\u00c1rea: {cuadrado.area()}\")  # 25\nprint(f\"Per\u00edmetro: {cuadrado.perimetro()}\")  # 20\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#109-metodos-magicos-dunder-methods","title":"10.9. M\u00e9todos M\u00e1gicos (Dunder Methods)","text":"<p>Los m\u00e9todos m\u00e1gicos (double underscore) permiten personalizar el comportamiento de los objetos.</p>"},{"location":"conocimientos-basicos/10-poo/#representacion","title":"Representaci\u00f3n","text":"<pre><code>class Punto:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __str__(self):\n        \"\"\"Para print() y str()\"\"\"\n        return f\"Punto({self.x}, {self.y})\"\n\n    def __repr__(self):\n        \"\"\"Para representaci\u00f3n t\u00e9cnica\"\"\"\n        return f\"Punto(x={self.x}, y={self.y})\"\n\npunto = Punto(3, 4)\nprint(punto)        # Punto(3, 4)\nprint(repr(punto))  # Punto(x=3, y=4)\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#operadores-aritmeticos","title":"Operadores Aritm\u00e9ticos","text":"<pre><code>class Vector:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __add__(self, otro):\n        \"\"\"Para + \"\"\"\n        return Vector(self.x + otro.x, self.y + otro.y)\n\n    def __sub__(self, otro):\n        \"\"\"Para - \"\"\"\n        return Vector(self.x - otro.x, self.y - otro.y)\n\n    def __mul__(self, escalar):\n        \"\"\"Para * (con escalar)\"\"\"\n        return Vector(self.x * escalar, self.y * escalar)\n\n    def __rmul__(self, escalar):\n        \"\"\"Para escalar * vector\"\"\"\n        return self.__mul__(escalar)\n\n    def __abs__(self):\n        \"\"\"Para abs()\"\"\"\n        return (self.x**2 + self.y**2) ** 0.5\n\n    def __str__(self):\n        return f\"Vector({self.x}, {self.y})\"\n\nv1 = Vector(3, 4)\nv2 = Vector(1, 2)\n\nprint(v1 + v2)      # Vector(4, 6)\nprint(v1 - v2)      # Vector(2, 2)\nprint(v1 * 2)       # Vector(6, 8)\nprint(3 * v1)       # Vector(9, 12)\nprint(abs(v1))      # 5.0\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#comparacion","title":"Comparaci\u00f3n","text":"<pre><code>class Estudiante:\n    def __init__(self, nombre, nota):\n        self.nombre = nombre\n        self.nota = nota\n\n    def __eq__(self, otro):\n        \"\"\"Para == \"\"\"\n        return self.nota == otro.nota\n\n    def __lt__(self, otro):\n        \"\"\"Para &lt; \"\"\"\n        return self.nota &lt; otro.nota\n\n    def __le__(self, otro):\n        \"\"\"Para &lt;= \"\"\"\n        return self.nota &lt;= otro.nota\n\n    def __gt__(self, otro):\n        \"\"\"Para &gt; \"\"\"\n        return self.nota &gt; otro.nota\n\n    def __str__(self):\n        return f\"{self.nombre}: {self.nota}\"\n\nestudiantes = [\n    Estudiante(\"Ana\", 8),\n    Estudiante(\"Luis\", 9),\n    Estudiante(\"Mar\u00eda\", 7)\n]\n\n# sorted() usa __lt__\nordenados = sorted(estudiantes)\nfor e in ordenados:\n    print(e)\n# Mar\u00eda: 7\n# Ana: 8\n# Luis: 9\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#contenedores","title":"Contenedores","text":"<pre><code>class MiLista:\n    def __init__(self, elementos=None):\n        self._datos = elementos if elementos else []\n\n    def __len__(self):\n        \"\"\"Para len()\"\"\"\n        return len(self._datos)\n\n    def __getitem__(self, indice):\n        \"\"\"Para [] lectura\"\"\"\n        return self._datos[indice]\n\n    def __setitem__(self, indice, valor):\n        \"\"\"Para [] escritura\"\"\"\n        self._datos[indice] = valor\n\n    def __contains__(self, item):\n        \"\"\"Para 'in'\"\"\"\n        return item in self._datos\n\n    def __iter__(self):\n        \"\"\"Para iteraci\u00f3n\"\"\"\n        return iter(self._datos)\n\n    def append(self, item):\n        self._datos.append(item)\n\nlista = MiLista([1, 2, 3])\nprint(len(lista))      # 3\nprint(lista[0])        # 1\nprint(2 in lista)      # True\n\nfor item in lista:\n    print(item)\n\nlista[0] = 10\nprint(lista[0])  # 10\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#contexto-with","title":"Contexto (with)","text":"<pre><code>class GestorArchivo:\n    def __init__(self, nombre, modo):\n        self.nombre = nombre\n        self.modo = modo\n        self.archivo = None\n\n    def __enter__(self):\n        \"\"\"Al entrar en 'with'\"\"\"\n        print(f\"Abriendo {self.nombre}\")\n        self.archivo = open(self.nombre, self.modo)\n        return self.archivo\n\n    def __exit__(self, tipo_exc, valor_exc, tb):\n        \"\"\"Al salir de 'with'\"\"\"\n        print(f\"Cerrando {self.nombre}\")\n        if self.archivo:\n            self.archivo.close()\n        return False  # No suprimir excepciones\n\n# Uso\nwith GestorArchivo(\"test.txt\", \"w\") as f:\n    f.write(\"Hola mundo\")\n# Abriendo test.txt\n# Cerrando test.txt\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#1010-composicion-vs-herencia","title":"10.10. Composici\u00f3n vs Herencia","text":""},{"location":"conocimientos-basicos/10-poo/#herencia-es-un","title":"Herencia (\"es un\")","text":"<pre><code>class Vehiculo:\n    def mover(self):\n        print(\"El veh\u00edculo se mueve\")\n\nclass Coche(Vehiculo):  # Un coche ES UN veh\u00edculo\n    pass\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#composicion-tiene-un","title":"Composici\u00f3n (\"tiene un\")","text":"<pre><code>class Motor:\n    def arrancar(self):\n        print(\"Motor arrancado\")\n\n    def detener(self):\n        print(\"Motor detenido\")\n\nclass Rueda:\n    def rodar(self):\n        print(\"La rueda rueda\")\n\nclass Coche:  # Un coche TIENE UN motor y TIENE ruedas\n    def __init__(self):\n        self.motor = Motor()\n        self.ruedas = [Rueda() for _ in range(4)]\n\n    def arrancar(self):\n        self.motor.arrancar()\n\n    def conducir(self):\n        for rueda in self.ruedas:\n            rueda.rodar()\n\ncoche = Coche()\ncoche.arrancar()\ncoche.conducir()\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#cuando-usar-cada-una","title":"Cu\u00e1ndo Usar Cada Una","text":"<ul> <li>Herencia: Cuando hay una relaci\u00f3n clara \"es un\".</li> <li>Composici\u00f3n: Cuando hay una relaci\u00f3n \"tiene un\" o para mayor flexibilidad.</li> </ul>"},{"location":"conocimientos-basicos/10-poo/#1011-ejercicio-completo-sistema-de-gestion","title":"10.11. Ejercicio Completo: Sistema de Gesti\u00f3n","text":"<pre><code>from abc import ABC, abstractmethod\nfrom datetime import datetime\n\nclass Persona(ABC):\n    \"\"\"Clase abstracta base para personas.\"\"\"\n\n    def __init__(self, nombre, email):\n        self.nombre = nombre\n        self.email = email\n        self._fecha_registro = datetime.now()\n\n    @abstractmethod\n    def descripcion(self):\n        pass\n\n    def __str__(self):\n        return f\"{self.__class__.__name__}: {self.nombre}\"\n\nclass Empleado(Persona):\n    \"\"\"Empleado de la empresa.\"\"\"\n\n    _contador = 0\n\n    def __init__(self, nombre, email, puesto, salario):\n        super().__init__(nombre, email)\n        Empleado._contador += 1\n        self.id = f\"EMP{Empleado._contador:04d}\"\n        self.puesto = puesto\n        self._salario = salario\n        self.proyectos = []\n\n    @property\n    def salario(self):\n        return self._salario\n\n    @salario.setter\n    def salario(self, valor):\n        if valor &lt; 0:\n            raise ValueError(\"El salario no puede ser negativo\")\n        self._salario = valor\n\n    def descripcion(self):\n        return f\"{self.nombre} - {self.puesto} ({self.id})\"\n\n    def asignar_proyecto(self, proyecto):\n        self.proyectos.append(proyecto)\n        proyecto.miembros.append(self)\n\nclass Cliente(Persona):\n    \"\"\"Cliente de la empresa.\"\"\"\n\n    _contador = 0\n\n    def __init__(self, nombre, email, empresa):\n        super().__init__(nombre, email)\n        Cliente._contador += 1\n        self.id = f\"CLI{Cliente._contador:04d}\"\n        self.empresa = empresa\n        self.pedidos = []\n\n    def descripcion(self):\n        return f\"{self.nombre} de {self.empresa} ({self.id})\"\n\nclass Proyecto:\n    \"\"\"Proyecto de la empresa.\"\"\"\n\n    def __init__(self, nombre, descripcion, presupuesto):\n        self.nombre = nombre\n        self.descripcion = descripcion\n        self.presupuesto = presupuesto\n        self.miembros = []\n        self.tareas = []\n        self._completado = False\n\n    def agregar_tarea(self, tarea):\n        self.tareas.append(tarea)\n\n    @property\n    def progreso(self):\n        if not self.tareas:\n            return 0\n        completadas = sum(1 for t in self.tareas if t.completada)\n        return (completadas / len(self.tareas)) * 100\n\n    def __str__(self):\n        return f\"Proyecto: {self.nombre} ({self.progreso:.0f}% completado)\"\n\nclass Tarea:\n    \"\"\"Tarea de un proyecto.\"\"\"\n\n    def __init__(self, titulo, descripcion):\n        self.titulo = titulo\n        self.descripcion = descripcion\n        self.completada = False\n\n    def completar(self):\n        self.completada = True\n\n    def __str__(self):\n        estado = \"\u2713\" if self.completada else \"\u25cb\"\n        return f\"{estado} {self.titulo}\"\n\n# --- Uso del sistema ---\n\n# Crear empleados\nemp1 = Empleado(\"Ana Garc\u00eda\", \"ana@empresa.com\", \"Desarrolladora Senior\", 45000)\nemp2 = Empleado(\"Luis Mart\u00ednez\", \"luis@empresa.com\", \"Dise\u00f1ador UX\", 38000)\n\n# Crear proyecto\nproyecto = Proyecto(\"App M\u00f3vil\", \"Desarrollo de app para clientes\", 50000)\n\n# Asignar empleados\nemp1.asignar_proyecto(proyecto)\nemp2.asignar_proyecto(proyecto)\n\n# Agregar tareas\nproyecto.agregar_tarea(Tarea(\"Dise\u00f1o de interfaz\", \"Crear mockups\"))\nproyecto.agregar_tarea(Tarea(\"Backend API\", \"Desarrollar endpoints\"))\nproyecto.agregar_tarea(Tarea(\"Testing\", \"Pruebas unitarias\"))\n\n# Completar tareas\nproyecto.tareas[0].completar()\nproyecto.tareas[1].completar()\n\n# Mostrar informaci\u00f3n\nprint(emp1.descripcion())\nprint(proyecto)\nfor tarea in proyecto.tareas:\n    print(f\"  {tarea}\")\n\n# Crear cliente\ncliente = Cliente(\"Mar\u00eda L\u00f3pez\", \"maria@cliente.com\", \"TechCorp\")\nprint(cliente.descripcion())\n</code></pre>"},{"location":"conocimientos-basicos/10-poo/#1012-resumen-de-poo","title":"10.12. Resumen de POO","text":"Concepto Descripci\u00f3n Clase Plantilla para crear objetos Objeto Instancia de una clase Atributo Variable de un objeto M\u00e9todo Funci\u00f3n de un objeto <code>__init__</code> Constructor <code>self</code> Referencia al objeto actual Encapsulamiento Ocultar datos internos Herencia Crear clases basadas en otras Polimorfismo Mismo m\u00e9todo, diferente comportamiento Abstracci\u00f3n Definir interfaces <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/11-json/","title":"\ud83d\udcc4 Trabajando con JSON en Python","text":"<p>JSON (JavaScript Object Notation) es un formato ligero de intercambio de datos, f\u00e1cil de leer y escribir para humanos, y f\u00e1cil de parsear y generar para m\u00e1quinas. Es el formato est\u00e1ndar para APIs web y configuraciones.</p>"},{"location":"conocimientos-basicos/11-json/#1-que-es-json","title":"1. \u00bfQu\u00e9 es JSON?","text":"<p>JSON es un formato de texto que representa datos estructurados. Su sintaxis es muy similar a los diccionarios y listas de Python.</p>"},{"location":"conocimientos-basicos/11-json/#estructura-de-json","title":"Estructura de JSON","text":"<pre><code>{\n    \"nombre\": \"Fran\",\n    \"edad\": 25,\n    \"activo\": true,\n    \"cursos\": [\"Python\", \"Machine Learning\", \"Deep Learning\"],\n    \"direccion\": {\n        \"ciudad\": \"Alicante\",\n        \"pais\": \"Espa\u00f1a\"\n    }\n}\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#tipos-de-datos-en-json","title":"Tipos de datos en JSON","text":"JSON Python <code>object</code> <code>{}</code> <code>dict</code> <code>array</code> <code>[]</code> <code>list</code> <code>string</code> <code>\"texto\"</code> <code>str</code> <code>number</code> <code>123</code> o <code>1.5</code> <code>int</code> o <code>float</code> <code>true</code> / <code>false</code> <code>True</code> / <code>False</code> <code>null</code> <code>None</code>"},{"location":"conocimientos-basicos/11-json/#2-el-modulo-json","title":"2. El M\u00f3dulo <code>json</code>","text":"<p>Python incluye el m\u00f3dulo <code>json</code> en su biblioteca est\u00e1ndar:</p> <pre><code>import json\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#funciones-principales","title":"Funciones Principales","text":"Funci\u00f3n Descripci\u00f3n <code>json.loads()</code> Convierte string JSON \u2192 diccionario Python <code>json.dumps()</code> Convierte diccionario Python \u2192 string JSON <code>json.load()</code> Lee archivo JSON \u2192 diccionario Python <code>json.dump()</code> Escribe diccionario Python \u2192 archivo JSON"},{"location":"conocimientos-basicos/11-json/#3-convertir-json-string-a-diccionario","title":"3. Convertir JSON String a Diccionario","text":""},{"location":"conocimientos-basicos/11-json/#jsonloads-parse-de-string","title":"json.loads() - Parse de String","text":"<pre><code>import json\n\n# JSON como string (nota las comillas triples)\nprofesor_string = \"\"\"\n{\n    \"nombre\": \"Fran\",\n    \"apellido\": \"Garc\u00eda\",\n    \"edad\": 25,\n    \"domicilio\": {\n        \"direccion\": \"Lillo Juan, 128\",\n        \"ciudad\": \"San Vicente del Raspeig\",\n        \"comunidad\": \"Valenciana\",\n        \"codigoPostal\": \"03690\"\n    },\n    \"numerosTelefonos\": [\n        {\"tipo\": \"casa\", \"numero\": \"666 666 666\"},\n        {\"tipo\": \"movil\", \"numero\": \"777 777 777\"}\n    ]\n}\n\"\"\"\n\n# Convertir JSON string a diccionario Python\nprofesor_dict = json.loads(profesor_string)\n\n# Verificar el tipo\nprint(type(profesor_dict))  # &lt;class 'dict'&gt;\n\n# Acceder a los datos\nprint(profesor_dict['nombre'])      # Fran\nprint(profesor_dict['edad'])        # 25\n\n# Acceder a datos anidados\nprint(profesor_dict['domicilio']['ciudad'])  # San Vicente del Raspeig\n\n# Acceder a listas\nprint(profesor_dict['numerosTelefonos'][0]['numero'])  # 666 666 666\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#4-convertir-diccionario-a-json-string","title":"4. Convertir Diccionario a JSON String","text":""},{"location":"conocimientos-basicos/11-json/#jsondumps-serializacion","title":"json.dumps() - Serializaci\u00f3n","text":"<pre><code>import json\n\n# Diccionario Python\nusuario = {\n    \"nombre\": \"Ana\",\n    \"edad\": 30,\n    \"activo\": True,\n    \"cursos\": [\"Python\", \"Data Science\"],\n    \"puntuacion\": None\n}\n\n# Convertir a JSON string\njson_string = json.dumps(usuario)\nprint(json_string)\n# {\"nombre\": \"Ana\", \"edad\": 30, \"activo\": true, \"cursos\": [\"Python\", \"Data Science\"], \"puntuacion\": null}\n\n# Con formato legible (indentaci\u00f3n)\njson_bonito = json.dumps(usuario, indent=4)\nprint(json_bonito)\n# {\n#     \"nombre\": \"Ana\",\n#     \"edad\": 30,\n#     \"activo\": true,\n#     \"cursos\": [\n#         \"Python\",\n#         \"Data Science\"\n#     ],\n#     \"puntuacion\": null\n# }\n\n# Con caracteres especiales (espa\u00f1ol)\ndatos = {\"ciudad\": \"San Jos\u00e9\", \"pa\u00eds\": \"Espa\u00f1a\"}\nprint(json.dumps(datos))  # {\"ciudad\": \"San Jos\\u00e9\", \"pa\\u00eds\": \"Espa\\u00f1a\"}\nprint(json.dumps(datos, ensure_ascii=False))  # {\"ciudad\": \"San Jos\u00e9\", \"pa\u00eds\": \"Espa\u00f1a\"}\n\n# Ordenar claves alfab\u00e9ticamente\nprint(json.dumps(usuario, sort_keys=True, indent=2))\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#5-leer-json-desde-archivo","title":"5. Leer JSON desde Archivo","text":""},{"location":"conocimientos-basicos/11-json/#jsonload-lectura-de-archivo","title":"json.load() - Lectura de Archivo","text":"<pre><code>import json\n\n# M\u00e9todo 1: Leer y parsear directamente\nwith open('profesor.json', 'r', encoding='utf-8') as archivo:\n    datos = json.load(archivo)\n\nprint(datos['nombre'])  # Fran\nprint(datos['domicilio']['ciudad'])  # San Vicente del Raspeig\n\n# M\u00e9todo 2: Leer como string y luego parsear\nwith open('profesor.json', 'r', encoding='utf-8') as archivo:\n    contenido = archivo.read()  # String con el JSON\n    datos = json.loads(contenido)  # Parsear el string\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#ejemplo-con-archivo-lukejson","title":"Ejemplo con archivo luke.json","text":"<pre><code>import json\n\nwith open('luke.json', 'r', encoding='utf-8') as f:\n    luke = json.load(f)\n\nprint(luke['name'])    # Luke Skywalker\nprint(luke['height'])  # 172\nprint(luke['films'])   # Lista de URLs de pel\u00edculas\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#6-escribir-json-en-archivo","title":"6. Escribir JSON en Archivo","text":""},{"location":"conocimientos-basicos/11-json/#jsondump-escritura-en-archivo","title":"json.dump() - Escritura en Archivo","text":"<pre><code>import json\n\n# Datos a guardar\nusuario = {\n    \"nombre\": \"Fran\",\n    \"apellido\": \"Garc\u00eda\",\n    \"edad\": 48,\n    \"ciudad\": \"Alicante\",\n    \"habilidades\": [\"Python\", \"Machine Learning\", \"SQL\"]\n}\n\n# Guardar en archivo JSON\nwith open('usuario.json', 'w', encoding='utf-8') as archivo:\n    json.dump(usuario, archivo)\n\n# Con formato legible\nwith open('usuario_bonito.json', 'w', encoding='utf-8') as archivo:\n    json.dump(usuario, archivo, indent=4, ensure_ascii=False)\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#archivo-resultante-usuario_bonitojson","title":"Archivo resultante (usuario_bonito.json):","text":"<pre><code>{\n    \"nombre\": \"Fran\",\n    \"apellido\": \"Garc\u00eda\",\n    \"edad\": 48,\n    \"ciudad\": \"Alicante\",\n    \"habilidades\": [\n        \"Python\",\n        \"Machine Learning\",\n        \"SQL\"\n    ]\n}\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#7-leer-json-desde-internet-apis","title":"7. Leer JSON desde Internet (APIs)","text":""},{"location":"conocimientos-basicos/11-json/#usando-la-libreria-requests","title":"Usando la librer\u00eda <code>requests</code>","text":"<pre><code># Primero instalar: pip install requests\nimport requests\nimport json\n\n# Hacer petici\u00f3n GET a una API\nurl = \"https://jsonplaceholder.typicode.com/todos/1\"\nresponse = requests.get(url)\n\n# M\u00e9todo 1: Usar .json() de requests (m\u00e1s simple)\ndatos = response.json()\nprint(datos)\n# {'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n\n# M\u00e9todo 2: Parsear el texto de la respuesta\ndatos = json.loads(response.text)\n\n# Acceder a los datos\nprint(datos['userId'])     # 1\nprint(datos['title'])      # delectus aut autem\nprint(datos['completed'])  # False\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#obtener-lista-de-datos","title":"Obtener Lista de Datos","text":"<pre><code>import requests\n\nurl = \"https://jsonplaceholder.typicode.com/todos\"\nresponse = requests.get(url)\ntareas = response.json()  # Lista de diccionarios\n\nprint(type(tareas))  # &lt;class 'list'&gt;\nprint(len(tareas))   # 200\n\n# Filtrar con list comprehension\npendientes = [t for t in tareas if not t['completed']]\ncompletadas = [t for t in tareas if t['completed']]\n\nprint(f\"Pendientes: {len(pendientes)}\")   # 110\nprint(f\"Completadas: {len(completadas)}\") # 90\n\n# Mostrar las primeras 3 tareas pendientes\nfor tarea in pendientes[:3]:\n    print(f\"ID: {tarea['id']} - {tarea['title']}\")\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#api-con-autenticacion-headers","title":"API con Autenticaci\u00f3n (Headers)","text":"<pre><code>import requests\nimport json\n\n# API que requiere token de autenticaci\u00f3n\ntoken = \"XXXX\"  # Reemplazar con tu token real\n\nurl = \"https://api.football-data.org/v4/teams/86/matches?status=SCHEDULED\"\n\n# Configurar cabeceras\ncabeceras = {\n    \"X-Auth-Token\": token,\n    \"User-Agent\": \"PostmanRuntime/7.26.8\"\n}\n\nresponse = requests.get(url, headers=cabeceras)\n\nif response.status_code == 200:\n    datos = response.json()\n    print(datos)\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#ejemplo-api-de-star-wars","title":"Ejemplo: API de Star Wars","text":"<pre><code>import requests\nimport json\n\ncodigo_personaje = input(\"Introduce el c\u00f3digo del personaje: \")\n\nurl = f\"https://swapi.dev/api/people/{codigo_personaje}/?format=json\"\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    personaje = response.json()\n\n    print(f\"Nombre: {personaje['name']}\")\n    print(f\"Altura: {personaje['height']} cm\")\n    print(f\"Peso: {personaje['mass']} kg\")\n    print(f\"Color de ojos: {personaje['eye_color']}\")\n\n    # Obtener informaci\u00f3n de las pel\u00edculas\n    print(\"\\nPel\u00edculas:\")\n    for url_pelicula in personaje['films']:\n        resp_pelicula = requests.get(url_pelicula)\n        pelicula = resp_pelicula.json()\n        print(f\"  - {pelicula['title']} ({pelicula['release_date']})\")\nelse:\n    print(\"Personaje no encontrado\")\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#8-manejo-de-errores-en-json","title":"8. Manejo de Errores en JSON","text":"<pre><code>import json\n\n# JSON mal formado\njson_invalido = '{\"nombre\": \"Ana\", \"edad\": }'\n\ntry:\n    datos = json.loads(json_invalido)\nexcept json.JSONDecodeError as e:\n    print(f\"Error al parsear JSON: {e}\")\n    # Error al parsear JSON: Expecting value: line 1 column 27 (char 26)\n\n# Archivo que no existe\ntry:\n    with open('no_existe.json', 'r') as f:\n        datos = json.load(f)\nexcept FileNotFoundError:\n    print(\"El archivo no existe\")\nexcept json.JSONDecodeError:\n    print(\"El archivo no contiene JSON v\u00e1lido\")\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#9-json-y-clases-python-serializacion-avanzada","title":"9. JSON y Clases Python (Serializaci\u00f3n Avanzada)","text":""},{"location":"conocimientos-basicos/11-json/#problema-las-clases-no-son-serializables-directamente","title":"Problema: Las clases no son serializables directamente","text":"<pre><code>import json\n\nclass Persona:\n    def __init__(self, nombre, edad):\n        self.nombre = nombre\n        self.edad = edad\n\npersona = Persona(\"Ana\", 25)\n\n# Esto genera error\n# json.dumps(persona)  # TypeError: Object of type Persona is not JSON serializable\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#solucion-1-convertir-a-diccionario-manualmente","title":"Soluci\u00f3n 1: Convertir a diccionario manualmente","text":"<pre><code>import json\n\nclass Persona:\n    def __init__(self, nombre, edad):\n        self.nombre = nombre\n        self.edad = edad\n\n    def to_dict(self):\n        return {\n            \"nombre\": self.nombre,\n            \"edad\": self.edad\n        }\n\npersona = Persona(\"Ana\", 25)\njson_string = json.dumps(persona.to_dict())\nprint(json_string)  # {\"nombre\": \"Ana\", \"edad\": 25}\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#solucion-2-usar-__dict__","title":"Soluci\u00f3n 2: Usar <code>__dict__</code>","text":"<pre><code>import json\n\nclass Persona:\n    def __init__(self, nombre, edad):\n        self.nombre = nombre\n        self.edad = edad\n\npersona = Persona(\"Ana\", 25)\njson_string = json.dumps(persona.__dict__)\nprint(json_string)  # {\"nombre\": \"Ana\", \"edad\": 25}\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#solucion-3-encoder-personalizado","title":"Soluci\u00f3n 3: Encoder Personalizado","text":"<pre><code>import json\n\nclass Persona:\n    def __init__(self, nombre, edad):\n        self.nombre = nombre\n        self.edad = edad\n\nclass PersonaEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Persona):\n            return {\n                \"nombre\": obj.nombre,\n                \"edad\": obj.edad\n            }\n        return super().default(obj)\n\npersona = Persona(\"Ana\", 25)\njson_string = json.dumps(persona, cls=PersonaEncoder)\nprint(json_string)  # {\"nombre\": \"Ana\", \"edad\": 25}\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#decoder-personalizado-json-objeto","title":"Decoder Personalizado (JSON \u2192 Objeto)","text":"<pre><code>import json\n\nclass Tarea:\n    def __init__(self, user_id, id, title, completed):\n        self.user_id = user_id\n        self.id = id\n        self.title = title\n        self.completed = completed\n\n    def __repr__(self):\n        return f\"Tarea({self.id}: {self.title})\"\n\nclass TareaDecoder(json.JSONDecoder):\n    def __init__(self):\n        super().__init__(object_hook=self.object_hook)\n\n    def object_hook(self, json_dict):\n        # Solo convertir si tiene las claves esperadas\n        if 'userId' in json_dict and 'title' in json_dict:\n            return Tarea(\n                json_dict.get('userId'),\n                json_dict.get('id'),\n                json_dict.get('title'),\n                json_dict.get('completed')\n            )\n        return json_dict\n\n# Usar el decoder\nimport requests\n\nurl = \"https://jsonplaceholder.typicode.com/todos/1\"\nresponse = requests.get(url)\n\ntarea = json.loads(response.text, cls=TareaDecoder)\nprint(type(tarea))       # &lt;class 'Tarea'&gt;\nprint(tarea.id)          # 1\nprint(tarea.title)       # delectus aut autem\nprint(tarea.completed)   # False\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#10-ejemplo-practico-consumir-api-y-crear-dataset","title":"10. Ejemplo Pr\u00e1ctico: Consumir API y Crear Dataset","text":"<pre><code>import requests\nimport json\nimport csv\nfrom datetime import datetime\n\n# Configuraci\u00f3n\nAPI_KEY = \"XXXX\"  # Reemplazar con tu API key\nBASE_URL = \"https://api.rawg.io/api/games\"\n\n# Clase para videojuegos\nclass Videojuego:\n    def __init__(self, nombre, anyo, imagen, valoracion):\n        self.nombre = nombre\n        self.anyo = anyo\n        self.imagen = imagen\n        self.valoracion = valoracion\n\n    def __str__(self):\n        return f\"{self.nombre},{self.anyo},{self.imagen},{self.valoracion}\"\n\n# Obtener datos de la API\nvideojuegos = []\n\nfor pagina in range(1, 3):  # 2 p\u00e1ginas\n    url = f\"{BASE_URL}?key={API_KEY}&amp;page={pagina}\"\n\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        datos = response.json()\n\n        for juego in datos['results']:\n            # Crear objeto Videojuego\n            vj = Videojuego(\n                nombre=juego['name'],\n                anyo=juego['released'][:4] if juego['released'] else 'N/A',\n                imagen=juego['background_image'] or 'Sin imagen',\n                valoracion=juego['rating']\n            )\n            videojuegos.append(vj)\n\nprint(f\"Total videojuegos obtenidos: {len(videojuegos)}\")\n\n# Filtrar los mejor valorados\nmejores = [vj for vj in videojuegos if vj.valoracion &gt; 4.0]\nmejores.sort(key=lambda x: x.valoracion, reverse=True)\n\n# Guardar en CSV\nwith open('videojuegos.csv', 'w', newline='', encoding='utf-8') as f:\n    f.write(\"Nombre,A\u00f1o,Imagen,Rating\\n\")\n    for vj in mejores:\n        f.write(f\"{vj}\\n\")\n\nprint(f\"Guardados {len(mejores)} videojuegos con rating &gt; 4.0\")\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#11-trabajar-con-json-anidados-complejos","title":"11. Trabajar con JSON Anidados Complejos","text":"<pre><code>import json\n\n# JSON complejo con m\u00faltiples niveles\nempresa_json = \"\"\"\n{\n    \"nombre\": \"TechCorp\",\n    \"fundacion\": 2010,\n    \"departamentos\": [\n        {\n            \"nombre\": \"Desarrollo\",\n            \"empleados\": [\n                {\"nombre\": \"Ana\", \"cargo\": \"Senior Dev\", \"salario\": 45000},\n                {\"nombre\": \"Luis\", \"cargo\": \"Junior Dev\", \"salario\": 28000}\n            ]\n        },\n        {\n            \"nombre\": \"Marketing\",\n            \"empleados\": [\n                {\"nombre\": \"Mar\u00eda\", \"cargo\": \"Manager\", \"salario\": 52000}\n            ]\n        }\n    ],\n    \"activa\": true\n}\n\"\"\"\n\nempresa = json.loads(empresa_json)\n\n# Navegar por la estructura\nprint(empresa['nombre'])  # TechCorp\n\n# Iterar departamentos\nfor depto in empresa['departamentos']:\n    print(f\"\\nDepartamento: {depto['nombre']}\")\n    for emp in depto['empleados']:\n        print(f\"  - {emp['nombre']} ({emp['cargo']}): {emp['salario']}\u20ac\")\n\n# Calcular salario total\nsalario_total = sum(\n    emp['salario'] \n    for depto in empresa['departamentos'] \n    for emp in depto['empleados']\n)\nprint(f\"\\nSalario total empresa: {salario_total}\u20ac\")\n\n# Encontrar empleado mejor pagado\ntodos_empleados = [\n    emp \n    for depto in empresa['departamentos'] \n    for emp in depto['empleados']\n]\nmejor_pagado = max(todos_empleados, key=lambda x: x['salario'])\nprint(f\"Mejor pagado: {mejor_pagado['nombre']} - {mejor_pagado['salario']}\u20ac\")\n</code></pre>"},{"location":"conocimientos-basicos/11-json/#12-resumen-de-funciones","title":"12. Resumen de Funciones","text":"Funci\u00f3n Entrada Salida Uso <code>json.loads(s)</code> String JSON Dict/List Python Parsear string <code>json.dumps(obj)</code> Dict/List Python String JSON Serializar a string <code>json.load(f)</code> Archivo JSON Dict/List Python Leer archivo <code>json.dump(obj, f)</code> Dict/List + Archivo - Escribir archivo"},{"location":"conocimientos-basicos/11-json/#parametros-utiles-de-dumps-y-dump","title":"Par\u00e1metros \u00datiles de <code>dumps()</code> y <code>dump()</code>","text":"Par\u00e1metro Descripci\u00f3n Ejemplo <code>indent</code> Indentaci\u00f3n para formato legible <code>indent=4</code> <code>sort_keys</code> Ordenar claves alfab\u00e9ticamente <code>sort_keys=True</code> <code>ensure_ascii</code> Permitir caracteres no ASCII <code>ensure_ascii=False</code> <code>cls</code> Encoder personalizado <code>cls=MiEncoder</code>"},{"location":"conocimientos-basicos/11-json/#13-buenas-practicas","title":"13. Buenas Pr\u00e1cticas","text":"<pre><code># \u2705 HACER:\n# 1. Usar encoding='utf-8' al abrir archivos\n# 2. Manejar excepciones (JSONDecodeError, FileNotFoundError)\n# 3. Usar ensure_ascii=False para caracteres especiales\n# 4. Validar la estructura del JSON antes de acceder a claves\n# 5. Usar .get() para acceso seguro a claves\n\ndatos = {\"nombre\": \"Ana\"}\nedad = datos.get('edad', 0)  # Devuelve 0 si 'edad' no existe\n\n# \u274c NO HACER:\n# 1. Confiar ciegamente en la estructura del JSON externo\n# 2. Olvidar cerrar archivos (usar 'with')\n# 3. Ignorar los c\u00f3digos de estado en peticiones HTTP\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/12-faker/","title":"\ud83c\udfad Faker - Generaci\u00f3n de Datos Ficticios","text":"<p>Faker es una librer\u00eda de Python que permite generar datos falsos de manera realista. Es extremadamente \u00fatil para pruebas, desarrollo, poblar bases de datos y crear datasets de ejemplo.</p>"},{"location":"conocimientos-basicos/12-faker/#1-instalacion-e-importacion","title":"1. Instalaci\u00f3n e Importaci\u00f3n","text":"<pre><code># Instalaci\u00f3n\n# pip install faker\n\n# Importaci\u00f3n\nfrom faker import Faker\n\n# Crear instancia (por defecto en ingl\u00e9s)\nfake = Faker()\n\n# Crear instancia en espa\u00f1ol\nfake = Faker('es_ES')\n\n# M\u00faltiples idiomas\nfake = Faker(['es_ES', 'en_US', 'fr_FR', 'ja_JP'])\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#2-datos-personales-basicos","title":"2. Datos Personales B\u00e1sicos","text":""},{"location":"conocimientos-basicos/12-faker/#nombres","title":"Nombres","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Nombre completo\nprint(fake.name())              # Mar\u00eda Garc\u00eda L\u00f3pez\n\n# Nombre de pila\nprint(fake.first_name())        # Carlos\n\n# Apellido\nprint(fake.last_name())         # Fern\u00e1ndez\n\n# Nombre masculino\nprint(fake.first_name_male())   # Antonio\n\n# Nombre femenino\nprint(fake.first_name_female()) # Laura\n\n# Prefijo (Sr., Sra., etc.)\nprint(fake.prefix())            # Sra.\n\n# Sufijo\nprint(fake.suffix())            # Jr.\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#generar-multiples-nombres","title":"Generar M\u00faltiples Nombres","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Generar 5 nombres\nfor _ in range(5):\n    print(fake.name())\n\n# Salida:\n# Jos\u00e9 Mart\u00edn Ruiz\n# Ana Mar\u00eda L\u00f3pez Garc\u00eda\n# Pedro S\u00e1nchez Torres\n# Carmen Rodr\u00edguez P\u00e9rez\n# Francisco G\u00f3mez D\u00edaz\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#3-direcciones","title":"3. Direcciones","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Direcci\u00f3n completa\nprint(fake.address())\n# Ronda de Ana Mar\u00eda L\u00f3pez 45\n# Cuenca, 04267\n\n# Componentes individuales\nprint(fake.street_name())       # Calle de San Pedro\nprint(fake.street_address())    # Paseo de Garc\u00eda 78\nprint(fake.city())              # Sevilla\nprint(fake.state())             # Madrid\nprint(fake.postcode())          # 28001\nprint(fake.country())           # Espa\u00f1a\n\n# Coordenadas geogr\u00e1ficas\nprint(fake.latitude())          # 40.4168\nprint(fake.longitude())         # -3.7038\nprint(fake.coordinate())        # (40.4168, -3.7038)\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#4-contacto","title":"4. Contacto","text":""},{"location":"conocimientos-basicos/12-faker/#email-y-telefono","title":"Email y Tel\u00e9fono","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Email\nprint(fake.email())                    # juan.garcia@example.com\nprint(fake.free_email())               # maria_lopez@gmail.com\nprint(fake.company_email())            # ana.martinez@empresa.es\nprint(fake.safe_email())               # pedro@example.org\n\n# Email personalizado\nprint(fake.ascii_email())              # carlos.sanchez@example.net\n\n# Tel\u00e9fono\nprint(fake.phone_number())             # +34 612 345 678\nprint(fake.msisdn())                   # 34612345678\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#internet","title":"Internet","text":"<pre><code>from faker import Faker\n\nfake = Faker()\n\n# Usuario\nprint(fake.user_name())         # juan_garcia\nprint(fake.password())          # aB3$kL9mN\n\n# Password personalizado\nprint(fake.password(\n    length=12,\n    special_chars=True,\n    digits=True,\n    upper_case=True,\n    lower_case=True\n))  # Xk9$mN2pL@qR\n\n# URLs y dominios\nprint(fake.url())               # https://www.example.com/\nprint(fake.domain_name())       # garcia.com\nprint(fake.domain_word())       # martinez\nprint(fake.tld())               # es\n\n# IP\nprint(fake.ipv4())              # 192.168.1.100\nprint(fake.ipv6())              # 2001:0db8:85a3:0000:0000:8a2e:0370:7334\nprint(fake.mac_address())       # 00:1A:2B:3C:4D:5E\n\n# User Agent\nprint(fake.user_agent())        # Mozilla/5.0 (Windows NT 10.0; Win64; x64)...\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#5-fechas-y-tiempos","title":"5. Fechas y Tiempos","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Fecha\nprint(fake.date())                      # 2023-05-15\nprint(fake.date_this_year())            # Fecha de este a\u00f1o\nprint(fake.date_this_month())           # Fecha de este mes\nprint(fake.date_this_decade())          # Fecha de esta d\u00e9cada\n\n# Fecha en rango\nprint(fake.date_between(\n    start_date='-30y',\n    end_date='today'\n))  # Fecha entre hace 30 a\u00f1os y hoy\n\n# Fecha de nacimiento\nprint(fake.date_of_birth(\n    minimum_age=18,\n    maximum_age=65\n))  # 1975-03-22\n\n# Hora\nprint(fake.time())                      # 14:35:22\n\n# Fecha y hora completa\nprint(fake.date_time())                 # 2023-05-15 14:35:22\nprint(fake.date_time_this_year())\n\n# Timestamp Unix\nprint(fake.unix_time())                 # 1684159522\n\n# D\u00eda y mes\nprint(fake.day_of_week())               # Mi\u00e9rcoles\nprint(fake.month_name())                # Mayo\nprint(fake.year())                      # 2023\n\n# Zona horaria\nprint(fake.timezone())                  # Europe/Madrid\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#6-texto","title":"6. Texto","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Palabra\nprint(fake.word())                      # casa\n\n# Palabras\nprint(fake.words(nb=5))                 # ['casa', 'perro', 'mesa', 'libro', 'agua']\n\n# Frase\nprint(fake.sentence())                  # El r\u00e1pido zorro marr\u00f3n salta.\nprint(fake.sentence(nb_words=10))       # Frase de ~10 palabras\n\n# P\u00e1rrafo\nprint(fake.paragraph())                 # P\u00e1rrafo con varias frases\nprint(fake.paragraph(nb_sentences=5))   # P\u00e1rrafo con 5 frases\n\n# M\u00faltiples p\u00e1rrafos\nprint(fake.paragraphs(nb=3))            # Lista de 3 p\u00e1rrafos\n\n# Texto largo\nprint(fake.text())                      # Texto de ~200 caracteres\nprint(fake.text(max_nb_chars=500))      # Texto de hasta 500 caracteres\n\n# Lorem Ipsum\nprint(fake.catch_phrase())              # Frase promocional\nprint(fake.bs())                        # Jerga empresarial\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#7-numeros-y-datos-financieros","title":"7. N\u00fameros y Datos Financieros","text":""},{"location":"conocimientos-basicos/12-faker/#numeros","title":"N\u00fameros","text":"<pre><code>from faker import Faker\n\nfake = Faker()\n\n# Enteros\nprint(fake.random_int(min=1, max=100))      # 42\nprint(fake.random_digit())                   # 7\nprint(fake.random_number(digits=5))          # 45678\n\n# Decimales\nprint(fake.pyfloat(min_value=0, max_value=100, right_digits=2))  # 45.67\nprint(fake.pydecimal(left_digits=3, right_digits=2))             # 123.45\n\n# Booleano\nprint(fake.boolean())                        # True\nprint(fake.boolean(chance_of_getting_true=75))  # 75% probabilidad de True\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#datos-financieros","title":"Datos Financieros","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Tarjeta de cr\u00e9dito\nprint(fake.credit_card_number())        # 4532015112830366\nprint(fake.credit_card_provider())      # Visa\nprint(fake.credit_card_expire())        # 03/25\nprint(fake.credit_card_security_code()) # 123\nprint(fake.credit_card_full())          # Informaci\u00f3n completa\n\n# IBAN\nprint(fake.iban())                      # ES9121000418450200051332\n\n# Moneda\nprint(fake.currency())                  # ('EUR', 'Euro')\nprint(fake.currency_code())             # EUR\nprint(fake.currency_name())             # Euro\nprint(fake.pricetag())                  # 45,99 \u20ac\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#8-empresas-y-trabajo","title":"8. Empresas y Trabajo","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Empresa\nprint(fake.company())                   # Garc\u00eda e Hijos S.L.\nprint(fake.company_suffix())            # S.A.\nprint(fake.catch_phrase())              # Soluciones innovadoras para el futuro\n\n# Trabajo\nprint(fake.job())                       # Ingeniero de Software\n\n# NIF/CIF (Espa\u00f1a)\nprint(fake.nif())                       # 12345678Z\nprint(fake.nie())                       # X1234567L\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#9-perfiles-completos","title":"9. Perfiles Completos","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Perfil simple\nperfil = fake.simple_profile()\nprint(perfil)\n# {\n#     'username': 'maria_garcia',\n#     'name': 'Mar\u00eda Garc\u00eda L\u00f3pez',\n#     'sex': 'F',\n#     'address': 'Calle Mayor 45, Madrid 28001',\n#     'mail': 'maria.garcia@example.com',\n#     'birthdate': datetime.date(1985, 3, 15)\n# }\n\n# Acceder a campos individuales\nprint(perfil['name'])\nprint(perfil['mail'])\nprint(perfil['birthdate'].year)\n\n# Perfil completo\nperfil_completo = fake.profile()\nprint(perfil_completo)\n# Incluye: job, company, ssn, residence, blood_group, website...\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#10-colores","title":"10. Colores","text":"<pre><code>from faker import Faker\n\nfake = Faker()\n\nprint(fake.color_name())        # Azul\nprint(fake.hex_color())         # #3498db\nprint(fake.rgb_color())         # 52,152,219\nprint(fake.rgb_css_color())     # rgb(52,152,219)\nprint(fake.safe_color_name())   # blue\nprint(fake.safe_hex_color())    # #0000ff\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#11-archivos-y-rutas","title":"11. Archivos y Rutas","text":"<pre><code>from faker import Faker\n\nfake = Faker()\n\n# Nombres de archivo\nprint(fake.file_name())                 # documento.pdf\nprint(fake.file_name(extension='xlsx')) # datos.xlsx\nprint(fake.file_extension())            # pdf\n\n# Tipos MIME\nprint(fake.mime_type())                 # application/pdf\n\n# Rutas\nprint(fake.file_path())                 # /home/user/docs/file.txt\nprint(fake.file_path(depth=3))          # Ruta con 3 niveles\n\n# UUID\nprint(fake.uuid4())                     # a1b2c3d4-e5f6-7890-abcd-ef1234567890\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#12-datos-cientificos","title":"12. Datos Cient\u00edficos","text":"<pre><code>from faker import Faker\n\nfake = Faker()\n\n# Python\nprint(fake.pylist(nb_elements=5))       # Lista de 5 elementos aleatorios\nprint(fake.pydict(nb_elements=3))       # Diccionario de 3 elementos\nprint(fake.pytuple(nb_elements=4))      # Tupla de 4 elementos\nprint(fake.pyset(nb_elements=5))        # Set de 5 elementos\n\n# Estructuras espec\u00edficas\nprint(fake.pylist(\n    nb_elements=3,\n    variable_nb_elements=False,\n    value_types=[int]\n))  # [42, 17, 89]\n\nprint(fake.pydict(\n    nb_elements=2,\n    value_types=[str]\n))  # {'key1': 'valor1', 'key2': 'valor2'}\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#13-reproducibilidad-con-semilla","title":"13. Reproducibilidad con Semilla","text":"<pre><code>from faker import Faker\n\n# Establecer semilla para resultados reproducibles\nFaker.seed(12345)\nfake = Faker('es_ES')\n\n# Siempre generar\u00e1 los mismos datos\nprint(fake.name())  # Siempre el mismo nombre\nprint(fake.email()) # Siempre el mismo email\n\n# Tambi\u00e9n puedes usar seed por instancia\nfake1 = Faker('es_ES')\nfake1.seed_instance(42)\n\nfake2 = Faker('es_ES')\nfake2.seed_instance(42)\n\nprint(fake1.name() == fake2.name())  # True\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#14-multiples-idiomas","title":"14. M\u00faltiples Idiomas","text":"<pre><code>from faker import Faker\n\n# Instancia multiidioma\nfake = Faker(['es_ES', 'en_US', 'fr_FR', 'de_DE', 'ja_JP'])\n\n# Genera datos aleatorios de cualquier idioma\nfor _ in range(5):\n    print(fake.name())\n\n# Salida:\n# Mar\u00eda Garc\u00eda (espa\u00f1ol)\n# John Smith (ingl\u00e9s)\n# Pierre Dubois (franc\u00e9s)\n# Hans M\u00fcller (alem\u00e1n)\n# \u7530\u4e2d\u592a\u90ce (japon\u00e9s)\n\n# Acceder a un idioma espec\u00edfico\nfake_es = Faker('es_ES')\nfake_en = Faker('en_US')\n\nprint(fake_es.name())  # Nombre espa\u00f1ol\nprint(fake_en.name())  # Nombre ingl\u00e9s\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#idiomas-disponibles-algunos-ejemplos","title":"Idiomas Disponibles (algunos ejemplos)","text":"C\u00f3digo Idioma <code>es_ES</code> Espa\u00f1ol (Espa\u00f1a) <code>es_MX</code> Espa\u00f1ol (M\u00e9xico) <code>en_US</code> Ingl\u00e9s (EEUU) <code>en_GB</code> Ingl\u00e9s (Reino Unido) <code>fr_FR</code> Franc\u00e9s <code>de_DE</code> Alem\u00e1n <code>it_IT</code> Italiano <code>pt_BR</code> Portugu\u00e9s (Brasil) <code>ja_JP</code> Japon\u00e9s <code>zh_CN</code> Chino <code>ru_RU</code> Ruso"},{"location":"conocimientos-basicos/12-faker/#15-generar-csv-con-faker","title":"15. Generar CSV con Faker","text":""},{"location":"conocimientos-basicos/12-faker/#ejemplo-usuarios-de-red-social","title":"Ejemplo: Usuarios de Red Social","text":"<pre><code>from faker import Faker\nimport csv\n\n# Configuraci\u00f3n\nfaker = Faker(['es_ES', 'en_US', 'fr_FR'])\ncantidad_usuarios = 1000\nruta_fichero = 'usuarios_facebook.csv'\ncampos = [\"id\", \"nombre\", \"apellidos\", \"correo\", \"password\", \n          \"dia\", \"mes\", \"anyo\", \"genero\"]\n\n# Generar CSV\nwith open(ruta_fichero, 'w', newline='', encoding='utf-8') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=campos)\n    writer.writeheader()\n\n    for i in range(1, cantidad_usuarios + 1):\n        perfil = faker.simple_profile()\n        nombre_completo = perfil['name'].split()\n\n        writer.writerow({\n            \"id\": i,\n            \"nombre\": nombre_completo[0],\n            \"apellidos\": ' '.join(nombre_completo[1:]) if len(nombre_completo) &gt; 1 else faker.last_name(),\n            \"correo\": perfil['mail'],\n            \"password\": faker.password(length=10, special_chars=True, digits=True),\n            \"dia\": perfil['birthdate'].day,\n            \"mes\": perfil['birthdate'].month,\n            \"anyo\": perfil['birthdate'].year,\n            \"genero\": perfil['sex']\n        })\n\nprint(f\"Archivo '{ruta_fichero}' creado con {cantidad_usuarios} usuarios.\")\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#ejemplo-dataset-de-empleados","title":"Ejemplo: Dataset de Empleados","text":"<pre><code>from faker import Faker\nimport csv\n\nfake = Faker('es_ES')\n\ncampos = ['id', 'nombre', 'apellidos', 'email', 'departamento', \n          'cargo', 'salario', 'fecha_contratacion', 'telefono']\n\ndepartamentos = ['Ventas', 'Marketing', 'IT', 'RRHH', 'Finanzas', 'Operaciones']\n\nwith open('empleados.csv', 'w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=campos)\n    writer.writeheader()\n\n    for i in range(1, 501):\n        writer.writerow({\n            'id': i,\n            'nombre': fake.first_name(),\n            'apellidos': fake.last_name(),\n            'email': fake.company_email(),\n            'departamento': fake.random_element(departamentos),\n            'cargo': fake.job(),\n            'salario': fake.random_int(min=20000, max=80000),\n            'fecha_contratacion': fake.date_between(start_date='-10y', end_date='today'),\n            'telefono': fake.phone_number()\n        })\n\nprint(\"Archivo 'empleados.csv' creado con 500 empleados.\")\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#16-generar-json-con-faker","title":"16. Generar JSON con Faker","text":"<pre><code>from faker import Faker\nimport json\n\nfake = Faker('es_ES')\n\n# Generar lista de usuarios\nusuarios = []\nfor i in range(100):\n    usuario = {\n        \"id\": i + 1,\n        \"nombre\": fake.name(),\n        \"email\": fake.email(),\n        \"telefono\": fake.phone_number(),\n        \"direccion\": {\n            \"calle\": fake.street_address(),\n            \"ciudad\": fake.city(),\n            \"codigo_postal\": fake.postcode(),\n            \"pais\": fake.country()\n        },\n        \"fecha_registro\": str(fake.date_this_year()),\n        \"activo\": fake.boolean(chance_of_getting_true=80)\n    }\n    usuarios.append(usuario)\n\n# Guardar en archivo JSON\nwith open('usuarios.json', 'w', encoding='utf-8') as f:\n    json.dump(usuarios, f, indent=4, ensure_ascii=False)\n\nprint(f\"Archivo 'usuarios.json' creado con {len(usuarios)} usuarios.\")\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#17-uso-con-pandas","title":"17. Uso con Pandas","text":"<pre><code>from faker import Faker\nimport pandas as pd\n\nfake = Faker('es_ES')\n\n# Crear DataFrame directamente\nn = 1000\n\ndf = pd.DataFrame({\n    'id': range(1, n + 1),\n    'nombre': [fake.name() for _ in range(n)],\n    'email': [fake.email() for _ in range(n)],\n    'ciudad': [fake.city() for _ in range(n)],\n    'fecha_nacimiento': [fake.date_of_birth(minimum_age=18, maximum_age=70) for _ in range(n)],\n    'salario': [fake.random_int(min=18000, max=100000) for _ in range(n)],\n    'departamento': [fake.random_element(['IT', 'Ventas', 'Marketing', 'RRHH']) for _ in range(n)]\n})\n\nprint(df.head())\nprint(f\"\\nEstad\u00edsticas de salario:\")\nprint(df['salario'].describe())\n\n# Guardar en diferentes formatos\ndf.to_csv('datos_faker.csv', index=False)\ndf.to_excel('datos_faker.xlsx', index=False)\ndf.to_json('datos_faker.json', orient='records', indent=2)\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#18-proveedores-personalizados","title":"18. Proveedores Personalizados","text":"<pre><code>from faker import Faker\nfrom faker.providers import BaseProvider\n\n# Crear proveedor personalizado\nclass VideoGameProvider(BaseProvider):\n    def video_game_genre(self):\n        genres = ['RPG', 'FPS', 'Aventura', 'Estrategia', 'Deportes', \n                  'Simulaci\u00f3n', 'Puzzle', 'Plataformas']\n        return self.random_element(genres)\n\n    def video_game_platform(self):\n        platforms = ['PC', 'PlayStation 5', 'Xbox Series X', 'Nintendo Switch', \n                     'Steam Deck', 'Mobile']\n        return self.random_element(platforms)\n\n    def video_game_name(self):\n        adjectives = ['Dark', 'Epic', 'Final', 'Eternal', 'Lost', 'Hidden']\n        nouns = ['Kingdom', 'Quest', 'Legacy', 'Warriors', 'Legends', 'Chronicles']\n        return f\"{self.random_element(adjectives)} {self.random_element(nouns)}\"\n\n# Usar el proveedor\nfake = Faker('es_ES')\nfake.add_provider(VideoGameProvider)\n\nfor _ in range(5):\n    print(f\"{fake.video_game_name()} - {fake.video_game_genre()} ({fake.video_game_platform()})\")\n\n# Salida:\n# Epic Quest - RPG (PlayStation 5)\n# Dark Chronicles - FPS (PC)\n# Final Warriors - Aventura (Nintendo Switch)\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#19-unique-valores-unicos","title":"19. Unique (Valores \u00danicos)","text":"<pre><code>from faker import Faker\n\nfake = Faker('es_ES')\n\n# Generar emails \u00fanicos (sin repetici\u00f3n)\nemails_unicos = [fake.unique.email() for _ in range(10)]\nprint(emails_unicos)\n\n# Resetear el registro de \u00fanicos\nfake.unique.clear()\n\n# Generar nombres \u00fanicos\ntry:\n    nombres = [fake.unique.first_name() for _ in range(1000)]\nexcept Exception as e:\n    print(f\"Error: Se agotaron los nombres \u00fanicos disponibles\")\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#20-ejemplos-practicos-completos","title":"20. Ejemplos Pr\u00e1cticos Completos","text":""},{"location":"conocimientos-basicos/12-faker/#dataset-de-tienda-online","title":"Dataset de Tienda Online","text":"<pre><code>from faker import Faker\nimport csv\nimport random\n\nfake = Faker('es_ES')\n\n# Productos\ncategorias = ['Electr\u00f3nica', 'Ropa', 'Hogar', 'Deportes', 'Libros']\n\nwith open('productos.csv', 'w', newline='', encoding='utf-8') as f:\n    campos = ['id', 'nombre', 'categoria', 'precio', 'stock', 'descripcion']\n    writer = csv.DictWriter(f, fieldnames=campos)\n    writer.writeheader()\n\n    for i in range(200):\n        writer.writerow({\n            'id': i + 1,\n            'nombre': fake.catch_phrase(),\n            'categoria': fake.random_element(categorias),\n            'precio': round(random.uniform(9.99, 999.99), 2),\n            'stock': fake.random_int(min=0, max=500),\n            'descripcion': fake.text(max_nb_chars=200)\n        })\n\n# Clientes\nwith open('clientes.csv', 'w', newline='', encoding='utf-8') as f:\n    campos = ['id', 'nombre', 'email', 'telefono', 'direccion', 'ciudad', 'cp']\n    writer = csv.DictWriter(f, fieldnames=campos)\n    writer.writeheader()\n\n    for i in range(500):\n        writer.writerow({\n            'id': i + 1,\n            'nombre': fake.name(),\n            'email': fake.unique.email(),\n            'telefono': fake.phone_number(),\n            'direccion': fake.street_address(),\n            'ciudad': fake.city(),\n            'cp': fake.postcode()\n        })\n\n# Pedidos\nwith open('pedidos.csv', 'w', newline='', encoding='utf-8') as f:\n    campos = ['id', 'cliente_id', 'producto_id', 'cantidad', 'fecha', 'estado']\n    writer = csv.DictWriter(f, fieldnames=campos)\n    writer.writeheader()\n\n    estados = ['Pendiente', 'Enviado', 'Entregado', 'Cancelado']\n\n    for i in range(1000):\n        writer.writerow({\n            'id': i + 1,\n            'cliente_id': fake.random_int(min=1, max=500),\n            'producto_id': fake.random_int(min=1, max=200),\n            'cantidad': fake.random_int(min=1, max=5),\n            'fecha': fake.date_between(start_date='-1y', end_date='today'),\n            'estado': fake.random_element(estados)\n        })\n\nprint(\"Datasets de tienda online creados:\")\nprint(\"- productos.csv (200 productos)\")\nprint(\"- clientes.csv (500 clientes)\")\nprint(\"- pedidos.csv (1000 pedidos)\")\n</code></pre>"},{"location":"conocimientos-basicos/12-faker/#21-resumen-de-metodos-principales","title":"21. Resumen de M\u00e9todos Principales","text":"Categor\u00eda M\u00e9todos Nombres <code>name()</code>, <code>first_name()</code>, <code>last_name()</code> Direcciones <code>address()</code>, <code>city()</code>, <code>street_address()</code>, <code>postcode()</code> Contacto <code>email()</code>, <code>phone_number()</code>, <code>url()</code> Fechas <code>date()</code>, <code>date_of_birth()</code>, <code>date_between()</code> Texto <code>text()</code>, <code>sentence()</code>, <code>paragraph()</code>, <code>word()</code> N\u00fameros <code>random_int()</code>, <code>pyfloat()</code>, <code>boolean()</code> Finanzas <code>credit_card_number()</code>, <code>iban()</code>, <code>pricetag()</code> Empresa <code>company()</code>, <code>job()</code>, <code>catch_phrase()</code> Perfil <code>simple_profile()</code>, <code>profile()</code> Internet <code>user_name()</code>, <code>password()</code>, <code>ipv4()</code> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/13-numpy/","title":"\ud83d\udcda NumPy - Computaci\u00f3n Num\u00e9rica","text":"<p>NumPy (Numerical Python) es la librer\u00eda fundamental para computaci\u00f3n num\u00e9rica en Python. Proporciona soporte para arrays multidimensionales y operaciones matem\u00e1ticas de alto rendimiento.</p>"},{"location":"conocimientos-basicos/13-numpy/#1-instalacion-e-importacion","title":"1. Instalaci\u00f3n e Importaci\u00f3n","text":"<pre><code># Instalaci\u00f3n\n# pip install numpy\n\n# Importaci\u00f3n (convenci\u00f3n est\u00e1ndar)\nimport numpy as np\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#2-arrays-de-numpy","title":"2. Arrays de NumPy","text":""},{"location":"conocimientos-basicos/13-numpy/#crear-arrays","title":"Crear Arrays","text":"<pre><code>import numpy as np\n\n# Desde lista\narr1 = np.array([1, 2, 3, 4, 5])\nprint(arr1)  # [1 2 3 4 5]\n\n# Array 2D (matriz)\narr2d = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr2d)\n# [[1 2 3]\n#  [4 5 6]]\n\n# Array 3D\narr3d = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nprint(arr3d.shape)  # (2, 2, 2)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#funciones-para-crear-arrays","title":"Funciones para Crear Arrays","text":"<pre><code># Array de ceros\nzeros = np.zeros((3, 4))\nprint(zeros)\n# [[0. 0. 0. 0.]\n#  [0. 0. 0. 0.]\n#  [0. 0. 0. 0.]]\n\n# Array de unos\nones = np.ones((2, 3))\nprint(ones)\n# [[1. 1. 1.]\n#  [1. 1. 1.]]\n\n# Array vac\u00edo (valores aleatorios)\nempty = np.empty((2, 2))\n\n# Array con valor espec\u00edfico\nfull = np.full((3, 3), 7)\nprint(full)\n# [[7 7 7]\n#  [7 7 7]\n#  [7 7 7]]\n\n# Matriz identidad\nidentidad = np.eye(3)\nprint(identidad)\n# [[1. 0. 0.]\n#  [0. 1. 0.]\n#  [0. 0. 1.]]\n\n# Rango de valores\nrango = np.arange(0, 10, 2)  # inicio, fin, paso\nprint(rango)  # [0 2 4 6 8]\n\n# Valores espaciados uniformemente\nlinspace = np.linspace(0, 1, 5)  # inicio, fin, cantidad\nprint(linspace)  # [0.   0.25 0.5  0.75 1.  ]\n\n# Valores aleatorios\naleatorio = np.random.rand(3, 3)  # Uniforme [0, 1)\nnormal = np.random.randn(3, 3)    # Normal (0, 1)\nenteros = np.random.randint(0, 100, (3, 3))  # Enteros aleatorios\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#3-propiedades-de-arrays","title":"3. Propiedades de Arrays","text":"<pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr.shape)   # (2, 3) - dimensiones\nprint(arr.ndim)    # 2 - n\u00famero de dimensiones\nprint(arr.size)    # 6 - total de elementos\nprint(arr.dtype)   # int64 - tipo de datos\nprint(arr.itemsize)  # 8 - bytes por elemento\nprint(arr.nbytes)  # 48 - total de bytes\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#tipos-de-datos","title":"Tipos de Datos","text":"<pre><code># Especificar tipo al crear\narr_int = np.array([1, 2, 3], dtype=np.int32)\narr_float = np.array([1, 2, 3], dtype=np.float64)\narr_bool = np.array([1, 0, 1], dtype=np.bool_)\n\n# Convertir tipo\narr = np.array([1.5, 2.7, 3.9])\narr_int = arr.astype(np.int32)\nprint(arr_int)  # [1 2 3]\n\n# Tipos comunes\n# np.int8, np.int16, np.int32, np.int64\n# np.float16, np.float32, np.float64\n# np.bool_, np.complex64\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#4-indexacion-y-slicing","title":"4. Indexaci\u00f3n y Slicing","text":""},{"location":"conocimientos-basicos/13-numpy/#arrays-1d","title":"Arrays 1D","text":"<pre><code>arr = np.array([10, 20, 30, 40, 50])\n\n# Indexaci\u00f3n\nprint(arr[0])    # 10\nprint(arr[-1])   # 50\n\n# Slicing\nprint(arr[1:4])    # [20 30 40]\nprint(arr[:3])     # [10 20 30]\nprint(arr[2:])     # [30 40 50]\nprint(arr[::2])    # [10 30 50] (cada 2)\nprint(arr[::-1])   # [50 40 30 20 10] (invertido)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#arrays-2d","title":"Arrays 2D","text":"<pre><code>arr = np.array([[1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]])\n\n# Acceder a elemento\nprint(arr[0, 0])   # 1\nprint(arr[1, 2])   # 7\nprint(arr[2, -1])  # 12\n\n# Fila completa\nprint(arr[0])      # [1 2 3 4]\nprint(arr[1, :])   # [5 6 7 8]\n\n# Columna completa\nprint(arr[:, 0])   # [1 5 9]\nprint(arr[:, -1])  # [4 8 12]\n\n# Submatriz\nprint(arr[0:2, 1:3])\n# [[2 3]\n#  [6 7]]\n\n# Filas y columnas espec\u00edficas\nprint(arr[[0, 2], :])  # Filas 0 y 2\nprint(arr[:, [0, 3]])  # Columnas 0 y 3\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#indexacion-booleana","title":"Indexaci\u00f3n Booleana","text":"<pre><code>arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Crear m\u00e1scara booleana\nmascara = arr &gt; 5\nprint(mascara)  # [False False False False False  True  True  True  True  True]\n\n# Filtrar elementos\nmayores = arr[arr &gt; 5]\nprint(mayores)  # [ 6  7  8  9 10]\n\n# Condiciones m\u00faltiples\npares_mayores = arr[(arr &gt; 3) &amp; (arr % 2 == 0)]\nprint(pares_mayores)  # [ 4  6  8 10]\n\n# Modificar con condici\u00f3n\narr[arr &lt; 5] = 0\nprint(arr)  # [ 0  0  0  0  5  6  7  8  9 10]\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#fancy-indexing","title":"Fancy Indexing","text":"<pre><code>arr = np.array([10, 20, 30, 40, 50])\n\n# \u00cdndices espec\u00edficos\nindices = [0, 2, 4]\nprint(arr[indices])  # [10 30 50]\n\n# Para matrices\nmatriz = np.arange(1, 10).reshape(3, 3)\nprint(matriz)\n# [[1 2 3]\n#  [4 5 6]\n#  [7 8 9]]\n\nfilas = [0, 1, 2]\ncols = [2, 1, 0]\nprint(matriz[filas, cols])  # [3 5 7] (diagonal inversa)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#5-manipulacion-de-arrays","title":"5. Manipulaci\u00f3n de Arrays","text":""},{"location":"conocimientos-basicos/13-numpy/#reshape-cambiar-forma","title":"Reshape (Cambiar Forma)","text":"<pre><code>arr = np.arange(12)\nprint(arr)  # [ 0  1  2  3  4  5  6  7  8  9 10 11]\n\n# Reshape a 3x4\nmatriz = arr.reshape(3, 4)\nprint(matriz)\n# [[ 0  1  2  3]\n#  [ 4  5  6  7]\n#  [ 8  9 10 11]]\n\n# Reshape a 2x2x3\ntensor = arr.reshape(2, 2, 3)\n\n# -1 calcula autom\u00e1ticamente\nauto = arr.reshape(4, -1)  # 4 filas, columnas autom\u00e1ticas\nprint(auto.shape)  # (4, 3)\n\n# Aplanar\nplano = matriz.flatten()  # Copia\nplano = matriz.ravel()    # Vista\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#concatenar-y-dividir","title":"Concatenar y Dividir","text":"<pre><code>a = np.array([[1, 2], [3, 4]])\nb = np.array([[5, 6], [7, 8]])\n\n# Concatenar\nvertical = np.vstack([a, b])  # Vertical\nprint(vertical)\n# [[1 2]\n#  [3 4]\n#  [5 6]\n#  [7 8]]\n\nhorizontal = np.hstack([a, b])  # Horizontal\nprint(horizontal)\n# [[1 2 5 6]\n#  [3 4 7 8]]\n\n# concatenate con axis\nconcat_v = np.concatenate([a, b], axis=0)  # Vertical\nconcat_h = np.concatenate([a, b], axis=1)  # Horizontal\n\n# Dividir\narr = np.arange(16).reshape(4, 4)\npartes = np.split(arr, 2)  # Dividir en 2\nv1, v2 = np.vsplit(arr, 2)  # Vertical\nh1, h2 = np.hsplit(arr, 2)  # Horizontal\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#transponer","title":"Transponer","text":"<pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\nprint(arr.shape)  # (2, 3)\n\ntranspuesta = arr.T\nprint(transpuesta.shape)  # (3, 2)\nprint(transpuesta)\n# [[1 4]\n#  [2 5]\n#  [3 6]]\n\n# Tambi\u00e9n: np.transpose(arr)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#anadir-dimensiones","title":"A\u00f1adir Dimensiones","text":"<pre><code>arr = np.array([1, 2, 3])\nprint(arr.shape)  # (3,)\n\n# A\u00f1adir dimensi\u00f3n\narr_fila = arr[np.newaxis, :]  # (1, 3)\narr_col = arr[:, np.newaxis]   # (3, 1)\n\n# expand_dims\narr_exp = np.expand_dims(arr, axis=0)  # (1, 3)\narr_exp = np.expand_dims(arr, axis=1)  # (3, 1)\n\n# squeeze (eliminar dimensiones de tama\u00f1o 1)\narr = np.array([[[1, 2, 3]]])  # (1, 1, 3)\narr_squeeze = arr.squeeze()  # (3,)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#6-operaciones-matematicas","title":"6. Operaciones Matem\u00e1ticas","text":""},{"location":"conocimientos-basicos/13-numpy/#operaciones-elemento-a-elemento","title":"Operaciones Elemento a Elemento","text":"<pre><code>a = np.array([1, 2, 3, 4])\nb = np.array([10, 20, 30, 40])\n\n# Aritm\u00e9ticas\nprint(a + b)   # [11 22 33 44]\nprint(a - b)   # [ -9 -18 -27 -36]\nprint(a * b)   # [ 10  40  90 160]\nprint(a / b)   # [0.1 0.1 0.1 0.1]\nprint(a ** 2)  # [ 1  4  9 16]\nprint(a % 2)   # [1 0 1 0]\n\n# Con escalares\nprint(a + 10)  # [11 12 13 14]\nprint(a * 2)   # [2 4 6 8]\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#funciones-matematicas","title":"Funciones Matem\u00e1ticas","text":"<pre><code>arr = np.array([1, 4, 9, 16, 25])\n\n# Ra\u00edz cuadrada\nprint(np.sqrt(arr))  # [1. 2. 3. 4. 5.]\n\n# Exponencial y logaritmo\nprint(np.exp(arr))   # Exponencial\nprint(np.log(arr))   # Logaritmo natural\nprint(np.log10(arr)) # Logaritmo base 10\n\n# Trigonom\u00e9tricas\nangulos = np.array([0, np.pi/2, np.pi])\nprint(np.sin(angulos))  # [0.0000000e+00 1.0000000e+00 1.2246468e-16]\nprint(np.cos(angulos))  # [ 1.000000e+00  6.123234e-17 -1.000000e+00]\n\n# Redondeo\narr = np.array([1.2, 2.5, 3.7, 4.1])\nprint(np.round(arr))   # [1. 2. 4. 4.]\nprint(np.floor(arr))   # [1. 2. 3. 4.]\nprint(np.ceil(arr))    # [2. 3. 4. 5.]\nprint(np.trunc(arr))   # [1. 2. 3. 4.]\n\n# Valor absoluto\narr = np.array([-1, -2, 3, -4])\nprint(np.abs(arr))  # [1 2 3 4]\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#funciones-de-agregacion","title":"Funciones de Agregaci\u00f3n","text":"<pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(np.sum(arr))       # 21\nprint(np.mean(arr))      # 3.5\nprint(np.std(arr))       # Desviaci\u00f3n est\u00e1ndar\nprint(np.var(arr))       # Varianza\nprint(np.min(arr))       # 1\nprint(np.max(arr))       # 6\nprint(np.prod(arr))      # Producto (720)\n\n# Por eje\nprint(np.sum(arr, axis=0))  # Por columnas: [5 7 9]\nprint(np.sum(arr, axis=1))  # Por filas: [ 6 15]\n\nprint(np.mean(arr, axis=0))  # Media por columnas\nprint(np.mean(arr, axis=1))  # Media por filas\n\n# \u00cdndices de min/max\nprint(np.argmin(arr))  # 0 (\u00edndice del m\u00ednimo)\nprint(np.argmax(arr))  # 5 (\u00edndice del m\u00e1ximo)\n\n# Acumulativos\nprint(np.cumsum(arr))    # [ 1  3  6 10 15 21]\nprint(np.cumprod(arr))   # [  1   2   6  24 120 720]\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#7-algebra-lineal","title":"7. \u00c1lgebra Lineal","text":"<pre><code># Producto punto\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nprint(np.dot(a, b))  # 32\n\n# Multiplicaci\u00f3n de matrices\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nprint(np.dot(A, B))  # Producto matricial\nprint(A @ B)         # Igual que np.dot para matrices\n# [[19 22]\n#  [43 50]]\n\n# Multiplicaci\u00f3n elemento a elemento (NO matricial)\nprint(A * B)\n# [[ 5 12]\n#  [21 32]]\n\n# Transpuesta\nprint(A.T)\n\n# Determinante\nprint(np.linalg.det(A))  # -2.0\n\n# Matriz inversa\nprint(np.linalg.inv(A))\n\n# Autovalores y autovectores\nvalores, vectores = np.linalg.eig(A)\nprint(\"Autovalores:\", valores)\nprint(\"Autovectores:\", vectores)\n\n# Resolver sistema de ecuaciones Ax = b\nA = np.array([[2, 1], [1, 3]])\nb = np.array([5, 5])\nx = np.linalg.solve(A, b)\nprint(\"Soluci\u00f3n:\", x)  # [2. 1.]\n\n# Norma\nv = np.array([3, 4])\nprint(np.linalg.norm(v))  # 5.0 (norma euclidiana)\n\n# Rango de matriz\nprint(np.linalg.matrix_rank(A))\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#8-broadcasting","title":"8. Broadcasting","text":"<p>Broadcasting permite operar arrays de diferentes formas.</p> <pre><code># Escalar con array\narr = np.array([1, 2, 3])\nprint(arr * 2)  # [2 4 6]\n\n# Array 1D con 2D\nmatriz = np.array([[1, 2, 3], [4, 5, 6]])\nfila = np.array([10, 20, 30])\n\nprint(matriz + fila)\n# [[11 22 33]\n#  [14 25 36]]\n\n# Columna con matriz\ncolumna = np.array([[100], [200]])\nprint(matriz + columna)\n# [[101 102 103]\n#  [204 205 206]]\n\n# Crear tabla de multiplicar con broadcasting\nfilas = np.arange(1, 11).reshape(10, 1)\ncols = np.arange(1, 11)\ntabla = filas * cols\nprint(tabla)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#reglas-de-broadcasting","title":"Reglas de Broadcasting","text":"<ol> <li>Si los arrays tienen diferente n\u00famero de dimensiones, se a\u00f1aden 1s a la izquierda.</li> <li>Arrays con tama\u00f1o 1 en una dimensi\u00f3n se estiran para coincidir.</li> <li>Si los tama\u00f1os no coinciden y ninguno es 1, error.</li> </ol> <pre><code># Ejemplo: (3,) y (3, 3)\na = np.array([1, 2, 3])       # (3,)\nb = np.ones((3, 3))           # (3, 3)\n# a se convierte en (1, 3) -&gt; broadcasting a (3, 3)\nprint(a + b)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#9-funciones-de-comparacion","title":"9. Funciones de Comparaci\u00f3n","text":"<pre><code>arr = np.array([1, 2, 3, 4, 5])\n\n# Comparaciones (devuelven arrays booleanos)\nprint(arr &gt; 3)   # [False False False  True  True]\nprint(arr == 3)  # [False False  True False False]\nprint(arr != 3)  # [ True  True False  True  True]\n\n# np.where (como if-else vectorizado)\nresultado = np.where(arr &gt; 3, \"Grande\", \"Peque\u00f1o\")\nprint(resultado)  # ['Peque\u00f1o' 'Peque\u00f1o' 'Peque\u00f1o' 'Grande' 'Grande']\n\n# np.where para encontrar \u00edndices\nindices = np.where(arr &gt; 3)\nprint(indices)  # (array([3, 4]),)\n\n# any y all\nprint(np.any(arr &gt; 3))   # True (al menos uno)\nprint(np.all(arr &gt; 0))   # True (todos)\nprint(np.any(arr &gt; 10))  # False\n\n# Comparar arrays\na = np.array([1, 2, 3])\nb = np.array([1, 2, 4])\nprint(np.array_equal(a, b))  # False\nprint(np.allclose(a, b, atol=1))  # True (tolerancia)\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#10-numeros-aleatorios","title":"10. N\u00fameros Aleatorios","text":"<pre><code># Establecer semilla para reproducibilidad\nnp.random.seed(42)\n\n# Distribuci\u00f3n uniforme [0, 1)\nuniform = np.random.rand(3, 3)\n\n# Distribuci\u00f3n normal (media=0, std=1)\nnormal = np.random.randn(3, 3)\n\n# Enteros aleatorios\nenteros = np.random.randint(0, 100, size=(5, 5))\n\n# Elegir de un array\narr = np.array([10, 20, 30, 40, 50])\neleccion = np.random.choice(arr, size=3, replace=False)\n\n# Mezclar array\nnp.random.shuffle(arr)\n\n# Permutaci\u00f3n (devuelve nuevo array)\npermutado = np.random.permutation(arr)\n\n# Otras distribuciones\nbinomial = np.random.binomial(n=10, p=0.5, size=100)\npoisson = np.random.poisson(lam=5, size=100)\nexponencial = np.random.exponential(scale=1.0, size=100)\n\n# Nueva API (recomendada)\nrng = np.random.default_rng(seed=42)\nvalores = rng.random((3, 3))\nenteros = rng.integers(0, 100, size=(3, 3))\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#11-copiar-arrays","title":"11. Copiar Arrays","text":"<pre><code># Vista (comparte memoria)\narr = np.array([1, 2, 3, 4, 5])\nvista = arr[1:4]\nvista[0] = 100\nprint(arr)  # [  1 100   3   4   5] - arr tambi\u00e9n cambi\u00f3\n\n# Copia (independiente)\narr = np.array([1, 2, 3, 4, 5])\ncopia = arr[1:4].copy()\ncopia[0] = 100\nprint(arr)  # [1 2 3 4 5] - arr no cambi\u00f3\n\n# Verificar si comparten memoria\nprint(np.shares_memory(arr, vista))  # True\nprint(np.shares_memory(arr, copia))  # False\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#12-guardar-y-cargar-arrays","title":"12. Guardar y Cargar Arrays","text":"<pre><code>arr = np.array([[1, 2, 3], [4, 5, 6]])\n\n# Formato binario NumPy (.npy)\nnp.save(\"array.npy\", arr)\ncargado = np.load(\"array.npy\")\n\n# M\u00faltiples arrays (.npz)\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nnp.savez(\"arrays.npz\", arr_a=a, arr_b=b)\n\ndatos = np.load(\"arrays.npz\")\nprint(datos[\"arr_a\"])\nprint(datos[\"arr_b\"])\n\n# Formato texto\nnp.savetxt(\"array.txt\", arr, delimiter=\",\")\ncargado = np.loadtxt(\"array.txt\", delimiter=\",\")\n\n# CSV con encabezado\nnp.savetxt(\"datos.csv\", arr, delimiter=\",\", \n           header=\"col1,col2,col3\", comments=\"\")\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#13-ejemplo-practico-analisis-de-datos","title":"13. Ejemplo Pr\u00e1ctico: An\u00e1lisis de Datos","text":"<pre><code>import numpy as np\n\n# Simular datos de ventas (100 d\u00edas, 5 productos)\nnp.random.seed(42)\nventas = np.random.randint(10, 100, size=(100, 5))\n\n# Estad\u00edsticas b\u00e1sicas\nprint(\"Total de ventas:\", np.sum(ventas))\nprint(\"Media diaria por producto:\", np.mean(ventas, axis=0))\nprint(\"Mejor d\u00eda (ventas totales):\", np.argmax(np.sum(ventas, axis=1)))\nprint(\"Producto m\u00e1s vendido:\", np.argmax(np.sum(ventas, axis=0)))\n\n# D\u00edas con ventas superiores al promedio\npromedio_diario = np.mean(np.sum(ventas, axis=1))\ndias_buenos = np.where(np.sum(ventas, axis=1) &gt; promedio_diario)[0]\nprint(f\"D\u00edas sobre el promedio: {len(dias_buenos)}\")\n\n# Normalizar datos\nventas_norm = (ventas - np.min(ventas)) / (np.max(ventas) - np.min(ventas))\n\n# Correlaci\u00f3n entre productos\ncorrelacion = np.corrcoef(ventas.T)\nprint(\"Correlaci\u00f3n producto 0 y 1:\", correlacion[0, 1])\n</code></pre>"},{"location":"conocimientos-basicos/13-numpy/#14-resumen-de-funciones","title":"14. Resumen de Funciones","text":"Funci\u00f3n Descripci\u00f3n <code>np.array()</code> Crear array <code>np.zeros()</code>, <code>np.ones()</code> Arrays de ceros/unos <code>np.arange()</code>, <code>np.linspace()</code> Rangos de valores <code>reshape()</code>, <code>flatten()</code> Cambiar forma <code>np.vstack()</code>, <code>np.hstack()</code> Concatenar <code>np.sum()</code>, <code>np.mean()</code> Agregaciones <code>np.dot()</code>, <code>@</code> Producto matricial <code>np.where()</code> Condici\u00f3n vectorizada <code>np.save()</code>, <code>np.load()</code> Guardar/cargar <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/14-pandas/","title":"\ud83d\udcda Pandas - An\u00e1lisis de Datos","text":"<p>Pandas es la librer\u00eda m\u00e1s popular para an\u00e1lisis y manipulaci\u00f3n de datos en Python. Proporciona estructuras de datos flexibles (Series y DataFrame) y herramientas para trabajar con datos tabulares.</p>"},{"location":"conocimientos-basicos/14-pandas/#1-instalacion-e-importacion","title":"1. Instalaci\u00f3n e Importaci\u00f3n","text":"<pre><code># Instalaci\u00f3n\n# pip install pandas\n\n# Importaci\u00f3n (convenci\u00f3n est\u00e1ndar)\nimport pandas as pd\nimport numpy as np\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#2-estructuras-de-datos","title":"2. Estructuras de Datos","text":""},{"location":"conocimientos-basicos/14-pandas/#series","title":"Series","text":"<p>Una Series es un array unidimensional etiquetado.</p> <pre><code># Crear Series desde lista\ns = pd.Series([10, 20, 30, 40, 50])\nprint(s)\n# 0    10\n# 1    20\n# 2    30\n# 3    40\n# 4    50\n# dtype: int64\n\n# Con \u00edndices personalizados\ns = pd.Series([10, 20, 30], index=[\"a\", \"b\", \"c\"])\nprint(s)\n# a    10\n# b    20\n# c    30\n\n# Desde diccionario\ndatos = {\"manzanas\": 10, \"naranjas\": 20, \"pl\u00e1tanos\": 15}\ns = pd.Series(datos)\nprint(s)\n\n# Acceso a elementos\nprint(s[\"manzanas\"])  # 10\nprint(s[0])           # 10 (por posici\u00f3n)\nprint(s.values)       # array NumPy\nprint(s.index)        # Index(['manzanas', 'naranjas', 'pl\u00e1tanos'])\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#dataframe","title":"DataFrame","text":"<p>Un DataFrame es una tabla 2D con filas y columnas etiquetadas.</p> <pre><code># Desde diccionario de listas\ndatos = {\n    \"nombre\": [\"Ana\", \"Luis\", \"Mar\u00eda\", \"Pedro\"],\n    \"edad\": [25, 30, 28, 35],\n    \"ciudad\": [\"Madrid\", \"Barcelona\", \"Valencia\", \"Sevilla\"]\n}\ndf = pd.DataFrame(datos)\nprint(df)\n#   nombre  edad     ciudad\n# 0    Ana    25     Madrid\n# 1   Luis    30  Barcelona\n# 2  Mar\u00eda    28   Valencia\n# 3  Pedro    35    Sevilla\n\n# Con \u00edndice personalizado\ndf = pd.DataFrame(datos, index=[\"a\", \"b\", \"c\", \"d\"])\n\n# Desde lista de diccionarios\nregistros = [\n    {\"nombre\": \"Ana\", \"edad\": 25},\n    {\"nombre\": \"Luis\", \"edad\": 30}\n]\ndf = pd.DataFrame(registros)\n\n# Desde array NumPy\narr = np.random.randint(0, 100, (4, 3))\ndf = pd.DataFrame(arr, columns=[\"A\", \"B\", \"C\"])\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#3-leer-y-escribir-datos","title":"3. Leer y Escribir Datos","text":""},{"location":"conocimientos-basicos/14-pandas/#csv","title":"CSV","text":"<pre><code># Leer CSV\ndf = pd.read_csv(\"datos.csv\")\n\n# Con opciones\ndf = pd.read_csv(\"datos.csv\",\n                 sep=\";\",              # Separador\n                 header=0,             # Fila de encabezados\n                 index_col=0,          # Columna como \u00edndice\n                 usecols=[\"a\", \"b\"],   # Solo estas columnas\n                 nrows=100,            # Primeras N filas\n                 encoding=\"utf-8\",     # Codificaci\u00f3n\n                 na_values=[\"N/A\"])    # Valores nulos\n\n# Escribir CSV\ndf.to_csv(\"salida.csv\", index=False)\ndf.to_csv(\"salida.csv\", sep=\";\", encoding=\"utf-8\")\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#excel","title":"Excel","text":"<pre><code># Leer Excel (requiere openpyxl o xlrd)\ndf = pd.read_excel(\"datos.xlsx\", sheet_name=\"Hoja1\")\n\n# Leer m\u00faltiples hojas\nhojas = pd.read_excel(\"datos.xlsx\", sheet_name=None)  # Diccionario\n\n# Escribir Excel\ndf.to_excel(\"salida.xlsx\", index=False, sheet_name=\"Datos\")\n\n# M\u00faltiples hojas\nwith pd.ExcelWriter(\"salida.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Hoja1\")\n    df2.to_excel(writer, sheet_name=\"Hoja2\")\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#otros-formatos","title":"Otros Formatos","text":"<pre><code># JSON\ndf = pd.read_json(\"datos.json\")\ndf.to_json(\"salida.json\", orient=\"records\")\n\n# SQL\nimport sqlite3\nconn = sqlite3.connect(\"base_datos.db\")\ndf = pd.read_sql(\"SELECT * FROM tabla\", conn)\ndf.to_sql(\"tabla_nueva\", conn, if_exists=\"replace\")\n\n# HTML\ntablas = pd.read_html(\"https://ejemplo.com/tabla\")  # Lista de DataFrames\ndf.to_html(\"tabla.html\")\n\n# Pickle (binario Python)\ndf.to_pickle(\"datos.pkl\")\ndf = pd.read_pickle(\"datos.pkl\")\n\n# Parquet (eficiente para big data)\ndf.to_parquet(\"datos.parquet\")\ndf = pd.read_parquet(\"datos.parquet\")\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#4-explorar-datos","title":"4. Explorar Datos","text":"<pre><code># Crear DataFrame de ejemplo\ndf = pd.DataFrame({\n    \"nombre\": [\"Ana\", \"Luis\", \"Mar\u00eda\", \"Pedro\", \"Carmen\"],\n    \"edad\": [25, 30, 28, 35, 22],\n    \"salario\": [35000, 45000, 40000, 55000, 32000],\n    \"departamento\": [\"IT\", \"Ventas\", \"IT\", \"RRHH\", \"Ventas\"]\n})\n\n# Primeras/\u00faltimas filas\nprint(df.head())      # Primeras 5\nprint(df.head(3))     # Primeras 3\nprint(df.tail())      # \u00daltimas 5\n\n# Informaci\u00f3n general\nprint(df.info())      # Tipos, nulos, memoria\nprint(df.shape)       # (5, 4)\nprint(df.columns)     # Nombres de columnas\nprint(df.index)       # \u00cdndice\nprint(df.dtypes)      # Tipos por columna\n\n# Estad\u00edsticas descriptivas\nprint(df.describe())         # Num\u00e9ricas\nprint(df.describe(include=\"all\"))  # Todas\nprint(df.describe(include=[\"object\"]))  # Solo texto\n\n# Valores \u00fanicos\nprint(df[\"departamento\"].unique())      # Array de valores \u00fanicos\nprint(df[\"departamento\"].nunique())     # Cantidad de \u00fanicos\nprint(df[\"departamento\"].value_counts())  # Frecuencia\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#5-seleccion-de-datos","title":"5. Selecci\u00f3n de Datos","text":""},{"location":"conocimientos-basicos/14-pandas/#seleccionar-columnas","title":"Seleccionar Columnas","text":"<pre><code># Una columna (Series)\nprint(df[\"nombre\"])\nprint(df.nombre)  # Equivalente si no tiene espacios\n\n# Varias columnas (DataFrame)\nprint(df[[\"nombre\", \"edad\"]])\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#seleccionar-filas","title":"Seleccionar Filas","text":"<pre><code># Por \u00edndice posicional (iloc)\nprint(df.iloc[0])       # Primera fila (Series)\nprint(df.iloc[0:3])     # Primeras 3 filas\nprint(df.iloc[[0, 2, 4]])  # Filas espec\u00edficas\n\n# Por etiqueta (loc)\nprint(df.loc[0])        # Fila con \u00edndice 0\nprint(df.loc[0:2])      # Filas 0 a 2 (incluido)\n\n# Filas y columnas\nprint(df.iloc[0:3, 0:2])      # Primeras 3 filas, primeras 2 columnas\nprint(df.loc[0:2, [\"nombre\", \"edad\"]])  # Por etiquetas\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#filtrar-con-condiciones","title":"Filtrar con Condiciones","text":"<pre><code># Condici\u00f3n simple\nmayores_30 = df[df[\"edad\"] &gt; 30]\nprint(mayores_30)\n\n# Condiciones m\u00faltiples\nfiltro = df[(df[\"edad\"] &gt; 25) &amp; (df[\"salario\"] &gt; 35000)]\nprint(filtro)\n\n# OR\nfiltro = df[(df[\"departamento\"] == \"IT\") | (df[\"departamento\"] == \"Ventas\")]\n\n# isin() para m\u00faltiples valores\nfiltro = df[df[\"departamento\"].isin([\"IT\", \"RRHH\"])]\n\n# Texto con str\nfiltro = df[df[\"nombre\"].str.startswith(\"M\")]\nfiltro = df[df[\"nombre\"].str.contains(\"ar\")]\n\n# query() - sintaxis m\u00e1s legible\nfiltro = df.query(\"edad &gt; 25 and salario &gt; 35000\")\nfiltro = df.query(\"departamento == 'IT'\")\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#6-modificar-datos","title":"6. Modificar Datos","text":""},{"location":"conocimientos-basicos/14-pandas/#anadirmodificar-columnas","title":"A\u00f1adir/Modificar Columnas","text":"<pre><code># Nueva columna\ndf[\"bonus\"] = df[\"salario\"] * 0.1\ndf[\"email\"] = df[\"nombre\"].str.lower() + \"@empresa.com\"\n\n# Columna condicional\ndf[\"senior\"] = df[\"edad\"] &gt; 30\ndf[\"categoria\"] = np.where(df[\"salario\"] &gt; 40000, \"Alto\", \"Normal\")\n\n# apply() para funciones personalizadas\ndef categorizar_edad(edad):\n    if edad &lt; 25:\n        return \"Joven\"\n    elif edad &lt; 35:\n        return \"Adulto\"\n    return \"Senior\"\n\ndf[\"grupo_edad\"] = df[\"edad\"].apply(categorizar_edad)\n\n# map() para reemplazar valores\nmapa_dept = {\"IT\": \"Tecnolog\u00eda\", \"RRHH\": \"Recursos Humanos\", \"Ventas\": \"Comercial\"}\ndf[\"dept_largo\"] = df[\"departamento\"].map(mapa_dept)\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#modificar-valores","title":"Modificar Valores","text":"<pre><code># Modificar valor espec\u00edfico\ndf.loc[0, \"salario\"] = 36000\ndf.iloc[0, 2] = 36000\n\n# Modificar con condici\u00f3n\ndf.loc[df[\"departamento\"] == \"IT\", \"salario\"] *= 1.1  # Aumento 10%\n\n# replace()\ndf[\"departamento\"] = df[\"departamento\"].replace(\"IT\", \"Tech\")\ndf = df.replace({\"IT\": \"Tech\", \"RRHH\": \"HR\"})\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#renombrar","title":"Renombrar","text":"<pre><code># Renombrar columnas\ndf = df.rename(columns={\"nombre\": \"empleado\", \"edad\": \"a\u00f1os\"})\n\n# Renombrar todas\ndf.columns = [\"col1\", \"col2\", \"col3\", \"col4\"]\n\n# Renombrar \u00edndice\ndf = df.rename(index={0: \"primero\", 1: \"segundo\"})\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#eliminar","title":"Eliminar","text":"<pre><code># Eliminar columnas\ndf = df.drop(columns=[\"bonus\", \"email\"])\ndf = df.drop(\"bonus\", axis=1)\n\n# Eliminar filas\ndf = df.drop([0, 1])  # Por \u00edndice\ndf = df.drop(df[df[\"edad\"] &lt; 25].index)  # Por condici\u00f3n\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#7-valores-nulos","title":"7. Valores Nulos","text":"<pre><code># Crear DataFrame con nulos\ndf = pd.DataFrame({\n    \"A\": [1, 2, None, 4],\n    \"B\": [None, 2, 3, 4],\n    \"C\": [1, None, None, 4]\n})\n\n# Detectar nulos\nprint(df.isnull())      # DataFrame booleano\nprint(df.isna())        # Igual\nprint(df.isnull().sum())  # Cantidad por columna\nprint(df.isnull().sum().sum())  # Total\n\n# Eliminar nulos\ndf_limpio = df.dropna()           # Filas con alg\u00fan nulo\ndf_limpio = df.dropna(how=\"all\")  # Filas completamente nulas\ndf_limpio = df.dropna(subset=[\"A\", \"B\"])  # Solo si nulo en A o B\n\n# Rellenar nulos\ndf_relleno = df.fillna(0)              # Con valor\ndf_relleno = df.fillna(df.mean())      # Con media\ndf_relleno = df.fillna(method=\"ffill\") # Forward fill\ndf_relleno = df.fillna(method=\"bfill\") # Backward fill\n\n# Interpolaci\u00f3n\ndf_interp = df.interpolate()\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#8-ordenar-datos","title":"8. Ordenar Datos","text":"<pre><code>df = pd.DataFrame({\n    \"nombre\": [\"Ana\", \"Luis\", \"Mar\u00eda\", \"Pedro\"],\n    \"edad\": [25, 30, 28, 35],\n    \"salario\": [35000, 45000, 40000, 55000]\n})\n\n# Ordenar por columna\ndf_ordenado = df.sort_values(\"edad\")\ndf_ordenado = df.sort_values(\"edad\", ascending=False)\n\n# Por m\u00faltiples columnas\ndf_ordenado = df.sort_values([\"edad\", \"salario\"], ascending=[True, False])\n\n# Por \u00edndice\ndf_ordenado = df.sort_index()\n\n# nlargest / nsmallest\ntop3 = df.nlargest(3, \"salario\")\nbottom3 = df.nsmallest(3, \"edad\")\n\n# Ranking\ndf[\"rank_salario\"] = df[\"salario\"].rank(ascending=False)\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#9-agrupar-datos-groupby","title":"9. Agrupar Datos (GroupBy)","text":"<pre><code>df = pd.DataFrame({\n    \"departamento\": [\"IT\", \"Ventas\", \"IT\", \"Ventas\", \"RRHH\"],\n    \"empleado\": [\"Ana\", \"Luis\", \"Mar\u00eda\", \"Pedro\", \"Carmen\"],\n    \"salario\": [35000, 45000, 40000, 55000, 38000],\n    \"a\u00f1os_empresa\": [2, 5, 3, 8, 4]\n})\n\n# Agrupar por una columna\ngrupo = df.groupby(\"departamento\")\n\n# Agregaciones\nprint(grupo[\"salario\"].mean())    # Media por departamento\nprint(grupo[\"salario\"].sum())     # Suma\nprint(grupo.size())               # Tama\u00f1o de cada grupo\n\n# M\u00faltiples agregaciones\nprint(grupo[\"salario\"].agg([\"mean\", \"sum\", \"count\", \"min\", \"max\"]))\n\n# Agregaciones diferentes por columna\nresultado = grupo.agg({\n    \"salario\": [\"mean\", \"sum\"],\n    \"a\u00f1os_empresa\": \"mean\"\n})\n\n# agg() con funciones personalizadas\ndef rango(x):\n    return x.max() - x.min()\n\nprint(grupo[\"salario\"].agg(rango))\n\n# Agrupar por m\u00faltiples columnas\ndf[\"senior\"] = df[\"a\u00f1os_empresa\"] &gt; 3\ngrupo2 = df.groupby([\"departamento\", \"senior\"])\nprint(grupo2[\"salario\"].mean())\n\n# transform() - mantiene forma original\ndf[\"salario_medio_dept\"] = grupo[\"salario\"].transform(\"mean\")\ndf[\"salario_vs_media\"] = df[\"salario\"] - df[\"salario_medio_dept\"]\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#10-combinar-dataframes","title":"10. Combinar DataFrames","text":""},{"location":"conocimientos-basicos/14-pandas/#concatenar","title":"Concatenar","text":"<pre><code>df1 = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\ndf2 = pd.DataFrame({\"A\": [5, 6], \"B\": [7, 8]})\n\n# Vertical (filas)\nconcat_v = pd.concat([df1, df2])\nconcat_v = pd.concat([df1, df2], ignore_index=True)\n\n# Horizontal (columnas)\nconcat_h = pd.concat([df1, df2], axis=1)\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#merge-join","title":"Merge (JOIN)","text":"<pre><code>empleados = pd.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"nombre\": [\"Ana\", \"Luis\", \"Mar\u00eda\", \"Pedro\"],\n    \"dept_id\": [10, 20, 10, 30]\n})\n\ndepartamentos = pd.DataFrame({\n    \"dept_id\": [10, 20, 30],\n    \"departamento\": [\"IT\", \"Ventas\", \"RRHH\"]\n})\n\n# Inner join (por defecto)\nresultado = pd.merge(empleados, departamentos, on=\"dept_id\")\n\n# Left join\nresultado = pd.merge(empleados, departamentos, on=\"dept_id\", how=\"left\")\n\n# Right join\nresultado = pd.merge(empleados, departamentos, on=\"dept_id\", how=\"right\")\n\n# Outer join\nresultado = pd.merge(empleados, departamentos, on=\"dept_id\", how=\"outer\")\n\n# Columnas con nombres diferentes\ndf1 = pd.DataFrame({\"id_emp\": [1, 2], \"nombre\": [\"A\", \"B\"]})\ndf2 = pd.DataFrame({\"emp_id\": [1, 2], \"salario\": [1000, 2000]})\nresultado = pd.merge(df1, df2, left_on=\"id_emp\", right_on=\"emp_id\")\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#join","title":"Join","text":"<pre><code>df1 = pd.DataFrame({\"A\": [1, 2]}, index=[\"a\", \"b\"])\ndf2 = pd.DataFrame({\"B\": [3, 4]}, index=[\"a\", \"b\"])\n\n# Join por \u00edndice\nresultado = df1.join(df2)\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#11-pivot-tables-y-reshape","title":"11. Pivot Tables y Reshape","text":""},{"location":"conocimientos-basicos/14-pandas/#pivot-table","title":"Pivot Table","text":"<pre><code>df = pd.DataFrame({\n    \"fecha\": [\"2024-01\", \"2024-01\", \"2024-02\", \"2024-02\"],\n    \"producto\": [\"A\", \"B\", \"A\", \"B\"],\n    \"ventas\": [100, 150, 120, 180],\n    \"cantidad\": [10, 15, 12, 18]\n})\n\n# Pivot table b\u00e1sico\npivot = df.pivot_table(\n    values=\"ventas\",\n    index=\"fecha\",\n    columns=\"producto\",\n    aggfunc=\"sum\"\n)\n\n# Con m\u00faltiples agregaciones\npivot = df.pivot_table(\n    values=\"ventas\",\n    index=\"fecha\",\n    columns=\"producto\",\n    aggfunc=[\"sum\", \"mean\"]\n)\n\n# Totales\npivot = df.pivot_table(\n    values=\"ventas\",\n    index=\"fecha\",\n    columns=\"producto\",\n    aggfunc=\"sum\",\n    margins=True,\n    margins_name=\"Total\"\n)\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#melt-de-ancho-a-largo","title":"Melt (de ancho a largo)","text":"<pre><code>df_ancho = pd.DataFrame({\n    \"id\": [1, 2],\n    \"nombre\": [\"Ana\", \"Luis\"],\n    \"enero\": [100, 150],\n    \"febrero\": [110, 160]\n})\n\ndf_largo = pd.melt(\n    df_ancho,\n    id_vars=[\"id\", \"nombre\"],\n    value_vars=[\"enero\", \"febrero\"],\n    var_name=\"mes\",\n    value_name=\"ventas\"\n)\nprint(df_largo)\n#    id nombre      mes  ventas\n# 0   1    Ana    enero     100\n# 1   2   Luis    enero     150\n# 2   1    Ana  febrero     110\n# 3   2   Luis  febrero     160\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#stackunstack","title":"Stack/Unstack","text":"<pre><code># Stack: de columnas a filas (MultiIndex)\ndf = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}, index=[\"x\", \"y\"])\nstacked = df.stack()\n\n# Unstack: de filas a columnas\nunstacked = stacked.unstack()\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#12-trabajar-con-fechas","title":"12. Trabajar con Fechas","text":"<pre><code># Crear columna datetime\ndf = pd.DataFrame({\n    \"fecha_str\": [\"2024-01-15\", \"2024-02-20\", \"2024-03-25\"]\n})\n\ndf[\"fecha\"] = pd.to_datetime(df[\"fecha_str\"])\n\n# Crear rango de fechas\nfechas = pd.date_range(\"2024-01-01\", periods=10, freq=\"D\")\nfechas = pd.date_range(\"2024-01-01\", \"2024-12-31\", freq=\"M\")\n\n# Extraer componentes\ndf[\"a\u00f1o\"] = df[\"fecha\"].dt.year\ndf[\"mes\"] = df[\"fecha\"].dt.month\ndf[\"dia\"] = df[\"fecha\"].dt.day\ndf[\"dia_semana\"] = df[\"fecha\"].dt.dayofweek  # 0=Lunes\ndf[\"nombre_dia\"] = df[\"fecha\"].dt.day_name()\ndf[\"trimestre\"] = df[\"fecha\"].dt.quarter\n\n# Filtrar por fechas\ndf = df[df[\"fecha\"] &gt; \"2024-02-01\"]\ndf = df[df[\"fecha\"].between(\"2024-01-01\", \"2024-06-30\")]\n\n# \u00cdndice temporal\ndf = df.set_index(\"fecha\")\ndf_2024 = df.loc[\"2024\"]\ndf_enero = df.loc[\"2024-01\"]\n\n# Resample (agrupar por tiempo)\ndf_mensual = df.resample(\"M\").sum()\ndf_semanal = df.resample(\"W\").mean()\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#13-operaciones-con-strings","title":"13. Operaciones con Strings","text":"<pre><code>df = pd.DataFrame({\n    \"nombre\": [\"  Ana Garc\u00eda  \", \"LUIS P\u00c9REZ\", \"mar\u00eda l\u00f3pez\"],\n    \"email\": [\"ana@email.com\", \"luis@empresa.es\", \"maria@otro.net\"]\n})\n\n# Acceso a m\u00e9todos string con .str\ndf[\"nombre_limpio\"] = df[\"nombre\"].str.strip()\ndf[\"nombre_upper\"] = df[\"nombre\"].str.upper()\ndf[\"nombre_lower\"] = df[\"nombre\"].str.lower()\ndf[\"nombre_title\"] = df[\"nombre\"].str.title()\n\n# Dividir\ndf[\"dominio\"] = df[\"email\"].str.split(\"@\").str[1]\n\n# Reemplazar\ndf[\"email_nuevo\"] = df[\"email\"].str.replace(\".com\", \".org\")\n\n# Contiene\ndf[\"es_empresa\"] = df[\"email\"].str.contains(\"empresa\")\n\n# Longitud\ndf[\"len_nombre\"] = df[\"nombre\"].str.len()\n\n# Extraer con regex\ndf[\"usuario\"] = df[\"email\"].str.extract(r\"(.+)@\")\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#14-ejemplo-completo-analisis-de-ventas","title":"14. Ejemplo Completo: An\u00e1lisis de Ventas","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Crear datos de ejemplo\nnp.random.seed(42)\nn = 1000\n\ndf = pd.DataFrame({\n    \"fecha\": pd.date_range(\"2024-01-01\", periods=n, freq=\"H\"),\n    \"producto\": np.random.choice([\"Laptop\", \"Mouse\", \"Teclado\", \"Monitor\"], n),\n    \"cantidad\": np.random.randint(1, 10, n),\n    \"precio_unitario\": np.random.choice([999, 29, 79, 299], n),\n    \"region\": np.random.choice([\"Norte\", \"Sur\", \"Este\", \"Oeste\"], n)\n})\n\ndf[\"total\"] = df[\"cantidad\"] * df[\"precio_unitario\"]\n\n# An\u00e1lisis\nprint(\"=== AN\u00c1LISIS DE VENTAS ===\\n\")\n\n# Resumen general\nprint(\"Resumen estad\u00edstico:\")\nprint(df[[\"cantidad\", \"total\"]].describe())\n\n# Ventas por producto\nprint(\"\\nVentas por producto:\")\nprint(df.groupby(\"producto\")[\"total\"].agg([\"sum\", \"mean\", \"count\"]))\n\n# Ventas por regi\u00f3n\nprint(\"\\nVentas por regi\u00f3n:\")\nprint(df.groupby(\"region\")[\"total\"].sum().sort_values(ascending=False))\n\n# Ventas mensuales\ndf[\"mes\"] = df[\"fecha\"].dt.to_period(\"M\")\nprint(\"\\nVentas mensuales:\")\nprint(df.groupby(\"mes\")[\"total\"].sum())\n\n# Top productos por regi\u00f3n\nprint(\"\\nProducto m\u00e1s vendido por regi\u00f3n:\")\nprint(df.groupby([\"region\", \"producto\"])[\"total\"].sum().unstack().idxmax(axis=1))\n\n# Pivot: productos vs regiones\nprint(\"\\nTabla resumen (productos x regiones):\")\npivot = df.pivot_table(\n    values=\"total\",\n    index=\"producto\",\n    columns=\"region\",\n    aggfunc=\"sum\",\n    margins=True\n)\nprint(pivot)\n</code></pre>"},{"location":"conocimientos-basicos/14-pandas/#15-resumen-de-funciones","title":"15. Resumen de Funciones","text":"Funci\u00f3n Descripci\u00f3n <code>pd.read_csv()</code> Leer CSV <code>df.to_csv()</code> Escribir CSV <code>df.head()</code>, <code>df.tail()</code> Ver filas <code>df.info()</code>, <code>df.describe()</code> Informaci\u00f3n <code>df.loc[]</code>, <code>df.iloc[]</code> Seleccionar <code>df.query()</code> Filtrar <code>df.groupby()</code> Agrupar <code>pd.merge()</code> Combinar <code>pd.concat()</code> Concatenar <code>df.pivot_table()</code> Tabla pivote <code>df.fillna()</code>, <code>df.dropna()</code> Manejar nulos <code>df.sort_values()</code> Ordenar <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/15-matplotlib/","title":"\ud83d\udcda Matplotlib - Visualizaci\u00f3n de Datos","text":"<p>Matplotlib es la librer\u00eda m\u00e1s utilizada para crear gr\u00e1ficos y visualizaciones en Python. Es altamente personalizable y sirve como base para otras librer\u00edas de visualizaci\u00f3n.</p>"},{"location":"conocimientos-basicos/15-matplotlib/#1-instalacion-e-importacion","title":"1. Instalaci\u00f3n e Importaci\u00f3n","text":"<pre><code># Instalaci\u00f3n\n# pip install matplotlib\n\n# Importaci\u00f3n (convenci\u00f3n est\u00e1ndar)\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#2-grafico-basico","title":"2. Gr\u00e1fico B\u00e1sico","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Datos\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 6, 8, 10]\n\n# Crear gr\u00e1fico\nplt.plot(x, y)\n\n# Mostrar\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#con-titulos-y-etiquetas","title":"Con T\u00edtulos y Etiquetas","text":"<pre><code>plt.plot(x, y)\nplt.title(\"Mi Primer Gr\u00e1fico\")\nplt.xlabel(\"Eje X\")\nplt.ylabel(\"Eje Y\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#guardar-figura","title":"Guardar Figura","text":"<pre><code>plt.plot(x, y)\nplt.title(\"Gr\u00e1fico\")\nplt.savefig(\"grafico.png\", dpi=300, bbox_inches=\"tight\")\nplt.savefig(\"grafico.pdf\")  # Tambi\u00e9n PDF, SVG, etc.\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#3-tipos-de-graficos","title":"3. Tipos de Gr\u00e1ficos","text":""},{"location":"conocimientos-basicos/15-matplotlib/#grafico-de-lineas","title":"Gr\u00e1fico de L\u00edneas","text":"<pre><code>x = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nplt.plot(x, y1, label=\"sin(x)\")\nplt.plot(x, y2, label=\"cos(x)\")\nplt.title(\"Funciones Trigonom\u00e9tricas\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#personalizar-lineas","title":"Personalizar L\u00edneas","text":"<pre><code>x = np.arange(0, 10, 1)\ny = x ** 2\n\n# Estilos de l\u00ednea\nplt.plot(x, y, color=\"red\", linewidth=2, linestyle=\"--\", marker=\"o\")\n\n# Formato corto: 'color-marcador-l\u00ednea'\nplt.plot(x, y, \"b-o\")   # Azul, l\u00ednea s\u00f3lida, c\u00edrculos\nplt.plot(x, y, \"r--s\")  # Rojo, l\u00ednea discontinua, cuadrados\nplt.plot(x, y, \"g:^\")   # Verde, l\u00ednea punteada, tri\u00e1ngulos\n\nplt.show()\n</code></pre> <p>Colores: <code>b</code>(blue), <code>g</code>(green), <code>r</code>(red), <code>c</code>(cyan), <code>m</code>(magenta), <code>y</code>(yellow), <code>k</code>(black), <code>w</code>(white)</p> <p>Marcadores: <code>o</code>(c\u00edrculo), <code>s</code>(cuadrado), <code>^</code>(tri\u00e1ngulo), <code>*</code>(estrella), <code>+</code>, <code>x</code>, <code>.</code></p> <p>L\u00edneas: <code>-</code>(s\u00f3lida), <code>--</code>(discontinua), <code>:</code>(punteada), <code>-.</code>(punto-raya)</p>"},{"location":"conocimientos-basicos/15-matplotlib/#grafico-de-dispersion-scatter","title":"Gr\u00e1fico de Dispersi\u00f3n (Scatter)","text":"<pre><code>np.random.seed(42)\nx = np.random.randn(100)\ny = np.random.randn(100)\ncolores = np.random.rand(100)\ntama\u00f1os = np.random.rand(100) * 500\n\nplt.scatter(x, y, c=colores, s=tama\u00f1os, alpha=0.6, cmap=\"viridis\")\nplt.colorbar(label=\"Valor\")\nplt.title(\"Gr\u00e1fico de Dispersi\u00f3n\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#grafico-de-barras","title":"Gr\u00e1fico de Barras","text":"<pre><code>categorias = [\"A\", \"B\", \"C\", \"D\", \"E\"]\nvalores = [23, 45, 56, 78, 32]\n\n# Barras verticales\nplt.bar(categorias, valores, color=\"steelblue\", edgecolor=\"black\")\nplt.title(\"Gr\u00e1fico de Barras\")\nplt.xlabel(\"Categor\u00eda\")\nplt.ylabel(\"Valor\")\nplt.show()\n\n# Barras horizontales\nplt.barh(categorias, valores, color=\"coral\")\nplt.title(\"Barras Horizontales\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#barras-agrupadas","title":"Barras Agrupadas","text":"<pre><code>categorias = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\nproducto_a = [20, 35, 30, 35]\nproducto_b = [25, 32, 34, 20]\n\nx = np.arange(len(categorias))\nancho = 0.35\n\nplt.bar(x - ancho/2, producto_a, ancho, label=\"Producto A\", color=\"steelblue\")\nplt.bar(x + ancho/2, producto_b, ancho, label=\"Producto B\", color=\"coral\")\n\nplt.xlabel(\"Trimestre\")\nplt.ylabel(\"Ventas\")\nplt.title(\"Ventas por Producto\")\nplt.xticks(x, categorias)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#barras-apiladas","title":"Barras Apiladas","text":"<pre><code>categorias = [\"A\", \"B\", \"C\", \"D\"]\nvalores1 = [20, 35, 30, 25]\nvalores2 = [25, 32, 34, 20]\n\nplt.bar(categorias, valores1, label=\"Serie 1\")\nplt.bar(categorias, valores2, bottom=valores1, label=\"Serie 2\")\nplt.legend()\nplt.title(\"Barras Apiladas\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#histograma","title":"Histograma","text":"<pre><code>datos = np.random.randn(1000)\n\nplt.hist(datos, bins=30, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\nplt.title(\"Histograma\")\nplt.xlabel(\"Valor\")\nplt.ylabel(\"Frecuencia\")\nplt.show()\n\n# M\u00faltiples histogramas\ndatos1 = np.random.randn(1000)\ndatos2 = np.random.randn(1000) + 2\n\nplt.hist(datos1, bins=30, alpha=0.5, label=\"Grupo 1\")\nplt.hist(datos2, bins=30, alpha=0.5, label=\"Grupo 2\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#grafico-de-pastel-pie","title":"Gr\u00e1fico de Pastel (Pie)","text":"<pre><code>etiquetas = [\"Python\", \"JavaScript\", \"Java\", \"C++\", \"Otros\"]\ntama\u00f1os = [35, 25, 20, 10, 10]\ncolores = [\"#ff9999\", \"#66b3ff\", \"#99ff99\", \"#ffcc99\", \"#c2c2f0\"]\nexplode = (0.1, 0, 0, 0, 0)  # Destacar Python\n\nplt.pie(tama\u00f1os, explode=explode, labels=etiquetas, colors=colores,\n        autopct=\"%1.1f%%\", shadow=True, startangle=90)\nplt.title(\"Lenguajes de Programaci\u00f3n\")\nplt.axis(\"equal\")  # C\u00edrculo perfecto\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#diagrama-de-caja-box-plot","title":"Diagrama de Caja (Box Plot)","text":"<pre><code>datos = [np.random.randn(100) + i for i in range(4)]\n\nplt.boxplot(datos, labels=[\"A\", \"B\", \"C\", \"D\"])\nplt.title(\"Diagrama de Caja\")\nplt.ylabel(\"Valor\")\nplt.show()\n\n# Personalizado\nplt.boxplot(datos, notch=True, patch_artist=True,\n            boxprops=dict(facecolor=\"lightblue\"))\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#grafico-de-violin","title":"Gr\u00e1fico de Viol\u00edn","text":"<pre><code>datos = [np.random.randn(100) * (i+1) for i in range(4)]\n\nplt.violinplot(datos, showmeans=True, showmedians=True)\nplt.xticks([1, 2, 3, 4], [\"A\", \"B\", \"C\", \"D\"])\nplt.title(\"Gr\u00e1fico de Viol\u00edn\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#mapa-de-calor-heatmap","title":"Mapa de Calor (Heatmap)","text":"<pre><code>datos = np.random.rand(10, 10)\n\nplt.imshow(datos, cmap=\"hot\", aspect=\"auto\")\nplt.colorbar(label=\"Valor\")\nplt.title(\"Mapa de Calor\")\nplt.show()\n\n# Con anotaciones\nfig, ax = plt.subplots()\nim = ax.imshow(datos[:5, :5], cmap=\"YlOrRd\")\nfor i in range(5):\n    for j in range(5):\n        ax.text(j, i, f\"{datos[i, j]:.2f}\", ha=\"center\", va=\"center\")\nplt.colorbar(im)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#grafico-de-area","title":"Gr\u00e1fico de \u00c1rea","text":"<pre><code>x = np.arange(10)\ny1 = np.random.randint(1, 10, 10)\ny2 = np.random.randint(1, 10, 10)\ny3 = np.random.randint(1, 10, 10)\n\nplt.stackplot(x, y1, y2, y3, labels=[\"A\", \"B\", \"C\"], alpha=0.7)\nplt.legend(loc=\"upper left\")\nplt.title(\"Gr\u00e1fico de \u00c1rea Apilada\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#4-subplots-multiples-graficos","title":"4. Subplots (M\u00faltiples Gr\u00e1ficos)","text":""},{"location":"conocimientos-basicos/15-matplotlib/#basico","title":"B\u00e1sico","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n# Gr\u00e1fico 1 (arriba izquierda)\naxes[0, 0].plot([1, 2, 3], [1, 4, 9])\naxes[0, 0].set_title(\"Gr\u00e1fico 1\")\n\n# Gr\u00e1fico 2 (arriba derecha)\naxes[0, 1].bar([\"A\", \"B\", \"C\"], [3, 7, 5])\naxes[0, 1].set_title(\"Gr\u00e1fico 2\")\n\n# Gr\u00e1fico 3 (abajo izquierda)\naxes[1, 0].scatter(np.random.rand(50), np.random.rand(50))\naxes[1, 0].set_title(\"Gr\u00e1fico 3\")\n\n# Gr\u00e1fico 4 (abajo derecha)\naxes[1, 1].hist(np.random.randn(100), bins=20)\naxes[1, 1].set_title(\"Gr\u00e1fico 4\")\n\nplt.tight_layout()  # Ajusta espaciado\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#una-fila-o-columna","title":"Una Fila o Columna","text":"<pre><code># Una fila\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\naxes[0].plot([1, 2, 3])\naxes[1].bar([\"A\", \"B\"], [1, 2])\naxes[2].scatter([1, 2], [1, 2])\nplt.tight_layout()\nplt.show()\n\n# Una columna\nfig, axes = plt.subplots(3, 1, figsize=(6, 10))\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#subplots-con-tamanos-diferentes","title":"Subplots con Tama\u00f1os Diferentes","text":"<pre><code>fig = plt.figure(figsize=(12, 6))\n\n# Gr\u00e1fico grande a la izquierda\nax1 = fig.add_subplot(1, 2, 1)\nax1.plot(np.sin(np.linspace(0, 10, 100)))\nax1.set_title(\"Gr\u00e1fico Principal\")\n\n# Dos gr\u00e1ficos peque\u00f1os a la derecha\nax2 = fig.add_subplot(2, 2, 2)\nax2.bar([\"A\", \"B\", \"C\"], [1, 2, 3])\nax2.set_title(\"Gr\u00e1fico 2\")\n\nax3 = fig.add_subplot(2, 2, 4)\nax3.scatter(np.random.rand(20), np.random.rand(20))\nax3.set_title(\"Gr\u00e1fico 3\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#5-personalizacion-avanzada","title":"5. Personalizaci\u00f3n Avanzada","text":""},{"location":"conocimientos-basicos/15-matplotlib/#configurar-figura","title":"Configurar Figura","text":"<pre><code># Tama\u00f1o y resoluci\u00f3n\nfig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n\n# Fondo\nfig.patch.set_facecolor(\"lightgray\")\nax.set_facecolor(\"white\")\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#ejes-y-ticks","title":"Ejes y Ticks","text":"<pre><code>x = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\nax.plot(x, y)\n\n# L\u00edmites de ejes\nax.set_xlim(0, 10)\nax.set_ylim(-1.5, 1.5)\n\n# Ticks personalizados\nax.set_xticks([0, np.pi, 2*np.pi, 3*np.pi])\nax.set_xticklabels([\"0\", \"\u03c0\", \"2\u03c0\", \"3\u03c0\"])\n\n# Rotar etiquetas\nplt.xticks(rotation=45)\n\n# Grid\nax.grid(True, linestyle=\"--\", alpha=0.7)\nax.grid(True, which=\"minor\", linestyle=\":\", alpha=0.5)\nax.minorticks_on()\n\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#leyenda","title":"Leyenda","text":"<pre><code>x = np.linspace(0, 10, 100)\n\nplt.plot(x, np.sin(x), label=\"sin(x)\")\nplt.plot(x, np.cos(x), label=\"cos(x)\")\n\n# Ubicaci\u00f3n\nplt.legend(loc=\"upper right\")\n# Opciones: 'best', 'upper left', 'lower right', 'center', etc.\n\n# Fuera del gr\u00e1fico\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n# Personalizada\nplt.legend(title=\"Funciones\", fontsize=10, framealpha=0.8,\n           facecolor=\"white\", edgecolor=\"black\")\n\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#anotaciones","title":"Anotaciones","text":"<pre><code>x = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nfig, ax = plt.subplots()\nax.plot(x, y)\n\n# Texto simple\nax.text(5, 0.5, \"Texto aqu\u00ed\", fontsize=12, color=\"red\")\n\n# Anotaci\u00f3n con flecha\nax.annotate(\"M\u00e1ximo\", xy=(np.pi/2, 1), xytext=(3, 1.2),\n            arrowprops=dict(arrowstyle=\"-&gt;\", color=\"black\"),\n            fontsize=10)\n\n# L\u00ednea vertical/horizontal\nax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\nax.axvline(x=np.pi, color=\"red\", linestyle=\":\", alpha=0.7)\n\n# Regi\u00f3n sombreada\nax.axvspan(2, 4, alpha=0.2, color=\"yellow\")\nax.axhspan(-0.5, 0.5, alpha=0.1, color=\"green\")\n\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#estilos-predefinidos","title":"Estilos Predefinidos","text":"<pre><code># Ver estilos disponibles\nprint(plt.style.available)\n\n# Usar estilo\nplt.style.use(\"seaborn-v0_8\")  # o 'ggplot', 'dark_background', etc.\n\nx = np.linspace(0, 10, 100)\nplt.plot(x, np.sin(x))\nplt.plot(x, np.cos(x))\nplt.title(\"Con estilo Seaborn\")\nplt.show()\n\n# Contexto temporal\nwith plt.style.context(\"dark_background\"):\n    plt.plot(x, np.sin(x))\n    plt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#colormaps","title":"Colormaps","text":"<pre><code># Ver colormaps disponibles\n# Sequential: 'viridis', 'plasma', 'magma', 'cividis', 'Blues', 'Reds'\n# Diverging: 'coolwarm', 'RdYlBu', 'seismic'\n# Categorical: 'Set1', 'Set2', 'Pastel1'\n\ndatos = np.random.rand(10, 10)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\ncmaps = [\"viridis\", \"plasma\", \"coolwarm\", \"RdYlBu\"]\nfor ax, cmap in zip(axes.flat, cmaps):\n    im = ax.imshow(datos, cmap=cmap)\n    ax.set_title(cmap)\n    plt.colorbar(im, ax=ax)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#6-graficos-3d","title":"6. Gr\u00e1ficos 3D","text":"<pre><code>from mpl_toolkits.mplot3d import Axes3D\n\n# Superficie 3D\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\n\nx = np.linspace(-5, 5, 50)\ny = np.linspace(-5, 5, 50)\nX, Y = np.meshgrid(x, y)\nZ = np.sin(np.sqrt(X**2 + Y**2))\n\nax.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.8)\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_zlabel(\"Z\")\nax.set_title(\"Superficie 3D\")\nplt.show()\n\n# Scatter 3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\nx = np.random.rand(100)\ny = np.random.rand(100)\nz = np.random.rand(100)\nc = np.random.rand(100)\n\nax.scatter(x, y, z, c=c, cmap=\"plasma\", s=50)\nplt.show()\n\n# L\u00ednea 3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\nt = np.linspace(0, 10*np.pi, 1000)\nx = np.sin(t)\ny = np.cos(t)\nz = t\n\nax.plot(x, y, z)\nax.set_title(\"H\u00e9lice 3D\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#7-interfaz-orientada-a-objetos-vs-pyplot","title":"7. Interfaz Orientada a Objetos vs pyplot","text":""},{"location":"conocimientos-basicos/15-matplotlib/#estilo-pyplot-rapido","title":"Estilo pyplot (r\u00e1pido)","text":"<pre><code>plt.plot([1, 2, 3], [1, 4, 9])\nplt.title(\"T\u00edtulo\")\nplt.xlabel(\"X\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#estilo-oo-mas-control","title":"Estilo OO (m\u00e1s control)","text":"<pre><code>fig, ax = plt.subplots()\nax.plot([1, 2, 3], [1, 4, 9])\nax.set_title(\"T\u00edtulo\")\nax.set_xlabel(\"X\")\nplt.show()\n</code></pre> <p>Recomendaci\u00f3n: Usar OO para gr\u00e1ficos complejos y m\u00faltiples subplots.</p>"},{"location":"conocimientos-basicos/15-matplotlib/#8-ejemplo-completo-dashboard","title":"8. Ejemplo Completo: Dashboard","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Datos\nnp.random.seed(42)\nmeses = [\"Ene\", \"Feb\", \"Mar\", \"Abr\", \"May\", \"Jun\"]\nventas = [120, 135, 145, 160, 180, 175]\ngastos = [80, 85, 90, 95, 100, 105]\nbeneficio = np.array(ventas) - np.array(gastos)\nproductos = [\"A\", \"B\", \"C\", \"D\"]\nventas_prod = [35, 25, 22, 18]\n\n# Crear figura\nfig = plt.figure(figsize=(14, 10))\nfig.suptitle(\"Dashboard de Ventas 2024\", fontsize=16, fontweight=\"bold\")\n\n# 1. Gr\u00e1fico de l\u00edneas - Evoluci\u00f3n\nax1 = fig.add_subplot(2, 2, 1)\nax1.plot(meses, ventas, \"b-o\", label=\"Ventas\", linewidth=2)\nax1.plot(meses, gastos, \"r--s\", label=\"Gastos\", linewidth=2)\nax1.fill_between(meses, ventas, gastos, alpha=0.2, color=\"green\")\nax1.set_title(\"Evoluci\u00f3n Ventas vs Gastos\")\nax1.set_ylabel(\"Miles \u20ac\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Gr\u00e1fico de barras - Beneficio\nax2 = fig.add_subplot(2, 2, 2)\ncolores = [\"green\" if b &gt; 0 else \"red\" for b in beneficio]\nax2.bar(meses, beneficio, color=colores, edgecolor=\"black\")\nax2.axhline(y=0, color=\"black\", linewidth=0.5)\nax2.set_title(\"Beneficio Mensual\")\nax2.set_ylabel(\"Miles \u20ac\")\nfor i, v in enumerate(beneficio):\n    ax2.text(i, v + 1, f\"{v}\", ha=\"center\", fontsize=9)\n\n# 3. Gr\u00e1fico de pastel - Productos\nax3 = fig.add_subplot(2, 2, 3)\nexplode = (0.05, 0, 0, 0)\nax3.pie(ventas_prod, labels=productos, autopct=\"%1.1f%%\",\n        explode=explode, colors=plt.cm.Pastel1.colors[:4],\n        shadow=True, startangle=90)\nax3.set_title(\"Distribuci\u00f3n por Producto\")\n\n# 4. Histograma - Distribuci\u00f3n de transacciones\nax4 = fig.add_subplot(2, 2, 4)\ntransacciones = np.random.exponential(50, 500)\nax4.hist(transacciones, bins=30, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\nax4.axvline(np.mean(transacciones), color=\"red\", linestyle=\"--\",\n            label=f\"Media: {np.mean(transacciones):.1f}\u20ac\")\nax4.set_title(\"Distribuci\u00f3n de Transacciones\")\nax4.set_xlabel(\"Valor (\u20ac)\")\nax4.set_ylabel(\"Frecuencia\")\nax4.legend()\n\nplt.tight_layout()\nplt.savefig(\"dashboard.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#9-animaciones-introduccion","title":"9. Animaciones (Introducci\u00f3n)","text":"<pre><code>import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\nx = np.linspace(0, 2*np.pi, 100)\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x + i/10))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, frames=100, interval=50, blit=True)\nplt.show()\n\n# Guardar como gif (requiere pillow)\n# ani.save(\"animacion.gif\", writer=\"pillow\", fps=30)\n</code></pre>"},{"location":"conocimientos-basicos/15-matplotlib/#10-resumen-de-funciones","title":"10. Resumen de Funciones","text":"Funci\u00f3n Descripci\u00f3n <code>plt.plot()</code> Gr\u00e1fico de l\u00edneas <code>plt.scatter()</code> Dispersi\u00f3n <code>plt.bar()</code>, <code>plt.barh()</code> Barras <code>plt.hist()</code> Histograma <code>plt.pie()</code> Pastel <code>plt.boxplot()</code> Diagrama de caja <code>plt.imshow()</code> Mapa de calor <code>plt.subplots()</code> M\u00faltiples gr\u00e1ficos <code>plt.title()</code>, <code>plt.xlabel()</code> T\u00edtulos <code>plt.legend()</code> Leyenda <code>plt.savefig()</code> Guardar <code>plt.show()</code> Mostrar <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/16-seaborn/","title":"\ud83d\udcda Seaborn - Visualizaci\u00f3n Estad\u00edstica","text":"<p>Seaborn es una librer\u00eda de visualizaci\u00f3n basada en Matplotlib que proporciona una interfaz de alto nivel para crear gr\u00e1ficos estad\u00edsticos atractivos y informativos.</p>"},{"location":"conocimientos-basicos/16-seaborn/#1-instalacion-e-importacion","title":"1. Instalaci\u00f3n e Importaci\u00f3n","text":"<pre><code># Instalaci\u00f3n\n# pip install seaborn\n\n# Importaci\u00f3n (convenci\u00f3n est\u00e1ndar)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#2-configuracion-inicial","title":"2. Configuraci\u00f3n Inicial","text":"<pre><code># Establecer estilo\nsns.set_theme()  # Estilo por defecto de Seaborn\n\n# Estilos disponibles\nsns.set_style(\"whitegrid\")   # Fondo blanco con grid\nsns.set_style(\"darkgrid\")    # Fondo gris con grid\nsns.set_style(\"white\")       # Fondo blanco sin grid\nsns.set_style(\"dark\")        # Fondo gris sin grid\nsns.set_style(\"ticks\")       # Ejes con marcas\n\n# Contexto (escala de elementos)\nsns.set_context(\"paper\")     # M\u00e1s peque\u00f1o\nsns.set_context(\"notebook\")  # Por defecto\nsns.set_context(\"talk\")      # Para presentaciones\nsns.set_context(\"poster\")    # M\u00e1s grande\n\n# Paleta de colores\nsns.set_palette(\"deep\")      # Por defecto\nsns.set_palette(\"pastel\")\nsns.set_palette(\"Set2\")\nsns.set_palette(\"husl\")\n\n# Configuraci\u00f3n completa\nsns.set_theme(style=\"whitegrid\", palette=\"deep\", context=\"notebook\")\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#3-datasets-de-ejemplo","title":"3. Datasets de Ejemplo","text":"<p>Seaborn incluye datasets para practicar:</p> <pre><code># Ver datasets disponibles\nprint(sns.get_dataset_names())\n\n# Cargar datasets\ntips = sns.load_dataset(\"tips\")       # Propinas en restaurante\niris = sns.load_dataset(\"iris\")       # Flores iris\ntitanic = sns.load_dataset(\"titanic\") # Pasajeros del Titanic\npenguins = sns.load_dataset(\"penguins\")  # Ping\u00fcinos\nflights = sns.load_dataset(\"flights\")    # Vuelos mensuales\n\nprint(tips.head())\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#4-graficos-de-distribucion","title":"4. Gr\u00e1ficos de Distribuci\u00f3n","text":""},{"location":"conocimientos-basicos/16-seaborn/#histograma-histplot","title":"Histograma (histplot)","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# Histograma b\u00e1sico\nsns.histplot(data=tips, x=\"total_bill\")\nplt.title(\"Distribuci\u00f3n de Cuentas\")\nplt.show()\n\n# Con KDE (densidad)\nsns.histplot(data=tips, x=\"total_bill\", kde=True)\nplt.show()\n\n# Por categor\u00eda\nsns.histplot(data=tips, x=\"total_bill\", hue=\"time\", kde=True)\nplt.show()\n\n# M\u00faltiples histogramas\nsns.histplot(data=tips, x=\"total_bill\", hue=\"day\", multiple=\"stack\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#kde-plot-densidad","title":"KDE Plot (Densidad)","text":"<pre><code># Densidad simple\nsns.kdeplot(data=tips, x=\"total_bill\")\nplt.show()\n\n# Por grupos\nsns.kdeplot(data=tips, x=\"total_bill\", hue=\"time\", fill=True, alpha=0.5)\nplt.show()\n\n# 2D\nsns.kdeplot(data=tips, x=\"total_bill\", y=\"tip\", fill=True, cmap=\"Blues\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#rug-plot","title":"Rug Plot","text":"<pre><code># Muestra puntos individuales en el eje\nsns.kdeplot(data=tips, x=\"total_bill\")\nsns.rugplot(data=tips, x=\"total_bill\", height=0.05)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#ecdf-plot-funcion-de-distribucion-acumulada","title":"ECDF Plot (Funci\u00f3n de Distribuci\u00f3n Acumulada)","text":"<pre><code>sns.ecdfplot(data=tips, x=\"total_bill\", hue=\"time\")\nplt.title(\"ECDF de Cuentas\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#displot-figura-completa","title":"Displot (Figura completa)","text":"<pre><code># Crea figura con facetas\nsns.displot(data=tips, x=\"total_bill\", col=\"time\", row=\"smoker\", kde=True)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#5-graficos-categoricos","title":"5. Gr\u00e1ficos Categ\u00f3ricos","text":""},{"location":"conocimientos-basicos/16-seaborn/#strip-plot-y-swarm-plot","title":"Strip Plot y Swarm Plot","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# Strip plot (puntos dispersos)\nsns.stripplot(data=tips, x=\"day\", y=\"total_bill\")\nplt.title(\"Strip Plot\")\nplt.show()\n\n# Swarm plot (puntos sin solapamiento)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\")\nplt.title(\"Swarm Plot\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#box-plot","title":"Box Plot","text":"<pre><code># B\u00e1sico\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\")\nplt.title(\"Box Plot\")\nplt.show()\n\n# Con hue\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\")\nplt.show()\n\n# Personalizado\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\",\n            palette=\"Set3\", linewidth=1.5, fliersize=3)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#violin-plot","title":"Violin Plot","text":"<pre><code># B\u00e1sico (muestra distribuci\u00f3n)\nsns.violinplot(data=tips, x=\"day\", y=\"total_bill\")\nplt.title(\"Violin Plot\")\nplt.show()\n\n# Split por categor\u00eda\nsns.violinplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", split=True)\nplt.show()\n\n# Con puntos internos\nsns.violinplot(data=tips, x=\"day\", y=\"total_bill\", inner=\"points\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#bar-plot-con-intervalos-de-confianza","title":"Bar Plot (con intervalos de confianza)","text":"<pre><code># Muestra media con intervalo de confianza\nsns.barplot(data=tips, x=\"day\", y=\"total_bill\")\nplt.title(\"Media de Cuenta por D\u00eda\")\nplt.show()\n\n# Por grupos\nsns.barplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\")\nplt.show()\n\n# Sin intervalos\nsns.barplot(data=tips, x=\"day\", y=\"total_bill\", errorbar=None)\nplt.show()\n\n# Con otra estad\u00edstica\nsns.barplot(data=tips, x=\"day\", y=\"total_bill\", estimator=np.median)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#count-plot","title":"Count Plot","text":"<pre><code># Cuenta frecuencias\nsns.countplot(data=tips, x=\"day\")\nplt.title(\"Frecuencia por D\u00eda\")\nplt.show()\n\n# Por grupos\nsns.countplot(data=tips, x=\"day\", hue=\"sex\")\nplt.show()\n\n# Horizontal\nsns.countplot(data=tips, y=\"day\", hue=\"smoker\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#point-plot","title":"Point Plot","text":"<pre><code># Muestra medias con l\u00edneas conectadas\nsns.pointplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\")\nplt.title(\"Point Plot\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#catplot-figura-completa-categorica","title":"Catplot (Figura completa categ\u00f3rica)","text":"<pre><code># Grid de gr\u00e1ficos categ\u00f3ricos\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", col=\"time\",\n            kind=\"box\", height=4, aspect=1)\nplt.show()\n\n# kind puede ser: 'strip', 'swarm', 'box', 'violin', 'bar', 'count', 'point'\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#6-graficos-de-relacion","title":"6. Gr\u00e1ficos de Relaci\u00f3n","text":""},{"location":"conocimientos-basicos/16-seaborn/#scatter-plot","title":"Scatter Plot","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# B\u00e1sico\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\")\nplt.title(\"Cuenta vs Propina\")\nplt.show()\n\n# Con hue (color por categor\u00eda)\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"day\")\nplt.show()\n\n# Con style (forma por categor\u00eda)\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\",\n                hue=\"day\", style=\"time\", size=\"size\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#line-plot","title":"Line Plot","text":"<pre><code>flights = sns.load_dataset(\"flights\")\n\n# L\u00ednea con intervalo de confianza\nsns.lineplot(data=flights, x=\"year\", y=\"passengers\")\nplt.title(\"Pasajeros por A\u00f1o\")\nplt.show()\n\n# Por grupos\nsns.lineplot(data=flights, x=\"year\", y=\"passengers\", hue=\"month\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#relplot-figura-completa-de-relaciones","title":"Relplot (Figura completa de relaciones)","text":"<pre><code>sns.relplot(data=tips, x=\"total_bill\", y=\"tip\",\n            col=\"time\", hue=\"smoker\", style=\"smoker\",\n            kind=\"scatter\", height=4, aspect=1)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#7-graficos-de-regresion","title":"7. Gr\u00e1ficos de Regresi\u00f3n","text":""},{"location":"conocimientos-basicos/16-seaborn/#regplot","title":"Regplot","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# Regresi\u00f3n lineal\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\")\nplt.title(\"Regresi\u00f3n Lineal\")\nplt.show()\n\n# Sin intervalo de confianza\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", ci=None)\nplt.show()\n\n# Regresi\u00f3n polin\u00f3mica\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", order=2)\nplt.show()\n\n# Regresi\u00f3n lowess (no param\u00e9trica)\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", lowess=True)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#lmplot-figura-con-facetas","title":"Lmplot (Figura con facetas)","text":"<pre><code># Grid de regresiones\nsns.lmplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"smoker\")\nplt.show()\n\n# Con facetas\nsns.lmplot(data=tips, x=\"total_bill\", y=\"tip\", col=\"time\", row=\"smoker\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#residplot-residuos","title":"Residplot (Residuos)","text":"<pre><code># Muestra residuos de la regresi\u00f3n\nsns.residplot(data=tips, x=\"total_bill\", y=\"tip\")\nplt.title(\"Residuos\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#8-graficos-matriciales","title":"8. Gr\u00e1ficos Matriciales","text":""},{"location":"conocimientos-basicos/16-seaborn/#heatmap-mapa-de-calor","title":"Heatmap (Mapa de Calor)","text":"<pre><code># Matriz de correlaci\u00f3n\ntips = sns.load_dataset(\"tips\")\ncorr = tips[[\"total_bill\", \"tip\", \"size\"]].corr()\n\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0)\nplt.title(\"Matriz de Correlaci\u00f3n\")\nplt.show()\n\n# Personalizado\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"YlOrRd\",\n            linewidths=0.5, square=True,\n            cbar_kws={\"shrink\": 0.8})\nplt.show()\n\n# Para datos de vuelos\nflights = sns.load_dataset(\"flights\")\nflights_pivot = flights.pivot(index=\"month\", columns=\"year\", values=\"passengers\")\n\nsns.heatmap(flights_pivot, cmap=\"YlGnBu\", annot=True, fmt=\"d\")\nplt.title(\"Pasajeros por Mes y A\u00f1o\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#clustermap","title":"Clustermap","text":"<pre><code># Mapa de calor con clustering jer\u00e1rquico\nflights_pivot = flights.pivot(index=\"month\", columns=\"year\", values=\"passengers\")\n\nsns.clustermap(flights_pivot, cmap=\"viridis\", standard_scale=1)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#9-pair-plot-matriz-de-dispersion","title":"9. Pair Plot (Matriz de Dispersi\u00f3n)","text":"<pre><code>iris = sns.load_dataset(\"iris\")\n\n# B\u00e1sico\nsns.pairplot(iris)\nplt.show()\n\n# Por especies\nsns.pairplot(iris, hue=\"species\")\nplt.show()\n\n# Personalizado\nsns.pairplot(iris, hue=\"species\",\n             diag_kind=\"kde\",      # En diagonal: 'hist' o 'kde'\n             markers=[\"o\", \"s\", \"D\"],\n             palette=\"Set2\",\n             corner=True)          # Solo tri\u00e1ngulo inferior\nplt.show()\n\n# Seleccionar variables\nsns.pairplot(iris, vars=[\"sepal_length\", \"sepal_width\", \"petal_length\"],\n             hue=\"species\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#10-joint-plot","title":"10. Joint Plot","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# B\u00e1sico (scatter + histogramas)\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\")\nplt.show()\n\n# Con regresi\u00f3n\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"reg\")\nplt.show()\n\n# Con densidad\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"kde\", fill=True)\nplt.show()\n\n# Hex\u00e1gonos\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", kind=\"hex\")\nplt.show()\n\n# Con hue\nsns.jointplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"time\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#11-facetgrid-grillas-de-graficos","title":"11. FacetGrid (Grillas de Gr\u00e1ficos)","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# Crear grid\ng = sns.FacetGrid(tips, col=\"time\", row=\"smoker\", height=4)\ng.map(sns.scatterplot, \"total_bill\", \"tip\")\ng.add_legend()\nplt.show()\n\n# Con m\u00e1s personalizaci\u00f3n\ng = sns.FacetGrid(tips, col=\"day\", col_wrap=2, height=4)\ng.map(sns.histplot, \"total_bill\", kde=True)\ng.set_titles(\"{col_name}\")\nplt.show()\n\n# Con hue\ng = sns.FacetGrid(tips, col=\"time\", hue=\"smoker\")\ng.map(sns.scatterplot, \"total_bill\", \"tip\", alpha=0.7)\ng.add_legend()\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#12-paletas-de-colores","title":"12. Paletas de Colores","text":"<pre><code># Ver paletas disponibles\n# Cualitativas: deep, muted, pastel, bright, dark, colorblind\n# Secuenciales: Blues, Greens, Reds, viridis, rocket, mako\n# Divergentes: coolwarm, RdBu, seismic\n\n# Mostrar paleta\nsns.palplot(sns.color_palette(\"deep\"))\nplt.show()\n\n# Crear paleta personalizada\ncustom = sns.color_palette([\"#ff0000\", \"#00ff00\", \"#0000ff\"])\n\n# Usar paleta\nsns.barplot(data=tips, x=\"day\", y=\"total_bill\", palette=\"Set2\")\nplt.show()\n\n# Paleta para datos cuantitativos\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\",\n                hue=\"size\", palette=\"viridis\")\nplt.show()\n\n# Color continuo\nsns.kdeplot(data=tips, x=\"total_bill\", y=\"tip\",\n            fill=True, cmap=\"YlOrRd\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#13-personalizacion-con-matplotlib","title":"13. Personalizaci\u00f3n con Matplotlib","text":"<pre><code>tips = sns.load_dataset(\"tips\")\n\n# Seaborn devuelve objetos de Matplotlib\nfig, ax = plt.subplots(figsize=(10, 6))\n\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\", ax=ax)\n\n# Personalizar con Matplotlib\nax.set_title(\"Distribuci\u00f3n de Cuentas por D\u00eda\", fontsize=14, fontweight=\"bold\")\nax.set_xlabel(\"D\u00eda de la Semana\", fontsize=12)\nax.set_ylabel(\"Total de la Cuenta ($)\", fontsize=12)\nax.tick_params(axis=\"both\", labelsize=10)\n\nplt.tight_layout()\nplt.savefig(\"grafico_personalizado.png\", dpi=150)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#14-ejemplo-completo-analisis-exploratorio","title":"14. Ejemplo Completo: An\u00e1lisis Exploratorio","text":"<pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Cargar datos\ntips = sns.load_dataset(\"tips\")\n\n# Configurar estilo\nsns.set_theme(style=\"whitegrid\", palette=\"deep\")\n\n# Crear figura con m\u00faltiples gr\u00e1ficos\nfig = plt.figure(figsize=(16, 12))\nfig.suptitle(\"An\u00e1lisis Exploratorio - Dataset Tips\", fontsize=16, fontweight=\"bold\")\n\n# 1. Distribuci\u00f3n de cuentas\nax1 = fig.add_subplot(2, 3, 1)\nsns.histplot(data=tips, x=\"total_bill\", kde=True, ax=ax1)\nax1.set_title(\"Distribuci\u00f3n de Cuentas\")\n\n# 2. Relaci\u00f3n cuenta-propina\nax2 = fig.add_subplot(2, 3, 2)\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"time\", ax=ax2)\nax2.set_title(\"Cuenta vs Propina\")\n\n# 3. Box plot por d\u00eda\nax3 = fig.add_subplot(2, 3, 3)\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\", palette=\"Set2\", ax=ax3)\nax3.set_title(\"Cuenta por D\u00eda\")\n\n# 4. Conteo por d\u00eda y momento\nax4 = fig.add_subplot(2, 3, 4)\nsns.countplot(data=tips, x=\"day\", hue=\"time\", ax=ax4)\nax4.set_title(\"Frecuencia por D\u00eda y Momento\")\nax4.legend(title=\"Momento\")\n\n# 5. Violin plot fumadores\nax5 = fig.add_subplot(2, 3, 5)\nsns.violinplot(data=tips, x=\"smoker\", y=\"tip\", hue=\"sex\", split=True, ax=ax5)\nax5.set_title(\"Propinas: Fumadores vs No Fumadores\")\n\n# 6. Regresi\u00f3n\nax6 = fig.add_subplot(2, 3, 6)\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", ax=ax6, scatter_kws={\"alpha\": 0.5})\nax6.set_title(\"Regresi\u00f3n Lineal\")\n\nplt.tight_layout()\nplt.savefig(\"analisis_tips.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n# Pair plot separado\nsns.pairplot(tips, hue=\"time\", diag_kind=\"kde\", corner=True)\nplt.savefig(\"pairplot_tips.png\", dpi=150)\nplt.show()\n\n# Heatmap de correlaci\u00f3n\nplt.figure(figsize=(8, 6))\nnumeric_tips = tips.select_dtypes(include=[np.number])\nsns.heatmap(numeric_tips.corr(), annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\nplt.title(\"Matriz de Correlaci\u00f3n\")\nplt.tight_layout()\nplt.savefig(\"correlacion_tips.png\", dpi=150)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/16-seaborn/#15-resumen-de-funciones","title":"15. Resumen de Funciones","text":"Funci\u00f3n Tipo Descripci\u00f3n <code>sns.histplot()</code> Distribuci\u00f3n Histograma <code>sns.kdeplot()</code> Distribuci\u00f3n Densidad <code>sns.boxplot()</code> Categ\u00f3rico Diagrama de caja <code>sns.violinplot()</code> Categ\u00f3rico Diagrama de viol\u00edn <code>sns.barplot()</code> Categ\u00f3rico Barras con media <code>sns.countplot()</code> Categ\u00f3rico Conteo <code>sns.scatterplot()</code> Relaci\u00f3n Dispersi\u00f3n <code>sns.lineplot()</code> Relaci\u00f3n L\u00edneas <code>sns.regplot()</code> Regresi\u00f3n Con l\u00ednea de regresi\u00f3n <code>sns.heatmap()</code> Matricial Mapa de calor <code>sns.pairplot()</code> Multivariable Matriz de dispersi\u00f3n <code>sns.jointplot()</code> Bivariable Scatter + distribuciones <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"conocimientos-basicos/17-scikit-learn/","title":"\ud83e\udd16 Scikit-learn - Introducci\u00f3n al Machine Learning","text":"<p>Scikit-learn es la librer\u00eda m\u00e1s popular de Python para Machine Learning. Proporciona herramientas simples y eficientes para an\u00e1lisis predictivo y modelado estad\u00edstico.</p>"},{"location":"conocimientos-basicos/17-scikit-learn/#1-instalacion-e-importacion","title":"1. Instalaci\u00f3n e Importaci\u00f3n","text":"<pre><code># Instalaci\u00f3n\n# pip install scikit-learn\n\n# Importaci\u00f3n (convenci\u00f3n est\u00e1ndar)\nimport sklearn\n\n# Importaciones espec\u00edficas comunes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#2-conceptos-fundamentales","title":"2. Conceptos Fundamentales","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#tipos-de-aprendizaje","title":"Tipos de Aprendizaje","text":"Tipo Descripci\u00f3n Ejemplos Supervisado Datos etiquetados (con respuesta) Clasificaci\u00f3n, Regresi\u00f3n No supervisado Sin etiquetas Clustering, Reducci\u00f3n dimensionalidad"},{"location":"conocimientos-basicos/17-scikit-learn/#terminos-importantes","title":"T\u00e9rminos Importantes","text":"<pre><code># X = Features (caracter\u00edsticas/variables independientes)\n# y = Target (objetivo/variable dependiente)\n# Modelo = Algoritmo que aprende patrones de los datos\n# Entrenamiento = Proceso de ajuste del modelo\n# Predicci\u00f3n = Uso del modelo para nuevos datos\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#3-flujo-de-trabajo-basico","title":"3. Flujo de Trabajo B\u00e1sico","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# 1. CARGAR DATOS\n# X = caracter\u00edsticas, y = objetivo\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# 2. DIVIDIR DATOS (entrenamiento y prueba)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 3. PREPROCESAR (opcional pero recomendado)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 4. CREAR Y ENTRENAR MODELO\nmodelo = LogisticRegression()\nmodelo.fit(X_train_scaled, y_train)\n\n# 5. HACER PREDICCIONES\ny_pred = modelo.predict(X_test_scaled)\n\n# 6. EVALUAR\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Precisi\u00f3n: {accuracy:.2%}\")  # Precisi\u00f3n: 100.00%\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#4-datasets-de-ejemplo","title":"4. Datasets de Ejemplo","text":"<pre><code>from sklearn import datasets\n\n# Clasificaci\u00f3n\niris = datasets.load_iris()           # 3 clases de flores\ndigits = datasets.load_digits()       # D\u00edgitos escritos\nwine = datasets.load_wine()           # Tipos de vino\nbreast_cancer = datasets.load_breast_cancer()  # C\u00e1ncer de mama\n\n# Regresi\u00f3n\nboston = datasets.load_boston()       # Precios de casas (deprecated)\ndiabetes = datasets.load_diabetes()   # Progresi\u00f3n diabetes\n\n# Informaci\u00f3n del dataset\nprint(iris.keys())\n# dict_keys(['data', 'target', 'frame', 'target_names', \n#            'DESCR', 'feature_names', 'filename'])\n\nprint(iris.DESCR[:500])  # Descripci\u00f3n\nprint(iris.feature_names)  # Nombres de caracter\u00edsticas\nprint(iris.target_names)   # Nombres de clases\n\n# Crear DataFrame para explorar\nimport pandas as pd\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['target'] = iris.target\nprint(df.head())\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#generar-datos-sinteticos","title":"Generar Datos Sint\u00e9ticos","text":"<pre><code>from sklearn.datasets import make_classification, make_regression, make_blobs\n\n# Para clasificaci\u00f3n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=20,\n    n_informative=10,\n    n_classes=2,\n    random_state=42\n)\n\n# Para regresi\u00f3n\nX, y = make_regression(\n    n_samples=1000,\n    n_features=10,\n    noise=10,\n    random_state=42\n)\n\n# Para clustering\nX, y = make_blobs(\n    n_samples=300,\n    centers=3,\n    cluster_std=1.0,\n    random_state=42\n)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#5-division-de-datos","title":"5. Divisi\u00f3n de Datos","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#train_test_split","title":"train_test_split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nX = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\ny = [0, 0, 1, 1, 1]\n\n# Divisi\u00f3n b\u00e1sica (80% entrenamiento, 20% prueba)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Entrenamiento: {len(X_train)} muestras\")\nprint(f\"Prueba: {len(X_test)} muestras\")\n\n# Par\u00e1metros importantes:\n# test_size: proporci\u00f3n para test (0.2 = 20%)\n# random_state: semilla para reproducibilidad\n# stratify: mantener proporci\u00f3n de clases\n# shuffle: mezclar datos antes de dividir\n\n# Con estratificaci\u00f3n (importante para clasificaci\u00f3n desbalanceada)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#division-en-3-partes","title":"Divisi\u00f3n en 3 partes","text":"<pre><code>from sklearn.model_selection import train_test_split\n\n# Primero separar test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Luego separar validaci\u00f3n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 de 0.8 = 0.2\n)\n\n# Resultado: 60% train, 20% val, 20% test\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#6-preprocesamiento-de-datos","title":"6. Preprocesamiento de Datos","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#escalado-de-features","title":"Escalado de Features","text":"<pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nimport numpy as np\n\nX = np.array([[1, 10, 100],\n              [2, 20, 200],\n              [3, 30, 300]])\n\n# StandardScaler: media=0, desviaci\u00f3n=1 (m\u00e1s com\u00fan)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(\"StandardScaler:\")\nprint(X_scaled)\n\n# MinMaxScaler: valores entre 0 y 1\nscaler_mm = MinMaxScaler()\nX_minmax = scaler_mm.fit_transform(X)\nprint(\"\\nMinMaxScaler:\")\nprint(X_minmax)\n\n# RobustScaler: resistente a outliers\nscaler_robust = RobustScaler()\nX_robust = scaler_robust.fit_transform(X)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#importante-fit-vs-transform","title":"\u26a0\ufe0f Importante: fit vs transform","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\n# CORRECTO: fit solo en train\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # fit + transform\nX_test_scaled = scaler.transform(X_test)        # solo transform\n\n# INCORRECTO: NO hacer fit en test\n# X_test_scaled = scaler.fit_transform(X_test)  # \u274c Fuga de datos\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#codificacion-de-variables-categoricas","title":"Codificaci\u00f3n de Variables Categ\u00f3ricas","text":"<pre><code>from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport numpy as np\n\n# LabelEncoder: convierte categor\u00edas a n\u00fameros\nle = LabelEncoder()\ncolores = [\"rojo\", \"verde\", \"azul\", \"rojo\", \"azul\"]\ncolores_encoded = le.fit_transform(colores)\nprint(colores_encoded)  # [2 1 0 2 0]\n\n# Inverso\nprint(le.inverse_transform([0, 1, 2]))  # ['azul' 'verde' 'rojo']\n\n# OneHotEncoder: crea columnas binarias\nohe = OneHotEncoder(sparse_output=False)\ncolores_array = np.array(colores).reshape(-1, 1)\ncolores_onehot = ohe.fit_transform(colores_array)\nprint(colores_onehot)\n# [[0. 0. 1.]   rojo\n#  [0. 1. 0.]   verde\n#  [1. 0. 0.]   azul\n#  [0. 0. 1.]   rojo\n#  [1. 0. 0.]]  azul\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#manejo-de-valores-faltantes","title":"Manejo de Valores Faltantes","text":"<pre><code>from sklearn.impute import SimpleImputer\nimport numpy as np\n\nX = np.array([[1, 2, np.nan],\n              [3, np.nan, 6],\n              [7, 8, 9],\n              [np.nan, 5, 3]])\n\n# Imputar con la media\nimputer = SimpleImputer(strategy=\"mean\")\nX_imputed = imputer.fit_transform(X)\nprint(X_imputed)\n\n# Estrategias disponibles:\n# \"mean\": media (num\u00e9ricos)\n# \"median\": mediana (num\u00e9ricos)\n# \"most_frequent\": moda (categ\u00f3ricos)\n# \"constant\": valor fijo (fill_value=0)\n\n# Con valor constante\nimputer_const = SimpleImputer(strategy=\"constant\", fill_value=0)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#7-modelos-de-clasificacion","title":"7. Modelos de Clasificaci\u00f3n","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#regresion-logistica","title":"Regresi\u00f3n Log\u00edstica","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Cargar datos\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, test_size=0.2, random_state=42\n)\n\n# Entrenar modelo\nmodelo = LogisticRegression(max_iter=200)\nmodelo.fit(X_train, y_train)\n\n# Predecir\ny_pred = modelo.predict(X_test)\n\n# Probabilidades\ny_proba = modelo.predict_proba(X_test)\nprint(\"Probabilidades:\", y_proba[0])\n\n# Evaluar\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#k-nearest-neighbors-knn","title":"K-Nearest Neighbors (KNN)","text":"<pre><code>from sklearn.neighbors import KNeighborsClassifier\n\n# Crear modelo\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Entrenar\nknn.fit(X_train, y_train)\n\n# Predecir\ny_pred = knn.predict(X_test)\nprint(f\"Accuracy KNN: {accuracy_score(y_test, y_pred):.2%}\")\n\n# Probar diferentes valores de k\nfor k in [1, 3, 5, 7, 9]:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    score = knn.score(X_test, y_test)\n    print(f\"k={k}: {score:.2%}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#arbol-de-decision","title":"\u00c1rbol de Decisi\u00f3n","text":"<pre><code>from sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# Crear modelo\ntree = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree.fit(X_train, y_train)\n\n# Predecir\ny_pred = tree.predict(X_test)\nprint(f\"Accuracy Tree: {accuracy_score(y_test, y_pred):.2%}\")\n\n# Visualizar \u00e1rbol\nplt.figure(figsize=(20, 10))\nplot_tree(tree, feature_names=iris.feature_names,\n          class_names=iris.target_names, filled=True)\nplt.show()\n\n# Importancia de features\nimportances = tree.feature_importances_\nfor name, importance in zip(iris.feature_names, importances):\n    print(f\"{name}: {importance:.4f}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#random-forest","title":"Random Forest","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Crear modelo\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predecir\ny_pred = rf.predict(X_test)\nprint(f\"Accuracy RF: {accuracy_score(y_test, y_pred):.2%}\")\n\n# Importancia de features\nimportances = rf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(importances)), importances[indices])\nplt.xticks(range(len(importances)), \n           [iris.feature_names[i] for i in indices], rotation=45)\nplt.title(\"Importancia de Features\")\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<pre><code>from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\n# SVM requiere escalado\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Crear modelo\nsvm = SVC(kernel=\"rbf\", C=1.0, random_state=42)\nsvm.fit(X_train_scaled, y_train)\n\n# Predecir\ny_pred = svm.predict(X_test_scaled)\nprint(f\"Accuracy SVM: {accuracy_score(y_test, y_pred):.2%}\")\n\n# Con probabilidades\nsvm_proba = SVC(kernel=\"rbf\", probability=True)\nsvm_proba.fit(X_train_scaled, y_train)\ny_proba = svm_proba.predict_proba(X_test_scaled)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#8-modelos-de-regresion","title":"8. Modelos de Regresi\u00f3n","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#regresion-lineal","title":"Regresi\u00f3n Lineal","text":"<pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Cargar datos\ndiabetes = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(\n    diabetes.data, diabetes.target, test_size=0.2, random_state=42\n)\n\n# Entrenar modelo\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Predecir\ny_pred = lr.predict(X_test)\n\n# Evaluar\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"R\u00b2: {r2:.4f}\")\n\n# Coeficientes\nprint(\"\\nCoeficientes:\")\nfor name, coef in zip(diabetes.feature_names, lr.coef_):\n    print(f\"{name}: {coef:.4f}\")\nprint(f\"Intercepto: {lr.intercept_:.4f}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#ridge-y-lasso-regularizacion","title":"Ridge y Lasso (Regularizaci\u00f3n)","text":"<pre><code>from sklearn.linear_model import Ridge, Lasso\n\n# Ridge (regularizaci\u00f3n L2)\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\nprint(f\"R\u00b2 Ridge: {ridge.score(X_test, y_test):.4f}\")\n\n# Lasso (regularizaci\u00f3n L1)\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\nprint(f\"R\u00b2 Lasso: {lasso.score(X_test, y_test):.4f}\")\n\n# Lasso hace selecci\u00f3n de features (algunos coeficientes = 0)\nprint(f\"Features usadas: {np.sum(lasso.coef_ != 0)}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#random-forest-regressor","title":"Random Forest Regressor","text":"<pre><code>from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100, random_state=42)\nrfr.fit(X_train, y_train)\n\ny_pred = rfr.predict(X_test)\nprint(f\"R\u00b2 RF: {rfr.score(X_test, y_test):.4f}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#9-metricas-de-evaluacion","title":"9. M\u00e9tricas de Evaluaci\u00f3n","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#clasificacion","title":"Clasificaci\u00f3n","text":"<pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score\n)\n\n# Datos de ejemplo\ny_true = [0, 1, 1, 0, 1, 1, 0, 0, 1, 0]\ny_pred = [0, 1, 0, 0, 1, 1, 0, 1, 1, 0]\n\n# M\u00e9tricas b\u00e1sicas\nprint(f\"Accuracy: {accuracy_score(y_true, y_pred):.2%}\")\nprint(f\"Precision: {precision_score(y_true, y_pred):.2%}\")\nprint(f\"Recall: {recall_score(y_true, y_pred):.2%}\")\nprint(f\"F1-Score: {f1_score(y_true, y_pred):.2%}\")\n\n# Matriz de confusi\u00f3n\ncm = confusion_matrix(y_true, y_pred)\nprint(\"\\nMatriz de Confusi\u00f3n:\")\nprint(cm)\n#          Pred 0  Pred 1\n# Real 0    [TN      FP]\n# Real 1    [FN      TP]\n\n# Reporte completo\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred))\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#visualizar-matriz-de-confusion","title":"Visualizar Matriz de Confusi\u00f3n","text":"<pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(y_test, y_pred)\n\n# Opci\u00f3n 1: Seaborn\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=iris.target_names,\n            yticklabels=iris.target_names)\nplt.xlabel(\"Predicho\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confusi\u00f3n\")\nplt.show()\n\n# Opci\u00f3n 2: sklearn\nConfusionMatrixDisplay.from_predictions(y_test, y_pred,\n                                        display_labels=iris.target_names)\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#regresion","title":"Regresi\u00f3n","text":"<pre><code>from sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score,\n    mean_absolute_percentage_error\n)\nimport numpy as np\n\ny_true = [100, 150, 200, 250, 300]\ny_pred = [110, 140, 190, 260, 310]\n\n# M\u00e9tricas\nmse = mean_squared_error(y_true, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_true, y_pred)\nr2 = r2_score(y_true, y_pred)\nmape = mean_absolute_percentage_error(y_true, y_pred)\n\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"R\u00b2: {r2:.4f}\")\nprint(f\"MAPE: {mape:.2%}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#curva-roc-y-auc","title":"Curva ROC y AUC","text":"<pre><code>from sklearn.metrics import roc_curve, auc, RocCurveDisplay\nimport matplotlib.pyplot as plt\n\n# Necesitas probabilidades\ny_proba = modelo.predict_proba(X_test)[:, 1]  # Probabilidad clase positiva\n\n# Calcular ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# Graficar\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Curva ROC')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#10-validacion-cruzada","title":"10. Validaci\u00f3n Cruzada","text":"<pre><code>from sklearn.model_selection import cross_val_score, KFold\n\n# Validaci\u00f3n cruzada simple\nmodelo = LogisticRegression(max_iter=200)\nscores = cross_val_score(modelo, X, y, cv=5)  # 5 folds\n\nprint(f\"Scores: {scores}\")\nprint(f\"Media: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n\n# KFold personalizado\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(modelo, X, y, cv=kfold)\n\n# Stratified para clasificaci\u00f3n (mantiene proporciones)\nfrom sklearn.model_selection import StratifiedKFold\nskfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(modelo, X, y, cv=skfold)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#11-busqueda-de-hiperparametros","title":"11. B\u00fasqueda de Hiperpar\u00e1metros","text":""},{"location":"conocimientos-basicos/17-scikit-learn/#grid-search","title":"Grid Search","text":"<pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Definir par\u00e1metros a buscar\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 10, None],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Crear b\u00fasqueda\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,  # Usar todos los cores\n    verbose=1\n)\n\n# Ejecutar b\u00fasqueda\ngrid_search.fit(X_train, y_train)\n\n# Mejores par\u00e1metros\nprint(f\"Mejores par\u00e1metros: {grid_search.best_params_}\")\nprint(f\"Mejor score: {grid_search.best_score_:.4f}\")\n\n# Usar mejor modelo\nmejor_modelo = grid_search.best_estimator_\ny_pred = mejor_modelo.predict(X_test)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#random-search","title":"Random Search","text":"<pre><code>from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\n# Distribuciones de par\u00e1metros\nparam_distributions = {\n    'n_estimators': randint(50, 300),\n    'max_depth': randint(3, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10)\n}\n\n# B\u00fasqueda aleatoria\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_distributions,\n    n_iter=50,  # N\u00famero de combinaciones a probar\n    cv=5,\n    scoring='accuracy',\n    random_state=42,\n    n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)\nprint(f\"Mejores par\u00e1metros: {random_search.best_params_}\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#12-pipelines","title":"12. Pipelines","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n# Crear pipeline\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', SVC())\n])\n\n# Usar como modelo normal\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n\n# Grid Search con Pipeline\nparam_grid = {\n    'classifier__C': [0.1, 1, 10],\n    'classifier__kernel': ['rbf', 'linear']\n}\n\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#pipeline-con-multiples-pasos","title":"Pipeline con m\u00faltiples pasos","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Pipeline para diferentes tipos de columnas\nnumeric_features = [0, 1, 2, 3]  # \u00edndices de columnas num\u00e9ricas\ncategorical_features = [4, 5]     # \u00edndices de columnas categ\u00f3ricas\n\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer([\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\n# Pipeline completo\nclf = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n\n# Usar\nclf.fit(X_train, y_train)\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#13-guardar-y-cargar-modelos","title":"13. Guardar y Cargar Modelos","text":"<pre><code>import joblib\nimport pickle\n\n# Opci\u00f3n 1: joblib (recomendado para sklearn)\n# Guardar\njoblib.dump(modelo, 'modelo.joblib')\n\n# Cargar\nmodelo_cargado = joblib.load('modelo.joblib')\ny_pred = modelo_cargado.predict(X_test)\n\n# Opci\u00f3n 2: pickle\nwith open('modelo.pkl', 'wb') as f:\n    pickle.dump(modelo, f)\n\nwith open('modelo.pkl', 'rb') as f:\n    modelo_cargado = pickle.load(f)\n\n# Guardar pipeline completo\njoblib.dump(pipe, 'pipeline_completo.joblib')\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#14-ejemplo-completo-clasificacion","title":"14. Ejemplo Completo: Clasificaci\u00f3n","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\n# 1. CARGAR DATOS\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nprint(f\"Forma de X: {X.shape}\")\nprint(f\"Clases: {data.target_names}\")\n\n# 2. EXPLORAR DATOS\ndf = pd.DataFrame(X, columns=data.feature_names)\ndf['target'] = y\nprint(df.describe())\n\n# 3. DIVIDIR DATOS\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 4. ESCALAR\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 5. COMPARAR MODELOS\nmodelos = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(random_state=42)\n}\n\nresultados = {}\nfor nombre, modelo in modelos.items():\n    # Validaci\u00f3n cruzada\n    scores = cross_val_score(modelo, X_train_scaled, y_train, cv=5)\n\n    # Entrenar y evaluar\n    modelo.fit(X_train_scaled, y_train)\n    y_pred = modelo.predict(X_test_scaled)\n    acc = accuracy_score(y_test, y_pred)\n\n    resultados[nombre] = {\n        'cv_mean': scores.mean(),\n        'cv_std': scores.std(),\n        'test_acc': acc\n    }\n\n    print(f\"\\n{nombre}:\")\n    print(f\"  CV: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n    print(f\"  Test: {acc:.4f}\")\n\n# 6. EVALUAR MEJOR MODELO\nmejor = max(resultados, key=lambda x: resultados[x]['test_acc'])\nprint(f\"\\nMejor modelo: {mejor}\")\n\n# Reentrenar\nmejor_modelo = modelos[mejor]\ny_pred = mejor_modelo.predict(X_test_scaled)\n\n# Reporte\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=data.target_names))\n\n# Matriz de confusi\u00f3n\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=data.target_names, yticklabels=data.target_names)\nplt.xlabel('Predicho')\nplt.ylabel('Real')\nplt.title(f'Matriz de Confusi\u00f3n - {mejor}')\nplt.show()\n\n# 7. GUARDAR MODELO\nimport joblib\njoblib.dump(mejor_modelo, 'mejor_modelo_cancer.joblib')\njoblib.dump(scaler, 'scaler_cancer.joblib')\nprint(\"\\nModelo guardado!\")\n</code></pre>"},{"location":"conocimientos-basicos/17-scikit-learn/#15-resumen-de-modulos-principales","title":"15. Resumen de M\u00f3dulos Principales","text":"M\u00f3dulo Descripci\u00f3n <code>sklearn.model_selection</code> Divisi\u00f3n de datos, validaci\u00f3n cruzada, b\u00fasqueda de hiperpar\u00e1metros <code>sklearn.preprocessing</code> Escalado, codificaci\u00f3n, transformaciones <code>sklearn.impute</code> Manejo de valores faltantes <code>sklearn.linear_model</code> Modelos lineales (regresi\u00f3n, log\u00edstica) <code>sklearn.tree</code> \u00c1rboles de decisi\u00f3n <code>sklearn.ensemble</code> Random Forest, Gradient Boosting <code>sklearn.svm</code> Support Vector Machines <code>sklearn.neighbors</code> K-Nearest Neighbors <code>sklearn.cluster</code> Algoritmos de clustering <code>sklearn.metrics</code> M\u00e9tricas de evaluaci\u00f3n <code>sklearn.pipeline</code> Pipelines de preprocesamiento + modelo <code>sklearn.datasets</code> Datasets de ejemplo"},{"location":"conocimientos-basicos/17-scikit-learn/#16-buenas-practicas","title":"16. Buenas Pr\u00e1cticas","text":"<pre><code># \u2705 HACER:\n# 1. Siempre dividir datos ANTES de cualquier preprocesamiento\n# 2. fit() solo en datos de entrenamiento\n# 3. transform() en train y test con el mismo objeto\n# 4. Usar pipelines para evitar fugas de datos\n# 5. Establecer random_state para reproducibilidad\n# 6. Usar validaci\u00f3n cruzada para evaluar\n# 7. Escalar datos para algoritmos sensibles (SVM, KNN, redes)\n# 8. Estratificar en clasificaci\u00f3n desbalanceada\n\n# \u274c NO HACER:\n# 1. Escalar todo el dataset antes de dividir\n# 2. Seleccionar features bas\u00e1ndose en todo el dataset\n# 3. Ignorar el balanceo de clases\n# 4. Evaluar solo en training\n# 5. Olvidar guardar el scaler junto con el modelo\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/","title":"\ud83e\udde0 Deep Learning","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Deep Learning! \ud83c\udf89</p>"},{"location":"deep-learning/#que-es-el-deep-learning","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Deep Learning?","text":"<p>El Deep Learning (Aprendizaje Profundo) es un subcampo del Machine Learning que utiliza redes neuronales artificiales con m\u00faltiples capas (de ah\u00ed el t\u00e9rmino \"profundo\") para aprender representaciones jer\u00e1rquicas de los datos.</p> <p>A diferencia de los algoritmos tradicionales de ML que requieren ingenier\u00eda de caracter\u00edsticas manual, el Deep Learning puede aprender autom\u00e1ticamente las caracter\u00edsticas relevantes directamente de los datos en bruto.</p>"},{"location":"deep-learning/#arquitecturas-principales","title":"\ud83e\udde0 Arquitecturas Principales","text":"<ol> <li> <p>Redes Neuronales Artificiales (ANN):    La base del Deep Learning, inspirada en el cerebro humano.    \ud83d\udccd Uso: Clasificaci\u00f3n, regresi\u00f3n.</p> </li> <li> <p>Redes Neuronales Convolucionales (CNN):    Especializadas en procesar datos con estructura de cuadr\u00edcula (im\u00e1genes).    \ud83d\udccd Uso: Visi\u00f3n por computadora, clasificaci\u00f3n de im\u00e1genes, detecci\u00f3n de objetos.</p> </li> <li> <p>Redes Neuronales Recurrentes (RNN):    Dise\u00f1adas para datos secuenciales con memoria de estados anteriores.    \ud83d\udccd Uso: Series temporales, texto.</p> </li> <li> <p>LSTM y GRU:    Variantes de RNN que solucionan el problema del gradiente desvaneciente.    \ud83d\udccd Uso: Secuencias largas, traducci\u00f3n.</p> </li> <li> <p>Transformers:    Arquitectura basada en mecanismos de atenci\u00f3n, revolucion\u00f3 el NLP y m\u00e1s.    \ud83d\udccd Uso: GPT, BERT, modelos de lenguaje grandes (LLMs).</p> </li> <li> <p>Autoencoders:    Redes que aprenden representaciones comprimidas de los datos.    \ud83d\udccd Uso: Reducci\u00f3n de dimensionalidad, generaci\u00f3n.</p> </li> <li> <p>GANs (Redes Generativas Adversarias):    Dos redes que compiten para generar datos realistas.    \ud83d\udccd Uso: Generaci\u00f3n de im\u00e1genes, arte, deepfakes.</p> </li> </ol>"},{"location":"deep-learning/#conceptos-fundamentales","title":"\u2699\ufe0f Conceptos Fundamentales","text":"<ul> <li>Neurona artificial: Unidad b\u00e1sica que aplica pesos, bias y funci\u00f3n de activaci\u00f3n</li> <li>Capas: Entrada, ocultas y salida</li> <li>Funciones de activaci\u00f3n: ReLU, Sigmoid, Tanh, Softmax</li> <li>Backpropagation: Algoritmo para calcular gradientes</li> <li>Optimizadores: SGD, Adam, RMSprop</li> <li>Regularizaci\u00f3n: Dropout, L1/L2, Batch Normalization</li> <li>Transfer Learning: Reutilizar modelos preentrenados</li> </ul>"},{"location":"deep-learning/#frameworks-populares","title":"\ud83d\udd0d Frameworks Populares","text":"<ul> <li>TensorFlow - Framework de Google</li> <li>Keras - API de alto nivel (integrada en TensorFlow)</li> <li>PyTorch - Framework de Meta, muy usado en investigaci\u00f3n</li> <li>JAX - Computaci\u00f3n num\u00e9rica de alto rendimiento</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/01-fundamentos/","title":"\ud83e\udde0 Unidad 1. Fundamentos del Deep Learning","text":"<p>El Deep Learning (Aprendizaje Profundo) es un subcampo del Machine Learning que utiliza redes neuronales artificiales con m\u00faltiples capas para aprender representaciones jer\u00e1rquicas de los datos.</p>"},{"location":"deep-learning/01-fundamentos/#11-que-es-el-deep-learning","title":"1.1. \u00bfQu\u00e9 es el Deep Learning?","text":"<p>El Deep Learning se diferencia del ML tradicional en su capacidad para aprender autom\u00e1ticamente las caracter\u00edsticas relevantes de los datos, sin necesidad de ingenier\u00eda manual de features.</p>"},{"location":"deep-learning/01-fundamentos/#machine-learning-vs-deep-learning","title":"Machine Learning vs Deep Learning","text":"Aspecto ML Tradicional Deep Learning Features Ingenier\u00eda manual Aprendizaje autom\u00e1tico Datos Funciona con menos datos Requiere muchos datos Hardware CPU suficiente GPU/TPU preferible Interpretabilidad Alta Baja (caja negra) Rendimiento (big data) Se estanca Mejora con m\u00e1s datos"},{"location":"deep-learning/01-fundamentos/#por-que-profundo","title":"\u00bfPor qu\u00e9 \"Profundo\"?","text":"<p>El t\u00e9rmino \"profundo\" se refiere a la cantidad de capas en la red neuronal. Mientras que las redes neuronales tradicionales ten\u00edan 1-2 capas ocultas, las redes profundas pueden tener cientos o miles de capas.</p> <pre><code>Red Neuronal Tradicional:    Red Neuronal Profunda:\n    Input                        Input\n      |                            |\n    [Capa]                      [Capa]\n      |                            |\n    Output                      [Capa]\n                                   |\n                                [Capa]\n                                   |\n                                  ...\n                                   |\n                                [Capa]\n                                   |\n                                Output\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#12-la-neurona-artificial-perceptron","title":"1.2. La Neurona Artificial (Perceptr\u00f3n)","text":"<p>La unidad b\u00e1sica de una red neuronal es la neurona artificial, inspirada (de forma simplificada) en las neuronas biol\u00f3gicas.</p>"},{"location":"deep-learning/01-fundamentos/#funcionamiento","title":"Funcionamiento","text":"<ol> <li>Entradas: Recibe valores \\(x_1, x_2, ..., x_n\\)</li> <li>Pesos: Cada entrada tiene un peso asociado \\(w_1, w_2, ..., w_n\\)</li> <li>Suma ponderada: \\(z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\\) (donde \\(b\\) es el bias)</li> <li>Activaci\u00f3n: \\(a = f(z)\\) (funci\u00f3n de activaci\u00f3n)</li> <li>Salida: El valor \\(a\\) se pasa a la siguiente capa</li> </ol>"},{"location":"deep-learning/01-fundamentos/#representacion-matematica","title":"Representaci\u00f3n Matem\u00e1tica","text":"\\[a = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = f(W^T X + b)\\] <pre><code>import numpy as np\n\ndef neurona(X, W, b, activacion='sigmoid'):\n    \"\"\"\n    Simula una neurona artificial.\n\n    X: vector de entradas\n    W: vector de pesos\n    b: bias\n    \"\"\"\n    # Suma ponderada\n    z = np.dot(W, X) + b\n\n    # Funci\u00f3n de activaci\u00f3n\n    if activacion == 'sigmoid':\n        a = 1 / (1 + np.exp(-z))\n    elif activacion == 'relu':\n        a = np.maximum(0, z)\n    elif activacion == 'tanh':\n        a = np.tanh(z)\n\n    return a\n\n# Ejemplo\nX = np.array([0.5, 0.3, 0.2])\nW = np.array([0.4, 0.6, 0.8])\nb = 0.1\n\nsalida = neurona(X, W, b)\nprint(f\"Salida de la neurona: {salida:.4f}\")\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#13-funciones-de-activacion","title":"1.3. Funciones de Activaci\u00f3n","text":"<p>Las funciones de activaci\u00f3n introducen no-linealidad en la red, permitiendo aprender relaciones complejas.</p>"},{"location":"deep-learning/01-fundamentos/#funciones-comunes","title":"Funciones Comunes","text":"Funci\u00f3n F\u00f3rmula Rango Uso Sigmoid \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) (0, 1) Clasificaci\u00f3n binaria (salida) Tanh \\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\) (-1, 1) Capas ocultas (alternativa a sigmoid) ReLU \\(f(z) = \\max(0, z)\\) [0, \u221e) Capas ocultas (m\u00e1s usada) Leaky ReLU \\(f(z) = \\max(0.01z, z)\\) (-\u221e, \u221e) Evita \"neuronas muertas\" Softmax \\(\\frac{e^{z_i}}{\\sum_j e^{z_j}}\\) (0, 1) Clasificaci\u00f3n multiclase (salida)"},{"location":"deep-learning/01-fundamentos/#visualizacion","title":"Visualizaci\u00f3n","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nz = np.linspace(-5, 5, 100)\n\n# Funciones de activaci\u00f3n\nsigmoid = 1 / (1 + np.exp(-z))\ntanh = np.tanh(z)\nrelu = np.maximum(0, z)\nleaky_relu = np.where(z &gt; 0, z, 0.01 * z)\n\n# Graficar\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\naxes[0, 0].plot(z, sigmoid)\naxes[0, 0].set_title('Sigmoid')\naxes[0, 0].grid(True)\n\naxes[0, 1].plot(z, tanh)\naxes[0, 1].set_title('Tanh')\naxes[0, 1].grid(True)\n\naxes[1, 0].plot(z, relu)\naxes[1, 0].set_title('ReLU')\naxes[1, 0].grid(True)\n\naxes[1, 1].plot(z, leaky_relu)\naxes[1, 1].set_title('Leaky ReLU')\naxes[1, 1].grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#por-que-relu-es-popular","title":"\u00bfPor qu\u00e9 ReLU es Popular?","text":"<ul> <li>Evita el problema del gradiente desvaneciente: La derivada es 1 para valores positivos.</li> <li>Computacionalmente eficiente: Solo comparaci\u00f3n y selecci\u00f3n.</li> <li>Promueve dispersi\u00f3n: Muchas neuronas producen 0, creando representaciones dispersas.</li> </ul>"},{"location":"deep-learning/01-fundamentos/#14-arquitectura-de-una-red-neuronal","title":"1.4. Arquitectura de una Red Neuronal","text":""},{"location":"deep-learning/01-fundamentos/#capas","title":"Capas","text":"<ol> <li>Capa de Entrada: Recibe los datos crudos. N\u00famero de neuronas = n\u00famero de features.</li> <li>Capas Ocultas: Procesan la informaci\u00f3n. El n\u00famero y tama\u00f1o define la capacidad del modelo.</li> <li>Capa de Salida: Produce la predicci\u00f3n final. Depende del tipo de problema.</li> </ol>"},{"location":"deep-learning/01-fundamentos/#ejemplo-de-arquitectura","title":"Ejemplo de Arquitectura","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Input\n\n# Red neuronal simple\nmodelo = Sequential([\n    Input(shape=(10,)),              # 10 features de entrada\n    Dense(64, activation='relu'),    # Primera capa oculta: 64 neuronas\n    Dense(32, activation='relu'),    # Segunda capa oculta: 32 neuronas\n    Dense(16, activation='relu'),    # Tercera capa oculta: 16 neuronas\n    Dense(1, activation='sigmoid')   # Salida: clasificaci\u00f3n binaria\n])\n\nmodelo.summary()\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#tipos-de-problemas-y-salidas","title":"Tipos de Problemas y Salidas","text":"Problema Capa de Salida Funci\u00f3n de Activaci\u00f3n Loss Function Regresi\u00f3n 1 neurona Lineal (ninguna) MSE Clasificaci\u00f3n Binaria 1 neurona Sigmoid Binary Crossentropy Clasificaci\u00f3n Multiclase N neuronas (N clases) Softmax Categorical Crossentropy"},{"location":"deep-learning/01-fundamentos/#15-forward-propagation","title":"1.5. Forward Propagation","text":"<p>Es el proceso de pasar los datos a trav\u00e9s de la red para obtener una predicci\u00f3n.</p> <pre><code>import numpy as np\n\ndef forward_propagation(X, pesos, biases, activaciones):\n    \"\"\"\n    Realiza forward propagation a trav\u00e9s de la red.\n    \"\"\"\n    activacion = X\n    activaciones_cache = [X]\n\n    for i, (W, b, func_act) in enumerate(zip(pesos, biases, activaciones)):\n        z = np.dot(activacion, W) + b\n\n        if func_act == 'relu':\n            activacion = np.maximum(0, z)\n        elif func_act == 'sigmoid':\n            activacion = 1 / (1 + np.exp(-z))\n        elif func_act == 'softmax':\n            exp_z = np.exp(z - np.max(z))\n            activacion = exp_z / exp_z.sum()\n\n        activaciones_cache.append(activacion)\n\n    return activacion, activaciones_cache\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#16-funcion-de-perdida-loss-function","title":"1.6. Funci\u00f3n de P\u00e9rdida (Loss Function)","text":"<p>La funci\u00f3n de p\u00e9rdida mide qu\u00e9 tan lejos est\u00e1n las predicciones de los valores reales.</p>"},{"location":"deep-learning/01-fundamentos/#funciones-comunes_1","title":"Funciones Comunes","text":"<p>Mean Squared Error (MSE) - Para regresi\u00f3n:</p> \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\] <p>Binary Cross-Entropy - Para clasificaci\u00f3n binaria:</p> \\[BCE = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\\] <p>Categorical Cross-Entropy - Para clasificaci\u00f3n multiclase:</p> \\[CCE = -\\sum_{i=1}^{n}\\sum_{c=1}^{C}y_{i,c}\\log(\\hat{y}_{i,c})\\] <pre><code>import numpy as np\n\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef binary_crossentropy(y_true, y_pred):\n    epsilon = 1e-15  # Evitar log(0)\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#17-backpropagation","title":"1.7. Backpropagation","text":"<p>El Backpropagation es el algoritmo que calcula los gradientes de la funci\u00f3n de p\u00e9rdida con respecto a cada peso, usando la regla de la cadena.</p>"},{"location":"deep-learning/01-fundamentos/#proceso","title":"Proceso","text":"<ol> <li>Forward pass: Calcular predicciones.</li> <li>Calcular p\u00e9rdida: Comparar predicci\u00f3n con valor real.</li> <li>Backward pass: Calcular gradientes desde la salida hacia la entrada.</li> <li>Actualizar pesos: Usar los gradientes para ajustar los pesos.</li> </ol>"},{"location":"deep-learning/01-fundamentos/#regla-de-la-cadena","title":"Regla de la Cadena","text":"\\[\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\\] <p>Donde: - \\(L\\) es la p\u00e9rdida - \\(a\\) es la activaci\u00f3n - \\(z\\) es la suma ponderada - \\(w\\) es el peso</p>"},{"location":"deep-learning/01-fundamentos/#18-optimizadores","title":"1.8. Optimizadores","text":"<p>Los optimizadores actualizan los pesos bas\u00e1ndose en los gradientes calculados.</p>"},{"location":"deep-learning/01-fundamentos/#gradient-descent","title":"Gradient Descent","text":"\\[w_{nuevo} = w_{actual} - \\eta \\cdot \\nabla L\\] <p>Donde \\(\\eta\\) es el learning rate (tasa de aprendizaje).</p>"},{"location":"deep-learning/01-fundamentos/#variantes-populares","title":"Variantes Populares","text":"Optimizador Descripci\u00f3n Cu\u00e1ndo usarlo SGD Actualiza con mini-batches Baseline, simple SGD + Momentum A\u00f1ade \"inercia\" a las actualizaciones Acelera convergencia RMSprop Adapta learning rate por par\u00e1metro Datos no estacionarios Adam Combina Momentum + RMSprop Default recomendado AdamW Adam con weight decay correcto Estado del arte"},{"location":"deep-learning/01-fundamentos/#ejemplo-en-keras","title":"Ejemplo en Keras","text":"<pre><code>from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n\n# Diferentes optimizadores\noptimizadores = {\n    'adam': Adam(learning_rate=0.001),\n    'sgd': SGD(learning_rate=0.01, momentum=0.9),\n    'rmsprop': RMSprop(learning_rate=0.001)\n}\n\n# Compilar modelo con Adam\nmodelo.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#19-ejemplo-completo-clasificacion-con-keras","title":"1.9. Ejemplo Completo: Clasificaci\u00f3n con Keras","text":"<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\n\n# Crear dataset sint\u00e9tico\nX, y = make_classification(\n    n_samples=1000, \n    n_features=20, \n    n_informative=15,\n    n_classes=2,\n    random_state=42\n)\n\n# Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Escalar (importante para redes neuronales)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Construir modelo\nmodelo = Sequential([\n    Input(shape=(20,)),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dropout(0.3),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# Compilar\nmodelo.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Entrenar\nhistoria = modelo.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\n# Evaluar\nloss, accuracy = modelo.evaluate(X_test, y_test)\nprint(f\"\\nAccuracy en test: {accuracy:.4f}\")\n\n# Visualizar entrenamiento\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(historia.history['loss'], label='Train')\nplt.plot(historia.history['val_loss'], label='Validation')\nplt.title('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(historia.history['accuracy'], label='Train')\nplt.plot(historia.history['val_accuracy'], label='Validation')\nplt.title('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/01-fundamentos/#110-frameworks-de-deep-learning","title":"1.10. Frameworks de Deep Learning","text":""},{"location":"deep-learning/01-fundamentos/#tensorflow-keras","title":"TensorFlow / Keras","text":"<ul> <li>Desarrollado por Google.</li> <li>Keras es la API de alto nivel.</li> <li>Excelente para producci\u00f3n y despliegue.</li> </ul>"},{"location":"deep-learning/01-fundamentos/#pytorch","title":"PyTorch","text":"<ul> <li>Desarrollado por Meta (Facebook).</li> <li>Muy popular en investigaci\u00f3n.</li> <li>Definici\u00f3n din\u00e1mica del grafo computacional.</li> </ul> <pre><code># Mismo modelo en PyTorch\nimport torch\nimport torch.nn as nn\n\nclass RedNeuronal(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(20, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nmodelo_pytorch = RedNeuronal()\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/02-cnn/","title":"\ud83e\udde0 Unidad 2. Redes Neuronales Convolucionales (CNN)","text":"<p>Las Redes Neuronales Convolucionales (Convolutional Neural Networks - CNN) son arquitecturas especializadas en procesar datos con estructura de cuadr\u00edcula, como im\u00e1genes. Son el est\u00e1ndar para tareas de visi\u00f3n por computadora.</p>"},{"location":"deep-learning/02-cnn/#21-por-que-cnn-para-imagenes","title":"2.1. \u00bfPor qu\u00e9 CNN para Im\u00e1genes?","text":""},{"location":"deep-learning/02-cnn/#limitaciones-de-las-redes-fully-connected","title":"Limitaciones de las Redes Fully Connected","text":"<p>Una imagen de 224x224 p\u00edxeles con 3 canales (RGB) tiene: \\(\\(224 \\times 224 \\times 3 = 150,528 \\text{ valores de entrada}\\)\\)</p> <p>Si conectamos esto a una capa oculta de 1000 neuronas: \\(\\(150,528 \\times 1000 = 150,528,000 \\text{ par\u00e1metros}\\)\\)</p> <p>Problemas:</p> <ul> <li>Demasiados par\u00e1metros (overfitting, memoria).</li> <li>No aprovecha la estructura espacial de la imagen.</li> <li>No es invariante a traslaciones.</li> </ul>"},{"location":"deep-learning/02-cnn/#ventajas-de-las-cnn","title":"Ventajas de las CNN","text":"<ul> <li>Conexiones locales: Cada neurona ve solo una regi\u00f3n peque\u00f1a.</li> <li>Compartici\u00f3n de pesos: Los mismos filtros se aplican en toda la imagen.</li> <li>Invariancia a traslaciones: Detectan patrones sin importar su posici\u00f3n.</li> <li>Jerarqu\u00eda de caracter\u00edsticas: Capas iniciales detectan bordes, las profundas detectan objetos.</li> </ul>"},{"location":"deep-learning/02-cnn/#22-la-operacion-de-convolucion","title":"2.2. La Operaci\u00f3n de Convoluci\u00f3n","text":"<p>La convoluci\u00f3n es una operaci\u00f3n matem\u00e1tica que aplica un filtro (kernel) sobre la imagen.</p>"},{"location":"deep-learning/02-cnn/#proceso","title":"Proceso","text":"<ol> <li>El filtro se desliza sobre la imagen.</li> <li>En cada posici\u00f3n, se calcula el producto punto entre el filtro y la regi\u00f3n de la imagen.</li> <li>El resultado forma un mapa de caracter\u00edsticas (feature map).</li> </ol>"},{"location":"deep-learning/02-cnn/#visualizacion","title":"Visualizaci\u00f3n","text":"<pre><code>Imagen (5x5):                Filtro (3x3):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1  2  3  4  5   \u2502          \u2502 1  0 -1 \u2502\n\u2502 6  7  8  9  10  \u2502    *     \u2502 1  0 -1 \u2502  =  Feature Map\n\u2502 11 12 13 14 15  \u2502          \u2502 1  0 -1 \u2502\n\u2502 16 17 18 19 20  \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 21 22 23 24 25  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-learning/02-cnn/#implementacion","title":"Implementaci\u00f3n","text":"<pre><code>import numpy as np\n\ndef convolucion_2d(imagen, filtro, stride=1, padding=0):\n    \"\"\"\n    Realiza convoluci\u00f3n 2D.\n    \"\"\"\n    # Padding\n    if padding &gt; 0:\n        imagen = np.pad(imagen, padding, mode='constant')\n\n    h_img, w_img = imagen.shape\n    h_filtro, w_filtro = filtro.shape\n\n    # Tama\u00f1o de salida\n    h_out = (h_img - h_filtro) // stride + 1\n    w_out = (w_img - w_filtro) // stride + 1\n\n    output = np.zeros((h_out, w_out))\n\n    for i in range(h_out):\n        for j in range(w_out):\n            region = imagen[i*stride:i*stride+h_filtro, j*stride:j*stride+w_filtro]\n            output[i, j] = np.sum(region * filtro)\n\n    return output\n\n# Ejemplo: Detector de bordes verticales\nimagen = np.array([\n    [10, 10, 10, 0, 0, 0],\n    [10, 10, 10, 0, 0, 0],\n    [10, 10, 10, 0, 0, 0],\n    [10, 10, 10, 0, 0, 0],\n    [10, 10, 10, 0, 0, 0],\n    [10, 10, 10, 0, 0, 0]\n])\n\nfiltro_vertical = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\nresultado = convolucion_2d(imagen, filtro_vertical)\nprint(\"Feature Map:\\n\", resultado)\n</code></pre>"},{"location":"deep-learning/02-cnn/#23-componentes-de-una-cnn","title":"2.3. Componentes de una CNN","text":""},{"location":"deep-learning/02-cnn/#capa-convolucional-conv2d","title":"Capa Convolucional (Conv2D)","text":"<p>Aplica m\u00faltiples filtros para extraer diferentes caracter\u00edsticas.</p> <pre><code>from tensorflow.keras.layers import Conv2D\n\n# 32 filtros de 3x3\nconv_layer = Conv2D(\n    filters=32,           # N\u00famero de filtros\n    kernel_size=(3, 3),   # Tama\u00f1o del filtro\n    strides=(1, 1),       # Paso del deslizamiento\n    padding='same',       # 'same' mantiene dimensiones, 'valid' no usa padding\n    activation='relu'\n)\n</code></pre>"},{"location":"deep-learning/02-cnn/#parametros-importantes","title":"Par\u00e1metros Importantes","text":"Par\u00e1metro Descripci\u00f3n <code>filters</code> N\u00famero de filtros (profundidad del output) <code>kernel_size</code> Tama\u00f1o del filtro (t\u00edpicamente 3x3 o 5x5) <code>strides</code> Paso del deslizamiento <code>padding</code> 'same' (mantiene tama\u00f1o) o 'valid' (reduce)"},{"location":"deep-learning/02-cnn/#capa-de-pooling-maxpool-avgpool","title":"Capa de Pooling (MaxPool, AvgPool)","text":"<p>Reduce las dimensiones espaciales, manteniendo las caracter\u00edsticas m\u00e1s importantes.</p> <pre><code>from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n\n# Max Pooling: toma el valor m\u00e1ximo de cada regi\u00f3n\nmax_pool = MaxPooling2D(pool_size=(2, 2))\n\n# Average Pooling: toma el promedio\navg_pool = AveragePooling2D(pool_size=(2, 2))\n</code></pre> <p>Max Pooling 2x2:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1  3 \u2502 2  4 \u2502    \u2502 3 \u2502 4 \u2502\n\u2502 5  6 \u2502 7  8 \u2502 \u2192  \u2502 6 \u2502 8 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2524      \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 9  2 \u2502 3  1 \u2502\n\u2502 3  4 \u2502 2  5 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-learning/02-cnn/#flatten-y-capas-densas","title":"Flatten y Capas Densas","text":"<p>Despu\u00e9s de las convoluciones, se \"aplana\" el tensor para conectar con capas densas.</p> <pre><code>from tensorflow.keras.layers import Flatten, Dense\n\n# Flatten: convierte tensor 3D a 1D\nflatten = Flatten()\n\n# Dense: capas completamente conectadas\ndense = Dense(128, activation='relu')\n</code></pre>"},{"location":"deep-learning/02-cnn/#24-arquitectura-tipica-de-cnn","title":"2.4. Arquitectura T\u00edpica de CNN","text":"<pre><code>Input \u2192 [Conv \u2192 ReLU \u2192 Pool] \u00d7 N \u2192 Flatten \u2192 Dense \u2192 Output\n\nEjemplo para clasificaci\u00f3n de im\u00e1genes:\n- Input: 224x224x3\n- Conv2D(32, 3x3) \u2192 ReLU \u2192 MaxPool(2x2)  \u2192  112x112x32\n- Conv2D(64, 3x3) \u2192 ReLU \u2192 MaxPool(2x2)  \u2192  56x56x64\n- Conv2D(128, 3x3) \u2192 ReLU \u2192 MaxPool(2x2) \u2192  28x28x128\n- Flatten                                 \u2192  100,352\n- Dense(256) \u2192 ReLU                       \u2192  256\n- Dense(10) \u2192 Softmax                     \u2192  10 clases\n</code></pre>"},{"location":"deep-learning/02-cnn/#implementacion-en-keras","title":"Implementaci\u00f3n en Keras","text":"<pre><code>from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n\ndef crear_cnn(input_shape, num_clases):\n    \"\"\"\n    Crea una CNN b\u00e1sica para clasificaci\u00f3n de im\u00e1genes.\n    \"\"\"\n    modelo = Sequential([\n        Input(shape=input_shape),\n\n        # Bloque 1\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),\n\n        # Bloque 2\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),\n\n        # Bloque 3\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),\n\n        # Clasificador\n        Flatten(),\n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(num_clases, activation='softmax')\n    ])\n\n    return modelo\n\n# Crear modelo para CIFAR-10 (im\u00e1genes 32x32x3, 10 clases)\nmodelo = crear_cnn((32, 32, 3), 10)\nmodelo.summary()\n</code></pre>"},{"location":"deep-learning/02-cnn/#25-ejemplo-completo-clasificacion-cifar-10","title":"2.5. Ejemplo Completo: Clasificaci\u00f3n CIFAR-10","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\n# Cargar datos\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Nombres de las clases\nclases = ['avi\u00f3n', 'auto', 'p\u00e1jaro', 'gato', 'venado', \n          'perro', 'rana', 'caballo', 'barco', 'cami\u00f3n']\n\n# Preprocesamiento\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\nprint(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n\n# Crear modelo\nmodelo = crear_cnn((32, 32, 3), 10)\n\n# Compilar\nmodelo.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Entrenar\nhistoria = modelo.fit(\n    X_train, y_train,\n    epochs=20,\n    batch_size=64,\n    validation_split=0.1,\n    verbose=1\n)\n\n# Evaluar\nloss, accuracy = modelo.evaluate(X_test, y_test)\nprint(f\"\\nAccuracy en test: {accuracy:.4f}\")\n\n# Visualizar algunas predicciones\npredicciones = modelo.predict(X_test[:10])\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_test[i])\n    pred_clase = clases[predicciones[i].argmax()]\n    real_clase = clases[y_test[i].argmax()]\n    color = 'green' if pred_clase == real_clase else 'red'\n    ax.set_title(f'Pred: {pred_clase}\\nReal: {real_clase}', color=color)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/02-cnn/#26-arquitecturas-famosas","title":"2.6. Arquitecturas Famosas","text":""},{"location":"deep-learning/02-cnn/#lenet-5-1998","title":"LeNet-5 (1998)","text":"<p>La primera CNN exitosa, dise\u00f1ada para reconocimiento de d\u00edgitos.</p> <pre><code>def lenet5(input_shape=(28, 28, 1), num_clases=10):\n    return Sequential([\n        Input(shape=input_shape),\n        Conv2D(6, (5, 5), activation='tanh'),\n        MaxPooling2D((2, 2)),\n        Conv2D(16, (5, 5), activation='tanh'),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(120, activation='tanh'),\n        Dense(84, activation='tanh'),\n        Dense(num_clases, activation='softmax')\n    ])\n</code></pre>"},{"location":"deep-learning/02-cnn/#vgg-2014","title":"VGG (2014)","text":"<p>Arquitectura profunda con filtros peque\u00f1os (3x3).</p> <pre><code>def vgg_block(num_convs, num_filters):\n    \"\"\"Un bloque VGG: m\u00faltiples Conv2D seguidas de MaxPool.\"\"\"\n    layers = []\n    for _ in range(num_convs):\n        layers.append(Conv2D(num_filters, (3, 3), activation='relu', padding='same'))\n    layers.append(MaxPooling2D((2, 2)))\n    return layers\n</code></pre>"},{"location":"deep-learning/02-cnn/#resnet-2015","title":"ResNet (2015)","text":"<p>Introdujo las conexiones residuales (skip connections) para entrenar redes muy profundas.</p> <pre><code>from tensorflow.keras.layers import Add\n\ndef bloque_residual(x, filtros):\n    \"\"\"Bloque residual de ResNet.\"\"\"\n    # Rama principal\n    fx = Conv2D(filtros, (3, 3), padding='same', activation='relu')(x)\n    fx = Conv2D(filtros, (3, 3), padding='same')(fx)\n\n    # Skip connection\n    out = Add()([x, fx])\n    out = tf.keras.activations.relu(out)\n\n    return out\n</code></pre>"},{"location":"deep-learning/02-cnn/#comparacion","title":"Comparaci\u00f3n","text":"Arquitectura A\u00f1o Capas Par\u00e1metros Top-5 Error (ImageNet) LeNet-5 1998 5 60K - AlexNet 2012 8 60M 16.4% VGG-16 2014 16 138M 7.3% ResNet-50 2015 50 25M 3.6% EfficientNet-B7 2019 66M 66M 2.9%"},{"location":"deep-learning/02-cnn/#27-data-augmentation","title":"2.7. Data Augmentation","text":"<p>T\u00e9cnica para aumentar artificialmente el dataset aplicando transformaciones a las im\u00e1genes.</p> <pre><code>from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Generador con aumentaciones\ndatagen = ImageDataGenerator(\n    rotation_range=15,          # Rotaci\u00f3n aleatoria\n    width_shift_range=0.1,      # Desplazamiento horizontal\n    height_shift_range=0.1,     # Desplazamiento vertical\n    horizontal_flip=True,       # Volteo horizontal\n    zoom_range=0.1,             # Zoom aleatorio\n    shear_range=0.1,            # Cizallamiento\n    fill_mode='nearest'         # C\u00f3mo rellenar p\u00edxeles vac\u00edos\n)\n\n# Usar en entrenamiento\nmodelo.fit(\n    datagen.flow(X_train, y_train, batch_size=32),\n    epochs=50,\n    validation_data=(X_test, y_test)\n)\n</code></pre>"},{"location":"deep-learning/02-cnn/#data-augmentation-con-tfkeraslayers","title":"Data Augmentation con tf.keras.layers","text":"<pre><code>from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n\n# Capa de aumentaci\u00f3n como parte del modelo\ndata_augmentation = Sequential([\n    RandomFlip(\"horizontal\"),\n    RandomRotation(0.1),\n    RandomZoom(0.1),\n])\n\n# Aplicar solo durante entrenamiento\nmodelo = Sequential([\n    Input(shape=(224, 224, 3)),\n    data_augmentation,\n    # ... resto del modelo\n])\n</code></pre>"},{"location":"deep-learning/02-cnn/#28-transfer-learning","title":"2.8. Transfer Learning","text":"<p>Usar modelos preentrenados en grandes datasets (ImageNet) y adaptarlos a nuestro problema.</p>"},{"location":"deep-learning/02-cnn/#estrategias","title":"Estrategias","text":"<ol> <li>Feature Extraction: Congelar las capas convolucionales, entrenar solo el clasificador.</li> <li>Fine-tuning: Descongelar algunas capas superiores y entrenarlas con learning rate bajo.</li> </ol> <pre><code>from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\n\n# Cargar modelo preentrenado (sin la cabeza de clasificaci\u00f3n)\nbase_model = VGG16(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(224, 224, 3)\n)\n\n# Congelar capas del modelo base\nbase_model.trainable = False\n\n# A\u00f1adir nuevo clasificador\nmodelo = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')  # 10 nuevas clases\n])\n\n# Entrenar solo las capas nuevas\nmodelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodelo.fit(X_train, y_train, epochs=10)\n\n# Fine-tuning: descongelar \u00faltimas capas\nbase_model.trainable = True\nfor layer in base_model.layers[:-4]:  # Congelar todas menos las \u00faltimas 4\n    layer.trainable = False\n\n# Recompilar con learning rate bajo\nmodelo.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),  # LR muy bajo\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodelo.fit(X_train, y_train, epochs=10)\n</code></pre>"},{"location":"deep-learning/02-cnn/#29-aplicaciones-de-cnn","title":"2.9. Aplicaciones de CNN","text":""},{"location":"deep-learning/02-cnn/#clasificacion-de-imagenes","title":"Clasificaci\u00f3n de Im\u00e1genes","text":"<p>Asignar una etiqueta a toda la imagen.</p>"},{"location":"deep-learning/02-cnn/#deteccion-de-objetos","title":"Detecci\u00f3n de Objetos","text":"<p>Localizar y clasificar m\u00faltiples objetos en una imagen.</p> <ul> <li>YOLO (You Only Look Once)</li> <li>Faster R-CNN</li> <li>SSD (Single Shot Detector)</li> </ul>"},{"location":"deep-learning/02-cnn/#segmentacion-semantica","title":"Segmentaci\u00f3n Sem\u00e1ntica","text":"<p>Clasificar cada p\u00edxel de la imagen.</p> <ul> <li>U-Net (para im\u00e1genes m\u00e9dicas)</li> <li>Mask R-CNN (segmentaci\u00f3n de instancias)</li> </ul>"},{"location":"deep-learning/02-cnn/#reconocimiento-facial","title":"Reconocimiento Facial","text":"<ul> <li>Detecci\u00f3n de caras</li> <li>Verificaci\u00f3n de identidad</li> <li>An\u00e1lisis de expresiones</li> </ul>"},{"location":"deep-learning/02-cnn/#210-visualizacion-de-filtros-y-feature-maps","title":"2.10. Visualizaci\u00f3n de Filtros y Feature Maps","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Visualizar filtros de la primera capa convolucional\nfiltros = modelo.layers[0].get_weights()[0]\n\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; filtros.shape[-1]:\n        # Normalizar filtro para visualizaci\u00f3n\n        f = filtros[:, :, :, i]\n        f = (f - f.min()) / (f.max() - f.min())\n        ax.imshow(f)\n    ax.axis('off')\nplt.suptitle('Filtros de la Primera Capa Convolucional')\nplt.tight_layout()\nplt.show()\n\n# Visualizar feature maps\nfrom tensorflow.keras.models import Model\n\n# Crear modelo que devuelve feature maps intermedios\ncapa_intermedia = Model(\n    inputs=modelo.input,\n    outputs=modelo.layers[2].output  # Despu\u00e9s de primera convoluci\u00f3n\n)\n\n# Obtener feature maps para una imagen\nimg = X_test[0:1]  # Una imagen\nfeature_maps = capa_intermedia.predict(img)\n\n# Visualizar\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; feature_maps.shape[-1]:\n        ax.imshow(feature_maps[0, :, :, i], cmap='viridis')\n    ax.axis('off')\nplt.suptitle('Feature Maps de la Primera Capa')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/03-rnn-lstm/","title":"\ud83e\udde0 Unidad 3. Redes Neuronales Recurrentes (RNN, LSTM, GRU)","text":"<p>Las Redes Neuronales Recurrentes (RNN) son arquitecturas dise\u00f1adas para procesar datos secuenciales, donde el orden de los elementos importa. Son fundamentales para tareas como procesamiento de texto, series temporales y audio.</p>"},{"location":"deep-learning/03-rnn-lstm/#31-por-que-redes-recurrentes","title":"3.1. \u00bfPor qu\u00e9 Redes Recurrentes?","text":""},{"location":"deep-learning/03-rnn-lstm/#limitaciones-de-las-redes-feed-forward","title":"Limitaciones de las Redes Feed-Forward","text":"<p>Las redes neuronales tradicionales:</p> <ul> <li>Procesan entradas de tama\u00f1o fijo.</li> <li>No tienen \"memoria\" de entradas anteriores.</li> <li>Tratan cada entrada de forma independiente.</li> </ul>"},{"location":"deep-learning/03-rnn-lstm/#datos-secuenciales","title":"Datos Secuenciales","text":"<p>Muchos problemas involucran secuencias donde el contexto importa:</p> <ul> <li>Texto: \"No me gusta\" vs \"Me gusta mucho\" - el significado cambia por contexto.</li> <li>Series Temporales: El precio de hoy depende de precios anteriores.</li> <li>Audio: Los fonemas se interpretan seg\u00fan los anteriores.</li> <li>Video: Los frames tienen continuidad temporal.</li> </ul>"},{"location":"deep-learning/03-rnn-lstm/#32-arquitectura-de-una-rnn","title":"3.2. Arquitectura de una RNN","text":"<p>La idea clave es que la red tiene un estado oculto que se actualiza en cada paso temporal y captura informaci\u00f3n de pasos anteriores.</p>"},{"location":"deep-learning/03-rnn-lstm/#estructura","title":"Estructura","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                                  \u2502\n        \u25bc                                  \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n    \u2502 h(t-1)\u2502\u2500\u2500\u25b6\u2502  h(t) \u2502\u2500\u2500\u25b6\u2502 h(t+1)\u2502\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u25b2           \u25b2           \u25b2\n        \u2502           \u2502           \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 x(t-1)\u2502   \u2502  x(t) \u2502   \u2502 x(t+1)\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Entrada     Entrada     Entrada\n    paso t-1    paso t      paso t+1\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#ecuaciones","title":"Ecuaciones","text":"<p>En cada paso temporal \\(t\\):</p> \\[h_t = \\tanh(W_{hh} \\cdot h_{t-1} + W_{xh} \\cdot x_t + b_h)\\] \\[y_t = W_{hy} \\cdot h_t + b_y\\] <p>Donde:</p> <ul> <li>\\(h_t\\) = Estado oculto en tiempo \\(t\\)</li> <li>\\(x_t\\) = Entrada en tiempo \\(t\\)</li> <li>\\(y_t\\) = Salida en tiempo \\(t\\)</li> <li>\\(W_{hh}, W_{xh}, W_{hy}\\) = Matrices de pesos (compartidas en todos los pasos)</li> </ul>"},{"location":"deep-learning/03-rnn-lstm/#implementacion-basica","title":"Implementaci\u00f3n B\u00e1sica","text":"<pre><code>import numpy as np\n\nclass RNNSimple:\n    def __init__(self, input_size, hidden_size, output_size):\n        # Inicializar pesos\n        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n        self.bh = np.zeros((hidden_size, 1))\n        self.by = np.zeros((output_size, 1))\n\n    def forward(self, inputs, h_prev):\n        \"\"\"\n        Forward pass a trav\u00e9s de la secuencia.\n        inputs: lista de vectores de entrada\n        h_prev: estado oculto inicial\n        \"\"\"\n        h = h_prev\n        outputs = []\n        hidden_states = [h]\n\n        for x in inputs:\n            # Actualizar estado oculto\n            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n            # Calcular salida\n            y = np.dot(self.Why, h) + self.by\n\n            outputs.append(y)\n            hidden_states.append(h)\n\n        return outputs, hidden_states\n\n# Ejemplo\nrnn = RNNSimple(input_size=10, hidden_size=20, output_size=5)\nsecuencia = [np.random.randn(10, 1) for _ in range(5)]\nh0 = np.zeros((20, 1))\n\noutputs, states = rnn.forward(secuencia, h0)\nprint(f\"N\u00famero de salidas: {len(outputs)}\")\nprint(f\"Forma de cada salida: {outputs[0].shape}\")\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#33-el-problema-del-gradiente-desvaneciente","title":"3.3. El Problema del Gradiente Desvaneciente","text":""},{"location":"deep-learning/03-rnn-lstm/#el-problema","title":"El Problema","text":"<p>Durante backpropagation a trav\u00e9s del tiempo (BPTT), los gradientes se multiplican repetidamente. Esto causa:</p> <ul> <li>Gradiente Desvaneciente: Los gradientes se hacen muy peque\u00f1os, y la red no puede aprender dependencias a largo plazo.</li> <li>Gradiente Explosivo: Los gradientes crecen exponencialmente (menos com\u00fan, se mitiga con gradient clipping).</li> </ul>"},{"location":"deep-learning/03-rnn-lstm/#consecuencia","title":"Consecuencia","text":"<p>Las RNN simples tienen dificultad para aprender relaciones entre elementos distantes en la secuencia.</p> <pre><code>\"El gato, que estaba en el jard\u00edn persiguiendo a un p\u00e1jaro, ___\"\n                    \u2191                                         \u2191\n              (muchas palabras entre \"gato\" y el verbo final)\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#34-lstm-long-short-term-memory","title":"3.4. LSTM (Long Short-Term Memory)","text":"<p>Las LSTM (Hochreiter &amp; Schmidhuber, 1997) resuelven el problema del gradiente desvaneciente mediante puertas (gates) que controlan el flujo de informaci\u00f3n.</p>"},{"location":"deep-learning/03-rnn-lstm/#componentes","title":"Componentes","text":"<ol> <li>Cell State (\\(C_t\\)): La \"memoria a largo plazo\" que fluye a trav\u00e9s de la red.</li> <li>Forget Gate (\\(f_t\\)): Decide qu\u00e9 informaci\u00f3n descartar de la memoria.</li> <li>Input Gate (\\(i_t\\)): Decide qu\u00e9 informaci\u00f3n nueva a\u00f1adir.</li> <li>Output Gate (\\(o_t\\)): Decide qu\u00e9 parte de la memoria usar para la salida.</li> </ol>"},{"location":"deep-learning/03-rnn-lstm/#ecuaciones_1","title":"Ecuaciones","text":"<p>Forget Gate: \\(\\(f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\\)\\)</p> <p>Input Gate: \\(\\(i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\\)\\) \\(\\(\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\\)\\)</p> <p>Actualizaci\u00f3n de Celda: \\(\\(C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\\)\\)</p> <p>Output Gate: \\(\\(o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\\)\\) \\(\\(h_t = o_t \\odot \\tanh(C_t)\\)\\)</p>"},{"location":"deep-learning/03-rnn-lstm/#diagrama","title":"Diagrama","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502              Cell State                  \u2502\n    C(t-1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500[\u00d7]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[+]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502\u2500\u2500\u2500\u25b6 C(t)\n                    \u2502        \u2191           \u2191                     \u2502\n                    \u2502     forget      input                    \u2502\n                    \u2502      gate       gate                     \u2502\n                    \u2502        \u2502           \u2502                     \u2502\n                    \u2502   \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510               \u2502\n                    \u2502   \u2502    \u03c3   \u2502 \u2502   \u03c3 \u00d7 tanh\u2502              \u2502\n                    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n                    \u2502        \u2191           \u2191                     \u2502\n    h(t-1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500[\u00d7]\u2500\u2500\u2500\u2500\u2500\u25b6\u2502\u2500\u2500\u2500\u25b6 h(t)\n                    \u2502                              \u2191           \u2502\n                    \u2502                           output         \u2502\n                    \u2502                            gate          \u2502\n                    \u2502                         \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510        \u2502\n                    \u2502                         \u2502   \u03c3   \u2502        \u2502\n                    \u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2191\n    x(t) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#implementacion-en-keras","title":"Implementaci\u00f3n en Keras","text":"<pre><code>from tensorflow.keras.layers import LSTM, Input\nfrom tensorflow.keras import Sequential\n\n# LSTM simple\nmodelo = Sequential([\n    Input(shape=(100, 50)),  # Secuencia de 100 pasos, 50 features cada uno\n    LSTM(128),               # 128 unidades LSTM\n    Dense(1, activation='sigmoid')\n])\n\n# LSTM apiladas (stacked)\nmodelo_profundo = Sequential([\n    Input(shape=(100, 50)),\n    LSTM(128, return_sequences=True),  # return_sequences=True para apilar\n    LSTM(64, return_sequences=True),\n    LSTM(32),                          # \u00daltima capa: return_sequences=False\n    Dense(1)\n])\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#35-gru-gated-recurrent-unit","title":"3.5. GRU (Gated Recurrent Unit)","text":"<p>Las GRU (Cho et al., 2014) son una simplificaci\u00f3n de las LSTM con menos par\u00e1metros.</p>"},{"location":"deep-learning/03-rnn-lstm/#diferencias-con-lstm","title":"Diferencias con LSTM","text":"<ul> <li>Combina forget e input gates en un solo update gate.</li> <li>No tiene cell state separado.</li> <li>Menos par\u00e1metros \u2192 m\u00e1s r\u00e1pido de entrenar.</li> <li>Rendimiento similar en muchas tareas.</li> </ul>"},{"location":"deep-learning/03-rnn-lstm/#ecuaciones_2","title":"Ecuaciones","text":"<p>Update Gate: \\(\\(z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])\\)\\)</p> <p>Reset Gate: \\(\\(r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])\\)\\)</p> <p>Candidato: \\(\\(\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t])\\)\\)</p> <p>Estado Oculto: \\(\\(h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\\)\\)</p>"},{"location":"deep-learning/03-rnn-lstm/#implementacion","title":"Implementaci\u00f3n","text":"<pre><code>from tensorflow.keras.layers import GRU\n\nmodelo_gru = Sequential([\n    Input(shape=(100, 50)),\n    GRU(128),\n    Dense(1)\n])\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#comparacion-lstm-vs-gru","title":"Comparaci\u00f3n LSTM vs GRU","text":"Caracter\u00edstica LSTM GRU Gates 3 (forget, input, output) 2 (update, reset) Par\u00e1metros M\u00e1s Menos Memoria Cell state separado Solo hidden state Velocidad M\u00e1s lento M\u00e1s r\u00e1pido Rendimiento Similar Similar"},{"location":"deep-learning/03-rnn-lstm/#36-rnn-bidireccionales","title":"3.6. RNN Bidireccionales","text":"<p>Procesan la secuencia en ambas direcciones, capturando contexto pasado y futuro.</p> <pre><code>Forward:   x1 \u2192 x2 \u2192 x3 \u2192 x4 \u2192 x5\nBackward:  x1 \u2190 x2 \u2190 x3 \u2190 x4 \u2190 x5\n\nSalida en t: [forward_h(t), backward_h(t)]\n</code></pre> <pre><code>from tensorflow.keras.layers import Bidirectional\n\n# LSTM Bidireccional\nmodelo_bi = Sequential([\n    Input(shape=(100, 50)),\n    Bidirectional(LSTM(64)),  # Salida: 128 (64 forward + 64 backward)\n    Dense(1)\n])\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#37-tipos-de-arquitecturas-rnn","title":"3.7. Tipos de Arquitecturas RNN","text":""},{"location":"deep-learning/03-rnn-lstm/#segun-entradasalida","title":"Seg\u00fan Entrada/Salida","text":"Tipo Entrada Salida Uso Many-to-One Secuencia Un valor Clasificaci\u00f3n de texto One-to-Many Un valor Secuencia Generaci\u00f3n de texto Many-to-Many (igual) Secuencia Secuencia (misma longitud) POS Tagging, NER Many-to-Many (diferente) Secuencia Secuencia (otra longitud) Traducci\u00f3n (Seq2Seq) <pre><code># Many-to-One: Clasificaci\u00f3n de sentimiento\nmodelo_m2o = Sequential([\n    Input(shape=(100, 50)),\n    LSTM(64),\n    Dense(2, activation='softmax')  # Positivo/Negativo\n])\n\n# Many-to-Many: Etiquetado de secuencia\nmodelo_m2m = Sequential([\n    Input(shape=(100, 50)),\n    LSTM(64, return_sequences=True),\n    Dense(10, activation='softmax')  # Una etiqueta por paso\n])\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#38-ejemplo-completo-prediccion-de-series-temporales","title":"3.8. Ejemplo Completo: Predicci\u00f3n de Series Temporales","text":"<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Input\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Generar datos sint\u00e9ticos (serie temporal sinusoidal con ruido)\nnp.random.seed(42)\nt = np.linspace(0, 100, 1000)\nserie = np.sin(t) + 0.1 * np.random.randn(1000)\n\n# Normalizar\nscaler = MinMaxScaler()\nserie_normalizada = scaler.fit_transform(serie.reshape(-1, 1))\n\n# Crear secuencias para entrenamiento\ndef crear_secuencias(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i+window_size])\n        y.append(data[i+window_size])\n    return np.array(X), np.array(y)\n\nwindow_size = 50\nX, y = crear_secuencias(serie_normalizada, window_size)\n\n# Divisi\u00f3n train/test\nsplit = int(len(X) * 0.8)\nX_train, X_test = X[:split], X[split:]\ny_train, y_test = y[:split], y[split:]\n\nprint(f\"X_train shape: {X_train.shape}\")  # (N, 50, 1)\nprint(f\"y_train shape: {y_train.shape}\")  # (N, 1)\n\n# Crear modelo LSTM\nmodelo = Sequential([\n    Input(shape=(window_size, 1)),\n    LSTM(50, return_sequences=True),\n    LSTM(50),\n    Dense(25, activation='relu'),\n    Dense(1)\n])\n\nmodelo.compile(optimizer='adam', loss='mse')\n\n# Entrenar\nhistoria = modelo.fit(\n    X_train, y_train,\n    epochs=20,\n    batch_size=32,\n    validation_split=0.1,\n    verbose=1\n)\n\n# Predecir\npredicciones = modelo.predict(X_test)\n\n# Visualizar\nplt.figure(figsize=(14, 6))\nplt.plot(y_test, label='Real', alpha=0.7)\nplt.plot(predicciones, label='Predicci\u00f3n', alpha=0.7)\nplt.title('Predicci\u00f3n de Serie Temporal con LSTM')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#39-ejemplo-clasificacion-de-texto-con-lstm","title":"3.9. Ejemplo: Clasificaci\u00f3n de Texto con LSTM","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Cargar dataset IMDB\nmax_features = 10000  # Vocabulario\nmaxlen = 200          # Longitud m\u00e1xima de secuencia\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n\n# Padding para longitud uniforme\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\n# Modelo\nfrom tensorflow.keras.layers import Embedding\n\nmodelo = Sequential([\n    Input(shape=(maxlen,)),\n    Embedding(max_features, 128),  # Embedding de palabras\n    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n    Dense(1, activation='sigmoid')\n])\n\nmodelo.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Entrenar\nhistoria = modelo.fit(\n    X_train, y_train,\n    epochs=5,\n    batch_size=64,\n    validation_split=0.2\n)\n\n# Evaluar\nloss, accuracy = modelo.evaluate(X_test, y_test)\nprint(f\"Accuracy en test: {accuracy:.4f}\")\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#310-tecnicas-de-regularizacion-para-rnn","title":"3.10. T\u00e9cnicas de Regularizaci\u00f3n para RNN","text":""},{"location":"deep-learning/03-rnn-lstm/#dropout","title":"Dropout","text":"<pre><code>from tensorflow.keras.layers import Dropout\n\nmodelo = Sequential([\n    Input(shape=(100, 50)),\n    LSTM(64, dropout=0.2, recurrent_dropout=0.2),  # Dropout en LSTM\n    Dropout(0.5),  # Dropout despu\u00e9s de LSTM\n    Dense(1)\n])\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#batch-normalization","title":"Batch Normalization","text":"<pre><code>from tensorflow.keras.layers import BatchNormalization\n\nmodelo = Sequential([\n    Input(shape=(100, 50)),\n    LSTM(64, return_sequences=True),\n    BatchNormalization(),\n    LSTM(32),\n    Dense(1)\n])\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#gradient-clipping","title":"Gradient Clipping","text":"<pre><code>optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\nmodelo.compile(optimizer=optimizer, loss='mse')\n</code></pre>"},{"location":"deep-learning/03-rnn-lstm/#311-consideraciones-practicas","title":"3.11. Consideraciones Pr\u00e1cticas","text":""},{"location":"deep-learning/03-rnn-lstm/#cuando-usar-rnnlstmgru","title":"Cu\u00e1ndo usar RNN/LSTM/GRU","text":"<ul> <li>Secuencias cortas a medianas: RNN simple puede funcionar.</li> <li>Secuencias largas: LSTM o GRU (mejor manejo de dependencias largas).</li> <li>Recursos limitados: GRU (menos par\u00e1metros).</li> <li>Contexto bidireccional necesario: BiLSTM/BiGRU.</li> </ul>"},{"location":"deep-learning/03-rnn-lstm/#alternativas-modernas","title":"Alternativas Modernas","text":"<p>Para muchas tareas de NLP, los Transformers han superado a las RNN:</p> <ul> <li>Paralelizaci\u00f3n m\u00e1s eficiente.</li> <li>Mejor captura de dependencias largas.</li> <li>Modelos preentrenados disponibles (BERT, GPT).</li> </ul> <p>Sin embargo, las RNN siguen siendo \u00fatiles para:</p> <ul> <li>Series temporales.</li> <li>Streaming de datos (procesamiento en tiempo real).</li> <li>Recursos computacionales limitados.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/04-transformers-dl/","title":"\ud83e\udd16 Unidad 4. Arquitectura Transformer","text":"<p>Los Transformers (Vaswani et al., 2017) revolucionaron el campo del Deep Learning al introducir el mecanismo de atenci\u00f3n como \u00fanico componente para modelar secuencias, eliminando la necesidad de recurrencia.</p>"},{"location":"deep-learning/04-transformers-dl/#41-motivacion-limitaciones-de-las-rnn","title":"4.1. Motivaci\u00f3n: Limitaciones de las RNN","text":"Limitaci\u00f3n RNN Soluci\u00f3n Transformer Procesamiento secuencial (no paralelizable) Procesamiento paralelo completo Dificultad con dependencias largas Atenci\u00f3n directa entre cualquier par de posiciones Gradientes que se desvanecen Conexiones directas mediante atenci\u00f3n"},{"location":"deep-learning/04-transformers-dl/#42-mecanismo-de-atencion","title":"4.2. Mecanismo de Atenci\u00f3n","text":""},{"location":"deep-learning/04-transformers-dl/#que-es-la-atencion","title":"\u00bfQu\u00e9 es la Atenci\u00f3n?","text":"<p>La atenci\u00f3n permite que cada elemento de una secuencia \"preste atenci\u00f3n\" a todos los dem\u00e1s elementos, ponderando su importancia.</p> <p>Intuici\u00f3n:</p> <pre><code>Frase: \"El gato negro que vive en mi casa est\u00e1 durmiendo en el sof\u00e1\"\n                 \u2191                              \u2191\n              \"est\u00e1\" presta m\u00e1s atenci\u00f3n a \"gato\" que a \"casa\"\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#scaled-dot-product-attention","title":"Scaled Dot-Product Attention","text":"<p>La atenci\u00f3n se calcula usando tres vectores para cada elemento:</p> <ul> <li>Query (Q): Lo que estamos buscando.</li> <li>Key (K): Etiqueta de cada elemento.</li> <li>Value (V): El contenido real.</li> </ul> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>Donde \\(d_k\\) es la dimensi\u00f3n de las keys (para estabilidad num\u00e9rica).</p> <pre><code>import numpy as np\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Calcula la atenci\u00f3n escalada por producto punto.\n\n    Q: queries (batch, seq_len, d_k)\n    K: keys (batch, seq_len, d_k)\n    V: values (batch, seq_len, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n\n    # Calcular scores\n    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n\n    # Aplicar m\u00e1scara si existe\n    if mask is not None:\n        scores = scores + (mask * -1e9)\n\n    # Softmax para obtener pesos de atenci\u00f3n\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n\n    # Multiplicar por valores\n    output = np.matmul(attention_weights, V)\n\n    return output, attention_weights\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#multi-head-attention","title":"Multi-Head Attention","text":"<p>En lugar de una sola atenci\u00f3n, usamos m\u00faltiples \"cabezas\" que aprenden diferentes tipos de relaciones:</p> \\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\\] <p>Donde cada cabeza es: \\(\\(\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\)\\)</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Divide la \u00faltima dimensi\u00f3n en (num_heads, depth).\"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask=None):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        # Scaled dot-product attention\n        scaled_attention = tf.matmul(q, k, transpose_b=True)\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention = scaled_attention / tf.math.sqrt(dk)\n\n        if mask is not None:\n            scaled_attention += (mask * -1e9)\n\n        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n        output = tf.matmul(attention_weights, v)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])\n        output = tf.reshape(output, (batch_size, -1, self.d_model))\n\n        return self.dense(output)\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#43-arquitectura-completa-del-transformer","title":"4.3. Arquitectura Completa del Transformer","text":""},{"location":"deep-learning/04-transformers-dl/#diagrama-general","title":"Diagrama General","text":"<pre><code>         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502           DECODER                    \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Linear + Softmax \u2192 Output   \u2502  \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2502              \u2191                       \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Multi-Head Attention        \u2502  \u2502 \u2190 Cross-attention\n         \u2502  \u2502   (queries del decoder,       \u2502  \u2502    (conecta encoder\n         \u2502  \u2502    keys/values del encoder)   \u2502  \u2502     y decoder)\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2502              \u2191                       \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Masked Multi-Head           \u2502  \u2502\n         \u2502  \u2502   Self-Attention              \u2502  \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2502              \u2191                       \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Output Embedding +          \u2502  \u2502\n         \u2502  \u2502   Positional Encoding         \u2502  \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2191\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502           ENCODER                    \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Feed Forward Neural Net     \u2502  \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2502              \u2191                       \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Multi-Head Self-Attention   \u2502  \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2502              \u2191                       \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n         \u2502  \u2502   Input Embedding +           \u2502  \u2502\n         \u2502  \u2502   Positional Encoding         \u2502  \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2191\n                   Input Tokens\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#44-componentes-del-transformer","title":"4.4. Componentes del Transformer","text":""},{"location":"deep-learning/04-transformers-dl/#positional-encoding","title":"Positional Encoding","text":"<p>Como no hay recurrencia, necesitamos inyectar informaci\u00f3n de posici\u00f3n:</p> \\[PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$ $$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\\] <pre><code>def positional_encoding(position, d_model):\n    \"\"\"Genera positional encodings.\"\"\"\n    angle_rads = get_angles(\n        np.arange(position)[:, np.newaxis],\n        np.arange(d_model)[np.newaxis, :],\n        d_model\n    )\n\n    # Seno a \u00edndices pares\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # Coseno a \u00edndices impares\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    return pos * angle_rates\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#feed-forward-network","title":"Feed-Forward Network","text":"<p>Despu\u00e9s de la atenci\u00f3n, cada posici\u00f3n pasa por una red feed-forward:</p> \\[\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\\] <pre><code>def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),\n        tf.keras.layers.Dense(d_model)\n    ])\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#layer-normalization-y-residual-connections","title":"Layer Normalization y Residual Connections","text":"<p>Cada sublayer tiene:</p> <ol> <li>Residual Connection: \\(\\text{output} = x + \\text{Sublayer}(x)\\)</li> <li>Layer Normalization: Normaliza las activaciones.</li> </ol> <pre><code>class EncoderLayer(Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super().__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        # Multi-head attention + residual + norm\n        attn_output = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        # Feed-forward + residual + norm\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n\n        return out2\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#45-self-attention-vs-cross-attention","title":"4.5. Self-Attention vs Cross-Attention","text":""},{"location":"deep-learning/04-transformers-dl/#self-attention","title":"Self-Attention","text":"<p>Q, K, V vienen de la misma secuencia:</p> <pre><code># En el encoder\noutput = self_attention(x, x, x)  # Q=K=V=x\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#cross-attention","title":"Cross-Attention","text":"<p>Q viene del decoder, K y V del encoder:</p> <pre><code># En el decoder (despu\u00e9s del self-attention)\noutput = cross_attention(\n    q=decoder_output,\n    k=encoder_output,\n    v=encoder_output\n)\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#masked-self-attention","title":"Masked Self-Attention","text":"<p>En el decoder, cada posici\u00f3n solo puede atender a posiciones anteriores (para generaci\u00f3n autoregresiva):</p> <pre><code>def create_look_ahead_mask(size):\n    \"\"\"M\u00e1scara triangular para evitar mirar el futuro.\"\"\"\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (size, size)\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#46-implementacion-completa-de-un-transformer","title":"4.6. Implementaci\u00f3n Completa de un Transformer","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Embedding, Dense, Dropout, LayerNormalization\n\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, \n                 input_vocab_size, target_vocab_size, \n                 pe_input, pe_target, rate=0.1):\n        super().__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n                               input_vocab_size, pe_input, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n                               target_vocab_size, pe_target, rate)\n\n        self.final_layer = Dense(target_vocab_size)\n\n    def call(self, inputs, training):\n        inp, tar = inputs\n\n        enc_padding_mask = create_padding_mask(inp)\n        dec_padding_mask = create_padding_mask(inp)\n        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n        dec_target_padding_mask = create_padding_mask(tar)\n        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n        dec_output = self.decoder(tar, enc_output, training, \n                                   combined_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)\n        return final_output\n\nclass Encoder(Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, \n                 vocab_size, maximum_position_encoding, rate=0.1):\n        super().__init__()\n\n        self.d_model = d_model\n        self.embedding = Embedding(vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                          for _ in range(num_layers)]\n        self.dropout = Dropout(rate)\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for enc_layer in self.enc_layers:\n            x = enc_layer(x, training, mask)\n\n        return x\n\nclass Decoder(Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, \n                 vocab_size, maximum_position_encoding, rate=0.1):\n        super().__init__()\n\n        self.d_model = d_model\n        self.embedding = Embedding(vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                          for _ in range(num_layers)]\n        self.dropout = Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for dec_layer in self.dec_layers:\n            x = dec_layer(x, enc_output, training, look_ahead_mask, padding_mask)\n\n        return x\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#47-variantes-de-transformers","title":"4.7. Variantes de Transformers","text":""},{"location":"deep-learning/04-transformers-dl/#encoder-only-bert","title":"Encoder-Only (BERT)","text":"<p>Solo usa el encoder. Ideal para:</p> <ul> <li>Clasificaci\u00f3n de texto.</li> <li>NER.</li> <li>Question Answering extractivo.</li> </ul> <pre><code># Usando Hugging Face\nfrom transformers import BertModel, BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ninputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\noutputs = model(**inputs)\n# outputs.last_hidden_state: (batch, seq_len, hidden_size)\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#decoder-only-gpt","title":"Decoder-Only (GPT)","text":"<p>Solo usa el decoder con masked self-attention. Ideal para:</p> <ul> <li>Generaci\u00f3n de texto.</li> <li>Completar oraciones.</li> <li>Modelos de lenguaje.</li> </ul> <pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\ninput_ids = tokenizer.encode(\"Once upon a time\", return_tensors='pt')\noutputs = model.generate(input_ids, max_length=50, num_return_sequences=1)\nprint(tokenizer.decode(outputs[0]))\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#encoder-decoder-t5-bart","title":"Encoder-Decoder (T5, BART)","text":"<p>Arquitectura completa. Ideal para:</p> <ul> <li>Traducci\u00f3n.</li> <li>Resumen.</li> <li>Question Answering generativo.</li> </ul> <pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')\n\n# Traducci\u00f3n\ninput_text = \"translate English to German: Hello, how are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#48-ejemplo-practico-fine-tuning-de-bert","title":"4.8. Ejemplo Pr\u00e1ctico: Fine-tuning de BERT","text":"<pre><code>from transformers import (\n    BertTokenizer, \n    TFBertForSequenceClassification,\n    create_optimizer\n)\nimport tensorflow as tf\n\n# Cargar modelo y tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Datos de ejemplo\ntextos = [\n    \"I love this movie!\",\n    \"This film was terrible.\",\n    \"Great acting and plot.\",\n    \"Waste of time.\"\n]\netiquetas = [1, 0, 1, 0]  # 1=positivo, 0=negativo\n\n# Tokenizar\nencodings = tokenizer(\n    textos,\n    truncation=True,\n    padding=True,\n    max_length=128,\n    return_tensors='tf'\n)\n\n# Crear dataset\ndataset = tf.data.Dataset.from_tensor_slices((\n    dict(encodings),\n    etiquetas\n)).batch(2)\n\n# Configurar optimizador\nbatch_size = 2\nnum_train_steps = len(textos) // batch_size * 3  # 3 \u00e9pocas\noptimizer, schedule = create_optimizer(\n    init_lr=2e-5,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=0\n)\n\n# Compilar\nmodel.compile(\n    optimizer=optimizer,\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\n# Entrenar\nmodel.fit(dataset, epochs=3)\n\n# Predecir\nnuevo_texto = \"Amazing film, highly recommended!\"\ninputs = tokenizer(nuevo_texto, return_tensors='tf', truncation=True, padding=True)\noutputs = model(inputs)\nprediccion = tf.nn.softmax(outputs.logits, axis=-1)\nprint(f\"Probabilidades: {prediccion.numpy()}\")\n</code></pre>"},{"location":"deep-learning/04-transformers-dl/#49-comparacion-de-arquitecturas","title":"4.9. Comparaci\u00f3n de Arquitecturas","text":"Caracter\u00edstica RNN/LSTM Transformer Paralelizaci\u00f3n Secuencial Total Dependencias largas Dif\u00edcil F\u00e1cil (atenci\u00f3n directa) Complejidad computacional \\(O(n)\\) \\(O(n^2)\\) Memoria \\(O(1)\\) por paso \\(O(n^2)\\) para atenci\u00f3n Preentrenamiento Limitado Muy efectivo"},{"location":"deep-learning/04-transformers-dl/#410-tendencias-actuales","title":"4.10. Tendencias Actuales","text":""},{"location":"deep-learning/04-transformers-dl/#efficient-transformers","title":"Efficient Transformers","text":"<p>Para reducir la complejidad \\(O(n^2)\\):</p> <ul> <li>Sparse Attention: Solo atiende a un subconjunto de posiciones.</li> <li>Linear Attention: Aproximaciones lineales.</li> <li>Longformer, BigBird: Para secuencias muy largas.</li> </ul>"},{"location":"deep-learning/04-transformers-dl/#large-language-models-llms","title":"Large Language Models (LLMs)","text":"<ul> <li>GPT-4, Claude, Llama: Miles de millones de par\u00e1metros.</li> <li>Instruction Tuning: Afinados para seguir instrucciones.</li> <li>RLHF: Entrenamiento con feedback humano.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/05-autoencoders/","title":"\ud83d\udd04 Unidad 5. Autoencoders y Representaci\u00f3n Latente","text":"<p>Los Autoencoders son redes neuronales que aprenden representaciones comprimidas de los datos de forma no supervisada. Son fundamentales para reducci\u00f3n de dimensionalidad, detecci\u00f3n de anomal\u00edas y generaci\u00f3n de datos.</p>"},{"location":"deep-learning/05-autoencoders/#51-que-es-un-autoencoder","title":"5.1. \u00bfQu\u00e9 es un Autoencoder?","text":"<p>Un autoencoder es una red que aprende a codificar datos en una representaci\u00f3n comprimida (espacio latente) y luego decodificarlos para reconstruir la entrada original.</p>"},{"location":"deep-learning/05-autoencoders/#arquitectura","title":"Arquitectura","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Entrada   \u2502      \u2502   Espacio   \u2502      \u2502   Salida    \u2502\n\u2502    (x)      \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Latente   \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502    (x')     \u2502\n\u2502  (784 dim)  \u2502      \u2502   (32 dim)  \u2502      \u2502  (784 dim)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u2502\u2500\u2500\u2500 ENCODER \u2500\u2500\u2500\u2502    \u2502\u2500\u2500 LATENTE \u2500\u2500\u2502     \u2502\u2500\u2500\u2500 DECODER \u2500\u2500\u2500\u2502\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#objetivo","title":"Objetivo","text":"<p>Minimizar la diferencia entre entrada y salida:</p> \\[\\mathcal{L} = ||x - x'||^2 = ||x - \\text{Decoder}(\\text{Encoder}(x))||^2\\]"},{"location":"deep-learning/05-autoencoders/#implementacion-basica","title":"Implementaci\u00f3n B\u00e1sica","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Input\n\n# Dimensiones\ninput_dim = 784  # Por ejemplo, im\u00e1genes 28x28\nencoding_dim = 32  # Dimensi\u00f3n del espacio latente\n\n# Encoder\nencoder = Sequential([\n    Input(shape=(input_dim,)),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(encoding_dim, activation='relu')  # Espacio latente\n], name='encoder')\n\n# Decoder\ndecoder = Sequential([\n    Input(shape=(encoding_dim,)),\n    Dense(128, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(input_dim, activation='sigmoid')  # Reconstrucci\u00f3n\n], name='decoder')\n\n# Autoencoder completo\nautoencoder_input = Input(shape=(input_dim,))\nencoded = encoder(autoencoder_input)\ndecoded = decoder(encoded)\nautoencoder = Model(autoencoder_input, decoded, name='autoencoder')\n\nautoencoder.compile(optimizer='adam', loss='mse')\nautoencoder.summary()\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#52-entrenamiento-de-autoencoder","title":"5.2. Entrenamiento de Autoencoder","text":"<pre><code>from tensorflow.keras.datasets import mnist\nimport numpy as np\n\n# Cargar datos\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# Preprocesar\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"x_test shape: {x_test.shape}\")\n\n# Entrenar\n# La entrada Y la salida son los mismos datos\nhistoria = autoencoder.fit(\n    x_train, x_train,  # Mismo dato como entrada y objetivo\n    epochs=50,\n    batch_size=256,\n    shuffle=True,\n    validation_data=(x_test, x_test)\n)\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#visualizar-resultados","title":"Visualizar Resultados","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Reconstruir im\u00e1genes de test\ndecoded_imgs = autoencoder.predict(x_test)\n\n# Visualizar originales vs reconstruidas\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n    plt.title(\"Original\")\n    plt.axis('off')\n\n    # Reconstruida\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n    plt.title(\"Reconstruida\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#53-aplicaciones-de-autoencoders","title":"5.3. Aplicaciones de Autoencoders","text":""},{"location":"deep-learning/05-autoencoders/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de Dimensionalidad","text":"<p>Similar a PCA, pero no lineal:</p> <pre><code># Extraer representaciones latentes\nencoded_imgs = encoder.predict(x_test)\nprint(f\"Dimensi\u00f3n original: {x_test.shape[1]}\")\nprint(f\"Dimensi\u00f3n latente: {encoded_imgs.shape[1]}\")\n\n# Visualizar en 2D (si encoding_dim=2)\nfrom sklearn.manifold import TSNE\n\n# Si encoding_dim &gt; 2, usar t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nencoded_2d = tsne.fit_transform(encoded_imgs[:1000])\n\nplt.figure(figsize=(10, 8))\nplt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], c=y_test[:1000], cmap='tab10')\nplt.colorbar()\nplt.title('Espacio Latente del Autoencoder')\nplt.show()\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#deteccion-de-anomalias","title":"Detecci\u00f3n de Anomal\u00edas","text":"<p>Las anomal\u00edas tienen mayor error de reconstrucci\u00f3n:</p> <pre><code>def detectar_anomalias(autoencoder, datos, umbral=None):\n    \"\"\"\n    Detecta anomal\u00edas bas\u00e1ndose en el error de reconstrucci\u00f3n.\n    \"\"\"\n    reconstrucciones = autoencoder.predict(datos)\n    errores = np.mean(np.square(datos - reconstrucciones), axis=1)\n\n    if umbral is None:\n        # Calcular umbral autom\u00e1ticamente\n        umbral = np.mean(errores) + 2 * np.std(errores)\n\n    anomalias = errores &gt; umbral\n    return anomalias, errores\n\n# Ejemplo con datos normales y anomal\u00edas\ndatos_normales = x_test[:100]\n# Crear \"anomal\u00edas\" artificiales (ruido aleatorio)\ndatos_anomalos = np.random.random((100, 784)).astype('float32')\n\n# Detectar\ntodos_datos = np.vstack([datos_normales, datos_anomalos])\nes_anomalia, errores = detectar_anomalias(autoencoder, todos_datos)\n\nprint(f\"Anomal\u00edas detectadas en datos normales: {es_anomalia[:100].sum()}\")\nprint(f\"Anomal\u00edas detectadas en datos an\u00f3malos: {es_anomalia[100:].sum()}\")\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#eliminacion-de-ruido-denoising","title":"Eliminaci\u00f3n de Ruido (Denoising)","text":"<pre><code># A\u00f1adir ruido a las im\u00e1genes\nnoise_factor = 0.5\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n# Entrenar para reconstruir im\u00e1genes limpias desde ruidosas\ndenoising_autoencoder = autoencoder\ndenoising_autoencoder.fit(\n    x_train_noisy, x_train,  # Entrada ruidosa, objetivo limpio\n    epochs=50,\n    batch_size=256,\n    validation_data=(x_test_noisy, x_test)\n)\n\n# Visualizar denoising\ndenoised_imgs = denoising_autoencoder.predict(x_test_noisy)\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#54-autoencoder-convolucional","title":"5.4. Autoencoder Convolucional","text":"<p>Para im\u00e1genes, usar capas convolucionales es m\u00e1s efectivo:</p> <pre><code>from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n\n# Encoder Convolucional\nencoder_conv = Sequential([\n    Input(shape=(28, 28, 1)),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2), padding='same'),\n    Conv2D(16, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2), padding='same'),\n    Conv2D(8, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2), padding='same')\n], name='encoder_conv')\n\n# Decoder Convolucional\ndecoder_conv = Sequential([\n    Input(shape=(4, 4, 8)),\n    Conv2D(8, (3, 3), activation='relu', padding='same'),\n    UpSampling2D((2, 2)),\n    Conv2D(16, (3, 3), activation='relu', padding='same'),\n    UpSampling2D((2, 2)),\n    Conv2D(32, (3, 3), activation='relu'),\n    UpSampling2D((2, 2)),\n    Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n], name='decoder_conv')\n\n# Autoencoder completo\ninput_img = Input(shape=(28, 28, 1))\nencoded = encoder_conv(input_img)\ndecoded = decoder_conv(encoded)\nconv_autoencoder = Model(input_img, decoded)\n\nconv_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#55-variational-autoencoder-vae","title":"5.5. Variational Autoencoder (VAE)","text":"<p>Los VAE son autoencoders generativos que aprenden una distribuci\u00f3n probabil\u00edstica en el espacio latente.</p>"},{"location":"deep-learning/05-autoencoders/#diferencias-con-autoencoder-normal","title":"Diferencias con Autoencoder Normal","text":"Autoencoder VAE Espacio latente: vectores fijos Espacio latente: distribuci\u00f3n probabil\u00edstica No puede generar nuevos datos Puede generar datos muestreando del espacio latente Minimiza error de reconstrucci\u00f3n Minimiza reconstrucci\u00f3n + KL divergence"},{"location":"deep-learning/05-autoencoders/#espacio-latente-del-vae","title":"Espacio Latente del VAE","text":"<p>En lugar de codificar a un vector \\(z\\), codificamos a:</p> <ul> <li>\\(\\mu\\) (media)</li> <li>\\(\\sigma\\) (desviaci\u00f3n est\u00e1ndar)</li> </ul> <p>Y muestreamos: \\(z = \\mu + \\sigma \\cdot \\epsilon\\), donde \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\)</p>"},{"location":"deep-learning/05-autoencoders/#funcion-de-perdida","title":"Funci\u00f3n de P\u00e9rdida","text":"\\[\\mathcal{L} = \\mathcal{L}_{reconstrucci\u00f3n} + D_{KL}(\\mathcal{N}(\\mu, \\sigma) || \\mathcal{N}(0, 1))\\] <p>El t\u00e9rmino KL fuerza que la distribuci\u00f3n latente se parezca a una normal est\u00e1ndar.</p>"},{"location":"deep-learning/05-autoencoders/#implementacion-de-vae","title":"Implementaci\u00f3n de VAE","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Lambda\n\nclass Sampling(tf.keras.layers.Layer):\n    \"\"\"Capa de muestreo usando el truco de reparametrizaci\u00f3n.\"\"\"\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.random.normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n# Dimensiones\noriginal_dim = 784\nintermediate_dim = 256\nlatent_dim = 2  # Usamos 2 para visualizaci\u00f3n\n\n# Encoder\nencoder_inputs = Input(shape=(original_dim,), name='encoder_input')\nx = Dense(intermediate_dim, activation='relu')(encoder_inputs)\nz_mean = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\nz = Sampling()([z_mean, z_log_var])\nencoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n\n# Decoder\nlatent_inputs = Input(shape=(latent_dim,), name='z_sampling')\nx = Dense(intermediate_dim, activation='relu')(latent_inputs)\ndecoder_outputs = Dense(original_dim, activation='sigmoid')(x)\ndecoder = Model(latent_inputs, decoder_outputs, name='decoder')\n\n# VAE completo\nclass VAE(Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data)\n            reconstruction = self.decoder(z)\n\n            # P\u00e9rdida de reconstrucci\u00f3n\n            reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    tf.keras.losses.binary_crossentropy(data, reconstruction),\n                    axis=-1\n                )\n            )\n\n            # KL Divergence\n            kl_loss = -0.5 * tf.reduce_mean(\n                tf.reduce_sum(\n                    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n                    axis=-1\n                )\n            )\n\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {\n            \"loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }\n\n# Crear y entrenar VAE\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=tf.keras.optimizers.Adam())\nvae.fit(x_train, epochs=30, batch_size=128)\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#visualizar-espacio-latente-del-vae","title":"Visualizar Espacio Latente del VAE","text":"<pre><code># Codificar datos de test\nz_mean, _, _ = vae.encoder.predict(x_test)\n\n# Visualizar espacio latente 2D\nplt.figure(figsize=(12, 10))\nscatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test, cmap='tab10', alpha=0.5)\nplt.colorbar(scatter)\nplt.xlabel('z[0]')\nplt.ylabel('z[1]')\nplt.title('Espacio Latente del VAE')\nplt.show()\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#generar-nuevos-datos","title":"Generar Nuevos Datos","text":"<pre><code>def generar_digitos(decoder, n=15, digit_size=28, figsize=15):\n    \"\"\"Genera una cuadr\u00edcula de d\u00edgitos muestreando del espacio latente.\"\"\"\n    # Cuadr\u00edcula en el espacio latente\n    grid_x = np.linspace(-3, 3, n)\n    grid_y = np.linspace(-3, 3, n)[::-1]\n\n    figure = np.zeros((digit_size * n, digit_size * n))\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample, verbose=0)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[\n                i * digit_size: (i + 1) * digit_size,\n                j * digit_size: (j + 1) * digit_size\n            ] = digit\n\n    plt.figure(figsize=(figsize, figsize))\n    plt.imshow(figure, cmap='Greys')\n    plt.axis('off')\n    plt.title('D\u00edgitos Generados desde el Espacio Latente')\n    plt.show()\n\ngenerar_digitos(vae.decoder)\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#56-tipos-de-autoencoders","title":"5.6. Tipos de Autoencoders","text":"Tipo Caracter\u00edstica Uso Principal Undercomplete Dimensi\u00f3n latente &lt; input Compresi\u00f3n, reducci\u00f3n dimensionalidad Overcomplete Dimensi\u00f3n latente &gt; input Necesita regularizaci\u00f3n extra Sparse Penaliza activaciones densas Features dispersas Denoising Entrena con ruido Eliminaci\u00f3n de ruido Contractive Penaliza sensibilidad a perturbaciones Representaciones robustas Variational (VAE) Espacio latente probabil\u00edstico Generaci\u00f3n de datos"},{"location":"deep-learning/05-autoencoders/#sparse-autoencoder","title":"Sparse Autoencoder","text":"<pre><code>from tensorflow.keras import regularizers\n\n# Autoencoder con regularizaci\u00f3n L1 (sparsity)\nsparse_encoder = Sequential([\n    Input(shape=(784,)),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu', \n          activity_regularizer=regularizers.l1(1e-5)),  # Sparsity\n    Dense(32, activation='relu')\n])\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#contractive-autoencoder","title":"Contractive Autoencoder","text":"<pre><code>class ContractiveAutoencoder(Model):\n    def __init__(self, encoder, decoder, lambda_contractive=1e-4):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.lambda_contractive = lambda_contractive\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            tape.watch(data)\n            with tf.GradientTape() as inner_tape:\n                inner_tape.watch(data)\n                encoded = self.encoder(data)\n\n            # Jacobiano del encoder\n            jacobian = inner_tape.batch_jacobian(encoded, data)\n\n            decoded = self.decoder(encoded)\n\n            # P\u00e9rdida de reconstrucci\u00f3n\n            reconstruction_loss = tf.reduce_mean(tf.square(data - decoded))\n\n            # Penalizaci\u00f3n contractiva (norma Frobenius del Jacobiano)\n            contractive_loss = tf.reduce_mean(tf.reduce_sum(tf.square(jacobian), axis=[1, 2]))\n\n            total_loss = reconstruction_loss + self.lambda_contractive * contractive_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return {\"loss\": total_loss}\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#57-vae-para-generacion-de-imagenes","title":"5.7. VAE para Generaci\u00f3n de Im\u00e1genes","text":"<pre><code>from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Flatten, Reshape\n\n# VAE Convolucional para im\u00e1genes\nlatent_dim = 2\n\n# Encoder\nencoder_inputs = Input(shape=(28, 28, 1))\nx = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\nx = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = Flatten()(x)\nx = Dense(16, activation=\"relu\")(x)\nz_mean = Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\nz = Sampling()([z_mean, z_log_var])\nencoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\n# Decoder\nlatent_inputs = Input(shape=(latent_dim,))\nx = Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\nx = Reshape((7, 7, 64))(x)\nx = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\ndecoder_outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#58-aplicaciones-avanzadas","title":"5.8. Aplicaciones Avanzadas","text":""},{"location":"deep-learning/05-autoencoders/#interpolacion-en-el-espacio-latente","title":"Interpolaci\u00f3n en el Espacio Latente","text":"<pre><code>def interpolar(decoder, z1, z2, n_pasos=10):\n    \"\"\"Interpola entre dos puntos en el espacio latente.\"\"\"\n    ratios = np.linspace(0, 1, n_pasos)\n\n    plt.figure(figsize=(20, 2))\n    for i, ratio in enumerate(ratios):\n        z_interp = z1 * (1 - ratio) + z2 * ratio\n        z_interp = z_interp.reshape(1, -1)\n        img = decoder.predict(z_interp, verbose=0)\n\n        plt.subplot(1, n_pasos, i + 1)\n        plt.imshow(img.reshape(28, 28), cmap='gray')\n        plt.axis('off')\n\n    plt.suptitle('Interpolaci\u00f3n en el Espacio Latente')\n    plt.show()\n\n# Ejemplo: interpolar entre dos d\u00edgitos\nz_mean_test, _, _ = vae.encoder.predict(x_test[:2])\ninterpolar(vae.decoder, z_mean_test[0], z_mean_test[1])\n</code></pre>"},{"location":"deep-learning/05-autoencoders/#aritmetica-en-el-espacio-latente","title":"Aritm\u00e9tica en el Espacio Latente","text":"<pre><code># Concepto: z(sonrisa) = z(cara_sonriendo) - z(cara_neutral) + z(otra_cara_neutral)\n# Resultado: otra_cara deber\u00eda sonre\u00edr\n\ndef aritmetica_latente(encoder, decoder, img_a, img_b, img_c):\n    \"\"\"\n    Calcula: resultado = z_c + (z_a - z_b)\n    Ejemplo: cara_c + (cara_sonriente - cara_neutral)\n    \"\"\"\n    z_a, _, _ = encoder.predict(img_a.reshape(1, -1))\n    z_b, _, _ = encoder.predict(img_b.reshape(1, -1))\n    z_c, _, _ = encoder.predict(img_c.reshape(1, -1))\n\n    z_resultado = z_c + (z_a - z_b)\n    resultado = decoder.predict(z_resultado)\n\n    return resultado\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/06-gans/","title":"\ud83c\udfad Unidad 6. Redes Generativas Adversarias (GANs)","text":"<p>Las Generative Adversarial Networks (GANs) son un paradigma de aprendizaje donde dos redes neuronales compiten entre s\u00ed, permitiendo generar datos sint\u00e9ticos de alta calidad como im\u00e1genes, audio y texto.</p>"},{"location":"deep-learning/06-gans/#61-concepto-de-gans","title":"6.1. Concepto de GANs","text":"<p>Introducidas por Ian Goodfellow en 2014, las GANs consisten en dos redes que se entrenan simult\u00e1neamente:</p>"},{"location":"deep-learning/06-gans/#componentes","title":"Componentes","text":"<ul> <li>Generador (G): Genera datos falsos a partir de ruido aleatorio.</li> <li>Discriminador (D): Distingue entre datos reales y falsos.</li> </ul>"},{"location":"deep-learning/06-gans/#analogia","title":"Analog\u00eda","text":"<pre><code>Generador (Falsificador)           Discriminador (Detective)\n        \u2502                                    \u2502\n        \u2502 Crea billetes falsos               \u2502 Detecta billetes falsos\n        \u2502                                    \u2502\n        \u25bc                                    \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Ruido z \u2502\u2500\u2500\u25b6 G(z) \u2500\u2500\u25b6 Imagen \u2500\u2500\u25b6  \u2502   D     \u2502\u2500\u2500\u25b6 Real/Falso\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     falsa               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                             \u25b2\n                                             \u2502\n                                       Imagen real\n</code></pre>"},{"location":"deep-learning/06-gans/#objetivo","title":"Objetivo","text":"<ul> <li>G quiere: Enga\u00f1ar a D (que D piense que sus im\u00e1genes son reales).</li> <li>D quiere: Distinguir correctamente entre real y falso.</li> </ul> <p>Este \"juego\" lleva a que G genere datos cada vez m\u00e1s realistas.</p>"},{"location":"deep-learning/06-gans/#62-formulacion-matematica","title":"6.2. Formulaci\u00f3n Matem\u00e1tica","text":""},{"location":"deep-learning/06-gans/#funcion-de-perdida","title":"Funci\u00f3n de P\u00e9rdida","text":"<p>El entrenamiento de GANs es un juego minimax:</p> \\[\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\\] <p>Donde:</p> <ul> <li>\\(x\\) = datos reales</li> <li>\\(z\\) = ruido aleatorio (vector latente)</li> <li>\\(G(z)\\) = dato generado</li> <li>\\(D(x)\\) = probabilidad de que \\(x\\) sea real</li> </ul>"},{"location":"deep-learning/06-gans/#interpretacion","title":"Interpretaci\u00f3n","text":"<ul> <li>D maximiza: \\(\\log D(x)\\) (clasificar real como real) + \\(\\log(1-D(G(z)))\\) (clasificar falso como falso).</li> <li>G minimiza: \\(\\log(1-D(G(z)))\\) equivale a maximizar \\(\\log D(G(z))\\) (enga\u00f1ar a D).</li> </ul>"},{"location":"deep-learning/06-gans/#63-implementacion-basica-de-una-gan","title":"6.3. Implementaci\u00f3n B\u00e1sica de una GAN","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Hiperpar\u00e1metros\nlatent_dim = 100  # Dimensi\u00f3n del ruido de entrada\nimg_shape = (28, 28, 1)\nimg_dim = 28 * 28\n\n# =====================\n# GENERADOR\n# =====================\ndef build_generator():\n    model = Sequential([\n        Dense(256, input_dim=latent_dim),\n        LeakyReLU(alpha=0.2),\n        BatchNormalization(momentum=0.8),\n\n        Dense(512),\n        LeakyReLU(alpha=0.2),\n        BatchNormalization(momentum=0.8),\n\n        Dense(1024),\n        LeakyReLU(alpha=0.2),\n        BatchNormalization(momentum=0.8),\n\n        Dense(img_dim, activation='tanh'),\n        Reshape(img_shape)\n    ], name='generator')\n    return model\n\n# =====================\n# DISCRIMINADOR\n# =====================\ndef build_discriminator():\n    model = Sequential([\n        Flatten(input_shape=img_shape),\n\n        Dense(512),\n        LeakyReLU(alpha=0.2),\n\n        Dense(256),\n        LeakyReLU(alpha=0.2),\n\n        Dense(1, activation='sigmoid')  # Probabilidad de ser real\n    ], name='discriminator')\n    return model\n\n# Crear modelos\ngenerator = build_generator()\ndiscriminator = build_discriminator()\n\n# Compilar discriminador\ndiscriminator.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n    metrics=['accuracy']\n)\n\ngenerator.summary()\ndiscriminator.summary()\n</code></pre>"},{"location":"deep-learning/06-gans/#crear-el-modelo-gan-combinado","title":"Crear el Modelo GAN Combinado","text":"<pre><code># Para entrenar el generador, congelamos el discriminador\ndiscriminator.trainable = False\n\n# Modelo combinado: Generador + Discriminador\ngan_input = tf.keras.Input(shape=(latent_dim,))\ngenerated_img = generator(gan_input)\nvalidity = discriminator(generated_img)\n\ngan = tf.keras.Model(gan_input, validity, name='gan')\ngan.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(0.0002, 0.5)\n)\n</code></pre>"},{"location":"deep-learning/06-gans/#64-entrenamiento-de-la-gan","title":"6.4. Entrenamiento de la GAN","text":"<pre><code>def train_gan(epochs, batch_size=128, sample_interval=1000):\n    # Cargar datos\n    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # Preprocesar: normalizar a [-1, 1] (para tanh)\n    X_train = X_train / 127.5 - 1.0\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Etiquetas\n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    d_losses = []\n    g_losses = []\n\n    for epoch in range(epochs):\n        # =====================\n        # Entrenar Discriminador\n        # =====================\n\n        # Seleccionar batch aleatorio de im\u00e1genes reales\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        real_imgs = X_train[idx]\n\n        # Generar im\u00e1genes falsas\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        gen_imgs = generator.predict(noise, verbose=0)\n\n        # Entrenar discriminador\n        d_loss_real = discriminator.train_on_batch(real_imgs, real)\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # =====================\n        # Entrenar Generador\n        # =====================\n\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n        # Entrenar generador (queremos que D clasifique las falsas como reales)\n        g_loss = gan.train_on_batch(noise, real)\n\n        # Guardar p\u00e9rdidas\n        d_losses.append(d_loss[0])\n        g_losses.append(g_loss)\n\n        # Imprimir progreso\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch} - D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.1f}% - G loss: {g_loss:.4f}\")\n\n        # Guardar im\u00e1genes de muestra\n        if epoch % sample_interval == 0:\n            sample_images(epoch)\n\n    return d_losses, g_losses\n\ndef sample_images(epoch, n=5):\n    \"\"\"Genera y guarda im\u00e1genes de muestra.\"\"\"\n    noise = np.random.normal(0, 1, (n * n, latent_dim))\n    gen_imgs = generator.predict(noise, verbose=0)\n\n    # Reescalar a [0, 1]\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    fig, axes = plt.subplots(n, n, figsize=(10, 10))\n    cnt = 0\n    for i in range(n):\n        for j in range(n):\n            axes[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n            axes[i, j].axis('off')\n            cnt += 1\n    plt.suptitle(f'Epoch {epoch}')\n    plt.savefig(f'gan_images_epoch_{epoch}.png')\n    plt.close()\n\n# Entrenar\nd_losses, g_losses = train_gan(epochs=30000, batch_size=64, sample_interval=2000)\n</code></pre>"},{"location":"deep-learning/06-gans/#65-deep-convolutional-gan-dcgan","title":"6.5. Deep Convolutional GAN (DCGAN)","text":"<p>Las DCGAN usan capas convolucionales para generar im\u00e1genes de mayor calidad.</p>"},{"location":"deep-learning/06-gans/#principios-de-arquitectura-dcgan","title":"Principios de Arquitectura DCGAN","text":"<ol> <li>Usar convoluciones transpuestas en el generador.</li> <li>Usar BatchNormalization en ambas redes.</li> <li>Usar LeakyReLU en el discriminador.</li> <li>Usar ReLU en el generador (excepto salida con tanh).</li> </ol> <pre><code>from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout\n\ndef build_dcgan_generator(latent_dim):\n    model = Sequential([\n        # Entrada: vector de ruido\n        Dense(7 * 7 * 256, input_dim=latent_dim),\n        Reshape((7, 7, 256)),\n\n        # Upsample a 14x14\n        Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n        BatchNormalization(),\n        LeakyReLU(alpha=0.2),\n\n        # Upsample a 28x28\n        Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n        BatchNormalization(),\n        LeakyReLU(alpha=0.2),\n\n        # Salida: imagen 28x28x1\n        Conv2D(1, (7, 7), padding='same', activation='tanh')\n    ], name='dcgan_generator')\n    return model\n\ndef build_dcgan_discriminator():\n    model = Sequential([\n        # Entrada: imagen 28x28x1\n        Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=(28, 28, 1)),\n        LeakyReLU(alpha=0.2),\n        Dropout(0.25),\n\n        Conv2D(128, (3, 3), strides=(2, 2), padding='same'),\n        LeakyReLU(alpha=0.2),\n        Dropout(0.25),\n        BatchNormalization(),\n\n        Conv2D(256, (3, 3), strides=(2, 2), padding='same'),\n        LeakyReLU(alpha=0.2),\n        Dropout(0.25),\n        BatchNormalization(),\n\n        Flatten(),\n        Dense(1, activation='sigmoid')\n    ], name='dcgan_discriminator')\n    return model\n</code></pre>"},{"location":"deep-learning/06-gans/#66-problemas-comunes-y-soluciones","title":"6.6. Problemas Comunes y Soluciones","text":""},{"location":"deep-learning/06-gans/#mode-collapse","title":"Mode Collapse","text":"<p>El generador produce solo unos pocos tipos de salidas.</p> <p>Soluciones:</p> <ul> <li>Mini-batch discrimination.</li> <li>Unrolled GANs.</li> <li>Wasserstein GAN (WGAN).</li> </ul>"},{"location":"deep-learning/06-gans/#entrenamiento-inestable","title":"Entrenamiento Inestable","text":"<p>El discriminador o generador dominan.</p> <p>Soluciones:</p> <ul> <li>Balancear learning rates.</li> <li>Label smoothing.</li> <li>Spectral normalization.</li> </ul>"},{"location":"deep-learning/06-gans/#vanishing-gradients","title":"Vanishing Gradients","text":"<p>El discriminador se vuelve muy bueno y el generador no recibe gradientes \u00fatiles.</p> <p>Soluciones:</p> <ul> <li>Usar p\u00e9rdida de Wasserstein.</li> <li>Feature matching.</li> </ul>"},{"location":"deep-learning/06-gans/#67-wasserstein-gan-wgan","title":"6.7. Wasserstein GAN (WGAN)","text":"<p>WGAN usa la distancia de Wasserstein para una m\u00e9trica de entrenamiento m\u00e1s estable.</p>"},{"location":"deep-learning/06-gans/#cambios-principales","title":"Cambios Principales","text":"<ol> <li>P\u00e9rdida: Distancia de Wasserstein en lugar de binary crossentropy.</li> <li>Discriminador \u2192 Cr\u00edtico: No produce probabilidad, sino un score.</li> <li>Clipping de pesos: Los pesos del cr\u00edtico se limitan a [-c, c].</li> </ol> <pre><code>from tensorflow.keras import backend as K\n\ndef wasserstein_loss(y_true, y_pred):\n    \"\"\"P\u00e9rdida de Wasserstein.\"\"\"\n    return K.mean(y_true * y_pred)\n\ndef build_critic():\n    \"\"\"El cr\u00edtico de WGAN (no usa sigmoid en la salida).\"\"\"\n    model = Sequential([\n        Flatten(input_shape=(28, 28, 1)),\n        Dense(512),\n        LeakyReLU(alpha=0.2),\n        Dense(256),\n        LeakyReLU(alpha=0.2),\n        Dense(1)  # Sin activaci\u00f3n sigmoid\n    ])\n    return model\n\n# Compilar con p\u00e9rdida Wasserstein\ncritic = build_critic()\ncritic.compile(\n    loss=wasserstein_loss,\n    optimizer=tf.keras.optimizers.RMSprop(lr=0.00005)\n)\n\ndef train_wgan(epochs, batch_size=64, n_critic=5, clip_value=0.01):\n    \"\"\"\n    Entrenar WGAN.\n    n_critic: n\u00famero de veces que se entrena el cr\u00edtico por cada vez del generador.\n    \"\"\"\n    (X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n    X_train = X_train / 127.5 - 1.0\n    X_train = np.expand_dims(X_train, axis=3)\n\n    real = np.ones((batch_size, 1))\n    fake = -np.ones((batch_size, 1))  # -1 para falsas en WGAN\n\n    for epoch in range(epochs):\n        # Entrenar cr\u00edtico n_critic veces\n        for _ in range(n_critic):\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            real_imgs = X_train[idx]\n\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            gen_imgs = generator.predict(noise, verbose=0)\n\n            critic.train_on_batch(real_imgs, real)\n            critic.train_on_batch(gen_imgs, fake)\n\n            # Clipping de pesos\n            for layer in critic.layers:\n                weights = layer.get_weights()\n                weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n                layer.set_weights(weights)\n\n        # Entrenar generador\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        gan.train_on_batch(noise, real)\n</code></pre>"},{"location":"deep-learning/06-gans/#68-conditional-gan-cgan","title":"6.8. Conditional GAN (cGAN)","text":"<p>Las cGAN permiten condicionar la generaci\u00f3n en una etiqueta o clase.</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Ruido \u2502\u2500\u2500\u2500\u2500\u25b6\u2502           \u2502\n    \u2502   z   \u2502     \u2502 Generador \u2502\u2500\u2500\u25b6 Imagen de d\u00edgito \"7\"\n    \u2502       \u2502     \u2502     G     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502           \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502           \u2502\n    \u2502 Label \u2502\u2500\u2500\u2500\u2500\u25b6\u2502           \u2502\n    \u2502  \"7\"  \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-learning/06-gans/#implementacion","title":"Implementaci\u00f3n","text":"<pre><code>from tensorflow.keras.layers import Embedding, Concatenate, Input\nfrom tensorflow.keras import Model\n\nnum_classes = 10  # D\u00edgitos 0-9\n\ndef build_cgan_generator(latent_dim, num_classes):\n    # Entrada de ruido\n    noise_input = Input(shape=(latent_dim,))\n\n    # Entrada de etiqueta (embedding)\n    label_input = Input(shape=(1,), dtype='int32')\n    label_embedding = Embedding(num_classes, latent_dim)(label_input)\n    label_embedding = Flatten()(label_embedding)\n\n    # Concatenar ruido y etiqueta\n    merged = Concatenate()([noise_input, label_embedding])\n\n    # Generador\n    x = Dense(256)(merged)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization()(x)\n    x = Dense(512)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization()(x)\n    x = Dense(1024)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization()(x)\n    x = Dense(784, activation='tanh')(x)\n    output = Reshape((28, 28, 1))(x)\n\n    return Model([noise_input, label_input], output, name='cgan_generator')\n\ndef build_cgan_discriminator(num_classes):\n    # Entrada de imagen\n    img_input = Input(shape=(28, 28, 1))\n    img_flat = Flatten()(img_input)\n\n    # Entrada de etiqueta\n    label_input = Input(shape=(1,), dtype='int32')\n    label_embedding = Embedding(num_classes, 784)(label_input)\n    label_embedding = Flatten()(label_embedding)\n\n    # Concatenar imagen y etiqueta\n    merged = Concatenate()([img_flat, label_embedding])\n\n    x = Dense(512)(merged)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(256)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    output = Dense(1, activation='sigmoid')(x)\n\n    return Model([img_input, label_input], output, name='cgan_discriminator')\n\n# Crear modelos\ncgan_generator = build_cgan_generator(latent_dim, num_classes)\ncgan_discriminator = build_cgan_discriminator(num_classes)\n\n# Generar d\u00edgitos espec\u00edficos\ndef generate_digit(generator, digit, n=10):\n    \"\"\"Genera n im\u00e1genes del d\u00edgito especificado.\"\"\"\n    noise = np.random.normal(0, 1, (n, latent_dim))\n    labels = np.full((n, 1), digit)\n\n    gen_imgs = generator.predict([noise, labels], verbose=0)\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    fig, axes = plt.subplots(1, n, figsize=(20, 2))\n    for i in range(n):\n        axes[i].imshow(gen_imgs[i, :, :, 0], cmap='gray')\n        axes[i].axis('off')\n    plt.suptitle(f'D\u00edgito generado: {digit}')\n    plt.show()\n\n# Ejemplo: generar varios \"7\"\ngenerate_digit(cgan_generator, digit=7)\n</code></pre>"},{"location":"deep-learning/06-gans/#69-otras-variantes-de-gans","title":"6.9. Otras Variantes de GANs","text":"Variante Caracter\u00edstica Aplicaci\u00f3n DCGAN Usa convolutions Im\u00e1genes de mayor calidad WGAN P\u00e9rdida de Wasserstein Entrenamiento m\u00e1s estable cGAN Condicionado en etiquetas Generaci\u00f3n controlada Pix2Pix Image-to-image translation Convertir bocetos a im\u00e1genes CycleGAN Traducci\u00f3n sin pares Convertir fotos a estilo art\u00edstico StyleGAN Control de estilo por capas Caras realistas de alta resoluci\u00f3n ProGAN Entrenamiento progresivo Im\u00e1genes de muy alta resoluci\u00f3n"},{"location":"deep-learning/06-gans/#610-aplicaciones-de-gans","title":"6.10. Aplicaciones de GANs","text":""},{"location":"deep-learning/06-gans/#generacion-de-imagenes-realistas","title":"Generaci\u00f3n de Im\u00e1genes Realistas","text":"<pre><code># StyleGAN2 con TensorFlow Hub\nimport tensorflow_hub as hub\n\n# Cargar modelo preentrenado\nstylegan = hub.load('https://tfhub.dev/google/progan-128/1')\n\n# Generar im\u00e1genes\nlatent = tf.random.normal([1, 512])\nimages = stylegan(latent)['default']\n</code></pre>"},{"location":"deep-learning/06-gans/#super-resolucion-srgan","title":"Super-Resoluci\u00f3n (SRGAN)","text":"<pre><code># Aumentar resoluci\u00f3n de im\u00e1genes\n# Low-res (64x64) \u2192 High-res (256x256)\n</code></pre>"},{"location":"deep-learning/06-gans/#transferencia-de-estilo","title":"Transferencia de Estilo","text":"<pre><code># CycleGAN: Foto \u2192 Pintura de Monet\n# Sin necesidad de pares de entrenamiento\n</code></pre>"},{"location":"deep-learning/06-gans/#data-augmentation","title":"Data Augmentation","text":"<pre><code># Generar datos sint\u00e9ticos para entrenar otros modelos\n# \u00datil cuando hay pocos datos reales\n</code></pre>"},{"location":"deep-learning/06-gans/#611-metricas-de-evaluacion","title":"6.11. M\u00e9tricas de Evaluaci\u00f3n","text":""},{"location":"deep-learning/06-gans/#inception-score-is","title":"Inception Score (IS)","text":"<p>Mide calidad y diversidad:</p> \\[IS = \\exp(\\mathbb{E}_x[D_{KL}(p(y|x) || p(y))])\\] <pre><code># Calcular Inception Score (simplificado)\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\ndef inception_score(images, n_split=10, eps=1e-16):\n    inception = InceptionV3(include_top=False, pooling='avg')\n    # ... c\u00e1lculo completo\n    pass\n</code></pre>"},{"location":"deep-learning/06-gans/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>Compara distribuci\u00f3n de caracter\u00edsticas:</p> \\[FID = ||\\mu_r - \\mu_g||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r\\Sigma_g)^{1/2})\\] <p>Menor FID = mejor calidad y diversidad.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/07-transfer-learning/","title":"\ud83d\udd04 Unidad 7. Transfer Learning y Fine-Tuning","text":"<p>Transfer Learning es una t\u00e9cnica que aprovecha el conocimiento aprendido en un problema para aplicarlo a otro diferente. Es una de las t\u00e9cnicas m\u00e1s importantes en Deep Learning moderno.</p>"},{"location":"deep-learning/07-transfer-learning/#71-que-es-transfer-learning","title":"7.1. \u00bfQu\u00e9 es Transfer Learning?","text":""},{"location":"deep-learning/07-transfer-learning/#concepto","title":"Concepto","text":"<p>En lugar de entrenar una red desde cero, usamos una red preentrenada en un dataset grande (ImageNet, Wikipedia, etc.) y la adaptamos a nuestro problema espec\u00edfico.</p> <pre><code>Modelo Preentrenado          Tu Problema\n(ImageNet: 14M im\u00e1genes)     (Tu dataset: 1000 im\u00e1genes)\n        \u2502                            \u2502\n        \u2502 Conocimiento               \u2502 Datos espec\u00edficos\n        \u2502 general                    \u2502\n        \u25bc                            \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502         Transfer Learning           \u2502\n    \u2502    Combina ambos para obtener      \u2502\n    \u2502    un modelo de alta calidad       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#por-que-funciona","title":"\u00bfPor qu\u00e9 Funciona?","text":"<p>Las capas iniciales de una red aprenden caracter\u00edsticas gen\u00e9ricas:</p> <ul> <li>Capas bajas: Bordes, texturas, colores.</li> <li>Capas medias: Patrones m\u00e1s complejos (ojos, ruedas).</li> <li>Capas altas: Conceptos espec\u00edficos del problema.</li> </ul> <p>Las caracter\u00edsticas gen\u00e9ricas son transferibles a otros problemas.</p>"},{"location":"deep-learning/07-transfer-learning/#72-estrategias-de-transfer-learning","title":"7.2. Estrategias de Transfer Learning","text":""},{"location":"deep-learning/07-transfer-learning/#estrategia-1-feature-extraction","title":"Estrategia 1: Feature Extraction","text":"<p>Usar el modelo preentrenado como extractor de caracter\u00edsticas fijas.</p> <pre><code>from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Input\n\n# Cargar modelo preentrenado SIN las capas de clasificaci\u00f3n\nbase_model = VGG16(\n    weights='imagenet',      # Pesos preentrenados\n    include_top=False,       # Excluir capas de clasificaci\u00f3n\n    input_shape=(224, 224, 3)\n)\n\n# CONGELAR todas las capas del modelo base\nbase_model.trainable = False\n\n# A\u00f1adir nuestras capas de clasificaci\u00f3n\nmodel = Sequential([\n    base_model,\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dense(10, activation='softmax')  # 10 clases\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#estrategia-2-fine-tuning","title":"Estrategia 2: Fine-Tuning","text":"<p>Descongelar algunas capas superiores del modelo base y reentrenarlas.</p> <pre><code># Primero: entrenar solo las capas nuevas (feature extraction)\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n\n# Segundo: descongelar algunas capas del modelo base\nbase_model.trainable = True\n\n# Congelar todas las capas excepto las \u00faltimas N\nfor layer in base_model.layers[:-4]:  # Descongelar \u00faltimas 4 capas\n    layer.trainable = False\n\n# Recompilar con learning rate muy bajo\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),  # LR muy bajo\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Entrenar con fine-tuning\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#comparacion-de-estrategias","title":"Comparaci\u00f3n de Estrategias","text":"Estrategia Cu\u00e1ndo Usar Dataset Feature Extraction Dataset peque\u00f1o, similar al original &lt; 10,000 im\u00e1genes Fine-Tuning Parcial Dataset mediano, similar al original 10,000 - 100,000 Fine-Tuning Completo Dataset grande, diferente al original &gt; 100,000"},{"location":"deep-learning/07-transfer-learning/#73-modelos-preentrenados-populares","title":"7.3. Modelos Preentrenados Populares","text":""},{"location":"deep-learning/07-transfer-learning/#para-imagenes-computer-vision","title":"Para Im\u00e1genes (Computer Vision)","text":"<pre><code>from tensorflow.keras.applications import (\n    VGG16, VGG19,\n    ResNet50, ResNet101, ResNet152,\n    InceptionV3, InceptionResNetV2,\n    MobileNetV2, MobileNetV3Small, MobileNetV3Large,\n    EfficientNetB0, EfficientNetB7,\n    DenseNet121, DenseNet201\n)\n\n# Ejemplo: cargar diferentes modelos\nvgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nresnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nefficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#comparacion-de-modelos","title":"Comparaci\u00f3n de Modelos","text":"Modelo Par\u00e1metros Top-1 Acc ImageNet Uso MobileNetV2 3.4M 71.8% Dispositivos m\u00f3viles ResNet50 25.6M 76.0% Balance precisi\u00f3n/velocidad EfficientNetB0 5.3M 77.1% Eficiente EfficientNetB7 66M 84.3% M\u00e1xima precisi\u00f3n VGG16 138M 71.3% Cl\u00e1sico, f\u00e1cil de entender"},{"location":"deep-learning/07-transfer-learning/#74-ejemplo-completo-clasificacion-de-imagenes","title":"7.4. Ejemplo Completo: Clasificaci\u00f3n de Im\u00e1genes","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# =====================\n# PREPARACI\u00d3N DE DATOS\n# =====================\n\n# Data augmentation para entrenamiento\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\n# Solo rescale para validaci\u00f3n\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\n# Cargar datos desde directorio\ntrain_generator = train_datagen.flow_from_directory(\n    'data/train',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    subset='training'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    'data/train',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',\n    subset='validation'\n)\n\nnum_classes = train_generator.num_classes\n\n# =====================\n# CREAR MODELO\n# =====================\n\n# Modelo base preentrenado\nbase_model = EfficientNetB0(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(224, 224, 3)\n)\nbase_model.trainable = False  # Congelar\n\n# Modelo completo\nmodel = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dropout(0.2),\n    Dense(256, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# =====================\n# FASE 1: FEATURE EXTRACTION\n# =====================\n\nprint(\"Fase 1: Feature Extraction\")\nhistory1 = model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n    ]\n)\n\n# =====================\n# FASE 2: FINE-TUNING\n# =====================\n\nprint(\"\\nFase 2: Fine-Tuning\")\n\n# Descongelar las \u00faltimas capas\nbase_model.trainable = True\nfor layer in base_model.layers[:-20]:  # Mantener congeladas las primeras\n    layer.trainable = False\n\n# Recompilar con learning rate bajo\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory2 = model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n    ]\n)\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#75-transfer-learning-para-nlp","title":"7.5. Transfer Learning para NLP","text":""},{"location":"deep-learning/07-transfer-learning/#modelos-preentrenados-de-texto","title":"Modelos Preentrenados de Texto","text":"<ul> <li>BERT: Bidirectional Encoder Representations from Transformers.</li> <li>GPT: Generative Pre-trained Transformer.</li> <li>RoBERTa: Robustly optimized BERT.</li> <li>DistilBERT: Versi\u00f3n ligera de BERT.</li> <li>XLNet, ALBERT, T5, etc.</li> </ul>"},{"location":"deep-learning/07-transfer-learning/#ejemplo-con-hugging-face","title":"Ejemplo con Hugging Face","text":"<pre><code>from transformers import (\n    TFBertForSequenceClassification,\n    BertTokenizer,\n    create_optimizer\n)\nimport tensorflow as tf\n\n# Cargar modelo y tokenizer preentrenados\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = TFBertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2  # Clasificaci\u00f3n binaria\n)\n\n# Tokenizar datos\ndef encode_texts(texts, tokenizer, max_length=128):\n    return tokenizer(\n        texts,\n        truncation=True,\n        padding='max_length',\n        max_length=max_length,\n        return_tensors='tf'\n    )\n\n# Datos de ejemplo\ntrain_texts = [\"I love this movie!\", \"This film was terrible.\"]\ntrain_labels = [1, 0]\n\ntrain_encodings = encode_texts(train_texts, tokenizer)\n\n# Crear dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_labels\n)).batch(8)\n\n# Configurar optimizador con warmup\nnum_train_steps = len(train_dataset) * 3  # 3 \u00e9pocas\noptimizer, schedule = create_optimizer(\n    init_lr=2e-5,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_train_steps // 10\n)\n\n# Compilar\nmodel.compile(\n    optimizer=optimizer,\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\n# Entrenar (fine-tuning)\nmodel.fit(train_dataset, epochs=3)\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#estrategias-de-fine-tuning-para-bert","title":"Estrategias de Fine-Tuning para BERT","text":"<pre><code># Estrategia 1: Congelar BERT, solo entrenar clasificador\nfor layer in model.bert.layers:\n    layer.trainable = False\n\n# Estrategia 2: Descongelar \u00faltimas N capas\nfor layer in model.bert.encoder.layer[:-2]:  # Congelar excepto \u00faltimas 2\n    layer.trainable = False\n\n# Estrategia 3: Learning rate diferente por capa (discriminative fine-tuning)\n# Las capas m\u00e1s profundas tienen LR m\u00e1s bajo\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#76-transfer-learning-con-pytorch","title":"7.6. Transfer Learning con PyTorch","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Cargar modelo preentrenado\nmodel = models.resnet50(pretrained=True)\n\n# Congelar todas las capas\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Reemplazar la \u00faltima capa\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Linear(num_ftrs, 256),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(256, 10)  # 10 clases\n)\n\n# Solo los par\u00e1metros de la nueva capa se entrenan\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n\n# Para fine-tuning posterior\ndef unfreeze_layers(model, n_layers):\n    \"\"\"Descongela las \u00faltimas n_layers.\"\"\"\n    layers = list(model.children())\n    for layer in layers[-n_layers:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n\n# Despu\u00e9s de entrenar las capas nuevas\nunfreeze_layers(model, 3)\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=1e-5  # LR bajo para fine-tuning\n)\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#77-domain-adaptation","title":"7.7. Domain Adaptation","text":"<p>Cuando el dominio fuente y objetivo son diferentes.</p>"},{"location":"deep-learning/07-transfer-learning/#tecnicas","title":"T\u00e9cnicas","text":"<ul> <li>Feature Alignment: Alinear distribuciones de caracter\u00edsticas.</li> <li>Adversarial Training: Discriminador de dominios.</li> <li>Self-Training: Pseudo-etiquetas en dominio objetivo.</li> </ul> <pre><code># Domain Adversarial Neural Network (DANN)\nclass DomainAdversarialModel(tf.keras.Model):\n    def __init__(self, feature_extractor, classifier, domain_discriminator):\n        super().__init__()\n        self.feature_extractor = feature_extractor\n        self.classifier = classifier\n        self.domain_discriminator = domain_discriminator\n\n    def call(self, inputs, training=False):\n        features = self.feature_extractor(inputs)\n        class_output = self.classifier(features)\n\n        # Gradient reversal para el discriminador de dominio\n        reversed_features = gradient_reversal(features)\n        domain_output = self.domain_discriminator(reversed_features)\n\n        return class_output, domain_output\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#78-knowledge-distillation","title":"7.8. Knowledge Distillation","text":"<p>Transferir conocimiento de un modelo grande (teacher) a uno peque\u00f1o (student).</p> <pre><code>import tensorflow as tf\n\nclass DistillationLoss(tf.keras.losses.Loss):\n    def __init__(self, temperature=3.0, alpha=0.5):\n        super().__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n\n    def call(self, y_true, y_pred, teacher_logits):\n        # P\u00e9rdida est\u00e1ndar\n        hard_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n\n        # P\u00e9rdida de destilaci\u00f3n (soft targets)\n        soft_teacher = tf.nn.softmax(teacher_logits / self.temperature)\n        soft_student = tf.nn.softmax(y_pred / self.temperature)\n        soft_loss = tf.keras.losses.categorical_crossentropy(soft_teacher, soft_student)\n\n        # Combinar\n        return self.alpha * hard_loss + (1 - self.alpha) * self.temperature**2 * soft_loss\n\n# Entrenamiento con destilaci\u00f3n\ndef train_with_distillation(student, teacher, train_data, epochs):\n    teacher.trainable = False  # Teacher congelado\n\n    for epoch in range(epochs):\n        for x_batch, y_batch in train_data:\n            with tf.GradientTape() as tape:\n                # Predicciones del teacher\n                teacher_logits = teacher(x_batch, training=False)\n\n                # Predicciones del student\n                student_logits = student(x_batch, training=True)\n\n                # P\u00e9rdida combinada\n                loss = distillation_loss(y_batch, student_logits, teacher_logits)\n\n            gradients = tape.gradient(loss, student.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, student.trainable_variables))\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#79-mejores-practicas","title":"7.9. Mejores Pr\u00e1cticas","text":""},{"location":"deep-learning/07-transfer-learning/#preprocesamiento","title":"Preprocesamiento","text":"<p>Usar el mismo preprocesamiento que el modelo preentrenado:</p> <pre><code>from tensorflow.keras.applications.efficientnet import preprocess_input\n\n# EfficientNet espera valores en [0, 255]\nx_train_processed = preprocess_input(x_train)\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#learning-rate","title":"Learning Rate","text":"<ul> <li>Feature extraction: LR normal (1e-3 a 1e-4).</li> <li>Fine-tuning: LR muy bajo (1e-5 a 1e-6).</li> </ul>"},{"location":"deep-learning/07-transfer-learning/#batch-size","title":"Batch Size","text":"<ul> <li>Con datasets peque\u00f1os, usar batch sizes peque\u00f1os.</li> <li>Considerar acumulaci\u00f3n de gradientes si la GPU no tiene suficiente memoria.</li> </ul>"},{"location":"deep-learning/07-transfer-learning/#regularizacion","title":"Regularizaci\u00f3n","text":"<pre><code># A\u00f1adir regularizaci\u00f3n para evitar overfitting\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\n\nmodel = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(256, activation='relu', kernel_regularizer='l2'),\n    Dropout(0.3),\n    Dense(num_classes, activation='softmax')\n])\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#early-stopping","title":"Early Stopping","text":"<pre><code>callbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-7\n    )\n]\n</code></pre>"},{"location":"deep-learning/07-transfer-learning/#710-cuando-no-usar-transfer-learning","title":"7.10. Cu\u00e1ndo NO Usar Transfer Learning","text":"<ul> <li>Dominio muy diferente: Im\u00e1genes m\u00e9dicas \u2192 pocas similitudes con ImageNet.</li> <li>Mucho datos propios: Si tienes millones de im\u00e1genes etiquetadas.</li> <li>Requerimientos espec\u00edficos: Arquitectura o restricciones especiales.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"deep-learning/08-frameworks-practica/","title":"\ud83d\udee0\ufe0f Unidad 8. Frameworks y Pr\u00e1ctica con Deep Learning","text":"<p>Esta unidad cubre los principales frameworks de Deep Learning, herramientas de desarrollo, y mejores pr\u00e1cticas para proyectos de producci\u00f3n.</p>"},{"location":"deep-learning/08-frameworks-practica/#81-comparacion-de-frameworks","title":"8.1. Comparaci\u00f3n de Frameworks","text":""},{"location":"deep-learning/08-frameworks-practica/#tensorflow-vs-pytorch","title":"TensorFlow vs PyTorch","text":"Caracter\u00edstica TensorFlow PyTorch Desarrollador Google Facebook (Meta) Paradigma Grafo est\u00e1tico \u2192 Eager execution Eager execution (din\u00e1mico) API de Alto Nivel Keras (integrado) torch.nn Debugging TensorBoard, tf.debugging PyDB, hooks Producci\u00f3n TF Serving, TF Lite, TF.js TorchServe, ONNX Comunidad Industria, producci\u00f3n Investigaci\u00f3n, academia Curva de Aprendizaje Moderada M\u00e1s intuitivo"},{"location":"deep-learning/08-frameworks-practica/#recomendacion","title":"Recomendaci\u00f3n","text":"<ul> <li>TensorFlow/Keras: Producci\u00f3n, despliegue m\u00f3vil, principiantes.</li> <li>PyTorch: Investigaci\u00f3n, prototipado r\u00e1pido, flexibilidad.</li> </ul>"},{"location":"deep-learning/08-frameworks-practica/#82-tensorflow-y-keras","title":"8.2. TensorFlow y Keras","text":""},{"location":"deep-learning/08-frameworks-practica/#arquitectura-de-tensorflow-2x","title":"Arquitectura de TensorFlow 2.x","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Tu C\u00f3digo Python              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                 Keras API                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     TensorFlow Core (Operaciones)          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Hardware: CPU / GPU (CUDA) / TPU / Edge   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#formas-de-crear-modelos-en-keras","title":"Formas de Crear Modelos en Keras","text":""},{"location":"deep-learning/08-frameworks-practica/#sequential-api-mas-simple","title":"Sequential API (m\u00e1s simple)","text":"<pre><code>from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input\n\nmodel = Sequential([\n    Input(shape=(784,)),\n    Dense(256, activation='relu'),\n    Dropout(0.3),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#functional-api-mas-flexible","title":"Functional API (m\u00e1s flexible)","text":"<pre><code>from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\n\n# M\u00faltiples entradas\ninput_a = Input(shape=(32,), name='input_a')\ninput_b = Input(shape=(64,), name='input_b')\n\nx_a = Dense(16, activation='relu')(input_a)\nx_b = Dense(32, activation='relu')(input_b)\n\nmerged = Concatenate()([x_a, x_b])\nx = Dense(64, activation='relu')(merged)\noutput = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=[input_a, input_b], outputs=output)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#subclassing-maxima-flexibilidad","title":"Subclassing (m\u00e1xima flexibilidad)","text":"<pre><code>class MiModelo(Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = Dense(256, activation='relu')\n        self.dense2 = Dense(128, activation='relu')\n        self.dense3 = Dense(10, activation='softmax')\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        if training:\n            x = tf.nn.dropout(x, rate=0.3)\n        x = self.dense2(x)\n        return self.dense3(x)\n\nmodel = MiModelo()\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#entrenamiento-personalizado","title":"Entrenamiento Personalizado","text":"<pre><code># Training loop personalizado\noptimizer = tf.keras.optimizers.Adam()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\n@tf.function  # Compilar para mayor velocidad\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        predictions = model(x, training=True)\n        loss = loss_fn(y, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_acc_metric.update_state(y, predictions)\n    return loss\n\n# Loop de entrenamiento\nfor epoch in range(epochs):\n    for x_batch, y_batch in train_dataset:\n        loss = train_step(x_batch, y_batch)\n\n    train_acc = train_acc_metric.result()\n    print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {train_acc:.4f}\")\n    train_acc_metric.reset_states()\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#83-pytorch","title":"8.3. PyTorch","text":""},{"location":"deep-learning/08-frameworks-practica/#estructura-basica","title":"Estructura B\u00e1sica","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Definir modelo\nclass MiRed(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n        self.layer2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.layer2(x)\n        return x\n\n# Instanciar\nmodel = MiRed(784, 256, 10)\n\n# Mover a GPU si est\u00e1 disponible\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#entrenamiento-en-pytorch","title":"Entrenamiento en PyTorch","text":"<pre><code># Configurar\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Crear DataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Entrenar\nmodel.train()  # Modo entrenamiento\nfor epoch in range(epochs):\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward\n        loss.backward()\n\n        # Actualizar pesos\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch}: Loss = {running_loss/len(train_loader):.4f}\")\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#evaluacion","title":"Evaluaci\u00f3n","text":"<pre><code>model.eval()  # Modo evaluaci\u00f3n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # No calcular gradientes\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Accuracy: {100 * correct / total:.2f}%\")\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#84-callbacks-y-monitorizacion","title":"8.4. Callbacks y Monitorizaci\u00f3n","text":""},{"location":"deep-learning/08-frameworks-practica/#callbacks-en-keras","title":"Callbacks en Keras","text":"<pre><code>from tensorflow.keras.callbacks import (\n    ModelCheckpoint,\n    EarlyStopping,\n    ReduceLROnPlateau,\n    TensorBoard,\n    CSVLogger\n)\n\ncallbacks = [\n    # Guardar el mejor modelo\n    ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_loss',\n        save_best_only=True\n    ),\n\n    # Early stopping\n    EarlyStopping(\n        monitor='val_loss',\n        patience=10,\n        restore_best_weights=True\n    ),\n\n    # Reducir learning rate\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=5,\n        min_lr=1e-7\n    ),\n\n    # TensorBoard\n    TensorBoard(log_dir='./logs'),\n\n    # Log a CSV\n    CSVLogger('training_log.csv')\n]\n\nmodel.fit(X_train, y_train, callbacks=callbacks)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#tensorboard","title":"TensorBoard","text":"<pre><code># En terminal\ntensorboard --logdir=./logs\n# Abrir navegador en http://localhost:6006\n</code></pre> <pre><code># Logging personalizado\nimport tensorflow as tf\n\nlog_dir = \"logs/custom\"\nsummary_writer = tf.summary.create_file_writer(log_dir)\n\nwith summary_writer.as_default():\n    tf.summary.scalar('custom_metric', value, step=epoch)\n    tf.summary.image('generated', images, step=epoch)\n    tf.summary.histogram('weights', model.layers[0].weights[0], step=epoch)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#85-gpu-y-aceleracion","title":"8.5. GPU y Aceleraci\u00f3n","text":""},{"location":"deep-learning/08-frameworks-practica/#verificar-gpu","title":"Verificar GPU","text":"<pre><code># TensorFlow\nimport tensorflow as tf\nprint(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))\n\n# PyTorch\nimport torch\nprint(\"CUDA disponible:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#configurar-memoria-gpu-tensorflow","title":"Configurar Memoria GPU (TensorFlow)","text":"<pre><code># Crecimiento din\u00e1mico de memoria\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n# O limitar memoria m\u00e1xima\ntf.config.set_logical_device_configuration(\n    gpus[0],\n    [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # 4GB\n)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#entrenamiento-multi-gpu","title":"Entrenamiento Multi-GPU","text":"<pre><code># TensorFlow\nstrategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope():\n    model = create_model()\n    model.compile(...)\n\nmodel.fit(...)\n\n# PyTorch\nmodel = nn.DataParallel(model)  # Simple\n# O usar DistributedDataParallel para mejor rendimiento\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#86-optimizacion-del-pipeline-de-datos","title":"8.6. Optimizaci\u00f3n del Pipeline de Datos","text":""},{"location":"deep-learning/08-frameworks-practica/#tfdata-api","title":"tf.data API","text":"<pre><code>import tensorflow as tf\n\n# Crear dataset\ndataset = tf.data.Dataset.from_tensor_slices((X, y))\n\n# Pipeline optimizado\ndataset = (dataset\n    .cache()                    # Cachear en memoria\n    .shuffle(buffer_size=10000) # Mezclar\n    .batch(32)                  # Crear batches\n    .prefetch(tf.data.AUTOTUNE) # Cargar siguiente batch mientras se entrena\n)\n\n# Para im\u00e1genes desde directorio\ndataset = tf.keras.utils.image_dataset_from_directory(\n    'data/train',\n    image_size=(224, 224),\n    batch_size=32\n)\ndataset = dataset.prefetch(tf.data.AUTOTUNE)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#dataloader-en-pytorch","title":"DataLoader en PyTorch","text":"<pre><code>from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,      # Carga paralela\n    pin_memory=True,    # Acelera transferencia a GPU\n    persistent_workers=True\n)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#87-guardado-y-carga-de-modelos","title":"8.7. Guardado y Carga de Modelos","text":""},{"location":"deep-learning/08-frameworks-practica/#tensorflowkeras","title":"TensorFlow/Keras","text":"<pre><code># Guardar modelo completo\nmodel.save('modelo_completo.keras')\n\n# Cargar\nmodel = tf.keras.models.load_model('modelo_completo.keras')\n\n# Solo pesos\nmodel.save_weights('pesos.weights.h5')\nmodel.load_weights('pesos.weights.h5')\n\n# SavedModel (para producci\u00f3n)\nmodel.save('modelo_savedmodel', save_format='tf')\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#pytorch","title":"PyTorch","text":"<pre><code># Solo pesos (recomendado)\ntorch.save(model.state_dict(), 'modelo_pesos.pth')\n\n# Cargar\nmodel = MiRed(784, 256, 10)\nmodel.load_state_dict(torch.load('modelo_pesos.pth'))\nmodel.eval()\n\n# Checkpoint completo (para continuar entrenamiento)\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n}, 'checkpoint.pth')\n\n# Cargar checkpoint\ncheckpoint = torch.load('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#88-debugging-y-profiling","title":"8.8. Debugging y Profiling","text":""},{"location":"deep-learning/08-frameworks-practica/#debugging-en-tensorflow","title":"Debugging en TensorFlow","text":"<pre><code># Eager execution por defecto permite debugging normal\ntf.config.run_functions_eagerly(True)\n\n# Verificar valores\n@tf.function\ndef mi_funcion(x):\n    tf.debugging.assert_non_negative(x, message=\"x debe ser positivo\")\n    tf.print(\"Valor de x:\", x)\n    return x ** 2\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#profiling","title":"Profiling","text":"<pre><code># TensorFlow Profiler\nimport tensorflow as tf\n\n# En el callback de TensorBoard\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir='logs',\n    profile_batch=(10, 20)  # Perfilar batches 10-20\n)\n\n# PyTorch Profiler\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA\n    ],\n    on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n) as prof:\n    for step, (inputs, labels) in enumerate(train_loader):\n        outputs = model(inputs)\n        loss.backward()\n        prof.step()\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#89-despliegue-a-produccion","title":"8.9. Despliegue a Producci\u00f3n","text":""},{"location":"deep-learning/08-frameworks-practica/#tensorflow-serving","title":"TensorFlow Serving","text":"<pre><code># Exportar modelo\nmodel.save('modelo_produccion/1')\n\n# Docker\n# docker pull tensorflow/serving\n# docker run -p 8501:8501 --mount type=bind,source=/path/modelo_produccion,target=/models/mi_modelo -e MODEL_NAME=mi_modelo -t tensorflow/serving\n\n# Hacer predicci\u00f3n v\u00eda REST\nimport requests\nimport json\n\ndata = json.dumps({\"instances\": X_test[:5].tolist()})\nresponse = requests.post(\n    'http://localhost:8501/v1/models/mi_modelo:predict',\n    data=data,\n    headers={\"content-type\": \"application/json\"}\n)\npredictions = response.json()['predictions']\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#tensorflow-lite-movil","title":"TensorFlow Lite (M\u00f3vil)","text":"<pre><code># Convertir a TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # Cuantizaci\u00f3n\ntflite_model = converter.convert()\n\n# Guardar\nwith open('modelo.tflite', 'wb') as f:\n    f.write(tflite_model)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#onnx-interoperabilidad","title":"ONNX (Interoperabilidad)","text":"<pre><code># PyTorch a ONNX\nimport torch.onnx\n\ndummy_input = torch.randn(1, 784)\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"modelo.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output']\n)\n\n# TensorFlow a ONNX\nimport tf2onnx\nmodel_proto, _ = tf2onnx.convert.from_keras(model)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#810-mejores-practicas","title":"8.10. Mejores Pr\u00e1cticas","text":""},{"location":"deep-learning/08-frameworks-practica/#estructura-de-proyecto","title":"Estructura de Proyecto","text":"<pre><code>mi_proyecto/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 external/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 saved/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u2514\u2500\u2500 dataset.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 architectures.py\n\u2502   \u2502   \u2514\u2500\u2500 losses.py\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2502   \u2514\u2500\u2500 callbacks.py\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u2514\u2500\u2500 helpers.py\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 exploration.ipynb\n\u251c\u2500\u2500 configs/\n\u2502   \u2514\u2500\u2500 config.yaml\n\u251c\u2500\u2500 tests/\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#reproducibilidad","title":"Reproducibilidad","text":"<pre><code>import random\nimport numpy as np\nimport tensorflow as tf\n\ndef set_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    # PyTorch\n    # torch.manual_seed(seed)\n    # torch.cuda.manual_seed_all(seed)\n\nset_seeds(42)\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#configuracion-con-yaml","title":"Configuraci\u00f3n con YAML","text":"<pre><code># config.yaml\nmodel:\n  architecture: resnet50\n  num_classes: 10\n  dropout: 0.3\n\ntraining:\n  batch_size: 32\n  epochs: 100\n  learning_rate: 0.001\n\ndata:\n  train_dir: \"data/train\"\n  val_dir: \"data/val\"\n  image_size: [224, 224]\n</code></pre> <pre><code>import yaml\n\nwith open('configs/config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nbatch_size = config['training']['batch_size']\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#logging-con-weights-biases","title":"Logging con Weights &amp; Biases","text":"<pre><code>import wandb\n\nwandb.init(project=\"mi-proyecto\", config=config)\n\n# Durante entrenamiento\nwandb.log({\n    \"loss\": loss,\n    \"accuracy\": accuracy,\n    \"epoch\": epoch\n})\n\n# Al final\nwandb.finish()\n</code></pre>"},{"location":"deep-learning/08-frameworks-practica/#811-recursos-adicionales","title":"8.11. Recursos Adicionales","text":""},{"location":"deep-learning/08-frameworks-practica/#documentacion-oficial","title":"Documentaci\u00f3n Oficial","text":"<ul> <li>TensorFlow</li> <li>PyTorch</li> <li>Keras</li> <li>Hugging Face</li> </ul>"},{"location":"deep-learning/08-frameworks-practica/#cursos-recomendados","title":"Cursos Recomendados","text":"<ul> <li>Deep Learning Specialization (Coursera/Andrew Ng).</li> <li>Fast.ai (Practical Deep Learning).</li> <li>Stanford CS231n (Computer Vision).</li> <li>Stanford CS224n (NLP).</li> </ul>"},{"location":"deep-learning/08-frameworks-practica/#libros","title":"Libros","text":"<ul> <li>\"Deep Learning\" - Goodfellow, Bengio, Courville.</li> <li>\"Hands-On Machine Learning\" - Aur\u00e9lien G\u00e9ron.</li> <li>\"Deep Learning with Python\" - Fran\u00e7ois Chollet.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/","title":"\ud83d\udcac Procesamiento de Lenguaje Natural (NLP)","text":"<p>\u00a1Bienvenido a la secci\u00f3n de Procesamiento de Lenguaje Natural! \ud83c\udf89</p>"},{"location":"procesamiento-lenguaje-natural/#que-es-el-procesamiento-de-lenguaje-natural","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Procesamiento de Lenguaje Natural?","text":"<p>El Procesamiento de Lenguaje Natural (NLP), del ingl\u00e9s Natural Language Processing, es una rama de la Inteligencia Artificial que se centra en la interacci\u00f3n entre las computadoras y el lenguaje humano.</p> <p>El objetivo del NLP es permitir que las m\u00e1quinas comprendan, interpreten y generen lenguaje humano de manera \u00fatil y significativa.</p>"},{"location":"procesamiento-lenguaje-natural/#aplicaciones-del-nlp","title":"\ud83e\udde0 Aplicaciones del NLP","text":"<ol> <li> <p>An\u00e1lisis de Sentimientos:    Determinar si un texto expresa una opini\u00f3n positiva, negativa o neutral.    \ud83d\udccd Ejemplo: Analizar rese\u00f1as de productos.</p> </li> <li> <p>Chatbots y Asistentes Virtuales:    Sistemas que interact\u00faan con usuarios mediante lenguaje natural.    \ud83d\udccd Ejemplo: ChatGPT, Alexa, Siri.</p> </li> <li> <p>Traducci\u00f3n Autom\u00e1tica:    Traducir texto de un idioma a otro.    \ud83d\udccd Ejemplo: Google Translate.</p> </li> <li> <p>Extracci\u00f3n de Informaci\u00f3n:    Identificar entidades y relaciones en textos.    \ud83d\udccd Ejemplo: Reconocimiento de nombres, fechas, lugares.</p> </li> <li> <p>Generaci\u00f3n de Texto:    Crear texto coherente y relevante autom\u00e1ticamente.    \ud83d\udccd Ejemplo: Generaci\u00f3n de res\u00famenes, escritura asistida.</p> </li> </ol>"},{"location":"procesamiento-lenguaje-natural/#conceptos-fundamentales","title":"\u2699\ufe0f Conceptos Fundamentales","text":"<ul> <li>Tokenizaci\u00f3n: Divisi\u00f3n del texto en unidades (palabras, subpalabras, caracteres)</li> <li>Stemming y Lematizaci\u00f3n: Reducci\u00f3n de palabras a su ra\u00edz</li> <li>Stop Words: Palabras comunes que suelen filtrarse</li> <li>Bag of Words (BoW): Representaci\u00f3n de texto como frecuencia de palabras</li> <li>TF-IDF: Medida de importancia de palabras en documentos</li> <li>Word Embeddings: Representaciones vectoriales de palabras (Word2Vec, GloVe)</li> <li>Transformers: Arquitectura base de modelos modernos (BERT, GPT)</li> </ul>"},{"location":"procesamiento-lenguaje-natural/#bibliotecas-populares","title":"\ud83d\udd0d Bibliotecas Populares","text":"<ul> <li>NLTK - Natural Language Toolkit</li> <li>spaCy - Procesamiento industrial de NLP</li> <li>Hugging Face Transformers - Modelos preentrenados</li> <li>Gensim - Modelado de t\u00f3picos y word embeddings</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/","title":"\ud83d\udcac Unidad 1. Fundamentos del Procesamiento de Lenguaje Natural","text":"<p>El Procesamiento de Lenguaje Natural (NLP) es una disciplina en la intersecci\u00f3n de la ling\u00fc\u00edstica, la inform\u00e1tica y la inteligencia artificial. Su objetivo es permitir que las m\u00e1quinas comprendan, interpreten y generen lenguaje humano de forma \u00fatil.</p>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#11-que-es-el-nlp","title":"1.1. \u00bfQu\u00e9 es el NLP?","text":"<p>El NLP abarca un amplio rango de tareas computacionales que involucran el lenguaje humano:</p> <ul> <li>Comprensi\u00f3n: Extraer significado del texto (clasificaci\u00f3n, extracci\u00f3n de entidades, an\u00e1lisis de sentimientos).</li> <li>Generaci\u00f3n: Crear texto coherente (chatbots, res\u00famenes, traducci\u00f3n).</li> <li>Transformaci\u00f3n: Convertir texto de una forma a otra (correcci\u00f3n ortogr\u00e1fica, parafraseo).</li> </ul>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#por-que-es-dificil","title":"\u00bfPor qu\u00e9 es dif\u00edcil?","text":"<p>El lenguaje humano presenta desaf\u00edos \u00fanicos para las m\u00e1quinas:</p> <ul> <li>Ambig\u00fcedad: Una palabra puede tener m\u00faltiples significados (\"banco\" puede ser una instituci\u00f3n financiera o un asiento).</li> <li>Contexto: El significado cambia seg\u00fan el contexto (\"Hace fr\u00edo\" vs \"Es un tipo fr\u00edo\").</li> <li>Variabilidad: Sin\u00f3nimos, jerga, errores ortogr\u00e1ficos, diferentes idiomas.</li> <li>Conocimiento impl\u00edcito: Los humanos entienden iron\u00eda, sarcasmo y referencias culturales.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#12-pipeline-tipico-de-nlp","title":"1.2. Pipeline T\u00edpico de NLP","text":"<p>Un proyecto de NLP generalmente sigue estos pasos:</p> <pre><code>Texto Crudo \u2192 Preprocesamiento \u2192 Representaci\u00f3n Vectorial \u2192 Modelo ML/DL \u2192 Salida\n</code></pre> <ol> <li>Adquisici\u00f3n de Datos: Obtener el corpus de texto (scraping, APIs, datasets p\u00fablicos).</li> <li>Preprocesamiento: Limpiar y normalizar el texto.</li> <li>Representaci\u00f3n: Convertir texto a n\u00fameros (vectores).</li> <li>Modelado: Aplicar algoritmos de ML o Deep Learning.</li> <li>Evaluaci\u00f3n: Medir el rendimiento con m\u00e9tricas apropiadas.</li> <li>Despliegue: Poner el modelo en producci\u00f3n.</li> </ol>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#13-preprocesamiento-de-texto","title":"1.3. Preprocesamiento de Texto","text":"<p>El preprocesamiento es crucial para reducir el ruido y normalizar el texto.</p>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#tokenizacion","title":"Tokenizaci\u00f3n","text":"<p>Dividir el texto en unidades m\u00e1s peque\u00f1as llamadas tokens (palabras, subpalabras o caracteres).</p> <pre><code>import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\ntexto = \"El NLP es fascinante. Permite crear chatbots incre\u00edbles.\"\n\n# Tokenizaci\u00f3n por oraciones\noraciones = sent_tokenize(texto, language='spanish')\nprint(oraciones)\n# ['El NLP es fascinante.', 'Permite crear chatbots incre\u00edbles.']\n\n# Tokenizaci\u00f3n por palabras\npalabras = word_tokenize(texto, language='spanish')\nprint(palabras)\n# ['El', 'NLP', 'es', 'fascinante', '.', 'Permite', 'crear', 'chatbots', 'incre\u00edbles', '.']\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#normalizacion-de-texto","title":"Normalizaci\u00f3n de Texto","text":"<ul> <li>Lowercasing: Convertir todo a min\u00fasculas para que \"Casa\" y \"casa\" sean iguales.</li> <li>Eliminaci\u00f3n de puntuaci\u00f3n y caracteres especiales.</li> <li>Eliminaci\u00f3n de n\u00fameros (si no son relevantes).</li> <li>Eliminaci\u00f3n de espacios extra.</li> </ul> <pre><code>import re\n\ndef normalizar_texto(texto):\n    # Min\u00fasculas\n    texto = texto.lower()\n    # Eliminar caracteres especiales (mantener letras, n\u00fameros y espacios)\n    texto = re.sub(r'[^a-z\u00e1\u00e9\u00ed\u00f3\u00fa\u00fc\u00f10-9\\s]', '', texto)\n    # Eliminar espacios m\u00faltiples\n    texto = re.sub(r'\\s+', ' ', texto).strip()\n    return texto\n\nprint(normalizar_texto(\"\u00a1Hola! \u00bfC\u00f3mo est\u00e1s???  Muy   bien.\"))\n# 'hola c\u00f3mo est\u00e1s muy bien'\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#stop-words","title":"Stop Words","text":"<p>Las stop words son palabras muy comunes que generalmente no aportan significado sem\u00e1ntico (art\u00edculos, preposiciones, conjunciones).</p> <pre><code>from nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words('spanish'))\nprint(list(stop_words)[:10])\n# ['al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual']\n\npalabras_filtradas = [w for w in palabras if w.lower() not in stop_words]\nprint(palabras_filtradas)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#stemming-vs-lematizacion","title":"Stemming vs Lematizaci\u00f3n","text":"<p>Ambas t\u00e9cnicas reducen las palabras a su forma base, pero de forma diferente:</p> <ul> <li>Stemming: Corta la palabra de forma heur\u00edstica para obtener su ra\u00edz. Es r\u00e1pido pero puede producir ra\u00edces que no son palabras reales (\"correr\", \"corriendo\" \u2192 \"corr\").</li> </ul> <pre><code>from nltk.stem import SnowballStemmer\n\nstemmer = SnowballStemmer('spanish')\npalabras = [\"corriendo\", \"corr\u00ed\", \"correr\", \"corredores\"]\nstems = [stemmer.stem(p) for p in palabras]\nprint(stems)\n# ['corr', 'corr', 'corr', 'corred']\n</code></pre> <ul> <li>Lematizaci\u00f3n: Utiliza un diccionario para encontrar el lema (forma can\u00f3nica) de la palabra. Es m\u00e1s preciso pero m\u00e1s lento (\"corriendo\" \u2192 \"correr\").</li> </ul> <pre><code>import spacy\n\nnlp = spacy.load('es_core_news_sm')\ndoc = nlp(\"Los perros estaban corriendo por el parque\")\nlemmas = [token.lemma_ for token in doc]\nprint(lemmas)\n# ['el', 'perro', 'estar', 'correr', 'por', 'el', 'parque']\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#14-bibliotecas-fundamentales-de-nlp","title":"1.4. Bibliotecas Fundamentales de NLP","text":"Biblioteca Descripci\u00f3n Fortalezas NLTK Natural Language Toolkit. La biblioteca cl\u00e1sica para aprender NLP. Educativa, muchos recursos, corpus incluidos. spaCy Biblioteca industrial de NLP. R\u00e1pida, pipelines preentrenados, NER, POS. Transformers (Hugging Face) Modelos de lenguaje preentrenados (BERT, GPT, etc.). Estado del arte, f\u00e1cil uso, muchos modelos. Gensim Especializada en modelado de t\u00f3picos y embeddings. Word2Vec, Doc2Vec, LDA. TextBlob Interfaz simple para tareas comunes de NLP. F\u00e1cil de usar, an\u00e1lisis de sentimientos."},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#ejemplo-completo-con-spacy","title":"Ejemplo Completo con spaCy","text":"<pre><code>import spacy\n\n# Cargar modelo en espa\u00f1ol\nnlp = spacy.load('es_core_news_sm')\n\ntexto = \"Apple est\u00e1 buscando comprar una startup del Reino Unido por 1.000 millones de d\u00f3lares.\"\ndoc = nlp(texto)\n\n# Tokenizaci\u00f3n autom\u00e1tica\nprint(\"Tokens:\", [token.text for token in doc])\n\n# Part-of-Speech Tagging (Etiquetado gramatical)\nprint(\"\\nPOS Tagging:\")\nfor token in doc:\n    print(f\"  {token.text}: {token.pos_} ({token.dep_})\")\n\n# Named Entity Recognition (NER)\nprint(\"\\nEntidades:\")\nfor ent in doc.ents:\n    print(f\"  {ent.text}: {ent.label_}\")\n# Apple: ORG, Reino Unido: LOC, 1.000 millones de d\u00f3lares: MONEY\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#15-tareas-comunes-de-nlp","title":"1.5. Tareas Comunes de NLP","text":"<p>El campo del NLP abarca muchas tareas espec\u00edficas:</p>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#clasificacion-de-texto","title":"Clasificaci\u00f3n de Texto","text":"<p>Asignar una categor\u00eda a un documento.</p> <ul> <li>An\u00e1lisis de Sentimientos: Positivo / Negativo / Neutral.</li> <li>Detecci\u00f3n de Spam: Spam / No Spam.</li> <li>Clasificaci\u00f3n de Noticias: Deportes / Pol\u00edtica / Tecnolog\u00eda.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#extraccion-de-informacion","title":"Extracci\u00f3n de Informaci\u00f3n","text":"<ul> <li>Named Entity Recognition (NER): Identificar personas, lugares, organizaciones, fechas.</li> <li>Extracci\u00f3n de Relaciones: Encontrar relaciones entre entidades (\"Apple\" - adquiri\u00f3 - \"Startup\").</li> </ul>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#generacion-de-texto","title":"Generaci\u00f3n de Texto","text":"<ul> <li>Resumen Autom\u00e1tico: Crear versiones cortas de textos largos.</li> <li>Traducci\u00f3n Autom\u00e1tica: Convertir texto entre idiomas.</li> <li>Chatbots: Generar respuestas conversacionales.</li> <li>Completado de Texto: GPT, modelos de lenguaje.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#similitud-y-busqueda","title":"Similitud y B\u00fasqueda","text":"<ul> <li>B\u00fasqueda Sem\u00e1ntica: Encontrar documentos similares por significado.</li> <li>Question Answering (QA): Responder preguntas bas\u00e1ndose en un contexto.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#16-datasets-populares-en-nlp","title":"1.6. Datasets Populares en NLP","text":"Dataset Descripci\u00f3n Tarea IMDB Reviews 50,000 rese\u00f1as de pel\u00edculas An\u00e1lisis de sentimientos AG News 120,000 art\u00edculos de noticias Clasificaci\u00f3n de texto SQuAD Preguntas sobre art\u00edculos de Wikipedia Question Answering GLUE / SuperGLUE Benchmark de m\u00faltiples tareas Evaluaci\u00f3n de modelos CoNLL-2003 Corpus anotado con entidades NER"},{"location":"procesamiento-lenguaje-natural/01-fundamentos-nlp/#17-consideraciones-eticas-en-nlp","title":"1.7. Consideraciones \u00c9ticas en NLP","text":"<p>El NLP plantea importantes cuestiones \u00e9ticas:</p> <ul> <li>Sesgo en los Datos: Los modelos pueden perpetuar sesgos de g\u00e9nero, raza o cultura presentes en los datos de entrenamiento.</li> <li>Privacidad: Los modelos entrenados pueden memorizar informaci\u00f3n sensible.</li> <li>Desinformaci\u00f3n: La generaci\u00f3n de texto puede usarse para crear fake news.</li> <li>Interpretabilidad: Los modelos de Deep Learning son \"cajas negras\".</li> </ul> <p>Es responsabilidad del profesional ser consciente de estos riesgos y tomar medidas para mitigarlos.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/","title":"\ud83d\udcac Unidad 2. Representaci\u00f3n de Texto: BoW y TF-IDF","text":"<p>Para que los algoritmos de Machine Learning puedan procesar texto, necesitamos convertir las palabras en vectores num\u00e9ricos. Esta unidad cubre las t\u00e9cnicas cl\u00e1sicas de representaci\u00f3n de texto: Bag of Words (BoW) y TF-IDF.</p>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#21-el-problema-de-la-representacion","title":"2.1. El Problema de la Representaci\u00f3n","text":"<p>Los algoritmos de ML trabajan con n\u00fameros, pero el texto es:</p> <ul> <li>Discreto: Las palabras son s\u00edmbolos, no n\u00fameros.</li> <li>Variable: Los documentos tienen diferentes longitudes.</li> <li>Disperso: El vocabulario puede ser muy grande.</li> </ul> <p>La pregunta clave es: \u00bfC\u00f3mo convertimos texto en vectores num\u00e9ricos que capturen su significado?</p>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#22-bag-of-words-bow","title":"2.2. Bag of Words (BoW)","text":"<p>El modelo Bag of Words (Bolsa de Palabras) es la representaci\u00f3n m\u00e1s simple. Convierte un documento en un vector donde cada dimensi\u00f3n representa una palabra del vocabulario, y el valor es la frecuencia de esa palabra en el documento.</p>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Ignora el orden: \"El perro muerde al hombre\" y \"El hombre muerde al perro\" tendr\u00edan la misma representaci\u00f3n.</li> <li>Ignora la gram\u00e1tica: Solo cuenta palabras.</li> <li>Vector disperso: La mayor\u00eda de los valores son 0 (la mayor\u00eda de las palabras del vocabulario no aparecen en un documento dado).</li> </ul>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#proceso-de-construccion","title":"Proceso de Construcci\u00f3n","text":"<ol> <li>Crear el vocabulario: Lista de todas las palabras \u00fanicas en el corpus.</li> <li>Vectorizar: Para cada documento, contar la frecuencia de cada palabra del vocabulario.</li> </ol>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#ejemplo-manual","title":"Ejemplo Manual","text":"<pre><code>Corpus:\n- Doc1: \"el gato come pescado\"\n- Doc2: \"el perro come carne\"\n- Doc3: \"el gato y el perro juegan\"\n\nVocabulario: [el, gato, come, pescado, perro, carne, y, juegan]\n\nMatriz BoW:\n         el  gato  come  pescado  perro  carne  y  juegan\nDoc1:     1    1     1      1       0      0    0    0\nDoc2:     1    0     1      0       1      1    0    0\nDoc3:     2    1     0      0       1      0    1    1\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#implementacion-con-scikit-learn","title":"Implementaci\u00f3n con scikit-learn","text":"<pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    \"el gato come pescado\",\n    \"el perro come carne\",\n    \"el gato y el perro juegan\"\n]\n\n# Crear vectorizador\nvectorizer = CountVectorizer()\n\n# Ajustar y transformar\nX = vectorizer.fit_transform(corpus)\n\n# Ver vocabulario\nprint(\"Vocabulario:\", vectorizer.get_feature_names_out())\n# ['carne' 'come' 'el' 'gato' 'juegan' 'perro' 'pescado']\n\n# Ver matriz (convertida a array denso para visualizaci\u00f3n)\nprint(\"\\nMatriz BoW:\\n\", X.toarray())\n# [[0 1 1 1 0 0 1]\n#  [1 1 1 0 0 1 0]\n#  [0 0 2 1 1 1 0]]\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#parametros-importantes-de-countvectorizer","title":"Par\u00e1metros Importantes de CountVectorizer","text":"Par\u00e1metro Descripci\u00f3n Ejemplo <code>max_features</code> Limita el vocabulario a las N palabras m\u00e1s frecuentes <code>max_features=1000</code> <code>stop_words</code> Elimina stop words <code>stop_words='english'</code> o lista personalizada <code>ngram_range</code> Incluye n-gramas (secuencias de N palabras) <code>ngram_range=(1, 2)</code> para unigramas y bigramas <code>min_df</code> Ignora palabras que aparecen en menos de N documentos <code>min_df=2</code> <code>max_df</code> Ignora palabras que aparecen en m\u00e1s del X% de documentos <code>max_df=0.95</code> <code>binary</code> Solo indica presencia (1) o ausencia (0), no frecuencia <code>binary=True</code>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#n-gramas","title":"N-gramas","text":"<p>Los n-gramas son secuencias de N palabras consecutivas. Permiten capturar algo de contexto y orden.</p> <ul> <li>Unigrama (n=1): \"machine\", \"learning\"</li> <li>Bigrama (n=2): \"machine learning\"</li> <li>Trigrama (n=3): \"natural language processing\"</li> </ul> <pre><code># Incluir bigramas adem\u00e1s de unigramas\nvectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\nX_ngram = vectorizer_ngram.fit_transform(corpus)\nprint(\"Vocabulario con bigramas:\", vectorizer_ngram.get_feature_names_out())\n# ['carne', 'come', 'come carne', 'come pescado', 'el', 'el gato', ...]\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#23-limitaciones-de-bow","title":"2.3. Limitaciones de BoW","text":"<p>El modelo BoW tiene varias limitaciones importantes:</p> <ol> <li>Todas las palabras tienen el mismo peso: \"el\", \"de\", \"que\" pesan igual que palabras m\u00e1s informativas.</li> <li>Alta dimensionalidad: El vocabulario puede ser muy grande (miles o millones de palabras).</li> <li>Vectores muy dispersos: La mayor\u00eda de los elementos son 0.</li> <li>Ignora el significado: Palabras sin\u00f3nimas tienen representaciones completamente diferentes.</li> <li>Ignora el orden: Pierde informaci\u00f3n contextual crucial.</li> </ol>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#24-tf-idf-term-frequency-inverse-document-frequency","title":"2.4. TF-IDF (Term Frequency - Inverse Document Frequency)","text":"<p>TF-IDF mejora BoW al dar m\u00e1s peso a las palabras que son:</p> <ul> <li>Frecuentes en el documento (Term Frequency - TF)</li> <li>Pero raras en el corpus general (Inverse Document Frequency - IDF)</li> </ul> <p>Esto penaliza palabras comunes como \"el\", \"de\", \"que\" y da m\u00e1s importancia a palabras distintivas.</p>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#formulas-matematicas","title":"F\u00f3rmulas Matem\u00e1ticas","text":"<p>Term Frequency (TF): Frecuencia de un t\u00e9rmino en un documento.</p> \\[TF(t, d) = \\frac{\\text{N\u00famero de veces que } t \\text{ aparece en } d}{\\text{Total de t\u00e9rminos en } d}\\] <p>O simplemente el conteo crudo: \\(TF(t, d) = f_{t,d}\\)</p> <p>Inverse Document Frequency (IDF): Mide la rareza de un t\u00e9rmino en el corpus.</p> \\[IDF(t) = \\log\\left(\\frac{N}{df_t}\\right)\\] <p>Donde:</p> <ul> <li>\\(N\\) = N\u00famero total de documentos</li> <li>\\(df_t\\) = N\u00famero de documentos que contienen el t\u00e9rmino \\(t\\)</li> </ul> <p>TF-IDF: El producto de ambos.</p> \\[TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)\\]"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#ejemplo-manual_1","title":"Ejemplo Manual","text":"<pre><code>Corpus (3 documentos):\n- Doc1: \"machine learning is fun\"\n- Doc2: \"machine learning and deep learning\"\n- Doc3: \"deep learning is powerful\"\n\nPalabra: \"learning\"\n- TF en Doc1: 1/4 = 0.25\n- TF en Doc2: 2/5 = 0.40  (aparece 2 veces)\n- TF en Doc3: 1/4 = 0.25\n- IDF: log(3/3) = log(1) = 0  (aparece en TODOS los docs \u2192 poco informativa)\n- TF-IDF: 0 en todos los documentos\n\nPalabra: \"fun\"\n- TF en Doc1: 1/4 = 0.25\n- IDF: log(3/1) = log(3) \u2248 1.1  (aparece solo en 1 doc \u2192 muy informativa)\n- TF-IDF en Doc1: 0.25 \u00d7 1.1 \u2248 0.275\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#implementacion-con-scikit-learn_1","title":"Implementaci\u00f3n con scikit-learn","text":"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\n    \"el gato come pescado\",\n    \"el perro come carne\",\n    \"el gato y el perro juegan\"\n]\n\n# Crear vectorizador TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\n\n# Ajustar y transformar\nX_tfidf = tfidf_vectorizer.fit_transform(corpus)\n\n# Ver vocabulario\nprint(\"Vocabulario:\", tfidf_vectorizer.get_feature_names_out())\n\n# Ver matriz TF-IDF\nimport pandas as pd\ndf_tfidf = pd.DataFrame(\n    X_tfidf.toarray(),\n    columns=tfidf_vectorizer.get_feature_names_out()\n)\nprint(\"\\nMatriz TF-IDF:\")\nprint(df_tfidf.round(3))\n</code></pre> <p>Resultado:</p> carne come el gato juegan perro pescado Doc1 0.000 0.450 0.340 0.534 0.000 0.000 0.630 Doc2 0.630 0.450 0.340 0.000 0.000 0.534 0.000 Doc3 0.000 0.000 0.485 0.380 0.565 0.380 0.000 <p>Observa c\u00f3mo:</p> <ul> <li>\"pescado\" y \"carne\" tienen valores altos (palabras distintivas de cada documento).</li> <li>\"el\" tiene valores relativamente bajos (muy com\u00fan).</li> <li>\"come\" tiene valores moderados (aparece en 2 de 3 docs).</li> </ul>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#parametros-de-tfidfvectorizer","title":"Par\u00e1metros de TfidfVectorizer","text":"Par\u00e1metro Descripci\u00f3n <code>norm</code> Normalizaci\u00f3n del vector ('l1', 'l2' o None) <code>use_idf</code> Si usar IDF (True por defecto) <code>smooth_idf</code> Suaviza IDF para evitar divisi\u00f3n por cero <code>sublinear_tf</code> Aplica logaritmo a TF (1 + log(TF))"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#25-comparacion-bow-vs-tf-idf","title":"2.5. Comparaci\u00f3n BoW vs TF-IDF","text":"Caracter\u00edstica BoW TF-IDF Valores Frecuencias crudas (enteros) Pesos ponderados (decimales) Palabras comunes Alto peso (por frecuencia) Bajo peso (penalizadas por IDF) Palabras distintivas Peso proporcional a frecuencia Alto peso (frecuentes localmente, raras globalmente) Uso t\u00edpico Baseline simple, conteo r\u00e1pido Clasificaci\u00f3n de texto, b\u00fasqueda de documentos"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#26-ejemplo-practico-clasificacion-de-sentimientos","title":"2.6. Ejemplo Pr\u00e1ctico: Clasificaci\u00f3n de Sentimientos","text":"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Dataset de ejemplo\ntextos = [\n    \"me encanta esta pel\u00edcula, es genial\",\n    \"pel\u00edcula incre\u00edble, la recomiendo mucho\",\n    \"excelente actuaci\u00f3n y gran historia\",\n    \"la mejor pel\u00edcula que he visto\",\n    \"qu\u00e9 pel\u00edcula tan mala y aburrida\",\n    \"no me gust\u00f3 nada, muy decepcionante\",\n    \"terrible, la peor pel\u00edcula del a\u00f1o\",\n    \"muy aburrida, no la recomiendo\"\n]\netiquetas = [1, 1, 1, 1, 0, 0, 0, 0]  # 1=positivo, 0=negativo\n\n# Dividir datos\nX_train, X_test, y_train, y_test = train_test_split(\n    textos, etiquetas, test_size=0.25, random_state=42\n)\n\n# Vectorizar con TF-IDF\nvectorizer = TfidfVectorizer(max_features=100)\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Entrenar clasificador Naive Bayes\nclf = MultinomialNB()\nclf.fit(X_train_tfidf, y_train)\n\n# Predecir y evaluar\ny_pred = clf.predict(X_test_tfidf)\nprint(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n\n# Probar con texto nuevo\nnuevo_texto = [\"esta pel\u00edcula es fant\u00e1stica\"]\nnuevo_tfidf = vectorizer.transform(nuevo_texto)\nprediccion = clf.predict(nuevo_tfidf)\nprint(f\"\\nPredicci\u00f3n para '{nuevo_texto[0]}': {'Positivo' if prediccion[0] else 'Negativo'}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#27-aplicaciones-reales","title":"2.7. Aplicaciones Reales","text":"<ul> <li>Motores de B\u00fasqueda: TF-IDF es fundamental para ranking de documentos relevantes (aunque modernos usan t\u00e9cnicas m\u00e1s avanzadas).</li> <li>Sistemas de Recomendaci\u00f3n de Contenido: Encontrar art\u00edculos o noticias similares.</li> <li>Clasificaci\u00f3n de Documentos: Categorizar emails, tickets de soporte, documentos legales.</li> <li>Detecci\u00f3n de Plagio: Comparar similitud entre documentos.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/02-bow-tfidf/#28-limitaciones-y-proximos-pasos","title":"2.8. Limitaciones y Pr\u00f3ximos Pasos","text":"<p>Aunque BoW y TF-IDF son \u00fatiles, tienen limitaciones importantes:</p> <ul> <li>No capturan sem\u00e1ntica: \"bueno\" y \"excelente\" son vectores completamente diferentes.</li> <li>No capturan contexto: El significado de una palabra puede cambiar seg\u00fan el contexto.</li> <li>Alta dimensionalidad: Vocabularios grandes generan vectores enormes.</li> </ul> <p>Las t\u00e9cnicas modernas como Word Embeddings (Word2Vec, GloVe) y Transformers (BERT, GPT) resuelven muchas de estas limitaciones al aprender representaciones densas y sem\u00e1nticas.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/","title":"\ud83d\udcac Unidad 3. Word Embeddings: Word2Vec y GloVe","text":"<p>Los Word Embeddings (incrustaciones de palabras) son representaciones vectoriales densas que capturan el significado sem\u00e1ntico de las palabras. A diferencia de BoW/TF-IDF, los embeddings colocan palabras con significados similares cerca en el espacio vectorial.</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#31-limitaciones-de-bowtf-idf","title":"3.1. Limitaciones de BoW/TF-IDF","text":"<p>Los m\u00e9todos cl\u00e1sicos tienen problemas fundamentales:</p> <ul> <li>Vectores dispersos: Un vocabulario de 50,000 palabras genera vectores de 50,000 dimensiones, la mayor\u00eda con valor 0.</li> <li>Sin relaci\u00f3n sem\u00e1ntica: \"feliz\" y \"contento\" tienen representaciones completamente diferentes.</li> <li>Alta dimensionalidad: Costoso en memoria y computaci\u00f3n.</li> <li>Sin generalizaci\u00f3n: El modelo no puede inferir similitudes entre palabras no vistas.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#la-idea-de-los-embeddings","title":"La Idea de los Embeddings","text":"<p>\"Una palabra se conoce por la compa\u00f1\u00eda que tiene\" - J.R. Firth, 1957</p> <p>Los embeddings aprenden representaciones densas (t\u00edpicamente 50-300 dimensiones) donde:</p> <ul> <li>Palabras similares tienen vectores similares.</li> <li>Las relaciones sem\u00e1nticas se capturan como operaciones vectoriales.</li> <li>El famoso ejemplo: <code>vector(\"rey\") - vector(\"hombre\") + vector(\"mujer\") \u2248 vector(\"reina\")</code></li> </ul>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#32-word2vec","title":"3.2. Word2Vec","text":"<p>Word2Vec es un modelo desarrollado por Google en 2013 que aprende embeddings de palabras usando redes neuronales superficiales. Existen dos arquitecturas principales:</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#321-skip-gram","title":"3.2.1. Skip-gram","text":"<p>Dado una palabra del centro (target), predice las palabras del contexto (alrededor).</p> <pre><code>Oraci\u00f3n: \"El gato come pescado fresco\"\nVentana de contexto = 2\n\nSi target = \"come\":\n- Predice: \"gato\", \"pescado\" (contexto)\n</code></pre> <p>Intuici\u00f3n: Si dos palabras aparecen frecuentemente en contextos similares, tendr\u00e1n embeddings similares.</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#322-cbow-continuous-bag-of-words","title":"3.2.2. CBOW (Continuous Bag of Words)","text":"<p>Lo opuesto a Skip-gram: dado el contexto (palabras alrededor), predice la palabra del centro.</p> <pre><code>Contexto: [\"el\", \"gato\", \"pescado\", \"fresco\"]\nPredice: \"come\"\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#comparacion","title":"Comparaci\u00f3n","text":"Caracter\u00edstica Skip-gram CBOW Predice Contexto dado palabra Palabra dado contexto Rendimiento Mejor con palabras raras Mejor con palabras frecuentes Velocidad M\u00e1s lento M\u00e1s r\u00e1pido Datos peque\u00f1os Mejor Peor"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#implementacion-con-gensim","title":"Implementaci\u00f3n con Gensim","text":"<pre><code>from gensim.models import Word2Vec\n\n# Corpus de ejemplo (lista de listas de tokens)\ncorpus = [\n    [\"el\", \"gato\", \"come\", \"pescado\"],\n    [\"el\", \"perro\", \"come\", \"carne\"],\n    [\"el\", \"gato\", \"duerme\", \"mucho\"],\n    [\"el\", \"perro\", \"corre\", \"r\u00e1pido\"],\n    [\"el\", \"p\u00e1jaro\", \"vuela\", \"alto\"]\n]\n\n# Entrenar modelo Word2Vec\nmodel = Word2Vec(\n    sentences=corpus,\n    vector_size=100,    # Dimensiones del embedding\n    window=5,           # Tama\u00f1o de ventana de contexto\n    min_count=1,        # Frecuencia m\u00ednima de palabra\n    workers=4,          # Threads para entrenamiento\n    sg=1                # 1 = Skip-gram, 0 = CBOW\n)\n\n# Obtener el vector de una palabra\nvector_gato = model.wv['gato']\nprint(f\"Dimensiones: {vector_gato.shape}\")  # (100,)\nprint(f\"Vector 'gato': {vector_gato[:5]}...\")  # Primeros 5 valores\n\n# Palabras m\u00e1s similares\nsimilares = model.wv.most_similar('gato', topn=3)\nprint(f\"\\nPalabras similares a 'gato': {similares}\")\n\n# Similitud entre dos palabras\nsimilitud = model.wv.similarity('gato', 'perro')\nprint(f\"Similitud gato-perro: {similitud:.4f}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#hiperparametros-importantes","title":"Hiperpar\u00e1metros Importantes","text":"Par\u00e1metro Descripci\u00f3n Valor t\u00edpico <code>vector_size</code> Dimensiones del embedding 100-300 <code>window</code> Palabras de contexto a considerar 5-10 <code>min_count</code> Frecuencia m\u00ednima para incluir palabra 5 <code>sg</code> 0=CBOW, 1=Skip-gram 1 para datos peque\u00f1os <code>negative</code> Muestras negativas (optimizaci\u00f3n) 5-20 <code>epochs</code> Iteraciones sobre el corpus 5-15"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#33-glove-global-vectors","title":"3.3. GloVe (Global Vectors)","text":"<p>GloVe (Stanford, 2014) es otro m\u00e9todo popular para crear embeddings. A diferencia de Word2Vec que aprende de ventanas locales, GloVe utiliza estad\u00edsticas globales de co-ocurrencia del corpus.</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#diferencia-con-word2vec","title":"Diferencia con Word2Vec","text":"<ul> <li>Word2Vec: Aprende de predicci\u00f3n local (ventana de contexto).</li> <li>GloVe: Construye una matriz de co-ocurrencia global y factoriza esa matriz.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#usando-glove-preentrenado","title":"Usando GloVe Preentrenado","text":"<p>GloVe proporciona embeddings preentrenados en grandes corpus (Wikipedia, Twitter, Common Crawl).</p> <pre><code>import numpy as np\n\n# Cargar embeddings GloVe preentrenados (descargar primero)\ndef cargar_glove(path, dim=100):\n    embeddings = {}\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\n# Ejemplo de uso (requiere descargar glove.6B.100d.txt)\n# glove = cargar_glove('glove.6B.100d.txt')\n# print(glove['king'].shape)  # (100,)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#comparacion-word2vec-vs-glove","title":"Comparaci\u00f3n Word2Vec vs GloVe","text":"Caracter\u00edstica Word2Vec GloVe M\u00e9todo Predictivo (red neuronal) Factorizaci\u00f3n de matriz Informaci\u00f3n Local (ventana) Global (co-ocurrencia) Entrenamiento Incremental posible Requiere todo el corpus Rendimiento Competitivo Competitivo"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#34-operaciones-con-embeddings","title":"3.4. Operaciones con Embeddings","text":"<p>Una propiedad fascinante de los embeddings es que las relaciones sem\u00e1nticas se capturan como operaciones vectoriales.</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#analogias","title":"Analog\u00edas","text":"<pre><code># rey - hombre + mujer \u2248 reina\nresult = model.wv.most_similar(\n    positive=['rey', 'mujer'],\n    negative=['hombre'],\n    topn=1\n)\n# [('reina', 0.85)]\n\n# Par\u00eds - Francia + Espa\u00f1a \u2248 Madrid\nresult = model.wv.most_similar(\n    positive=['paris', 'espa\u00f1a'],\n    negative=['francia'],\n    topn=1\n)\n# [('madrid', 0.82)]\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#similitud-coseno","title":"Similitud Coseno","text":"<p>La similitud entre embeddings se calcula t\u00edpicamente con similitud coseno:</p> \\[\\text{similitud}(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||}\\] <p>Valores cercanos a 1 indican alta similitud, cercanos a 0 indican poca relaci\u00f3n.</p> <pre><code>from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Calcular similitud entre dos vectores\nvec1 = model.wv['gato']\nvec2 = model.wv['perro']\n\nsimilitud = cosine_similarity([vec1], [vec2])[0][0]\nprint(f\"Similitud coseno: {similitud:.4f}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#35-embeddings-preentrenados","title":"3.5. Embeddings Preentrenados","text":"<p>En la pr\u00e1ctica, es com\u00fan usar embeddings preentrenados en grandes corpus:</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#fuentes-populares","title":"Fuentes Populares","text":"Nombre Corpus de Entrenamiento Dimensiones Word2Vec (Google) Google News (100B palabras) 300 GloVe (Stanford) Wikipedia + Gigaword 50, 100, 200, 300 FastText (Facebook) Wikipedia + Common Crawl 300"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#usando-fasttext-mejor-para-palabras-oov","title":"Usando FastText (mejor para palabras OOV)","text":"<p>FastText extiende Word2Vec al considerar sub-palabras (n-gramas de caracteres). Esto permite generar embeddings para palabras no vistas (Out-of-Vocabulary).</p> <pre><code>import fasttext.util\n\n# Descargar modelo preentrenado en espa\u00f1ol\nfasttext.util.download_model('es', if_exists='ignore')\nft = fasttext.load_model('cc.es.300.bin')\n\n# Obtener embedding\nvector = ft.get_word_vector('gato')\n\n# Funciona con palabras no vistas (OOV)\nvector_typo = ft.get_word_vector('gatito')  # Funciona!\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#36-embeddings-para-documentos","title":"3.6. Embeddings para Documentos","text":"<p>Los embeddings de palabras pueden extenderse a documentos completos:</p>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#promedio-de-embeddings","title":"Promedio de Embeddings","text":"<p>La forma m\u00e1s simple: promediar los embeddings de todas las palabras.</p> <pre><code>import numpy as np\n\ndef documento_a_vector(documento, model, dim=100):\n    \"\"\"Convierte un documento a vector promediando embeddings de palabras.\"\"\"\n    tokens = documento.lower().split()\n    vectores = []\n\n    for token in tokens:\n        if token in model.wv:\n            vectores.append(model.wv[token])\n\n    if vectores:\n        return np.mean(vectores, axis=0)\n    else:\n        return np.zeros(dim)\n\n# Uso\ndoc = \"el gato come pescado\"\nvec_doc = documento_a_vector(doc, model)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#doc2vec","title":"Doc2Vec","text":"<p>Doc2Vec extiende Word2Vec para aprender embeddings de documentos directamente.</p> <pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n# Preparar documentos etiquetados\ndocumentos = [\n    TaggedDocument(words=['el', 'gato', 'come', 'pescado'], tags=['doc1']),\n    TaggedDocument(words=['el', 'perro', 'come', 'carne'], tags=['doc2']),\n]\n\n# Entrenar modelo\nmodel_doc2vec = Doc2Vec(documentos, vector_size=50, window=2, min_count=1, epochs=100)\n\n# Obtener vector de un documento\nvector_doc1 = model_doc2vec.dv['doc1']\n\n# Inferir vector para documento nuevo\nnuevo_doc = ['el', 'gato', 'duerme']\nvector_nuevo = model_doc2vec.infer_vector(nuevo_doc)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#37-ejemplo-practico-busqueda-semantica","title":"3.7. Ejemplo Pr\u00e1ctico: B\u00fasqueda Sem\u00e1ntica","text":"<pre><code>from gensim.models import Word2Vec\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Corpus de documentos\ndocumentos = [\n    \"el gato duerme en el sof\u00e1\",\n    \"el perro corre por el parque\",\n    \"el p\u00e1jaro vuela sobre los \u00e1rboles\",\n    \"el pez nada en el acuario\",\n    \"el gato caza ratones por la noche\"\n]\n\n# Preparar corpus tokenizado\ncorpus_tokens = [doc.split() for doc in documentos]\n\n# Entrenar Word2Vec\nmodel = Word2Vec(corpus_tokens, vector_size=50, window=3, min_count=1, epochs=50)\n\ndef busqueda_semantica(query, documentos, model, topn=3):\n    \"\"\"Busca documentos similares a una query.\"\"\"\n    # Vector de la query\n    query_tokens = query.lower().split()\n    query_vecs = [model.wv[t] for t in query_tokens if t in model.wv]\n\n    if not query_vecs:\n        return []\n\n    query_vec = np.mean(query_vecs, axis=0).reshape(1, -1)\n\n    # Vectores de documentos\n    doc_vecs = []\n    for doc in documentos:\n        tokens = doc.split()\n        vecs = [model.wv[t] for t in tokens if t in model.wv]\n        if vecs:\n            doc_vecs.append(np.mean(vecs, axis=0))\n        else:\n            doc_vecs.append(np.zeros(50))\n\n    doc_vecs = np.array(doc_vecs)\n\n    # Calcular similitudes\n    similitudes = cosine_similarity(query_vec, doc_vecs)[0]\n\n    # Ordenar por similitud\n    indices = similitudes.argsort()[::-1][:topn]\n\n    return [(documentos[i], similitudes[i]) for i in indices]\n\n# Buscar\nquery = \"felino descansa\"\nresultados = busqueda_semantica(query, documentos, model)\n\nprint(f\"Query: '{query}'\")\nprint(\"\\nResultados:\")\nfor doc, score in resultados:\n    print(f\"  [{score:.3f}] {doc}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#38-consideraciones-y-limitaciones","title":"3.8. Consideraciones y Limitaciones","text":""},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#ventajas-de-word-embeddings","title":"Ventajas de Word Embeddings","text":"<ul> <li>Representaci\u00f3n densa: Vectores peque\u00f1os (100-300 dim) vs miles en BoW.</li> <li>Captura sem\u00e1ntica: Palabras similares est\u00e1n cerca en el espacio vectorial.</li> <li>Transferencia: Embeddings preentrenados pueden usarse en m\u00faltiples tareas.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/03-word-embeddings/#limitaciones","title":"Limitaciones","text":"<ul> <li>Una representaci\u00f3n por palabra: \"banco\" (asiento) y \"banco\" (instituci\u00f3n) tienen el mismo vector.</li> <li>Est\u00e1ticos: No cambian seg\u00fan el contexto de la oraci\u00f3n.</li> <li>Requieren mucho texto: Para entrenar buenos embeddings propios.</li> <li>Sesgos: Pueden capturar sesgos presentes en los datos de entrenamiento.</li> </ul> <p>Los modelos contextuales como BERT y GPT resuelven la limitaci\u00f3n de representaciones est\u00e1ticas generando embeddings diferentes para la misma palabra seg\u00fan su contexto.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/","title":"\ud83d\udcac Unidad 4. An\u00e1lisis de Sentimientos","text":"<p>El An\u00e1lisis de Sentimientos (Sentiment Analysis), tambi\u00e9n conocido como miner\u00eda de opiniones, es una de las aplicaciones m\u00e1s populares del NLP. Consiste en determinar la actitud, emoci\u00f3n u opini\u00f3n expresada en un texto.</p>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#41-que-es-el-analisis-de-sentimientos","title":"4.1. \u00bfQu\u00e9 es el An\u00e1lisis de Sentimientos?","text":"<p>Es el proceso computacional de identificar y categorizar opiniones expresadas en texto para determinar si la actitud del escritor hacia un tema es:</p> <ul> <li>Positiva: \"Me encanta este producto, es incre\u00edble\"</li> <li>Negativa: \"Terrible servicio, muy decepcionado\"</li> <li>Neutral: \"El producto lleg\u00f3 ayer\"</li> </ul>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#niveles-de-analisis","title":"Niveles de An\u00e1lisis","text":"Nivel Descripci\u00f3n Ejemplo Documento Sentimiento global del texto Rese\u00f1a completa de un producto Oraci\u00f3n Sentimiento de cada oraci\u00f3n Cada frase de una rese\u00f1a Aspecto Sentimiento sobre aspectos espec\u00edficos \"La bater\u00eda es excelente, pero la c\u00e1mara es mala\" Entidad Sentimiento hacia entidades espec\u00edficas Opiniones sobre diferentes marcas en un texto"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#tipos-de-salida","title":"Tipos de Salida","text":"<ul> <li>Binaria: Positivo / Negativo</li> <li>Ternaria: Positivo / Neutral / Negativo</li> <li>Escala: Rating num\u00e9rico (1-5 estrellas)</li> <li>Emociones: Alegr\u00eda, Tristeza, Ira, Miedo, Sorpresa, Disgusto</li> </ul>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#42-enfoques-para-analisis-de-sentimientos","title":"4.2. Enfoques para An\u00e1lisis de Sentimientos","text":""},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#enfoque-basado-en-lexico","title":"Enfoque Basado en L\u00e9xico","text":"<p>Utiliza diccionarios de palabras con polaridad predefinida.</p> <p>Ventajas:</p> <ul> <li>No requiere datos de entrenamiento.</li> <li>Interpretable y transparente.</li> <li>Funciona bien en dominios generales.</li> </ul> <p>Desventajas:</p> <ul> <li>No captura contexto ni negaciones bien.</li> <li>Depende de la calidad del l\u00e9xico.</li> <li>Dificultad con sarcasmo e iron\u00eda.</li> </ul> <pre><code># Ejemplo simple de enfoque l\u00e9xico\nlexico_positivo = {'bueno', 'excelente', 'genial', 'incre\u00edble', 'mejor', 'feliz', 'encanta'}\nlexico_negativo = {'malo', 'terrible', 'horrible', 'peor', 'triste', 'odio', 'decepcionante'}\n\ndef analisis_lexico(texto):\n    tokens = texto.lower().split()\n    score = 0\n\n    for token in tokens:\n        if token in lexico_positivo:\n            score += 1\n        elif token in lexico_negativo:\n            score -= 1\n\n    if score &gt; 0:\n        return \"Positivo\"\n    elif score &lt; 0:\n        return \"Negativo\"\n    else:\n        return \"Neutral\"\n\n# Ejemplos\nprint(analisis_lexico(\"Este producto es excelente y genial\"))  # Positivo\nprint(analisis_lexico(\"Terrible servicio, muy malo\"))  # Negativo\nprint(analisis_lexico(\"El paquete lleg\u00f3 ayer\"))  # Neutral\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#enfoque-basado-en-machine-learning","title":"Enfoque Basado en Machine Learning","text":"<p>Entrena clasificadores supervisados con datos etiquetados.</p> <p>Proceso t\u00edpico:</p> <ol> <li>Recolectar datos etiquetados (rese\u00f1as con ratings).</li> <li>Preprocesar texto (tokenizaci\u00f3n, limpieza).</li> <li>Vectorizar (BoW, TF-IDF, embeddings).</li> <li>Entrenar clasificador (Naive Bayes, SVM, Logistic Regression).</li> <li>Evaluar y ajustar.</li> </ol> <p>Ventajas:</p> <ul> <li>Captura patrones complejos.</li> <li>Adaptable a dominios espec\u00edficos.</li> <li>Mejor rendimiento general.</li> </ul> <p>Desventajas:</p> <ul> <li>Requiere datos etiquetados.</li> <li>Puede no generalizar a otros dominios.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#43-implementacion-con-machine-learning","title":"4.3. Implementaci\u00f3n con Machine Learning","text":""},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#dataset-imdb-movie-reviews","title":"Dataset: IMDB Movie Reviews","text":"<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Cargar datos (ejemplo simplificado)\n# En la pr\u00e1ctica, usar datasets como IMDB de Keras o Hugging Face\nrese\u00f1as = [\n    (\"me encanta esta pel\u00edcula, actuaciones incre\u00edbles\", 1),\n    (\"pel\u00edcula brillante, la recomiendo totalmente\", 1),\n    (\"una historia hermosa y conmovedora\", 1),\n    (\"excelente direcci\u00f3n y gui\u00f3n\", 1),\n    (\"muy entretenida, la mejor del a\u00f1o\", 1),\n    (\"qu\u00e9 pel\u00edcula tan aburrida y larga\", 0),\n    (\"terrible actuaci\u00f3n, muy decepcionante\", 0),\n    (\"perd\u00ed mi tiempo viendo esto\", 0),\n    (\"la peor pel\u00edcula que he visto\", 0),\n    (\"no la recomiendo para nada, muy mala\", 0),\n]\n\ntextos = [r[0] for r in rese\u00f1as]\netiquetas = [r[1] for r in rese\u00f1as]\n\n# Divisi\u00f3n train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    textos, etiquetas, test_size=0.3, random_state=42\n)\n\n# Vectorizaci\u00f3n TF-IDF\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Entrenar m\u00faltiples modelos\nmodelos = {\n    \"Naive Bayes\": MultinomialNB(),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"SVM\": LinearSVC()\n}\n\nfor nombre, modelo in modelos.items():\n    modelo.fit(X_train_tfidf, y_train)\n    y_pred = modelo.predict(X_test_tfidf)\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"{nombre}: Accuracy = {acc:.4f}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#44-analisis-de-sentimientos-con-bibliotecas","title":"4.4. An\u00e1lisis de Sentimientos con Bibliotecas","text":""},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#textblob-rapido-y-simple","title":"TextBlob (R\u00e1pido y Simple)","text":"<pre><code>from textblob import TextBlob\n\ndef analizar_sentimiento_textblob(texto):\n    blob = TextBlob(texto)\n    # Polaridad: -1 (negativo) a 1 (positivo)\n    # Subjetividad: 0 (objetivo) a 1 (subjetivo)\n    return {\n        'texto': texto,\n        'polaridad': blob.sentiment.polarity,\n        'subjetividad': blob.sentiment.subjectivity,\n        'sentimiento': 'Positivo' if blob.sentiment.polarity &gt; 0 \n                       else 'Negativo' if blob.sentiment.polarity &lt; 0 \n                       else 'Neutral'\n    }\n\n# Ejemplos\ntextos = [\n    \"I love this product, it's amazing!\",\n    \"Terrible experience, very disappointed\",\n    \"The package arrived yesterday\"\n]\n\nfor texto in textos:\n    resultado = analizar_sentimiento_textblob(texto)\n    print(f\"{resultado['texto']}\")\n    print(f\"  \u2192 {resultado['sentimiento']} (pol: {resultado['polaridad']:.2f})\")\n    print()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#vader-especializado-en-redes-sociales","title":"VADER (Especializado en Redes Sociales)","text":"<p>VADER (Valence Aware Dictionary for Sentiment Reasoning) est\u00e1 espec\u00edficamente dise\u00f1ado para texto de redes sociales, manejando emojis, may\u00fasculas, puntuaci\u00f3n, etc.</p> <pre><code>from nltk.sentiment import SentimentIntensityAnalyzer\nimport nltk\nnltk.download('vader_lexicon')\n\nsia = SentimentIntensityAnalyzer()\n\ntextos = [\n    \"I LOVE this!!! \ud83d\ude0d\",\n    \"This is okay, nothing special\",\n    \"Worst product EVER!!! \ud83d\ude21\ud83d\ude21\ud83d\ude21\",\n    \"The movie was good, but the ending was bad\"\n]\n\nfor texto in textos:\n    scores = sia.polarity_scores(texto)\n    print(f\"'{texto}'\")\n    print(f\"  Scores: {scores}\")\n    print(f\"  Compound: {scores['compound']:.3f}\")\n    print()\n</code></pre> <p>Output de VADER:</p> <ul> <li><code>neg</code>: Proporci\u00f3n negativa</li> <li><code>neu</code>: Proporci\u00f3n neutral</li> <li><code>pos</code>: Proporci\u00f3n positiva</li> <li><code>compound</code>: Score normalizado entre -1 y 1</li> </ul>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#45-analisis-de-sentimientos-con-transformers","title":"4.5. An\u00e1lisis de Sentimientos con Transformers","text":"<p>Los modelos basados en Transformers (BERT, RoBERTa) ofrecen el mejor rendimiento actual.</p>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#usando-hugging-face","title":"Usando Hugging Face","text":"<pre><code>from transformers import pipeline\n\n# Cargar pipeline de an\u00e1lisis de sentimientos\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\n\n# An\u00e1lisis simple\ntextos = [\n    \"I love this movie, it's fantastic!\",\n    \"This is the worst product I've ever bought\",\n    \"The weather is nice today\"\n]\n\nfor texto in textos:\n    resultado = sentiment_pipeline(texto)[0]\n    print(f\"'{texto}'\")\n    print(f\"  \u2192 {resultado['label']} (score: {resultado['score']:.4f})\")\n    print()\n\n# Para espa\u00f1ol, usar modelo espec\u00edfico\nsentiment_es = pipeline(\n    \"sentiment-analysis\", \n    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n)\n\ntexto_es = \"Esta pel\u00edcula es absolutamente maravillosa\"\nresultado_es = sentiment_es(texto_es)\nprint(f\"Espa\u00f1ol: '{texto_es}' \u2192 {resultado_es}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#modelo-especifico-para-espanol","title":"Modelo Espec\u00edfico para Espa\u00f1ol","text":"<pre><code>from transformers import pipeline\n\n# Modelo entrenado espec\u00edficamente para espa\u00f1ol\nclassifier = pipeline(\n    \"text-classification\",\n    model=\"pysentimiento/robertuito-sentiment-analysis\"\n)\n\ntextos_es = [\n    \"Me encanta este restaurante, la comida es deliciosa\",\n    \"El servicio fue terrible y tardaron mucho\",\n    \"El pedido lleg\u00f3 a tiempo\"\n]\n\nfor texto in textos_es:\n    resultado = classifier(texto)[0]\n    print(f\"'{texto}'\")\n    print(f\"  \u2192 {resultado['label']} ({resultado['score']:.3f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#46-analisis-de-sentimientos-por-aspectos","title":"4.6. An\u00e1lisis de Sentimientos por Aspectos","text":"<p>El Aspect-Based Sentiment Analysis (ABSA) identifica sentimientos hacia aspectos espec\u00edficos de un producto o servicio.</p> <pre><code>\"La bater\u00eda del tel\u00e9fono dura mucho, pero la c\u00e1mara es terrible\"\n\nAspectos:\n- bater\u00eda \u2192 Positivo\n- c\u00e1mara \u2192 Negativo\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#implementacion-simple","title":"Implementaci\u00f3n Simple","text":"<pre><code>import spacy\n\nnlp = spacy.load('es_core_news_sm')\n\naspectos_positivos = {\n    'bater\u00eda': ['dura', 'excelente', 'incre\u00edble', 'buena'],\n    'c\u00e1mara': ['n\u00edtida', 'clara', 'buena', 'incre\u00edble'],\n    'pantalla': ['brillante', 'clara', 'grande', 'hermosa'],\n    'dise\u00f1o': ['elegante', 'bonito', 'moderno', 'hermoso']\n}\n\naspectos_negativos = {\n    'bater\u00eda': ['corta', 'mala', 'terrible', 'poco'],\n    'c\u00e1mara': ['borrosa', 'mala', 'terrible', 'p\u00e9sima'],\n    'pantalla': ['peque\u00f1a', 'oscura', 'mala'],\n    'dise\u00f1o': ['feo', 'malo', 'anticuado']\n}\n\ndef absa_simple(texto):\n    \"\"\"An\u00e1lisis de sentimiento por aspectos simple.\"\"\"\n    texto_lower = texto.lower()\n    resultados = {}\n\n    # Buscar aspectos y palabras de sentimiento cercanas\n    for aspecto in aspectos_positivos.keys():\n        if aspecto in texto_lower:\n            sentimiento = \"Neutral\"\n\n            # Buscar palabras positivas\n            for palabra in aspectos_positivos[aspecto]:\n                if palabra in texto_lower:\n                    sentimiento = \"Positivo\"\n                    break\n\n            # Buscar palabras negativas\n            for palabra in aspectos_negativos.get(aspecto, []):\n                if palabra in texto_lower:\n                    sentimiento = \"Negativo\"\n                    break\n\n            resultados[aspecto] = sentimiento\n\n    return resultados\n\n# Ejemplo\ntexto = \"La bater\u00eda es excelente y dura todo el d\u00eda, pero la c\u00e1mara es terrible y borrosa\"\nprint(absa_simple(texto))\n# {'bater\u00eda': 'Positivo', 'c\u00e1mara': 'Negativo'}\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#47-aplicaciones-reales","title":"4.7. Aplicaciones Reales","text":""},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#monitoreo-de-redes-sociales","title":"Monitoreo de Redes Sociales","text":"<pre><code># Ejemplo: Analizar sentimiento de menciones de una marca\nmenciones = [\n    \"@MarcaX Gran servicio al cliente, muy satisfecho!\",\n    \"@MarcaX El producto lleg\u00f3 da\u00f1ado, muy decepcionado\",\n    \"@MarcaX \u00bfCu\u00e1l es el horario de atenci\u00f3n?\",\n    \"@MarcaX Incre\u00edble calidad, lo recomiendo totalmente\",\n    \"@MarcaX El peor servicio que he recibido, jam\u00e1s volver\u00e9\"\n]\n\nfrom collections import Counter\n\ndef monitorear_marca(menciones, classifier):\n    \"\"\"Analiza el sentimiento de menciones de una marca.\"\"\"\n    resultados = []\n\n    for mencion in menciones:\n        analisis = classifier(mencion)[0]\n        resultados.append(analisis['label'])\n\n    # Resumen\n    conteo = Counter(resultados)\n    total = len(resultados)\n\n    print(\"=== Resumen de Sentimiento ===\")\n    for label, count in conteo.most_common():\n        porcentaje = (count / total) * 100\n        print(f\"{label}: {count} ({porcentaje:.1f}%)\")\n\n    return resultados\n\n# monitorear_marca(menciones, sentiment_pipeline)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#analisis-de-resenas-de-productos","title":"An\u00e1lisis de Rese\u00f1as de Productos","text":"<pre><code>def analizar_rese\u00f1as_producto(rese\u00f1as):\n    \"\"\"\n    Analiza rese\u00f1as de un producto y genera un resumen.\n    \"\"\"\n    positivas = []\n    negativas = []\n\n    for rese\u00f1a in rese\u00f1as:\n        # Aqu\u00ed ir\u00eda el clasificador\n        # resultado = classifier(rese\u00f1a)\n        # Por simplicidad, usamos longitud como proxy\n        if len(rese\u00f1a) &gt; 50 and \"excelente\" in rese\u00f1a.lower():\n            positivas.append(rese\u00f1a)\n        elif \"malo\" in rese\u00f1a.lower() or \"terrible\" in rese\u00f1a.lower():\n            negativas.append(rese\u00f1a)\n\n    return {\n        'total': len(rese\u00f1as),\n        'positivas': len(positivas),\n        'negativas': len(negativas),\n        'score': len(positivas) / max(len(rese\u00f1as), 1)\n    }\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#48-desafios-y-consideraciones","title":"4.8. Desaf\u00edos y Consideraciones","text":""},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#desafios-comunes","title":"Desaf\u00edos Comunes","text":"<ol> <li> <p>Sarcasmo e Iron\u00eda: \"Oh genial, otro producto que no funciona\" \u2192 Detectado err\u00f3neamente como positivo.</p> </li> <li> <p>Negaciones: \"No me gust\u00f3 nada\" \u2192 Palabras positivas (\"gust\u00f3\") en contexto negativo.</p> </li> <li> <p>Comparaciones: \"Mejor que la competencia pero a\u00fan malo\" \u2192 Mezcla de sentimientos.</p> </li> <li> <p>Contexto del Dominio: \"La pel\u00edcula es una bomba\" \u2192 Puede ser muy buena (\u00e9xito) o muy mala (fracaso).</p> </li> <li> <p>Multiling\u00fce: Modelos entrenados en ingl\u00e9s no funcionan bien en espa\u00f1ol sin adaptaci\u00f3n.</p> </li> </ol>"},{"location":"procesamiento-lenguaje-natural/04-analisis-sentimientos/#metricas-de-evaluacion","title":"M\u00e9tricas de Evaluaci\u00f3n","text":"M\u00e9trica Uso Accuracy Proporci\u00f3n de predicciones correctas Precision De los predichos positivos, cu\u00e1ntos son correctos Recall De los positivos reales, cu\u00e1ntos detectamos F1-Score Media arm\u00f3nica de Precision y Recall Macro F1 Promedio de F1 por clase (\u00fatil con clases desbalanceadas) <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/05-ner/","title":"\ud83d\udcac Unidad 5. Reconocimiento de Entidades Nombradas (NER)","text":"<p>El Reconocimiento de Entidades Nombradas (Named Entity Recognition - NER) es una tarea fundamental del NLP que consiste en identificar y clasificar entidades mencionadas en texto en categor\u00edas predefinidas como personas, organizaciones, lugares, fechas, etc.</p>"},{"location":"procesamiento-lenguaje-natural/05-ner/#51-que-es-ner","title":"5.1. \u00bfQu\u00e9 es NER?","text":"<p>NER extrae informaci\u00f3n estructurada del texto no estructurado, identificando:</p> <ul> <li>Qui\u00e9n: Personas, organizaciones</li> <li>D\u00f3nde: Lugares, direcciones</li> <li>Cu\u00e1ndo: Fechas, horas</li> <li>Qu\u00e9: Productos, eventos</li> <li>Cu\u00e1nto: Cantidades, porcentajes, dinero</li> </ul>"},{"location":"procesamiento-lenguaje-natural/05-ner/#ejemplo","title":"Ejemplo","text":"<pre><code>Texto: \"Apple anunci\u00f3 que Tim Cook visitar\u00e1 Madrid el 15 de enero para \n        presentar el nuevo iPhone 15 con un precio de 999 euros.\"\n\nEntidades:\n- Apple \u2192 ORGANIZACI\u00d3N\n- Tim Cook \u2192 PERSONA\n- Madrid \u2192 LUGAR\n- 15 de enero \u2192 FECHA\n- iPhone 15 \u2192 PRODUCTO\n- 999 euros \u2192 DINERO\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#52-categorias-comunes-de-entidades","title":"5.2. Categor\u00edas Comunes de Entidades","text":""},{"location":"procesamiento-lenguaje-natural/05-ner/#etiquetas-estandar-conll","title":"Etiquetas Est\u00e1ndar (CoNLL)","text":"Etiqueta Descripci\u00f3n Ejemplos PER Persona Tim Cook, Mar\u00eda Garc\u00eda ORG Organizaci\u00f3n Apple, Microsoft, ONU LOC Ubicaci\u00f3n Madrid, R\u00edo Amazonas MISC Miscel\u00e1neo iPhone, COVID-19"},{"location":"procesamiento-lenguaje-natural/05-ner/#etiquetas-extendidas","title":"Etiquetas Extendidas","text":"Etiqueta Descripci\u00f3n Ejemplos DATE Fecha 15 de enero, 2024 TIME Hora 3:00 PM, mediod\u00eda MONEY Dinero $100, 999 euros PERCENT Porcentaje 15%, 0.5% QUANTITY Cantidad 100 km, 5 kg EVENT Evento Copa Mundial, Premios Oscar PRODUCT Producto iPhone, Windows 11 LAW Ley/Regulaci\u00f3n GDPR, Constituci\u00f3n"},{"location":"procesamiento-lenguaje-natural/05-ner/#53-formato-de-anotacion-bio","title":"5.3. Formato de Anotaci\u00f3n: BIO","text":"<p>El esquema BIO (Beginning-Inside-Outside) es el formato est\u00e1ndar para etiquetar secuencias:</p> <ul> <li>B-XXX: Beginning - Primera palabra de la entidad XXX</li> <li>I-XXX: Inside - Palabra intermedia/final de la entidad XXX</li> <li>O: Outside - No es parte de ninguna entidad</li> </ul>"},{"location":"procesamiento-lenguaje-natural/05-ner/#ejemplo_1","title":"Ejemplo","text":"<pre><code>Texto:     Tim    Cook   visitar\u00e1   Madrid   el   pr\u00f3ximo   lunes\nEtiquetas: B-PER  I-PER  O          B-LOC    O    O         B-DATE\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#variantes","title":"Variantes","text":"<ul> <li>IOB1: B- solo cuando hay entidades consecutivas del mismo tipo.</li> <li>IOB2 (BIO): B- siempre al inicio de una entidad.</li> <li>BIOES: A\u00f1ade S (Single) para entidades de una sola palabra y E (End).</li> </ul>"},{"location":"procesamiento-lenguaje-natural/05-ner/#54-ner-con-spacy","title":"5.4. NER con spaCy","text":"<p>spaCy ofrece modelos preentrenados con excelente soporte para NER.</p> <pre><code>import spacy\n\n# Cargar modelo en espa\u00f1ol\nnlp = spacy.load('es_core_news_lg')  # Modelo grande para mejor precisi\u00f3n\n\ntexto = \"\"\"\nApple Inc. anunci\u00f3 que Tim Cook visitar\u00e1 la sede de Madrid el 15 de enero de 2024.\nLa compa\u00f1\u00eda presentar\u00e1 el iPhone 15 Pro con un precio de 1.199 euros.\n\"\"\"\n\ndoc = nlp(texto)\n\n# Extraer entidades\nprint(\"Entidades encontradas:\")\nprint(\"-\" * 50)\nfor ent in doc.ents:\n    print(f\"{ent.text:25} \u2192 {ent.label_:10} ({ent.start_char}-{ent.end_char})\")\n</code></pre> <p>Salida esperada:</p> <pre><code>Entidades encontradas:\n--------------------------------------------------\nApple Inc.                \u2192 ORG        (1-11)\nTim Cook                  \u2192 PER        (26-34)\nMadrid                    \u2192 LOC        (54-60)\n15 de enero de 2024       \u2192 DATE       (64-83)\niPhone 15 Pro             \u2192 MISC       (111-124)\n1.199 euros               \u2192 MONEY      (143-154)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#visualizacion-de-entidades","title":"Visualizaci\u00f3n de Entidades","text":"<pre><code>from spacy import displacy\n\n# Visualizaci\u00f3n en notebook o HTML\ndisplacy.render(doc, style=\"ent\", jupyter=True)\n\n# O guardar como HTML\nhtml = displacy.render(doc, style=\"ent\", page=True)\nwith open(\"entidades.html\", \"w\", encoding=\"utf-8\") as f:\n    f.write(html)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#55-ner-con-transformers-hugging-face","title":"5.5. NER con Transformers (Hugging Face)","text":"<p>Los modelos basados en BERT ofrecen el mejor rendimiento actual.</p> <pre><code>from transformers import pipeline\n\n# Pipeline de NER\nner_pipeline = pipeline(\"ner\", grouped_entities=True)\n\ntexto = \"Apple CEO Tim Cook announced the new iPhone 15 in California on September 12th.\"\n\nentidades = ner_pipeline(texto)\n\nprint(\"Entidades (BERT):\")\nfor ent in entidades:\n    print(f\"{ent['word']:20} \u2192 {ent['entity_group']:10} (score: {ent['score']:.3f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#modelo-especifico-para-espanol","title":"Modelo Espec\u00edfico para Espa\u00f1ol","text":"<pre><code>from transformers import pipeline\n\n# Modelo NER entrenado en espa\u00f1ol\nner_es = pipeline(\n    \"ner\",\n    model=\"mrm8488/bert-spanish-cased-finetuned-ner\",\n    grouped_entities=True\n)\n\ntexto_es = \"El presidente Pedro S\u00e1nchez visit\u00f3 Barcelona el martes pasado\"\nentidades_es = ner_es(texto_es)\n\nfor ent in entidades_es:\n    print(f\"{ent['word']:20} \u2192 {ent['entity_group']}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#56-entrenamiento-de-modelo-ner-personalizado","title":"5.6. Entrenamiento de Modelo NER Personalizado","text":""},{"location":"procesamiento-lenguaje-natural/05-ner/#con-spacy","title":"Con spaCy","text":"<pre><code>import spacy\nfrom spacy.training import Example\nimport random\n\n# Datos de entrenamiento en formato spaCy\nTRAIN_DATA = [\n    (\"Apple lanzar\u00e1 el iPhone 16 en septiembre\", {\n        \"entities\": [(0, 5, \"ORG\"), (18, 27, \"PRODUCT\"), (31, 41, \"DATE\")]\n    }),\n    (\"Microsoft anunci\u00f3 Windows 12 ayer\", {\n        \"entities\": [(0, 9, \"ORG\"), (18, 28, \"PRODUCT\"), (29, 33, \"DATE\")]\n    }),\n    (\"El CEO de Google, Sundar Pichai, visit\u00f3 Madrid\", {\n        \"entities\": [(10, 16, \"ORG\"), (18, 31, \"PER\"), (41, 47, \"LOC\")]\n    }),\n]\n\n# Crear modelo en blanco\nnlp = spacy.blank(\"es\")\n\n# A\u00f1adir componente NER\nif \"ner\" not in nlp.pipe_names:\n    ner = nlp.add_pipe(\"ner\")\nelse:\n    ner = nlp.get_pipe(\"ner\")\n\n# A\u00f1adir etiquetas\nfor _, annotations in TRAIN_DATA:\n    for ent in annotations.get(\"entities\"):\n        ner.add_label(ent[2])\n\n# Entrenamiento\noptimizer = nlp.begin_training()\n\nfor epoch in range(30):\n    random.shuffle(TRAIN_DATA)\n    losses = {}\n\n    for text, annotations in TRAIN_DATA:\n        doc = nlp.make_doc(text)\n        example = Example.from_dict(doc, annotations)\n        nlp.update([example], drop=0.5, losses=losses)\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}: Loss = {losses['ner']:.4f}\")\n\n# Guardar modelo\nnlp.to_disk(\"modelo_ner_custom\")\n\n# Probar\ndoc = nlp(\"Samsung presentar\u00e1 el Galaxy S25 en enero\")\nfor ent in doc.ents:\n    print(f\"{ent.text} \u2192 {ent.label_}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#57-metricas-de-evaluacion-para-ner","title":"5.7. M\u00e9tricas de Evaluaci\u00f3n para NER","text":""},{"location":"procesamiento-lenguaje-natural/05-ner/#metricas-principales","title":"M\u00e9tricas Principales","text":"M\u00e9trica F\u00f3rmula Descripci\u00f3n Precision TP / (TP + FP) De las entidades predichas, cu\u00e1ntas son correctas Recall TP / (TP + FN) De las entidades reales, cu\u00e1ntas encontramos F1-Score 2\u00d7(P\u00d7R)/(P+R) Media arm\u00f3nica de Precision y Recall"},{"location":"procesamiento-lenguaje-natural/05-ner/#tipos-de-match","title":"Tipos de Match","text":"<ul> <li>Exact Match: La entidad predicha coincide exactamente en texto y tipo.</li> <li>Partial Match: El texto coincide parcialmente.</li> <li>Type Match: El tipo es correcto aunque los l\u00edmites no sean exactos.</li> </ul> <pre><code>from seqeval.metrics import classification_report, f1_score\n\n# Etiquetas reales (formato BIO)\ny_true = [['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O']]\n# Etiquetas predichas\ny_pred = [['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O']]\n\nprint(classification_report(y_true, y_pred))\nprint(f\"F1 Score: {f1_score(y_true, y_pred):.4f}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#58-aplicaciones-reales-de-ner","title":"5.8. Aplicaciones Reales de NER","text":""},{"location":"procesamiento-lenguaje-natural/05-ner/#extraccion-de-informacion-de-noticias","title":"Extracci\u00f3n de Informaci\u00f3n de Noticias","text":"<pre><code>import spacy\n\nnlp = spacy.load('es_core_news_lg')\n\nnoticia = \"\"\"\nEl presidente de Estados Unidos, Joe Biden, se reuni\u00f3 con \nel canciller alem\u00e1n Olaf Scholz en Berl\u00edn el pasado mi\u00e9rcoles.\nDiscutieron sobre la situaci\u00f3n en Ucrania y el acuerdo comercial \nde 500 millones de d\u00f3lares entre ambos pa\u00edses.\n\"\"\"\n\ndoc = nlp(noticia)\n\n# Extraer informaci\u00f3n estructurada\ninfo = {\n    'personas': [],\n    'lugares': [],\n    'organizaciones': [],\n    'fechas': [],\n    'dinero': []\n}\n\nfor ent in doc.ents:\n    if ent.label_ == 'PER':\n        info['personas'].append(ent.text)\n    elif ent.label_ in ['LOC', 'GPE']:\n        info['lugares'].append(ent.text)\n    elif ent.label_ == 'ORG':\n        info['organizaciones'].append(ent.text)\n    elif ent.label_ == 'DATE':\n        info['fechas'].append(ent.text)\n    elif ent.label_ == 'MONEY':\n        info['dinero'].append(ent.text)\n\nprint(\"Informaci\u00f3n Extra\u00edda:\")\nfor key, values in info.items():\n    if values:\n        print(f\"  {key.upper()}: {', '.join(set(values))}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#anonimizacion-de-datos-gdpr","title":"Anonimizaci\u00f3n de Datos (GDPR)","text":"<pre><code>def anonimizar_texto(texto, nlp):\n    \"\"\"\n    Anonimiza informaci\u00f3n personal en el texto.\n    \"\"\"\n    doc = nlp(texto)\n\n    # Crear texto anonimizado\n    texto_anon = texto\n\n    # Reemplazar de atr\u00e1s hacia adelante para mantener \u00edndices\n    for ent in reversed(doc.ents):\n        if ent.label_ in ['PER', 'PERSON']:\n            reemplazo = '[NOMBRE]'\n        elif ent.label_ in ['LOC', 'GPE']:\n            reemplazo = '[UBICACI\u00d3N]'\n        elif ent.label_ == 'ORG':\n            reemplazo = '[ORGANIZACI\u00d3N]'\n        elif ent.label_ == 'EMAIL':\n            reemplazo = '[EMAIL]'\n        elif ent.label_ in ['PHONE', 'CARDINAL'] and len(ent.text) &gt; 6:\n            reemplazo = '[TEL\u00c9FONO]'\n        else:\n            continue\n\n        texto_anon = texto_anon[:ent.start_char] + reemplazo + texto_anon[ent.end_char:]\n\n    return texto_anon\n\n# Ejemplo\ntexto = \"Juan Garc\u00eda de Madrid llam\u00f3 al 612345678 para hablar con Apple\"\ntexto_anonimizado = anonimizar_texto(texto, nlp)\nprint(f\"Original: {texto}\")\nprint(f\"Anonimizado: {texto_anonimizado}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#extraccion-de-cvs","title":"Extracci\u00f3n de CVs","text":"<pre><code>def extraer_info_cv(cv_texto, nlp):\n    \"\"\"\n    Extrae informaci\u00f3n estructurada de un CV.\n    \"\"\"\n    doc = nlp(cv_texto)\n\n    info_cv = {\n        'nombre': None,\n        'ubicacion': None,\n        'empresas': [],\n        'fechas': [],\n        'habilidades': []  # Requerir\u00eda un modelo personalizado\n    }\n\n    for ent in doc.ents:\n        if ent.label_ == 'PER' and info_cv['nombre'] is None:\n            info_cv['nombre'] = ent.text\n        elif ent.label_ in ['LOC', 'GPE']:\n            info_cv['ubicacion'] = ent.text\n        elif ent.label_ == 'ORG':\n            info_cv['empresas'].append(ent.text)\n        elif ent.label_ == 'DATE':\n            info_cv['fechas'].append(ent.text)\n\n    return info_cv\n\ncv = \"\"\"\nMar\u00eda L\u00f3pez Garc\u00eda\nDesarrolladora Senior | Madrid, Espa\u00f1a\n\nExperiencia:\n- Google (2020-2023): Ingeniera de Software\n- Microsoft (2018-2020): Desarrolladora Junior\n\nEducaci\u00f3n:\n- Universidad Complutense de Madrid (2014-2018)\n\"\"\"\n\nprint(extraer_info_cv(cv, nlp))\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/05-ner/#59-desafios-y-consideraciones","title":"5.9. Desaf\u00edos y Consideraciones","text":""},{"location":"procesamiento-lenguaje-natural/05-ner/#desafios-comunes","title":"Desaf\u00edos Comunes","text":"<ol> <li>Entidades anidadas: \"Banco de Espa\u00f1a\" \u2192 ORG que contiene LOC.</li> <li>Ambig\u00fcedad: \"Apple\" puede ser la empresa o la fruta.</li> <li>Entidades discontinuas: \"Microsoft y Google Inc.\" \u2192 Dos ORGs.</li> <li>Variaci\u00f3n de nombres: \"EEUU\", \"Estados Unidos\", \"USA\" \u2192 Mismo lugar.</li> <li>Nuevas entidades: Nombres de productos, personas, empresas nuevas.</li> <li>Dominio espec\u00edfico: Entidades m\u00e9dicas, legales, financieras requieren modelos especializados.</li> </ol>"},{"location":"procesamiento-lenguaje-natural/05-ner/#mejores-practicas","title":"Mejores Pr\u00e1cticas","text":"<ul> <li>Usar modelos grandes preentrenados como base.</li> <li>Fine-tuning con datos del dominio espec\u00edfico.</li> <li>Combinar reglas y modelos ML para mejor precisi\u00f3n.</li> <li>Validar y corregir errores manualmente para mejorar el modelo.</li> <li>Considerar Entity Linking para resolver ambig\u00fcedades.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/","title":"\ud83d\udcac Unidad 6. Clasificaci\u00f3n de Texto","text":"<p>La Clasificaci\u00f3n de Texto es una de las tareas m\u00e1s comunes y \u00fatiles del NLP. Consiste en asignar autom\u00e1ticamente una o m\u00e1s categor\u00edas predefinidas a un documento de texto.</p>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#61-que-es-la-clasificacion-de-texto","title":"6.1. \u00bfQu\u00e9 es la Clasificaci\u00f3n de Texto?","text":"<p>Es el proceso de categorizar texto en grupos organizados, utilizando t\u00e9cnicas de NLP y Machine Learning.</p>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#tipos-de-clasificacion","title":"Tipos de Clasificaci\u00f3n","text":"Tipo Descripci\u00f3n Ejemplo Binaria Dos clases Spam / No Spam Multiclase M\u00faltiples clases, una por documento Categor\u00eda de noticia (Deportes, Pol\u00edtica, Tecnolog\u00eda) Multi-etiqueta M\u00faltiples etiquetas por documento Tags de un art\u00edculo (puede ser \"Python\" Y \"Machine Learning\") Jer\u00e1rquica Categor\u00edas con estructura de \u00e1rbol Taxonom\u00edas de productos"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#aplicaciones-comunes","title":"Aplicaciones Comunes","text":"<ul> <li>Filtrado de Spam: Clasificar emails como spam o leg\u00edtimos.</li> <li>An\u00e1lisis de Sentimientos: Positivo / Negativo / Neutral.</li> <li>Categorizaci\u00f3n de Noticias: Deportes, Pol\u00edtica, Econom\u00eda, etc.</li> <li>Detecci\u00f3n de Idioma: Identificar el idioma de un texto.</li> <li>Clasificaci\u00f3n de Tickets de Soporte: Routing autom\u00e1tico.</li> <li>Detecci\u00f3n de Contenido T\u00f3xico: Moderaci\u00f3n de comentarios.</li> <li>Clasificaci\u00f3n de Intents: Para chatbots y asistentes virtuales.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#62-pipeline-de-clasificacion-de-texto","title":"6.2. Pipeline de Clasificaci\u00f3n de Texto","text":"<pre><code>1. Recolecci\u00f3n de Datos\n        \u2193\n2. Preprocesamiento\n        \u2193\n3. Representaci\u00f3n (Vectorizaci\u00f3n)\n        \u2193\n4. Divisi\u00f3n Train/Test\n        \u2193\n5. Entrenamiento del Modelo\n        \u2193\n6. Evaluaci\u00f3n\n        \u2193\n7. Optimizaci\u00f3n\n        \u2193\n8. Despliegue\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#63-clasificacion-con-machine-learning-tradicional","title":"6.3. Clasificaci\u00f3n con Machine Learning Tradicional","text":""},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#ejemplo-completo-clasificacion-de-noticias","title":"Ejemplo Completo: Clasificaci\u00f3n de Noticias","text":"<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Dataset de ejemplo (en pr\u00e1ctica usar datasets reales)\ndatos = {\n    'texto': [\n        \"El equipo gan\u00f3 el campeonato de f\u00fatbol este domingo\",\n        \"El partido de baloncesto termin\u00f3 con una victoria aplastante\",\n        \"El tenista espa\u00f1ol avanz\u00f3 a la final del torneo\",\n        \"El gobierno aprob\u00f3 nuevas medidas econ\u00f3micas\",\n        \"El presidente firm\u00f3 el acuerdo internacional\",\n        \"Las elecciones se celebrar\u00e1n el pr\u00f3ximo mes\",\n        \"Apple lanz\u00f3 su nuevo iPhone con mejoras en c\u00e1mara\",\n        \"Microsoft present\u00f3 actualizaciones de Windows\",\n        \"Google desarrolla nueva inteligencia artificial\",\n        \"Las acciones subieron tras el anuncio del banco central\",\n        \"La bolsa cerr\u00f3 con ganancias r\u00e9cord\",\n        \"El PIB creci\u00f3 un 3% el \u00faltimo trimestre\"\n    ],\n    'categoria': [\n        'deportes', 'deportes', 'deportes',\n        'politica', 'politica', 'politica',\n        'tecnologia', 'tecnologia', 'tecnologia',\n        'economia', 'economia', 'economia'\n    ]\n}\n\ndf = pd.DataFrame(datos)\n\n# Divisi\u00f3n de datos\nX_train, X_test, y_train, y_test = train_test_split(\n    df['texto'], df['categoria'], \n    test_size=0.25, \n    random_state=42,\n    stratify=df['categoria']  # Mantener proporci\u00f3n de clases\n)\n\n# Vectorizaci\u00f3n TF-IDF\nvectorizer = TfidfVectorizer(\n    max_features=5000,\n    ngram_range=(1, 2),  # Unigramas y bigramas\n    min_df=1\n)\n\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Entrenar y comparar m\u00faltiples modelos\nmodelos = {\n    'Naive Bayes': MultinomialNB(),\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'SVM': LinearSVC(),\n    'Random Forest': RandomForestClassifier(n_estimators=100)\n}\n\nresultados = {}\nfor nombre, modelo in modelos.items():\n    modelo.fit(X_train_tfidf, y_train)\n    y_pred = modelo.predict(X_test_tfidf)\n\n    # Calcular accuracy\n    accuracy = (y_pred == y_test).mean()\n    resultados[nombre] = accuracy\n\n    print(f\"\\n{'='*50}\")\n    print(f\"{nombre}\")\n    print(f\"{'='*50}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#seleccion-del-mejor-modelo","title":"Selecci\u00f3n del Mejor Modelo","text":"<pre><code># Comparaci\u00f3n visual de modelos\nplt.figure(figsize=(10, 6))\nplt.bar(resultados.keys(), resultados.values())\nplt.title('Comparaci\u00f3n de Modelos')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nfor i, (nombre, acc) in enumerate(resultados.items()):\n    plt.text(i, acc + 0.02, f'{acc:.3f}', ha='center')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#64-clasificacion-con-deep-learning","title":"6.4. Clasificaci\u00f3n con Deep Learning","text":""},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#red-neuronal-simple-con-keras","title":"Red Neuronal Simple con Keras","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\n# Preprocesamiento\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n\n# Padding para longitud uniforme\nmax_len = 100\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\nX_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n\n# Codificar etiquetas\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\nnum_classes = len(label_encoder.classes_)\n\n# Modelo\nmodel = Sequential([\n    tf.keras.layers.Embedding(5000, 128, input_length=max_len),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dropout(0.3),\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train_pad, y_train_encoded,\n    epochs=50,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\n# Evaluaci\u00f3n\nloss, accuracy = model.evaluate(X_test_pad, y_test_encoded)\nprint(f\"\\nAccuracy en test: {accuracy:.4f}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#65-clasificacion-con-transformers","title":"6.5. Clasificaci\u00f3n con Transformers","text":""},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#usando-hugging-face","title":"Usando Hugging Face","text":"<pre><code>from transformers import pipeline\n\n# Pipeline de clasificaci\u00f3n zero-shot\nclassifier = pipeline(\"zero-shot-classification\")\n\ntexto = \"El nuevo smartphone tiene una c\u00e1mara de 200 megap\u00edxeles\"\ncategorias = [\"tecnolog\u00eda\", \"deportes\", \"pol\u00edtica\", \"econom\u00eda\"]\n\nresultado = classifier(texto, categorias)\nprint(f\"Texto: {texto}\")\nprint(f\"Categor\u00eda: {resultado['labels'][0]} ({resultado['scores'][0]:.3f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#fine-tuning-de-bert-para-clasificacion","title":"Fine-tuning de BERT para Clasificaci\u00f3n","text":"<pre><code>from transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\nfrom datasets import Dataset\nimport numpy as np\n\n# Preparar datos\ntrain_data = Dataset.from_dict({\n    'text': list(X_train),\n    'label': list(y_train_encoded)\n})\n\ntest_data = Dataset.from_dict({\n    'text': list(X_test),\n    'label': list(y_test_encoded)\n})\n\n# Cargar tokenizer y modelo\nmodel_name = \"bert-base-multilingual-cased\"  # Para espa\u00f1ol\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'], \n        padding='max_length', \n        truncation=True,\n        max_length=128\n    )\n\ntrain_data = train_data.map(tokenize_function, batched=True)\ntest_data = test_data.map(tokenize_function, batched=True)\n\n# Cargar modelo\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_classes\n)\n\n# Configuraci\u00f3n de entrenamiento\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    evaluation_strategy=\"epoch\"\n)\n\n# M\u00e9trica\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = (predictions == labels).mean()\n    return {'accuracy': accuracy}\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    compute_metrics=compute_metrics\n)\n\n# Entrenar\ntrainer.train()\n\n# Evaluar\nresults = trainer.evaluate()\nprint(f\"Accuracy: {results['eval_accuracy']:.4f}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#66-clasificacion-multi-etiqueta","title":"6.6. Clasificaci\u00f3n Multi-etiqueta","text":"<p>Cuando un documento puede pertenecer a m\u00faltiples categor\u00edas simult\u00e1neamente.</p> <pre><code>from sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Datos multi-etiqueta\ntextos = [\n    \"Python es excelente para machine learning y an\u00e1lisis de datos\",\n    \"JavaScript permite crear aplicaciones web interactivas\",\n    \"El framework Django usa Python para desarrollo web\",\n    \"React y Node.js son populares en desarrollo web\"\n]\n\n# Cada texto puede tener m\u00faltiples etiquetas\netiquetas = [\n    ['python', 'machine-learning', 'data-science'],\n    ['javascript', 'web'],\n    ['python', 'web'],\n    ['javascript', 'web']\n]\n\n# Binarizar etiquetas\nmlb = MultiLabelBinarizer()\ny_multi = mlb.fit_transform(etiquetas)\n\nprint(\"Clases:\", mlb.classes_)\nprint(\"Etiquetas binarizadas:\\n\", y_multi)\n\n# Vectorizar textos\nvectorizer = TfidfVectorizer(max_features=1000)\nX = vectorizer.fit_transform(textos)\n\n# Modelo multi-etiqueta\nmodelo_multi = MultiOutputClassifier(LogisticRegression())\nmodelo_multi.fit(X, y_multi)\n\n# Predicci\u00f3n\nnuevo_texto = [\"Aprende Python para ciencia de datos y web\"]\nnuevo_X = vectorizer.transform(nuevo_texto)\nprediccion = modelo_multi.predict(nuevo_X)\n\n# Decodificar\netiquetas_pred = mlb.inverse_transform(prediccion)\nprint(f\"Predicci\u00f3n: {etiquetas_pred}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#67-deteccion-de-idioma","title":"6.7. Detecci\u00f3n de Idioma","text":"<p>Un caso especial de clasificaci\u00f3n de texto.</p> <pre><code>from langdetect import detect, detect_langs\n\ntextos = [\n    \"Hello, how are you today?\",\n    \"Hola, \u00bfc\u00f3mo est\u00e1s hoy?\",\n    \"Bonjour, comment allez-vous?\",\n    \"Guten Tag, wie geht es Ihnen?\",\n    \"Ciao, come stai oggi?\"\n]\n\nfor texto in textos:\n    idioma = detect(texto)\n    probabilidades = detect_langs(texto)\n    print(f\"'{texto[:30]}...'\")\n    print(f\"  Idioma: {idioma}\")\n    print(f\"  Probabilidades: {probabilidades}\")\n    print()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#68-clasificacion-de-intents-chatbots","title":"6.8. Clasificaci\u00f3n de Intents (Chatbots)","text":"<p>Para sistemas conversacionales, clasificamos la intenci\u00f3n del usuario.</p> <pre><code># Datos de ejemplo para chatbot\nintents_data = {\n    'texto': [\n        \"hola\", \"buenos d\u00edas\", \"qu\u00e9 tal\",\n        \"adi\u00f3s\", \"hasta luego\", \"nos vemos\",\n        \"cu\u00e1l es el precio\", \"cu\u00e1nto cuesta\", \"precio del producto\",\n        \"quiero comprar\", \"a\u00f1adir al carrito\", \"realizar pedido\",\n        \"d\u00f3nde est\u00e1 mi pedido\", \"estado del env\u00edo\", \"cu\u00e1ndo llega\"\n    ],\n    'intent': [\n        'saludo', 'saludo', 'saludo',\n        'despedida', 'despedida', 'despedida',\n        'consulta_precio', 'consulta_precio', 'consulta_precio',\n        'compra', 'compra', 'compra',\n        'seguimiento', 'seguimiento', 'seguimiento'\n    ]\n}\n\ndf_intents = pd.DataFrame(intents_data)\n\n# Entrenar clasificador de intents\nvectorizer_intent = TfidfVectorizer(ngram_range=(1, 2))\nX_intent = vectorizer_intent.fit_transform(df_intents['texto'])\n\nclf_intent = LogisticRegression()\nclf_intent.fit(X_intent, df_intents['intent'])\n\n# Funci\u00f3n para clasificar mensaje de usuario\ndef clasificar_intent(mensaje, vectorizer, modelo):\n    X = vectorizer.transform([mensaje])\n    intent = modelo.predict(X)[0]\n    proba = modelo.predict_proba(X).max()\n    return intent, proba\n\n# Probar\nmensajes_test = [\n    \"hola, buenas tardes\",\n    \"cu\u00e1nto vale este art\u00edculo\",\n    \"quiero hacer un pedido\",\n    \"d\u00f3nde est\u00e1 mi paquete\"\n]\n\nprint(\"Clasificaci\u00f3n de Intents:\")\nprint(\"-\" * 50)\nfor msg in mensajes_test:\n    intent, confianza = clasificar_intent(msg, vectorizer_intent, clf_intent)\n    print(f\"'{msg}'\")\n    print(f\"  \u2192 Intent: {intent} (confianza: {confianza:.2f})\")\n    print()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#69-metricas-de-evaluacion","title":"6.9. M\u00e9tricas de Evaluaci\u00f3n","text":""},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#para-clasificacion-multi-clase","title":"Para Clasificaci\u00f3n Multi-clase","text":"<pre><code>from sklearn.metrics import (\n    accuracy_score, \n    precision_score, \n    recall_score, \n    f1_score,\n    classification_report,\n    confusion_matrix\n)\nimport seaborn as sns\n\n# M\u00e9tricas\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision (macro):\", precision_score(y_test, y_pred, average='macro'))\nprint(\"Recall (macro):\", recall_score(y_test, y_pred, average='macro'))\nprint(\"F1-Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n\n# Reporte detallado\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Matriz de confusi\u00f3n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=label_encoder.classes_,\n            yticklabels=label_encoder.classes_)\nplt.title('Matriz de Confusi\u00f3n')\nplt.ylabel('Real')\nplt.xlabel('Predicho')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#promedios-para-multi-clase","title":"Promedios para Multi-clase","text":"Promedio Descripci\u00f3n micro Calcula m\u00e9tricas globalmente contando TP, FP, FN totales macro Promedio simple de m\u00e9tricas por clase (no pondera por tama\u00f1o) weighted Promedio ponderado por el n\u00famero de muestras por clase"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#610-consideraciones-practicas","title":"6.10. Consideraciones Pr\u00e1cticas","text":""},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#desbalance-de-clases","title":"Desbalance de Clases","text":"<pre><code>from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Calcular pesos de clase\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\n\n# Usar en modelo\nclf = LogisticRegression(class_weight='balanced')\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#validacion-cruzada","title":"Validaci\u00f3n Cruzada","text":"<pre><code>from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(clf, X_train_tfidf, y_train, cv=5, scoring='f1_macro')\nprint(f\"F1-Score (CV): {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/06-clasificacion-texto/#optimizacion-de-hiperparametros","title":"Optimizaci\u00f3n de Hiperpar\u00e1metros","text":"<pre><code>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'max_iter': [100, 500, 1000]\n}\n\ngrid_search = GridSearchCV(\n    LogisticRegression(), \n    param_grid, \n    cv=5, \n    scoring='f1_macro'\n)\ngrid_search.fit(X_train_tfidf, y_train)\n\nprint(f\"Mejores par\u00e1metros: {grid_search.best_params_}\")\nprint(f\"Mejor score: {grid_search.best_score_:.4f}\")\n</code></pre> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/07-transformers/","title":"\ud83d\udcac Unidad 7. Transformers y Modelos de Lenguaje","text":"<p>Los Transformers han revolucionado el NLP desde su introducci\u00f3n en 2017. Esta arquitectura es la base de modelos como BERT, GPT, T5 y los modernos Large Language Models (LLMs).</p>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#71-introduccion-a-los-transformers","title":"7.1. Introducci\u00f3n a los Transformers","text":""},{"location":"procesamiento-lenguaje-natural/07-transformers/#el-problema-con-las-rnn","title":"El Problema con las RNN","text":"<p>Antes de los Transformers, las Redes Neuronales Recurrentes (RNN) y sus variantes (LSTM, GRU) eran el est\u00e1ndar para procesamiento de secuencias. Sin embargo, ten\u00edan limitaciones:</p> <ul> <li>Procesamiento secuencial: No pueden paralelizarse eficientemente.</li> <li>Memoria a largo plazo: Dificultad para capturar dependencias lejanas.</li> <li>Entrenamiento lento: Debido a la naturaleza secuencial.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#la-solucion-atencion","title":"La Soluci\u00f3n: Atenci\u00f3n","text":"<p>El paper \"Attention Is All You Need\" (Vaswani et al., 2017) introdujo el mecanismo de Self-Attention que permite:</p> <ul> <li>Paralelizaci\u00f3n completa: Procesa toda la secuencia a la vez.</li> <li>Contexto global: Cada token puede atender a cualquier otro token.</li> <li>Escalabilidad: Permite entrenar modelos muy grandes.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#72-arquitectura-del-transformer","title":"7.2. Arquitectura del Transformer","text":""},{"location":"procesamiento-lenguaje-natural/07-transformers/#componentes-principales","title":"Componentes Principales","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  OUTPUT                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DECODER (x N)                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502    Feed Forward Neural Network      \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                    \u2191                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502      Cross-Attention                \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                    \u2191                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Masked Self-Attention              \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              ENCODER (x N)                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502    Feed Forward Neural Network      \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                    \u2191                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502         Self-Attention              \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Input Embedding + Positional Encoding      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2191\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  INPUT                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#self-attention","title":"Self-Attention","text":"<p>El mecanismo de Self-Attention permite que cada token \"atienda\" a todos los dem\u00e1s tokens de la secuencia.</p> <p>Para cada token, se calculan tres vectores:</p> <ul> <li>Query (Q): Lo que este token est\u00e1 \"buscando\"</li> <li>Key (K): Lo que este token \"ofrece\"</li> <li>Value (V): La informaci\u00f3n que contiene</li> </ul> <p>La f\u00f3rmula de atenci\u00f3n:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>Donde \\(d_k\\) es la dimensi\u00f3n de las keys.</p>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#multi-head-attention","title":"Multi-Head Attention","text":"<p>En lugar de una sola atenci\u00f3n, se usan m\u00faltiples \"cabezas\" de atenci\u00f3n que aprenden diferentes tipos de relaciones:</p> \\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\\] <p>Donde cada cabeza:</p> \\[\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\]"},{"location":"procesamiento-lenguaje-natural/07-transformers/#positional-encoding","title":"Positional Encoding","text":"<p>Como los Transformers no tienen noci\u00f3n inherente de posici\u00f3n, se a\u00f1ade informaci\u00f3n posicional mediante:</p> \\[PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$ $$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\\]"},{"location":"procesamiento-lenguaje-natural/07-transformers/#73-tipos-de-modelos-transformer","title":"7.3. Tipos de Modelos Transformer","text":""},{"location":"procesamiento-lenguaje-natural/07-transformers/#encoder-only-bert","title":"Encoder-only (BERT)","text":"<ul> <li>Procesa toda la secuencia bidireccionalmente.</li> <li>Ideal para: Clasificaci\u00f3n, NER, Question Answering extractivo.</li> <li>Ejemplos: BERT, RoBERTa, ALBERT, DistilBERT.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#decoder-only-gpt","title":"Decoder-only (GPT)","text":"<ul> <li>Genera texto de izquierda a derecha (autoregresivo).</li> <li>Ideal para: Generaci\u00f3n de texto, completado.</li> <li>Ejemplos: GPT-2, GPT-3, GPT-4, LLaMA, Mistral.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#encoder-decoder-t5","title":"Encoder-Decoder (T5)","text":"<ul> <li>Arquitectura completa original.</li> <li>Ideal para: Traducci\u00f3n, resumen, seq2seq.</li> <li>Ejemplos: T5, BART, mBART.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#74-bert-bidirectional-encoder-representations-from-transformers","title":"7.4. BERT (Bidirectional Encoder Representations from Transformers)","text":"<p>BERT fue un hito en NLP al introducir el preentrenamiento bidireccional.</p>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#preentrenamiento","title":"Preentrenamiento","text":"<p>BERT se preentrena con dos tareas:</p> <ol> <li> <p>Masked Language Model (MLM): Predecir tokens enmascarados.</p> <ul> <li>Entrada: \"El [MASK] come pescado\"</li> <li>Predicci\u00f3n: \"gato\"</li> </ul> </li> <li> <p>Next Sentence Prediction (NSP): Predecir si una oraci\u00f3n sigue a otra.</p> </li> </ol>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#usando-bert-con-hugging-face","title":"Usando BERT con Hugging Face","text":"<pre><code>from transformers import BertTokenizer, BertModel\nimport torch\n\n# Cargar modelo y tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained('bert-base-multilingual-cased')\n\n# Tokenizar texto\ntexto = \"El procesamiento de lenguaje natural es fascinante.\"\ninputs = tokenizer(texto, return_tensors='pt', padding=True, truncation=True)\n\n# Obtener embeddings\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Embedding del token [CLS] (representa toda la oraci\u00f3n)\ncls_embedding = outputs.last_hidden_state[:, 0, :]\nprint(f\"Dimensi\u00f3n del embedding: {cls_embedding.shape}\")  # [1, 768]\n\n# Embedding de cada token\nall_embeddings = outputs.last_hidden_state\nprint(f\"Embeddings por token: {all_embeddings.shape}\")  # [1, num_tokens, 768]\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#clasificacion-con-bert","title":"Clasificaci\u00f3n con BERT","text":"<pre><code>from transformers import BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Datos de ejemplo\ntrain_texts = [\"me encanta\", \"lo odio\", \"est\u00e1 bien\", \"incre\u00edble\", \"terrible\"]\ntrain_labels = [1, 0, 1, 1, 0]\n\n# Crear dataset\ntrain_dataset = Dataset.from_dict({\n    'text': train_texts,\n    'label': train_labels\n})\n\n# Tokenizar\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True)\n\n# Cargar modelo para clasificaci\u00f3n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-multilingual-cased',\n    num_labels=2\n)\n\n# Configurar entrenamiento\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    logging_steps=10\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\n# Entrenar\ntrainer.train()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#75-gpt-y-modelos-generativos","title":"7.5. GPT y Modelos Generativos","text":"<p>GPT (Generative Pre-trained Transformer) es un modelo autoregresivo que genera texto token por token.</p>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#generacion-de-texto","title":"Generaci\u00f3n de Texto","text":"<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Cargar modelo y tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Texto inicial (prompt)\nprompt = \"The future of artificial intelligence is\"\n\n# Tokenizar\ninput_ids = tokenizer.encode(prompt, return_tensors='pt')\n\n# Generar texto\noutput = model.generate(\n    input_ids,\n    max_length=100,\n    num_return_sequences=1,\n    temperature=0.7,  # Creatividad (0=determin\u00edstico, 1=m\u00e1s aleatorio)\n    top_k=50,         # Considerar top-k tokens\n    top_p=0.95,       # Nucleus sampling\n    do_sample=True,\n    pad_token_id=tokenizer.eos_token_id\n)\n\n# Decodificar\ntexto_generado = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(texto_generado)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#parametros-de-generacion","title":"Par\u00e1metros de Generaci\u00f3n","text":"Par\u00e1metro Descripci\u00f3n <code>max_length</code> Longitud m\u00e1xima del texto generado <code>temperature</code> Controla aleatoriedad (0=determin\u00edstico, &gt;1=m\u00e1s creativo) <code>top_k</code> Muestrear de los k tokens m\u00e1s probables <code>top_p</code> Nucleus sampling - muestrear del n\u00facleo de probabilidad p <code>num_beams</code> Beam search para mejor calidad <code>repetition_penalty</code> Penalizar repeticiones"},{"location":"procesamiento-lenguaje-natural/07-transformers/#76-modelos-en-espanol","title":"7.6. Modelos en Espa\u00f1ol","text":""},{"location":"procesamiento-lenguaje-natural/07-transformers/#beto-bert-en-espanol","title":"BETO (BERT en Espa\u00f1ol)","text":"<pre><code>from transformers import AutoTokenizer, AutoModel\n\n# BETO - BERT entrenado en espa\u00f1ol\ntokenizer = AutoTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\nmodel = AutoModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')\n\ntexto = \"El procesamiento de lenguaje natural es muy \u00fatil.\"\ninputs = tokenizer(texto, return_tensors='pt')\noutputs = model(**inputs)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#robertuito-tweets-en-espanol","title":"RoBERTuito (Tweets en Espa\u00f1ol)","text":"<pre><code>from transformers import pipeline\n\n# Modelo entrenado en tweets en espa\u00f1ol\nsentiment = pipeline(\n    'sentiment-analysis',\n    model='pysentimiento/robertuito-sentiment-analysis'\n)\n\ntextos = [\n    \"Me encanta este producto, es genial!\",\n    \"Qu\u00e9 mal servicio, no lo recomiendo\",\n    \"El paquete lleg\u00f3 bien\"\n]\n\nfor texto in textos:\n    result = sentiment(texto)[0]\n    print(f\"'{texto}' \u2192 {result['label']} ({result['score']:.3f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#77-sentence-transformers","title":"7.7. Sentence Transformers","text":"<p>Para obtener embeddings de oraciones completas que capturen el significado sem\u00e1ntico.</p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Cargar modelo de sentence embeddings\nmodel = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\n# Oraciones a comparar\noraciones = [\n    \"El gato est\u00e1 sentado en el sof\u00e1\",\n    \"Un felino descansa en el mueble\",\n    \"El perro corre por el parque\",\n    \"Machine learning es inteligencia artificial\"\n]\n\n# Obtener embeddings\nembeddings = model.encode(oraciones)\n\n# Calcular similitudes\nsimilitudes = cosine_similarity(embeddings)\n\nprint(\"Matriz de Similitud:\")\nfor i, s1 in enumerate(oraciones):\n    for j, s2 in enumerate(oraciones):\n        if i &lt; j:\n            print(f\"'{s1[:30]}...' vs '{s2[:30]}...'\")\n            print(f\"  Similitud: {similitudes[i][j]:.3f}\")\n            print()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#busqueda-semantica","title":"B\u00fasqueda Sem\u00e1ntica","text":"<pre><code>from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\n# Base de conocimiento\ndocumentos = [\n    \"Python es un lenguaje de programaci\u00f3n vers\u00e1til\",\n    \"Machine learning permite a las computadoras aprender de datos\",\n    \"El deep learning usa redes neuronales profundas\",\n    \"Los gatos son animales dom\u00e9sticos populares\",\n    \"El f\u00fatbol es el deporte m\u00e1s popular del mundo\"\n]\n\n# Codificar documentos\ndoc_embeddings = model.encode(documentos, convert_to_tensor=True)\n\n# Query de b\u00fasqueda\nquery = \"\u00bfQu\u00e9 es la inteligencia artificial?\"\nquery_embedding = model.encode(query, convert_to_tensor=True)\n\n# Buscar documentos similares\nscores = util.cos_sim(query_embedding, doc_embeddings)[0]\ntop_results = scores.argsort(descending=True)[:3]\n\nprint(f\"Query: '{query}'\")\nprint(\"\\nResultados:\")\nfor idx in top_results:\n    print(f\"  [{scores[idx]:.3f}] {documentos[idx]}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#78-large-language-models-llms","title":"7.8. Large Language Models (LLMs)","text":"<p>Los LLMs son modelos con miles de millones de par\u00e1metros entrenados en enormes cantidades de texto.</p>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#modelos-populares","title":"Modelos Populares","text":"Modelo Organizaci\u00f3n Par\u00e1metros Caracter\u00edsticas GPT-4 OpenAI ~1.8T Multimodal, mejor razonamiento Claude Anthropic ~100B+ Conversacional, seguro LLaMA 2 Meta 7B-70B Open source Mistral Mistral AI 7B Eficiente, open source Gemini Google ? Multimodal"},{"location":"procesamiento-lenguaje-natural/07-transformers/#usando-llms-con-la-api-de-openai","title":"Usando LLMs con la API de OpenAI","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key=\"tu-api-key\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Eres un experto en NLP.\"},\n        {\"role\": \"user\", \"content\": \"Explica qu\u00e9 es el mecanismo de atenci\u00f3n.\"}\n    ],\n    temperature=0.7,\n    max_tokens=500\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#usando-modelos-open-source-ollama","title":"Usando Modelos Open Source (Ollama)","text":"<pre><code>import ollama\n\nresponse = ollama.chat(model='llama2', messages=[\n    {'role': 'user', 'content': '\u00bfQu\u00e9 es el procesamiento de lenguaje natural?'}\n])\n\nprint(response['message']['content'])\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#79-fine-tuning-y-tecnicas-avanzadas","title":"7.9. Fine-tuning y T\u00e9cnicas Avanzadas","text":""},{"location":"procesamiento-lenguaje-natural/07-transformers/#parameter-efficient-fine-tuning-peft","title":"Parameter-Efficient Fine-Tuning (PEFT)","text":"<p>T\u00e9cnicas para adaptar modelos grandes sin entrenar todos los par\u00e1metros:</p> <ul> <li>LoRA (Low-Rank Adaptation): A\u00f1ade matrices de bajo rango a las capas.</li> <li>Prefix Tuning: A\u00f1ade tokens aprendibles al inicio.</li> <li>Prompt Tuning: Aprende embeddings de prompt.</li> </ul> <pre><code>from peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM\n\n# Configuraci\u00f3n LoRA\nlora_config = LoraConfig(\n    r=16,              # Rango de las matrices\n    lora_alpha=32,     # Factor de escalado\n    target_modules=[\"q_proj\", \"v_proj\"],  # M\u00f3dulos a adaptar\n    lora_dropout=0.1,\n    bias=\"none\"\n)\n\n# Cargar modelo y aplicar LoRA\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b\")\npeft_model = get_peft_model(model, lora_config)\n\n# Ver par\u00e1metros entrenables\npeft_model.print_trainable_parameters()\n# trainable params: 4,194,304 || all params: 6,738,415,616 || trainable%: 0.0622\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/07-transformers/#710-consideraciones-practicas","title":"7.10. Consideraciones Pr\u00e1cticas","text":""},{"location":"procesamiento-lenguaje-natural/07-transformers/#recursos-y-costos","title":"Recursos y Costos","text":"Modelo RAM GPU Tiempo Inferencia BERT-base ~1GB ~ms GPT-2 ~2GB ~ms LLaMA-7B ~14GB ~segundos LLaMA-70B ~140GB ~segundos"},{"location":"procesamiento-lenguaje-natural/07-transformers/#mejores-practicas","title":"Mejores Pr\u00e1cticas","text":"<ol> <li>Empezar simple: Usar modelos m\u00e1s peque\u00f1os primero.</li> <li>Cuantizaci\u00f3n: Reducir precisi\u00f3n para menor uso de memoria (int8, int4).</li> <li>Batching: Procesar m\u00faltiples entradas juntas.</li> <li>Caching: Cachear embeddings de documentos.</li> <li>Distillation: Crear modelos m\u00e1s peque\u00f1os que imiten a los grandes.</li> </ol> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/","title":"\ud83d\udcac Unidad 8. Aplicaciones Avanzadas de NLP","text":"<p>Esta unidad cubre aplicaciones avanzadas y casos de uso pr\u00e1cticos del NLP en el mundo real, incluyendo chatbots, sistemas de preguntas y respuestas, resumen autom\u00e1tico y traducci\u00f3n autom\u00e1tica.</p>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#81-sistemas-de-preguntas-y-respuestas-qa","title":"8.1. Sistemas de Preguntas y Respuestas (QA)","text":"<p>Los sistemas de Question Answering (QA) responden preguntas en lenguaje natural bas\u00e1ndose en un contexto o base de conocimiento.</p>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#tipos-de-qa","title":"Tipos de QA","text":"Tipo Descripci\u00f3n Ejemplo Extractivo Extrae la respuesta directamente del texto \"\u00bfQui\u00e9n fund\u00f3 Apple?\" \u2192 \"Steve Jobs\" (del contexto) Generativo Genera una respuesta en lenguaje natural Respuesta elaborada basada en m\u00faltiples fuentes Open-domain Responde sobre cualquier tema Wikipedia QA Closed-domain Responde sobre un dominio espec\u00edfico QA m\u00e9dico, legal"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#qa-extractivo-con-transformers","title":"QA Extractivo con Transformers","text":"<pre><code>from transformers import pipeline\n\n# Pipeline de QA\nqa_pipeline = pipeline(\"question-answering\")\n\n# Contexto\ncontexto = \"\"\"\nApple Inc. es una empresa tecnol\u00f3gica estadounidense fundada en 1976 por Steve Jobs, \nSteve Wozniak y Ronald Wayne. La compa\u00f1\u00eda tiene su sede en Cupertino, California.\nApple es conocida por productos como el iPhone, iPad, Mac y Apple Watch.\nEn 2023, Apple alcanz\u00f3 una valoraci\u00f3n de mercado de 3 billones de d\u00f3lares.\n\"\"\"\n\n# Preguntas\npreguntas = [\n    \"\u00bfQui\u00e9n fund\u00f3 Apple?\",\n    \"\u00bfD\u00f3nde est\u00e1 la sede de Apple?\",\n    \"\u00bfCu\u00e1l es la valoraci\u00f3n de Apple?\",\n    \"\u00bfCu\u00e1ndo se fund\u00f3 Apple?\"\n]\n\nprint(\"Sistema de Preguntas y Respuestas\")\nprint(\"=\" * 50)\n\nfor pregunta in preguntas:\n    resultado = qa_pipeline(question=pregunta, context=contexto)\n    print(f\"\\nP: {pregunta}\")\n    print(f\"R: {resultado['answer']} (confianza: {resultado['score']:.3f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#qa-con-retrieval-rag-retrieval-augmented-generation","title":"QA con Retrieval (RAG - Retrieval Augmented Generation)","text":"<pre><code>from sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\nimport numpy as np\n\n# Base de conocimiento\ndocumentos = [\n    \"Python es un lenguaje de programaci\u00f3n de alto nivel creado por Guido van Rossum en 1991.\",\n    \"JavaScript fue creado por Brendan Eich en 1995 para Netscape.\",\n    \"Java fue desarrollado por James Gosling en Sun Microsystems en 1995.\",\n    \"C++ fue creado por Bjarne Stroustrup comenzando en 1979.\",\n    \"Rust es un lenguaje de programaci\u00f3n de sistemas desarrollado por Mozilla desde 2010.\"\n]\n\n# Modelo de embeddings para retrieval\nembedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\ndoc_embeddings = embedder.encode(documentos)\n\n# Pipeline de QA\nqa_pipeline = pipeline(\"question-answering\")\n\ndef responder_pregunta_rag(pregunta, documentos, doc_embeddings, embedder, qa_pipeline, top_k=2):\n    \"\"\"\n    Sistema RAG: Retrieve + Generate\n    \"\"\"\n    # 1. Retrieve: Encontrar documentos relevantes\n    query_embedding = embedder.encode(pregunta)\n    similarities = np.dot(doc_embeddings, query_embedding)\n    top_indices = similarities.argsort()[-top_k:][::-1]\n\n    # Contexto relevante\n    contexto = \" \".join([documentos[i] for i in top_indices])\n\n    # 2. Generate: Responder usando el contexto\n    resultado = qa_pipeline(question=pregunta, context=contexto)\n\n    return {\n        'pregunta': pregunta,\n        'respuesta': resultado['answer'],\n        'confianza': resultado['score'],\n        'documentos_usados': [documentos[i] for i in top_indices]\n    }\n\n# Probar\npregunta = \"\u00bfQui\u00e9n cre\u00f3 Python?\"\nresultado = responder_pregunta_rag(pregunta, documentos, doc_embeddings, embedder, qa_pipeline)\n\nprint(f\"Pregunta: {resultado['pregunta']}\")\nprint(f\"Respuesta: {resultado['respuesta']}\")\nprint(f\"Confianza: {resultado['confianza']:.3f}\")\nprint(f\"Documentos consultados: {resultado['documentos_usados']}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#82-chatbots-y-asistentes-conversacionales","title":"8.2. Chatbots y Asistentes Conversacionales","text":""},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#arquitectura-de-un-chatbot","title":"Arquitectura de un Chatbot","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Usuario                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              NLU (Natural Language Understanding)    \u2502\n\u2502  - Intent Classification                             \u2502\n\u2502  - Entity Extraction                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Dialog Manager                          \u2502\n\u2502  - State Tracking                                    \u2502\n\u2502  - Policy Selection                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              NLG (Natural Language Generation)       \u2502\n\u2502  - Response Generation                               \u2502\n\u2502  - Template Filling                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Respuesta                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#chatbot-simple-con-reglas","title":"Chatbot Simple con Reglas","text":"<pre><code>import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nclass ChatbotSimple:\n    def __init__(self):\n        self.intents = {\n            'saludo': {\n                'patterns': ['hola', 'buenos d\u00edas', 'buenas tardes', 'qu\u00e9 tal'],\n                'responses': ['\u00a1Hola! \u00bfEn qu\u00e9 puedo ayudarte?', '\u00a1Buenos d\u00edas! \u00bfC\u00f3mo puedo asistirte?']\n            },\n            'despedida': {\n                'patterns': ['adi\u00f3s', 'hasta luego', 'chao', 'nos vemos'],\n                'responses': ['\u00a1Hasta luego!', '\u00a1Que tengas un buen d\u00eda!']\n            },\n            'precio': {\n                'patterns': ['cu\u00e1nto cuesta', 'precio', 'valor', 'cu\u00e1nto vale'],\n                'responses': ['Nuestros precios var\u00edan. \u00bfQu\u00e9 producto te interesa?']\n            },\n            'ayuda': {\n                'patterns': ['ayuda', 'necesito ayuda', 'c\u00f3mo funciona', 'help'],\n                'responses': ['Puedo ayudarte con informaci\u00f3n de productos, precios y pedidos.']\n            },\n            'default': {\n                'patterns': [],\n                'responses': ['No entiendo tu pregunta. \u00bfPodr\u00edas reformularla?']\n            }\n        }\n\n        self._entrenar_clasificador()\n\n    def _entrenar_clasificador(self):\n        textos = []\n        labels = []\n\n        for intent, data in self.intents.items():\n            if intent != 'default':\n                for pattern in data['patterns']:\n                    textos.append(pattern)\n                    labels.append(intent)\n\n        self.vectorizer = TfidfVectorizer()\n        X = self.vectorizer.fit_transform(textos)\n\n        self.clf = LogisticRegression()\n        self.clf.fit(X, labels)\n\n    def responder(self, mensaje):\n        import random\n\n        # Preprocesar\n        mensaje_limpio = mensaje.lower().strip()\n\n        # Clasificar intent\n        X = self.vectorizer.transform([mensaje_limpio])\n        intent = self.clf.predict(X)[0]\n        confianza = self.clf.predict_proba(X).max()\n\n        # Si confianza baja, usar default\n        if confianza &lt; 0.3:\n            intent = 'default'\n\n        # Seleccionar respuesta\n        respuestas = self.intents[intent]['responses']\n        respuesta = random.choice(respuestas)\n\n        return {\n            'intent': intent,\n            'confianza': confianza,\n            'respuesta': respuesta\n        }\n\n# Usar chatbot\nbot = ChatbotSimple()\n\nmensajes = [\n    \"Hola, buenos d\u00edas\",\n    \"\u00bfCu\u00e1nto cuesta el producto?\",\n    \"Necesito ayuda por favor\",\n    \"Adi\u00f3s, gracias\",\n    \"\u00bfCu\u00e1l es el clima hoy?\"  # Fuera de dominio\n]\n\nprint(\"Chatbot Demo\")\nprint(\"=\" * 50)\nfor msg in mensajes:\n    resultado = bot.responder(msg)\n    print(f\"\\nUsuario: {msg}\")\n    print(f\"Bot: {resultado['respuesta']}\")\n    print(f\"  (intent: {resultado['intent']}, conf: {resultado['confianza']:.2f})\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#chatbot-con-llm","title":"Chatbot con LLM","text":"<pre><code>from openai import OpenAI\n\nclass ChatbotLLM:\n    def __init__(self, api_key, system_prompt=None):\n        self.client = OpenAI(api_key=api_key)\n        self.system_prompt = system_prompt or \"\"\"\n        Eres un asistente virtual amable y \u00fatil. \n        Respondes de forma concisa y clara.\n        Si no sabes algo, lo admites honestamente.\n        \"\"\"\n        self.historial = []\n\n    def responder(self, mensaje_usuario):\n        # A\u00f1adir mensaje al historial\n        self.historial.append({\n            \"role\": \"user\",\n            \"content\": mensaje_usuario\n        })\n\n        # Llamar a la API\n        response = self.client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": self.system_prompt}\n            ] + self.historial,\n            temperature=0.7,\n            max_tokens=500\n        )\n\n        respuesta = response.choices[0].message.content\n\n        # A\u00f1adir respuesta al historial\n        self.historial.append({\n            \"role\": \"assistant\",\n            \"content\": respuesta\n        })\n\n        return respuesta\n\n    def reiniciar_conversacion(self):\n        self.historial = []\n\n# Uso\n# bot_llm = ChatbotLLM(api_key=\"tu-api-key\")\n# print(bot_llm.responder(\"\u00bfQu\u00e9 es el machine learning?\"))\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#83-resumen-automatico-de-texto","title":"8.3. Resumen Autom\u00e1tico de Texto","text":""},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#tipos-de-resumen","title":"Tipos de Resumen","text":"Tipo Descripci\u00f3n M\u00e9todo Extractivo Selecciona oraciones importantes del texto original TextRank, BERT extractivo Abstractivo Genera un nuevo texto que resume el contenido T5, BART, GPT"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#resumen-extractivo-textrank","title":"Resumen Extractivo (TextRank)","text":"<pre><code>from sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.text_rank import TextRankSummarizer\n\ndef resumir_extractivo(texto, num_oraciones=3):\n    \"\"\"\n    Resumen extractivo usando TextRank.\n    \"\"\"\n    parser = PlaintextParser.from_string(texto, Tokenizer(\"spanish\"))\n    summarizer = TextRankSummarizer()\n\n    resumen = summarizer(parser.document, num_oraciones)\n\n    return \" \".join([str(oracion) for oracion in resumen])\n\n# Texto largo de ejemplo\ntexto_largo = \"\"\"\nLa inteligencia artificial ha experimentado un crecimiento exponencial en los \u00faltimos a\u00f1os.\nLos avances en deep learning han permitido crear sistemas capaces de reconocer im\u00e1genes,\ntraducir idiomas y mantener conversaciones naturales con humanos.\n\nEmpresas como Google, Microsoft y OpenAI lideran la investigaci\u00f3n en este campo.\nLos modelos de lenguaje como GPT-4 pueden generar texto indistinguible del escrito por humanos.\nEsto ha generado debates sobre el futuro del trabajo y la \u00e9tica en la IA.\n\nA pesar de los avances, los expertos advierten que la IA actual no posee verdadera comprensi\u00f3n\no consciencia. Los sistemas actuales son herramientas sofisticadas de reconocimiento de patrones.\nEl camino hacia la inteligencia artificial general (AGI) a\u00fan es largo e incierto.\n\"\"\"\n\nresumen = resumir_extractivo(texto_largo, num_oraciones=2)\nprint(\"Resumen Extractivo:\")\nprint(resumen)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#resumen-abstractivo-con-transformers","title":"Resumen Abstractivo con Transformers","text":"<pre><code>from transformers import pipeline\n\n# Pipeline de resumen\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Texto en ingl\u00e9s (modelos en ingl\u00e9s funcionan mejor)\ntexto_en = \"\"\"\nArtificial intelligence has experienced exponential growth in recent years.\nAdvances in deep learning have enabled systems capable of recognizing images,\ntranslating languages, and having natural conversations with humans.\nCompanies like Google, Microsoft, and OpenAI lead research in this field.\nLanguage models like GPT-4 can generate text indistinguishable from human writing.\nThis has sparked debates about the future of work and AI ethics.\n\"\"\"\n\nresumen = summarizer(texto_en, max_length=50, min_length=20, do_sample=False)\nprint(\"Resumen Abstractivo:\")\nprint(resumen[0]['summary_text'])\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#resumen-en-espanol-con-mt5","title":"Resumen en Espa\u00f1ol con mT5","text":"<pre><code>from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n\n# Cargar modelo multiling\u00fce\nmodel_name = \"google/mt5-small\"\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\n\ndef resumir_mt5(texto, max_length=100):\n    \"\"\"\n    Resumen con mT5 (multiling\u00fce).\n    \"\"\"\n    # Prefijo para tarea de resumen\n    input_text = f\"summarize: {texto}\"\n\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        max_length=max_length,\n        min_length=20,\n        length_penalty=2.0,\n        num_beams=4,\n        early_stopping=True\n    )\n\n    resumen = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return resumen\n\n# Ejemplo\n# resumen_es = resumir_mt5(texto_largo)\n# print(resumen_es)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#84-traduccion-automatica","title":"8.4. Traducci\u00f3n Autom\u00e1tica","text":""},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#traduccion-con-marianmt","title":"Traducci\u00f3n con MarianMT","text":"<pre><code>from transformers import MarianMTModel, MarianTokenizer\n\ndef traducir(texto, src_lang=\"es\", tgt_lang=\"en\"):\n    \"\"\"\n    Traduce texto entre idiomas usando MarianMT.\n    \"\"\"\n    # Cargar modelo para el par de idiomas\n    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n\n    # Tokenizar\n    inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True)\n\n    # Traducir\n    translated = model.generate(**inputs)\n\n    # Decodificar\n    resultado = tokenizer.decode(translated[0], skip_special_tokens=True)\n    return resultado\n\n# Ejemplos\ntexto_es = \"El procesamiento de lenguaje natural permite a las m\u00e1quinas entender el lenguaje humano.\"\ntexto_en = traducir(texto_es, \"es\", \"en\")\nprint(f\"Original (ES): {texto_es}\")\nprint(f\"Traducido (EN): {texto_en}\")\n\n# Traducci\u00f3n inversa\ntexto_back = traducir(texto_en, \"en\", \"es\")\nprint(f\"Back-translation: {texto_back}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#traduccion-con-pipeline","title":"Traducci\u00f3n con Pipeline","text":"<pre><code>from transformers import pipeline\n\n# Pipeline de traducci\u00f3n\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n\ntextos_es = [\n    \"Buenos d\u00edas, \u00bfc\u00f3mo est\u00e1s?\",\n    \"Me gusta programar en Python.\",\n    \"El aprendizaje autom\u00e1tico es fascinante.\"\n]\n\nprint(\"Traducciones ES \u2192 EN:\")\nfor texto in textos_es:\n    resultado = translator(texto)[0]['translation_text']\n    print(f\"  {texto}\")\n    print(f\"  \u2192 {resultado}\")\n    print()\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#85-generacion-de-texto","title":"8.5. Generaci\u00f3n de Texto","text":""},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#completado-de-texto","title":"Completado de Texto","text":"<pre><code>from transformers import pipeline\n\n# Pipeline de generaci\u00f3n\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\n\n# Prompt\nprompt = \"The future of artificial intelligence will\"\n\n# Generar\nresultado = generator(\n    prompt,\n    max_length=100,\n    num_return_sequences=3,\n    temperature=0.8,\n    top_p=0.9\n)\n\nprint(\"Generaciones:\")\nfor i, gen in enumerate(resultado):\n    print(f\"\\n{i+1}. {gen['generated_text']}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#generacion-controlada","title":"Generaci\u00f3n Controlada","text":"<pre><code>from transformers import pipeline\n\n# Generador con control de estilo\ngenerator = pipeline(\"text-generation\", model=\"gpt2-medium\")\n\nprompts_con_estilo = [\n    \"Write a formal email: Dear Mr. Smith,\",\n    \"Write a casual text message: Hey!\",\n    \"Write a news headline: BREAKING:\"\n]\n\nfor prompt in prompts_con_estilo:\n    resultado = generator(prompt, max_length=50, num_return_sequences=1)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Output: {resultado[0]['generated_text']}\")\n    print(\"-\" * 50)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#86-deteccion-de-texto-generado-por-ia","title":"8.6. Detecci\u00f3n de Texto Generado por IA","text":"<p>Con el auge de los LLMs, detectar texto generado por IA es cada vez m\u00e1s importante.</p> <pre><code>from transformers import pipeline\n\n# Detector de texto IA (ejemplo conceptual)\n# Nota: Los detectores actuales no son 100% precisos\n\ndef analizar_texto_ia(texto):\n    \"\"\"\n    Heur\u00edsticas simples para detectar texto generado por IA.\n    \"\"\"\n    se\u00f1ales = {\n        'longitud_oraciones': [],\n        'palabras_repetidas': {},\n        'conectores_comunes': 0\n    }\n\n    # Analizar longitud de oraciones\n    oraciones = texto.split('.')\n    se\u00f1ales['longitud_oraciones'] = [len(o.split()) for o in oraciones if o.strip()]\n\n    # Variabilidad de longitud (IA tiende a ser m\u00e1s uniforme)\n    if se\u00f1ales['longitud_oraciones']:\n        variabilidad = np.std(se\u00f1ales['longitud_oraciones'])\n    else:\n        variabilidad = 0\n\n    # Conectores comunes en texto de IA\n    conectores_ia = ['adem\u00e1s', 'sin embargo', 'por lo tanto', 'en conclusi\u00f3n', \n                     'furthermore', 'however', 'therefore', 'in conclusion']\n\n    texto_lower = texto.lower()\n    for conector in conectores_ia:\n        if conector in texto_lower:\n            se\u00f1ales['conectores_comunes'] += 1\n\n    # Score simple\n    score_ia = 0.5  # Base\n\n    if variabilidad &lt; 5:  # Poca variabilidad\n        score_ia += 0.2\n\n    if se\u00f1ales['conectores_comunes'] &gt;= 3:\n        score_ia += 0.2\n\n    return {\n        'score_ia': min(score_ia, 1.0),\n        'se\u00f1ales': se\u00f1ales,\n        'prediccion': 'Probablemente IA' if score_ia &gt; 0.6 else 'Probablemente humano'\n    }\n\n# Ejemplo\nimport numpy as np\n\ntexto_sospechoso = \"\"\"\nLa inteligencia artificial ha revolucionado muchos campos. Sin embargo, tambi\u00e9n \npresenta desaf\u00edos. Por lo tanto, es importante considerarlos. Adem\u00e1s, debemos \nser conscientes de las implicaciones \u00e9ticas. En conclusi\u00f3n, la IA es una \nherramienta poderosa que debe usarse responsablemente.\n\"\"\"\n\nresultado = analizar_texto_ia(texto_sospechoso)\nprint(f\"Score IA: {resultado['score_ia']:.2f}\")\nprint(f\"Predicci\u00f3n: {resultado['prediccion']}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#87-aplicaciones-empresariales","title":"8.7. Aplicaciones Empresariales","text":""},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#extraccion-de-informacion-de-documentos","title":"Extracci\u00f3n de Informaci\u00f3n de Documentos","text":"<pre><code>import spacy\nfrom collections import defaultdict\n\nnlp = spacy.load('es_core_news_lg')\n\ndef extraer_informacion_contrato(texto):\n    \"\"\"\n    Extrae informaci\u00f3n clave de un contrato.\n    \"\"\"\n    doc = nlp(texto)\n\n    info = {\n        'partes': [],\n        'fechas': [],\n        'montos': [],\n        'ubicaciones': []\n    }\n\n    for ent in doc.ents:\n        if ent.label_ in ['PER', 'ORG']:\n            info['partes'].append(ent.text)\n        elif ent.label_ == 'DATE':\n            info['fechas'].append(ent.text)\n        elif ent.label_ == 'MONEY':\n            info['montos'].append(ent.text)\n        elif ent.label_ in ['LOC', 'GPE']:\n            info['ubicaciones'].append(ent.text)\n\n    # Eliminar duplicados\n    for key in info:\n        info[key] = list(set(info[key]))\n\n    return info\n\n# Ejemplo\ncontrato = \"\"\"\nEl presente contrato se celebra entre Empresa ABC S.A. y Juan P\u00e9rez Garc\u00eda,\ncon fecha 15 de enero de 2024 en Madrid, Espa\u00f1a.\nEl monto total del contrato es de 50.000 euros, pagaderos en 12 meses.\n\"\"\"\n\ninfo_contrato = extraer_informacion_contrato(contrato)\nprint(\"Informaci\u00f3n extra\u00edda del contrato:\")\nfor key, values in info_contrato.items():\n    if values:\n        print(f\"  {key}: {', '.join(values)}\")\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#analisis-de-feedback-de-clientes","title":"An\u00e1lisis de Feedback de Clientes","text":"<pre><code>from collections import Counter\nimport pandas as pd\n\ndef analizar_feedback(comentarios, sentiment_pipeline):\n    \"\"\"\n    Analiza feedback de clientes para obtener insights.\n    \"\"\"\n    resultados = []\n\n    for comentario in comentarios:\n        # An\u00e1lisis de sentimiento\n        sent = sentiment_pipeline(comentario)[0]\n\n        resultados.append({\n            'comentario': comentario[:50] + '...' if len(comentario) &gt; 50 else comentario,\n            'sentimiento': sent['label'],\n            'score': sent['score']\n        })\n\n    df = pd.DataFrame(resultados)\n\n    # Resumen\n    resumen = {\n        'total_comentarios': len(comentarios),\n        'distribucion_sentimiento': df['sentimiento'].value_counts().to_dict(),\n        'score_promedio': df['score'].mean(),\n        'ejemplos_negativos': df[df['sentimiento'] == 'NEGATIVE'].head(3)['comentario'].tolist()\n    }\n\n    return resumen\n\n# Ejemplo conceptual\n# comentarios = [\"Excelente producto!\", \"Muy mal servicio\", \"Normal, nada especial\"]\n# resumen = analizar_feedback(comentarios, sentiment_pipeline)\n</code></pre>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#88-consideraciones-eticas-y-mejores-practicas","title":"8.8. Consideraciones \u00c9ticas y Mejores Pr\u00e1cticas","text":""},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#sesgos-en-nlp","title":"Sesgos en NLP","text":"<ul> <li>Los modelos heredan sesgos de los datos de entrenamiento.</li> <li>Evaluar y mitigar sesgos de g\u00e9nero, raza, etc.</li> <li>Usar datasets diversos y representativos.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#privacidad","title":"Privacidad","text":"<ul> <li>Los modelos pueden memorizar informaci\u00f3n sensible.</li> <li>Implementar t\u00e9cnicas de anonimizaci\u00f3n.</li> <li>Cumplir con regulaciones (GDPR, etc.).</li> </ul>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#transparencia","title":"Transparencia","text":"<ul> <li>Documentar limitaciones de los modelos.</li> <li>Proporcionar explicaciones cuando sea posible.</li> <li>Informar a usuarios cuando interact\u00faan con IA.</li> </ul>"},{"location":"procesamiento-lenguaje-natural/08-aplicaciones-avanzadas/#uso-responsable","title":"Uso Responsable","text":"<ul> <li>No generar desinformaci\u00f3n o contenido da\u00f1ino.</li> <li>Implementar filtros y salvaguardas.</li> <li>Considerar el impacto social de las aplicaciones.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: Enero 2026 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"}]}